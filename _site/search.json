[
  {
    "objectID": "collections/software/awk.html",
    "href": "collections/software/awk.html",
    "title": "Awk",
    "section": "",
    "text": "An old goody! For quick command line analysis of data.\n\nThe following examples were taken from the tldr page for awk:\nPrint the fifth column (a.k.a. field) in a space-separated file:\nawk '{print $5}' filename\nPrint the second column of the lines containing “foo” in a space-separated file:\nawk '/foo/ {print $2}' filename\nPrint the last column of each line in a file, using a comma (instead of space) as a field separator:\nawk -F ',' '{print $NF}' filename\nSum the values in the first column of a file and print the total:\nawk '{s+=$1} END {print s}' filename\nPrint every third line starting from the first line:\nawk 'NR%3==1' filename\nPrint different values based on conditions:\nawk '{if ($1 == \"foo\") print \"Exact match foo\"; else if ($1 ~ \"bar\") print \"Partial match bar\"; else print \"Baz\"}' filename\nPrint all lines where the 10th column value equals the specified value:\nawk '($10 == value)'\nPrint all the lines which the 10th column value is between a min and a max:\nawk '($10 &gt;= min_value && $10 &lt;= max_value)'"
  },
  {
    "objectID": "collections/software/hamilton.html#why-choose-this-tool",
    "href": "collections/software/hamilton.html#why-choose-this-tool",
    "title": "Stitchfix Hamilton",
    "section": "Why choose this tool?",
    "text": "Why choose this tool?\nA scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections.html",
    "href": "collections.html",
    "title": "Collections",
    "section": "",
    "text": "Software\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nDec 3, 2023\n\n\nAwk\n\n\n \n\n\n\n\nDec 3, 2022\n\n\nPrefect-2.0\n\n\n \n\n\n\n\nDec 3, 2024\n\n\nStitchfix Hamilton\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rahul’s Blog",
    "section": "",
    "text": "Some Data Analysis about Congress\n\n\n\n\n\n\nmodel-comparison\n\n\ncongress\n\n\n\nHow to sitting president’s parties do with congress?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStitchfix Hamilton\n\n\n\n\n\n\ndata\n\n\ndata transformation\n\n\ndag\n\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\n\n\n\n\nmodels\n\n\nprobabilistic modeling\n\n\n\nModeling is not the end, its just the beginning!\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAwk\n\n\n\n\n\n\ndata transformation\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrefect-2.0\n\n\n\n\n\n\norchestration\n\n\npipeline\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\n\n\n\n\nStatistics\n\n\nMonteCarlo\n\n\n\nProbably the most important theorem in frequentist statistics\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\n\n\n\n\nVisualization\n\n\nCommunication\n\n\nStorytelling\n\n\n\nHow should you communicate your insights in Visualization?\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/vizasstory.html",
    "href": "posts/vizasstory.html",
    "title": "Visualization As Story",
    "section": "",
    "text": "There is this pretty famous book by Steve Krug, called “Dont Make Me Think”. Its a call to respect conventions for web elements, such as shopping carts (a cart should be on the upper right), so that the web experience is obvious to users.\n\n\n\nIn visualization, as in web development, your audience does not want to spend cognitive effort on things you could just show them, by convention, or by explicit writing. So, just point out the key facts and insights.\nFor example, in this great article in the financial times https://www.ft.com/content/0f11b219-0f1b-420e-8188-6651d1e749ff?hcb=1, the main point “Vaccines have made Covid-19 far less lethal” is written up-front.\n\n\n\nThe implications are made clear in the second sentence, comparing vaccinated 80 year-olds to un-vaccinated 50 year-olds. This implication is illustrated in the visualization as well, with a horizontal black line, and a caption.\nInstead of point markers, downwards pointing arrows are used on lines to reinforce the notion of lower risk. Captions and annotations are used to point out key insights. Extraneous frames and tick marks are removed.\nThis is an example of framing. It grabs the audience and leads it through the insights you want to share.\n\n\n\nThere’s been a lot of worry about breakthrough vaccination, especially with the news about the Provincetown cluster. Here is another visualization from the same article, telling us why the large number of breakthrough infections are to be expected.\n\n\n\nIt walks us through the entire calculation visually. And does it in two scenarios: high vaccination rates and low vaccination rates. We can ourselves see the larger hospitalization numbers in the low-vaccination scenario.\nThe visualization and explanation could have been framed in terms of base rates and conditional probabilities, but by illustrating the concepts with an example, they are made accessible to everyone. And the framing drives home the story: go get your shot!\nRead more on how to make good visualizations using R in this book by @khealy . If you are a pythonista, learn how to make good plots in @matplotlib using https://end-to-end-machine-learning.teachable.com/p/navigating-matplotlib-tutorial-how-to/ by @_brohrer_ ."
  },
  {
    "objectID": "posts/lawoflargenumbers.html",
    "href": "posts/lawoflargenumbers.html",
    "title": "The LLN",
    "section": "",
    "text": "Suppose that you toss a fair coin and catch it to see if you got heads or tails. Then you have this intuition that while you might get a streak of several heads in a row, in the long run the heads and tails are balanced.\nThis is actually an example of a famous law: the Law of Large numbers (LLN), which states that if you have a random variable X with a mean, the average value of X over a sample of size N converges i.e. gets close and closer to this mean as N becomes larger and larger.\n\n\n\nThe LLN was first proved by Jakob Bernoulli in Ars Conjectandi, published posthumously by his nephew Niklaus Bernoulli, who appropriated entire passages of it for his treatise on law. It is the basis of much of modern statistics, including the Monte-Carlo method.\nLets parse the law. A random variable is one that can take multiple values, each with some probability. So if X represents the flip of a coin, it will take values Heads and Tails with some probability. We’ll assign Heads the value 1 and Tails the value 0.\nThe probabilities attatched to the values a random variable takes is called a distribution, or probability mass function (pmf). For a fair coin, the “Bernoulli” Distribution attaches the probabilities 0.5 to value 1 and 0.5 to value 0. These probabilities must add to 1.\n\n\n\nAn unfair coin thats more likely to land on heads might have a distribution where 0 has attached probability 0.4 and 1 has attached probability 0.6. In this case the mean µ of the distribution is 0.4 x 0 + 0.6 x 1 = 0.6.\n\n\n\nThis mean does not need to be one of the allowed values of the distribution (here 0 and 1). The mean here simply indicates whats more likely: 0.6 means that heads is more likely than tails. What is the mean in the case of the fair coin?\nNow let us simulate the case of the fair coin. We’ll toss a sample of N coins, or 1 coin N times, using the magic of numpy. We’ll find the average of these N tosses. This is the fraction of heads! We’ll plot this sample average against the sample size N.\n\n\n\nWe find that these sample averages are quite close to 0.5. And, as we increase the sample size N, these sample averages become super close to 0.5. Indeed, as N becomes infinite, the sample averages approach the mean µ=0.5. This is the Law of Large Numbers.\n\n\n\nThe LLN can be tautologically used to define the probability of a fair coin showing heads as the asymptotic (infinite N) sampling average. This is the frequentist definition of “sampling probability”, the population frequency µ.\nBut we might also treat the mean µ as an intrinsic fraction of heads, a “parameter” of the Bernoulli distribution. Where does it come from in the first place? The value µ can be thought of as an “inferential probability” derived from symmetry and lack of knowledge.\n\n\n\nIf you have a coin (2 sides, 2 possibilities), and no additional information about the coin and toss physics (thus fair), you would guess fraction µ=0.5 for heads. The LLN then says that sampling probabilities converge to this “inferential probability”.\n\nwh"
  },
  {
    "objectID": "posts/boxloop.html",
    "href": "posts/boxloop.html",
    "title": "Box’s Loop",
    "section": "",
    "text": "In the 1960’s, the great statistician Box, along with his collaborators, formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box’s loop by Blei et. al., 1, and illustrated in the diagram (taken from the above linked paper) below:\n1 Blei, David M. “Build, compute, critique, repeat: Data analysis with latent variable models.” Annual Review of Statistics and Its Application 1 (2014): 203-232.\nBox himself focussed on the scientific method, but the loop is applicable at large to other examples of probabilistic modelling, such as the building of an information retrieval or recommendation system, exploratory data analysis, etc, etc\nWe:\n\nfirst build a model. This is as much as an art as a science if we are of the philosophical bent that we desire explainability. We bring in domain experts.\nWe compute a model using the observed data.\nWe then critique our model, studying how they succeed or fail and how they predict future data or on held out sets.\nIf we are satisfied with the performance of our model we apply it in the context of a predictive or explanatory system. If we are not, we go back to 1.\n\nIf we are Bayesians, we compute the posterior distribution (the distribution of the parameters conditioned on the data) of the (hidden) parameters of the model. Here we assume that the data is fixed and our stochasticity is in the parameters.\nIf we are Frequentists, we assume our data is a sample from a population and compute the parameters of our models abd confidence intervals for those parameters. Here we assume that the data is stochastic as in we could get multiple different samplkes, but that the parameter is fixed and given.\nWe could have mis-specified our model. It might be too simple or too complex. If so we go back to (1) and try again with another model specification."
  },
  {
    "objectID": "posts/votingforcongress/congress.html",
    "href": "posts/votingforcongress/congress.html",
    "title": "Some Data Analysis about Congress",
    "section": "",
    "text": "import pandas as pd\n\n\ntbl = pd.read_html(\"https://www.presidency.ucsb.edu/statistics/data/seats-congress-gainedlost-the-presidents-party-mid-term-elections\")\n\n\ndf = tbl[0]\ndf.columns = df.columns.to_flat_index()\ndf\n\n\n\n\n\n\n\n\n(Unnamed: 0_level_0, Year)\n(Unnamed: 1_level_0, Lame Duck?)\n(Unnamed: 2_level_0, President)\n(Unnamed: 3_level_0, President'sParty)\n(President's Job Approval Percentage (Gallup) As of:, Early Aug)\n(President's Job Approval Percentage (Gallup) As of:, Late Aug)\n(President's Job Approval Percentage (Gallup) As of:, Early Sep)\n(President's Job Approval Percentage (Gallup) As of:, Late Sep)\n(President's Job Approval Percentage (Gallup) As of:, Early Oct)\n(President's Job Approval Percentage (Gallup) As of:, Late Oct)\n(President's Party, House Seatsto Defend)\n(President's Party, Senate Seatsto Defend)\n(Seat Change, President's Party, House Seats)\n(Seat Change, President's Party, Senate Seats)\n\n\n\n\n0\n1934\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n--\n313\n14\n+9\n+9\n\n\n1\n1938\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n60\n334\n27\n-81\n-7\n\n\n2\n1942\nNaN\nFranklin D. Roosevelt\nD\n74\n--\n74\n--\n--\n--\n267\n25\n-46\n-9\n\n\n3\n1946\nNaN\nHarry S. Truman\nD\n--\n--\n33\n--\n--\n27\n244\n21\n-45\n-12\n\n\n4\n1950\nLD*\nHarry S. Truman\nD\nnd\n43\n35\n35\n43\n41\n263\n21\n-29\n-6\n\n\n5\n1954\nNaN\nDwight D. Eisenhower\nR\n67\n62\n--\n66\n62\n--\n221\n11\n-18\n-1\n\n\n6\n1958\nLD\nDwight D. Eisenhower\nR\n58\n56\n56\n54\n57\n--\n203\n20\n-48\n-13\n\n\n7\n1962\nNaN\nJohn F. Kennedy\nD\n--\n67\n--\n63\n--\n61\n264\n18\n-4\n+3\n\n\n8\n1966\n†\nLyndon B. Johnson\nD\n51\n47\n--\n--\n44\n44\n295\n21\n-47\n-4\n\n\n9\n1970\nNaN\nRichard Nixon\nR\n55\n55\n57\n51\n58\n--\n192\n7\n-12\n+2\n\n\n10\n1974\n±\nGerald R. Ford (Nixon)\nR\n71\n--\n66\n50\n53\n--\n192\n15\n-48\n-5\n\n\n11\n1978\nNaN\nJimmy Carter\nD\n43\n43\n48\n--\n49\n45\n292\n14\n-15\n-3\n\n\n12\n1982\nNaN\nRonald Reagan\nR\n41\n42\n--\n42\n--\n42\n192\n12\n-26\n+1\n\n\n13\n1986\nLD\nRonald Reagan\nR\n--\n64\n--\n63\n64\n--\n181\n22\n-5\n-8\n\n\n14\n1990\nNaN\nGeorge Bush\nR\n75\n73\n54\n--\n--\n57\n175\n17\n-8\n-1\n\n\n15\n1994\nNaN\nWilliam J. Clinton\nD\n43\n40\n40\n44\n43\n48\n258\n17\n-52\n-8\n\n\n16\n1998\nLD\nWilliam J. Clinton\nD\n65\n62\n63\n66\n65\n65\n207\n18\n+5\n0\n\n\n17\n2002\nNaN\nGeorge W. Bush\nR\n--\n66\n66\n66\n68\n67\n220\n20\n+8\n+2\n\n\n18\n2006\nLD\nGeorge W. Bush\nR\n37\n42\n39\n44\n37\n37\n233\n15\n-30\n-6\n\n\n19\n2010\nNaN\nBarack Obama\nD\n44\n44\n45\n45\n45\n45\n257\n15\n-63\n-6\n\n\n20\n2014\nLD\nBarack Obama\nD\n42\n42\n41\n43\n42\n41\n201\n20\n-13\n-9\n\n\n21\n2018\nNaN\nDonald J. Trump\nR\n41\n41\n39\n41\n44\n44\n241\n9\n-40\n+2\n\n\n22\n2022\nNaN\nJoseph R. Biden\nD\n38\n44\n44\n42\n42\nNaN\n222\n14\nTBD\nTBD"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Box’s Loop\n\n\n\n\n\n\nmodels\n\n\nprobabilistic modeling\n\n\n\nModeling is not the end, its just the beginning!\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\n\n\n\n\nmodel-comparison\n\n\ncongress\n\n\n\nHow to sitting president’s parties do with congress?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\n\n\n\n\nStatistics\n\n\nMonteCarlo\n\n\n\nProbably the most important theorem in frequentist statistics\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\n\n\n\n\nVisualization\n\n\nCommunication\n\n\nStorytelling\n\n\n\nHow should you communicate your insights in Visualization?\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "collections/software/prefect.html#why-choose-this-tool",
    "href": "collections/software/prefect.html#why-choose-this-tool",
    "title": "Prefect-2.0",
    "section": "Why choose this tool?",
    "text": "Why choose this tool?\nPrefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#more-about-the-tool",
    "href": "collections/software/prefect.html#more-about-the-tool",
    "title": "Prefect-2.0",
    "section": "More about the tool",
    "text": "More about the tool\nPrefect is organized around the notion of fllows. Flows can have subflows, and both of these can have tasks, but tasks cannot have sub-tasks. Flows have implementation as processes or as docker containers.\n\nflows can be run adhoc\nflows can be scheduled\nother DAG based software such as DVC pipelines, hamilton, and dbt can be run as prefect processes\nprefect does not seem to support event based activation of pipelines, although the ability to create deployments in python can enable us to create some such flow\nprefect is well integrated with dask, which we can then use for hyper-parameter optimizations on our cluster or other such distributed computations"
  },
  {
    "objectID": "collections/software/prefect.html#how-to-install",
    "href": "collections/software/prefect.html#how-to-install",
    "title": "Prefect-2.0",
    "section": "How to install",
    "text": "How to install\npip install -U prefect\nThe prefect orion UI will need proxying out of a cluster."
  },
  {
    "objectID": "collections/software/prefect.html#alternatives",
    "href": "collections/software/prefect.html#alternatives",
    "title": "Prefect-2.0",
    "section": "Alternatives",
    "text": "Alternatives\nSeveral alternatives exist. The old airflow and luigi are still around."
  }
]