[
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "Claude Session Context",
    "section": "Project Overview",
    "text": "Project Overview\nThis is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io. We’re designing new Bootstrap 5.3 themes with ColorBrewer palettes for matplotlib compatibility."
  },
  {
    "objectID": "CLAUDE.html#current-state",
    "href": "CLAUDE.html#current-state",
    "title": "Claude Session Context",
    "section": "Current State",
    "text": "Current State\n\nDesign Folders\n\ndesigns/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — NEEDS WORK (fork of depth, to be modernized)\ndesigns/design2-duality/ — PuOr diverging palette (rejected, too busy)\nOther folders are older iterations, can be ignored\n\n\n\nWhat User Likes About design1-depth\n\nClean, minimal aesthetic with good negative space\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nGreater line spacing (1.8 for body)\nColorBrewer Blues palette (#eff3ff → #08306b)\nYin-yang icon (just changed from layers icon)\nCircular brand mark (just changed from rounded square)\nSubtle code cell styling with just “In [n]” in corner\nDark/light mode support\n\n\n\nFiles in Each Design Folder\n\nindex.html — Landing page with hero, post cards, code section, interactive container\nboxloop.html — Box’s Loop article\nvizasstory.html — Visualization as Story article\ncongress.html — Congressional data analysis with pandas table\nearth-demo.html — Three.js rotating Earth demo"
  },
  {
    "objectID": "CLAUDE.html#next-session-task-modernize-design1-modern",
    "href": "CLAUDE.html#next-session-task-modernize-design1-modern",
    "title": "Claude Session Context",
    "section": "NEXT SESSION TASK: Modernize design1-modern",
    "text": "NEXT SESSION TASK: Modernize design1-modern\nTake designs/design1-modern/ and make it more cutting-edge while preserving what works:\n\nKeep\n\nSerif typography (Bitter + Source Serif 4)\nBlues ColorBrewer palette\nGood line spacing and negative space\nClean code block styling\nDark/light mode\n\n\n\nModernize With\n\nSubtle animations — Add micro-interactions on hover, smooth transitions, maybe a subtle parallax or scroll-triggered effect\nBolder hero — Consider a more dynamic hero section, perhaps with animated gradient backgrounds or geometric shapes\nGlassmorphism touches — Add subtle backdrop-filter blur effects on cards or nav (like sunearthmoon reference)\nModern layout flourishes — Asymmetric grids, overlapping elements, or diagonal/angled sections\nEnhanced interactive container — Make it feel more premium, better glow effects\nTypography polish — Maybe add variable font features, gradient text for hero title\nBetter visual hierarchy — Use the sequential blues more dramatically for depth layering\n\n\n\nReference Sites\n\nhttps://rahuldave.com/sunearthmoon/ — Cutting edge dark theme with Three.js, glassmorphism\nhttps://am207.github.io/2018fall/ — Clean academic (what to avoid being too much like)\n\n\n\nTechnical Notes\n\nBootstrap 5.3.3 via CDN\nMust expose CSS custom properties (–color-, –interactive-, –cb-blue-*) for embedded JS demos\nAll files are self-contained HTML with inline CSS\nThree.js demos should inherit theme colors\n\n\n\nColorBrewer Blues Palette (for reference)\n--cb-blue-50: #eff3ff   (lightest)\n--cb-blue-100: #c6dbef\n--cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6\n--cb-blue-400: #4292c6\n--cb-blue-500: #2171b5\n--cb-blue-600: #08519c\n--cb-blue-700: #084594\n--cb-blue-800: #08306b  (darkest)"
  },
  {
    "objectID": "CLAUDE.html#commands-to-preview",
    "href": "CLAUDE.html#commands-to-preview",
    "title": "Claude Session Context",
    "section": "Commands to Preview",
    "text": "Commands to Preview\nopen designs/design1-depth/index.html   # Current clean version\nopen designs/design1-modern/index.html  # To be modernized"
  },
  {
    "objectID": "collections/software/prefect.html#why-choose-this-tool",
    "href": "collections/software/prefect.html#why-choose-this-tool",
    "title": "Prefect-2.0",
    "section": "Why choose this tool?",
    "text": "Why choose this tool?\nPrefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#more-about-the-tool",
    "href": "collections/software/prefect.html#more-about-the-tool",
    "title": "Prefect-2.0",
    "section": "More about the tool",
    "text": "More about the tool\nPrefect is organized around the notion of fllows. Flows can have subflows, and both of these can have tasks, but tasks cannot have sub-tasks. Flows have implementation as processes or as docker containers.\n\nflows can be run adhoc\nflows can be scheduled\nother DAG based software such as DVC pipelines, hamilton, and dbt can be run as prefect processes\nprefect does not seem to support event based activation of pipelines, although the ability to create deployments in python can enable us to create some such flow\nprefect is well integrated with dask, which we can then use for hyper-parameter optimizations on our cluster or other such distributed computations"
  },
  {
    "objectID": "collections/software/prefect.html#how-to-install",
    "href": "collections/software/prefect.html#how-to-install",
    "title": "Prefect-2.0",
    "section": "How to install",
    "text": "How to install\npip install -U prefect\nThe prefect orion UI will need proxying out of a cluster."
  },
  {
    "objectID": "collections/software/prefect.html#alternatives",
    "href": "collections/software/prefect.html#alternatives",
    "title": "Prefect-2.0",
    "section": "Alternatives",
    "text": "Alternatives\nSeveral alternatives exist. The old airflow and luigi are still around."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Box’s Loop\n\n\n\nmodels\n\nprobabilistic modeling\n\n\n\nModeling is not the end, its just the beginning!\n\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRotating Earth\n\n\n\nvisualization\n\nthree.js\n\ninteractive\n\n\n\nA Three.js demonstration showing Earth’s rotation with the Modern theme.\n\n\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\n\nmodel-comparison\n\ncongress\n\n\n\nHow to sitting president’s parties do with congress?\n\n\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\n\nStatistics\n\nMonteCarlo\n\n\n\nProbably the most important theorem in frequentist statistics\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\n\nVisualization\n\nCommunication\n\nStorytelling\n\n\n\nHow should you communicate your insights in Visualization?\n\n\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "collections.html",
    "href": "collections.html",
    "title": "Collections",
    "section": "",
    "text": "Software\n\n\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nAwk\n\n\n \n\n\n\n\n\n\nDec 3, 2022\n\n\nPrefect-2.0\n\n\n \n\n\n\n\n\n\nDec 3, 2024\n\n\nStitchfix Hamilton\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Rahul’s Blog",
    "section": "Recent Posts",
    "text": "Recent Posts"
  },
  {
    "objectID": "posts/earth-demo/index.html",
    "href": "posts/earth-demo/index.html",
    "title": "Rotating Earth",
    "section": "",
    "text": "This interactive visualization demonstrates a rotating Earth rendered with Three.js. The visualization inherits the theme’s ColorBrewer Blues palette through CSS custom properties (--cb-blue-* and --interactive-*)."
  },
  {
    "objectID": "posts/earth-demo/index.html#how-it-works",
    "href": "posts/earth-demo/index.html#how-it-works",
    "title": "Rotating Earth",
    "section": "How It Works",
    "text": "How It Works\nThe Earth visualization uses Three.js to create a rotating sphere with:\n\nProcedural texture: Ocean gradients using the theme’s Blues palette\nSimplified continents: Elliptical shapes for landmasses\nIce caps: White polar regions\nAtmospheric glow: Semi-transparent outer sphere\nStars background: CSS-animated twinkling stars\n\nThe visualization reads CSS custom properties from the theme, so it automatically adapts to light/dark mode changes.\n# Example: Reading theme colors in JavaScript\nconst style = getComputedStyle(document.documentElement);\nconst primaryColor = style.getPropertyValue('--cb-blue-600');\nconst accentColor = style.getPropertyValue('--interactive-accent');"
  },
  {
    "objectID": "posts/vizasstory.html",
    "href": "posts/vizasstory.html",
    "title": "Visualization As Story",
    "section": "",
    "text": "There is this pretty famous book by Steve Krug, called “Dont Make Me Think”. Its a call to respect conventions for web elements, such as shopping carts (a cart should be on the upper right), so that the web experience is obvious to users.\n\n\n\nIn visualization, as in web development, your audience does not want to spend cognitive effort on things you could just show them, by convention, or by explicit writing. So, just point out the key facts and insights.\nFor example, in this great article in the financial times https://www.ft.com/content/0f11b219-0f1b-420e-8188-6651d1e749ff?hcb=1, the main point “Vaccines have made Covid-19 far less lethal” is written up-front.\n\n\n\nThe implications are made clear in the second sentence, comparing vaccinated 80 year-olds to un-vaccinated 50 year-olds. This implication is illustrated in the visualization as well, with a horizontal black line, and a caption.\nInstead of point markers, downwards pointing arrows are used on lines to reinforce the notion of lower risk. Captions and annotations are used to point out key insights. Extraneous frames and tick marks are removed.\nThis is an example of framing. It grabs the audience and leads it through the insights you want to share.\n\n\n\nThere’s been a lot of worry about breakthrough vaccination, especially with the news about the Provincetown cluster. Here is another visualization from the same article, telling us why the large number of breakthrough infections are to be expected.\n\n\n\nIt walks us through the entire calculation visually. And does it in two scenarios: high vaccination rates and low vaccination rates. We can ourselves see the larger hospitalization numbers in the low-vaccination scenario.\nThe visualization and explanation could have been framed in terms of base rates and conditional probabilities, but by illustrating the concepts with an example, they are made accessible to everyone. And the framing drives home the story: go get your shot!\nRead more on how to make good visualizations using R in this book by @khealy . If you are a pythonista, learn how to make good plots in @matplotlib using https://end-to-end-machine-learning.teachable.com/p/navigating-matplotlib-tutorial-how-to/ by @_brohrer_ ."
  },
  {
    "objectID": "posts/lawoflargenumbers.html",
    "href": "posts/lawoflargenumbers.html",
    "title": "The LLN",
    "section": "",
    "text": "Suppose that you toss a fair coin and catch it to see if you got heads or tails. Then you have this intuition that while you might get a streak of several heads in a row, in the long run the heads and tails are balanced.\nThis is actually an example of a famous law: the Law of Large numbers (LLN), which states that if you have a random variable X with a mean, the average value of X over a sample of size N converges i.e. gets close and closer to this mean as N becomes larger and larger.\n\n\n\nThe LLN was first proved by Jakob Bernoulli in Ars Conjectandi, published posthumously by his nephew Niklaus Bernoulli, who appropriated entire passages of it for his treatise on law. It is the basis of much of modern statistics, including the Monte-Carlo method.\nLets parse the law. A random variable is one that can take multiple values, each with some probability. So if X represents the flip of a coin, it will take values Heads and Tails with some probability. We’ll assign Heads the value 1 and Tails the value 0.\nThe probabilities attatched to the values a random variable takes is called a distribution, or probability mass function (pmf). For a fair coin, the “Bernoulli” Distribution attaches the probabilities 0.5 to value 1 and 0.5 to value 0. These probabilities must add to 1.\n\n\n\nAn unfair coin thats more likely to land on heads might have a distribution where 0 has attached probability 0.4 and 1 has attached probability 0.6. In this case the mean µ of the distribution is 0.4 x 0 + 0.6 x 1 = 0.6.\n\n\n\nThis mean does not need to be one of the allowed values of the distribution (here 0 and 1). The mean here simply indicates whats more likely: 0.6 means that heads is more likely than tails. What is the mean in the case of the fair coin?\nNow let us simulate the case of the fair coin. We’ll toss a sample of N coins, or 1 coin N times, using the magic of numpy. We’ll find the average of these N tosses. This is the fraction of heads! We’ll plot this sample average against the sample size N.\n\n\n\nWe find that these sample averages are quite close to 0.5. And, as we increase the sample size N, these sample averages become super close to 0.5. Indeed, as N becomes infinite, the sample averages approach the mean µ=0.5. This is the Law of Large Numbers.\n\n\n\nThe LLN can be tautologically used to define the probability of a fair coin showing heads as the asymptotic (infinite N) sampling average. This is the frequentist definition of “sampling probability”, the population frequency µ.\nBut we might also treat the mean µ as an intrinsic fraction of heads, a “parameter” of the Bernoulli distribution. Where does it come from in the first place? The value µ can be thought of as an “inferential probability” derived from symmetry and lack of knowledge.\n\n\n\nIf you have a coin (2 sides, 2 possibilities), and no additional information about the coin and toss physics (thus fair), you would guess fraction µ=0.5 for heads. The LLN then says that sampling probabilities converge to this “inferential probability”.\n\nwh"
  },
  {
    "objectID": "posts/boxloop.html",
    "href": "posts/boxloop.html",
    "title": "Box’s Loop",
    "section": "",
    "text": "In the 1960’s, the great statistician Box, along with his collaborators, formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box’s loop by Blei et. al., 1, and illustrated in the diagram (taken from the above linked paper) below:\n1 Blei, David M. “Build, compute, critique, repeat: Data analysis with latent variable models.” Annual Review of Statistics and Its Application 1 (2014): 203-232.\nBox himself focussed on the scientific method, but the loop is applicable at large to other examples of probabilistic modelling, such as the building of an information retrieval or recommendation system, exploratory data analysis, etc, etc\nWe:\n\nfirst build a model. This is as much as an art as a science if we are of the philosophical bent that we desire explainability. We bring in domain experts.\nWe compute a model using the observed data.\nWe then critique our model, studying how they succeed or fail and how they predict future data or on held out sets.\nIf we are satisfied with the performance of our model we apply it in the context of a predictive or explanatory system. If we are not, we go back to 1.\n\nIf we are Bayesians, we compute the posterior distribution (the distribution of the parameters conditioned on the data) of the (hidden) parameters of the model. Here we assume that the data is fixed and our stochasticity is in the parameters.\nIf we are Frequentists, we assume our data is a sample from a population and compute the parameters of our models abd confidence intervals for those parameters. Here we assume that the data is stochastic as in we could get multiple different samplkes, but that the parameter is fixed and given.\nWe could have mis-specified our model. It might be too simple or too complex. If so we go back to (1) and try again with another model specification."
  },
  {
    "objectID": "posts/votingforcongress/congress.html",
    "href": "posts/votingforcongress/congress.html",
    "title": "Some Data Analysis about Congress",
    "section": "",
    "text": "import pandas as pd\n\n\ntbl = pd.read_html(\"https://www.presidency.ucsb.edu/statistics/data/seats-congress-gainedlost-the-presidents-party-mid-term-elections\")\n\n\ndf = tbl[0]\ndf.columns = df.columns.to_flat_index()\ndf\n\n\n\n\n\n\n\n\n(Unnamed: 0_level_0, Year)\n(Unnamed: 1_level_0, Lame Duck?)\n(Unnamed: 2_level_0, President)\n(Unnamed: 3_level_0, President'sParty)\n(President's Job Approval Percentage (Gallup) As of:, Early Aug)\n(President's Job Approval Percentage (Gallup) As of:, Late Aug)\n(President's Job Approval Percentage (Gallup) As of:, Early Sep)\n(President's Job Approval Percentage (Gallup) As of:, Late Sep)\n(President's Job Approval Percentage (Gallup) As of:, Early Oct)\n(President's Job Approval Percentage (Gallup) As of:, Late Oct)\n(President's Party, House Seatsto Defend)\n(President's Party, Senate Seatsto Defend)\n(Seat Change, President's Party, House Seats)\n(Seat Change, President's Party, Senate Seats)\n\n\n\n\n0\n1934\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n--\n313\n14\n+9\n+9\n\n\n1\n1938\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n60\n334\n27\n-81\n-7\n\n\n2\n1942\nNaN\nFranklin D. Roosevelt\nD\n74\n--\n74\n--\n--\n--\n267\n25\n-46\n-9\n\n\n3\n1946\nNaN\nHarry S. Truman\nD\n--\n--\n33\n--\n--\n27\n244\n21\n-45\n-12\n\n\n4\n1950\nLD*\nHarry S. Truman\nD\nnd\n43\n35\n35\n43\n41\n263\n21\n-29\n-6\n\n\n5\n1954\nNaN\nDwight D. Eisenhower\nR\n67\n62\n--\n66\n62\n--\n221\n11\n-18\n-1\n\n\n6\n1958\nLD\nDwight D. Eisenhower\nR\n58\n56\n56\n54\n57\n--\n203\n20\n-48\n-13\n\n\n7\n1962\nNaN\nJohn F. Kennedy\nD\n--\n67\n--\n63\n--\n61\n264\n18\n-4\n+3\n\n\n8\n1966\n†\nLyndon B. Johnson\nD\n51\n47\n--\n--\n44\n44\n295\n21\n-47\n-4\n\n\n9\n1970\nNaN\nRichard Nixon\nR\n55\n55\n57\n51\n58\n--\n192\n7\n-12\n+2\n\n\n10\n1974\n±\nGerald R. Ford (Nixon)\nR\n71\n--\n66\n50\n53\n--\n192\n15\n-48\n-5\n\n\n11\n1978\nNaN\nJimmy Carter\nD\n43\n43\n48\n--\n49\n45\n292\n14\n-15\n-3\n\n\n12\n1982\nNaN\nRonald Reagan\nR\n41\n42\n--\n42\n--\n42\n192\n12\n-26\n+1\n\n\n13\n1986\nLD\nRonald Reagan\nR\n--\n64\n--\n63\n64\n--\n181\n22\n-5\n-8\n\n\n14\n1990\nNaN\nGeorge Bush\nR\n75\n73\n54\n--\n--\n57\n175\n17\n-8\n-1\n\n\n15\n1994\nNaN\nWilliam J. Clinton\nD\n43\n40\n40\n44\n43\n48\n258\n17\n-52\n-8\n\n\n16\n1998\nLD\nWilliam J. Clinton\nD\n65\n62\n63\n66\n65\n65\n207\n18\n+5\n0\n\n\n17\n2002\nNaN\nGeorge W. Bush\nR\n--\n66\n66\n66\n68\n67\n220\n20\n+8\n+2\n\n\n18\n2006\nLD\nGeorge W. Bush\nR\n37\n42\n39\n44\n37\n37\n233\n15\n-30\n-6\n\n\n19\n2010\nNaN\nBarack Obama\nD\n44\n44\n45\n45\n45\n45\n257\n15\n-63\n-6\n\n\n20\n2014\nLD\nBarack Obama\nD\n42\n42\n41\n43\n42\n41\n201\n20\n-13\n-9\n\n\n21\n2018\nNaN\nDonald J. Trump\nR\n41\n41\n39\n41\n44\n44\n241\n9\n-40\n+2\n\n\n22\n2022\nNaN\nJoseph R. Biden\nD\n38\n44\n44\n42\n42\nNaN\n222\n14\nTBD\nTBD"
  },
  {
    "objectID": "posts/probability.html#contents",
    "href": "posts/probability.html#contents",
    "title": "Probability",
    "section": "Contents",
    "text": "Contents\n{:.no_toc} * {: toc}\n\n%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', False)\nimport seaborn as sns\nsns.set_style(\"white\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/probability.html#what-is-probability",
    "href": "posts/probability.html#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability?\nSuppose you were to flip a coin. Then you expect not to be able to say whether the next toss would yield a heads or a tails. You might tell a friend that the odds of getting a heads is equal to to the odds of getting a tails, and that both are \\(1/2\\).\nThis intuitive notion of odds is a probability. It comes about because of our physical model of the world: say that because of our faith in the U.S. Mint, we might be willing to, without having seen any tosses, say that the coin is fair. In other words, there are two choices, both of which are equally likely.\n\nProbability from Symmetry\nConsider another example. If we were tossing a ‘fair’ six-sided dice, we may thus equivalently say that the odds of the dice falling on any one of its sides is \\(1/6\\). Indeed if there are \\(C\\) different equally likely possibilities, we’d expect that the probability of any one particular outcome would be \\(1/C\\).\nThe examples of the coin as well as the dice illustrate the notion of probability springing from symmetry. Here we think of probability of of the number 4 on the dice as the ratio:\n\\[\\frac{Number\\: of\\: cases\\: for\\: number\\: 4}{number\\: of\\: possibilities} = \\frac{1}{6},\\] assuming equally likely possibilities.\nIn other words, the symmetry refers to the notion that when there are multiple ways for an event to happen, and that then we have an intuitive model of fairness between these ways that tells us that none of these are any more likely than the other.\n\n\nProbability from a model\nThus one might think of symmetry as providing a model. There are also other kinds of models.\nThink of an event like an election, say a presidential election. You cant exactly run multiple trials of the election: its a one-off event. But you still want to talk about the likelyhood of a candidate winning. However people do make models of elections, based on inputs such as race, age, income, sampling polls, etc. They assign likeyhoods of candidates winning and run large numbers of simulations of the election, making predictions based on that. Forecasters like Nate Silver, Sam Wang, And Drew Linzer, made incredibly successfull predictions of the 2012 elections.\nOr consider what a weather forecaster means when he or she says there is a 90% chance of rain today. Presumably, this conclusion has been made from many computer simulations which take in the weather conditions known in the past, and propagated using physics to the current day. The simulations give different results based on the uncertainty in the measurement of past weather, and the inability of the physics to capture the phenomenon exactly (all physics is some approximation to the natural world). But 90% of these simulations show rain.\nIn all of these cases, there is either a model (a fair coin, an election forecasting model, a weather differential equation), or an experiment ( a large number of coin tosses) that is used to estimate a probability, or the odds, of an event \\(E\\) occuring.\n\nCombining models and observations\nIn all of these cases, probability is something we speak of, for observations we are to make in the future. And it is something we assign, based on the model or belief of the world we have, or on the basis of past observations that we have made, or that we might even imagine that we would make.\nConsider some additional examples. You might ask the probability of the Yankees winning the next baseball game against the Red Sox. Or you might ask for the probability of a launch failure for the next missile protecting Tel-Aviv. These are not academic questions: lots of betting money and lives depend upon them respectively. In both cases there is some past data, and some other inputs such as say, weather conditions, which might be used to construct a model, which is then used to predict the fate of the next game or launch.\nThey key takeaway is this: for some reasons, and possibly using some data, we have constructed a model of the universe. In other words, we have combined prior beliefs and past frequencies respectively. This notion of such combination is yet another notion of probability, called the Bayesian notion of probability. And we can now use this model to make predictions, such us the future odds of a particular event happening.\n\n\n\nProbability from frequency\nConsider doing a large number of coin flips. You would do, or imagine doing, a large number of flips or trials \\(N\\), and finding the number of times you got heads \\(N_H\\). Then the probability of getting heads would be \\[\\frac{N_H}{N}.\\]\nThis is the notion of probability as a relative frequency: if there are multiple ways an event like the tossing of a coin can happen, lets look at multiple trials of the event and see the fraction of times one or other of these ways happened.\nThis jibes with our general notion of probability from symmetry: indeed you can think of it as an experimental verificaltion of a symmetry based model.\n\nSimulating the results of the model\nWe dont have a coin right now. So let us simulate this process on a computer. To do this we will use a form of the random number generator built into numpy. In particular, we will use the function np.random.choice, which will with equal probability for all items pick an item from a list (thus if the list is of size 6, it will pick one of the six list items each time, with a probability 1/6).\n\ndef throw_a_coin(N):\n    return np.random.choice(['H','T'], size=N)\nthrows=throw_a_coin(40)\nprint(\"Throws:\",\" \".join(throws))\nprint(\"Number of Heads:\", np.sum(throws=='H'))\nprint(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/40.)\n\nThrows: T H T H T H H H H H T H H H T H T H T H T H H H T H H H H T T T T H T T H T T T\nNumber of Heads: 22\np1 = Number of Heads/Total Throws: 0.55\n\n\nNotice that you do not necessarily get 20 heads.\nNow say that we run the entire process again, a second replication to obtain a second sample. Then we ask the same question: what is the fraction of heads we get this time? Lets call the odds of heads in sample 2, then, \\(p_2\\):\n\ndef make_throws(N):\n    throws=throw_a_coin(N)\n    if N &lt;= 100:\n        print(\"Throws:\",\" \".join(throws))\n    else:\n        print(\"First 100 Throws:\",\" \".join(throws[:100]))\n    print(\"Number of Heads:\", np.sum(throws=='H'))\n    print(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/N)\nmake_throws(40)\n\nThrows: H T H T H H H H T H H H T T T H T H H H H T H H T T H H T H T H T H H H T T H H\nNumber of Heads: 25\np1 = Number of Heads/Total Throws: 0.625\n\n\nLet’s do many more trials\n\nmake_throws(1000)\n\nFirst 100 Throws: H H H T H H T T T H T H H H H T T T H H H H H H T H T T T H T H T T T H H T H T H H H H T H T H T T H H T T T T T T H T H H T H T H T H T H T T H T H H H T H T T T H H T H T H T H T H H H H T T T T H\nNumber of Heads: 521\np1 = Number of Heads/Total Throws: 0.521\n\n\nAnd even more:\n\nmake_throws(10000)\n\nFirst 100 Throws: H T T T H T T T H T T T T H T T H T H H H H T H T H T T H T H T H H H H T T H H H H T H H H H T H T T T H T H H T T T H T H T H T T T H T T T T H T H H H H H T T H H H T T H H H H H H T H T H T T T H\nNumber of Heads: 5047\np1 = Number of Heads/Total Throws: 0.5047\n\n\nAs you can see, the larger number of trials we do, the closer we seem to get to half the tosses showing up heads. Lets see this more systematically:\n\ntrials=np.arange(0, 40000, 1000)\nplt.plot(trials, [np.sum(throw_a_coin(j)=='H')/np.float(j) for j in trials], 'o-', alpha=0.2);\nplt.axhline(0.5, 0, 1, color='r');\nplt.xlabel('number of trials');\nplt.ylabel('probability of heads from simulation');\nplt.title('frequentist probability of heads');\n\n\n\n\n\n\n\n\nThus, the true odds fluctuate about their long-run value of 0.5, in accordance with the model of a fair coin (which we encoded in our simulation by having np.random.choice choose between two possibilities with equal probability), with the fluctuations becoming much smaller (we shall talk a lot more about this later in the book). These fluctations are what give rise to probability distributions.\nEach finite length run is called a sample, which has been obtained from the generative model of our fair coin. Its called generative as we can use the model to generate, using simulation, a set of samples we can play with to understand a model. Such simulation from a model is a key technique which we will come back to again and again in learning from data."
  },
  {
    "objectID": "posts/probability.html#the-rules-of-probability",
    "href": "posts/probability.html#the-rules-of-probability",
    "title": "Probability",
    "section": "The rules of probability",
    "text": "The rules of probability\nWe have seen multiple notions of probability so far. One might assign probabilities based on symmetry, for eg, 2 sides of a fair coin, or six sides of a fair dice. One might assign probabilities based on doing an experiment. such as the long run number of heads in many coin flips. One might assign probabilities based on beliefs; and one might even assign probabilities to events that have no chance of repeating, such as the 2012 presidential election, or the probability of rain between 2pm and 6pm today.\nThus, the very definition of probability seems to be wishy-washy and subjective. Thus you might wonder how you might work with such probabilities. For this, we turn to the rules of probability.\nThe rules dont care where our probabilities come from, as to how we estimated them, as long as they behave in intuitively sensible ways.\nConsider an example:\nE is the event of getting a heads in a first coin toss, and F is the same for a second coin toss. Here \\(\\Omega\\), the set of all possibilities that can happen when you toss two coins is \\(\\{HH, HT, TH, TT\\}\\). Since E only specifies that the first toss is heads, \\(E=\\{HT, HH\\}\\). Similarly \\(F= {HH, TH}\\) The set of all events that are not E then is \\(\\tilde{E} = {TH, TT}\\).\nThese sets, along with some others are captured in the venn diagram below:\n\n\n\n2 coin toss venn diagram\n\n\n\nThe Multiply/And/Intersection Formula for independent events\nIf E and F are independent events, the probability of both events happening together \\(P(EF)\\) or \\(P(E \\cap F)\\) (read as E and F or E intersection F, respectively) is the multiplication of the individual probabilities.\n\\[ P(EF) = P(E) P(F) .\\]\nIf you made the two independent coin tosses in our example, and you had a fair coin, the probability of both coming up heads is \\((1/2)*(1/2) = 1/4\\). This makes intuitive sense: half the time the first coin comes up heads, and then 1/2 the time the second coin comes up heads, so its 1/4 of the times that both come up heads.\n\n\nThe Plus/Or/Union Formula\nWe can now ask the question, what is \\(P(E+F)\\), the odds of E alone, F alone, or both together. Translated into English, we are asking, whats the probability that only the first toss was heads, or only the second toss was heads, or that both came up heads? Or in other words, what are the odds of at least one heads? The answer to this question is given by the rule:\n\\[P(E+F) = P(E) + P(F) - P(EF),\\]\nthe “plus” formula, where E+F, read as E or F (also \\(E \\cup F\\), reads as E union F) means “E alone, F alone, or both together”. This rule is a hard one to understand and has a lot of notation, so lets examine it in some detail.\nThere are four ways that these two tosses can arrange themselves, as illustrated by this diagram, adapted from the probability chapter in Feynman’s lectures on Physics..you should read it!.\n\n\n\n2 coin flips\n\n\nWe can have a HH, HT, TH, or TT. In three out of 4 of these cases, either the first toss was heads, or the second was heads. Thus \\(P(E+F)=3/4\\).\nThe formula says, add the odds that “the first toss was a heads, without worrying about the second one (1/2), to the probability that the second toss was a heads, without worrying about the first one” (1/2). Since this double counts the situation where both are heads; subtract that (1/4):\n\\[\\begin{eqnarray}\nP(E+F) \\, & = &\\, P(E) + P(F) - P(EF)\\\\\n\\frac{3}{4} \\, & = &\\, \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4}\n\\end{eqnarray}\\]\nArmed with these two formulas, we can tackle the world of conditional and marginal probabilities, and Bayes theorem!\n\n\nFormally summarizing the rules\nIf \\(X\\) and \\(Y\\) are two events and \\(p(X)\\) is the probability of the event \\(X\\) to happen. $X^- $ is the complement of \\(X\\), the event which is all the occurrences which are not in \\(X\\). \\(X+Y\\) is the union of \\(X\\) and \\(Y\\); \\(X,Y\\) is the intersection of \\(X\\) and \\(Y\\). (Both \\(X+Y\\) and \\(X,Y\\) are also events.)\n\n\nThe very fundamental rules of probability:\n\n\\(p(X) &gt;=0\\); probability must be non-negative\n\\(0 ≤ p(X) ≤ 1 \\;\\) \\(X\\) has probability range from 0 to 1.\n\\(p(X)+p(X^-)=1 \\;\\) \\(X\\) must either happen or not happen. These last two aximoms can be thought of as saying that the probabilities if all events put tohether must sum to 1.\n\\(p(X+Y)=p(X)+p(Y)−p(X,Y) \\;\\) \\(X\\) can happen and \\(Y\\) can happen but we must subtract the cases that are happening together so we do not over-count."
  },
  {
    "objectID": "posts/probability.html#random-variables",
    "href": "posts/probability.html#random-variables",
    "title": "Probability",
    "section": "Random Variables",
    "text": "Random Variables\nTo link the notion of events such as \\(E\\) and collections of events, or probability spaces \\(\\Omega\\) to data, we must introduce the concept of random variables. The following definition is taken from Larry Wasserman’s All of Stats.\nDefinition. A random variable is a mapping\n\\[ X: \\Omega \\rightarrow \\mathbb{R}\\]\nthat assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\). \\(\\Omega\\) is the sample space. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. Subsets of \\(\\Omega\\) are called Events. Say \\(\\omega = HHTTTTHTT\\) then \\(X(\\omega) = 3\\) if defined as number of heads in the sequence \\(\\omega\\).\nWe will assign a real number P(A) to every event A, called the probability of A. We also call P a probability distribution or a probability measure. To qualify as a probability, P must satisfy the three axioms (non-negative, \\(P(\\Omega)=1\\), disjoint probs add)."
  },
  {
    "objectID": "posts/probability.html#marginals-and-conditionals-and-bayes-theorem",
    "href": "posts/probability.html#marginals-and-conditionals-and-bayes-theorem",
    "title": "Probability",
    "section": "Marginals and conditionals, and Bayes Theorem",
    "text": "Marginals and conditionals, and Bayes Theorem\nThe diagram below taken from Bishop may be used to illustrate the concepts of conditionals and marginals. Consider two random variables, \\(X\\), which takes the values \\({x_i}\\) where \\(i = 1,...,M\\), and \\(Y\\), which takes the values \\({y_j}\\) where \\(j = 1,...,L\\). The number of instances for which \\(X = x_i\\) and \\(Y = y_j\\) is \\(n_{ij}\\). The number of points in column i where \\(X=x_i\\) is \\(c_i\\), and for the row where \\(Y = y_j\\) is \\(r_j\\).\n\n\n\nm:bishopprob\n\n\nThen the joint probability of having \\(p(X = x_i, Y= y_j)\\) is in the asymptotic limit of large numbers in the frequency sense of probability \\(n_{ij}/N\\) where is the total number of instances. The \\(X\\) marginal, \\(p(X=x_i)\\) can be obtained by summing instances in all the cells in the i’th column:\n\\[p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)\\]\nLets consider next only those instances for which \\(X=x_i\\). This means that we are limiting our analysis to the ith row. Then, we write the conditional probability of \\(Y = y_j\\) given \\(X = x_i\\) as \\(p(Y = y_j \\mid X = x_i)\\). This is the asymptotic fraction of these instances where \\(Y = y_j\\) and is obtained by dividing the instances in the cell by those in the comumn as\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i}.\\]\nA little algebraic rearrangement gives:\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i} = \\frac{n_{ij}}{N} / \\frac{c_i}{N},\\]\nor:\n\\[p(Y = y_j \\mid X = x_i) \\times p(X=x_i) =  p(X=x_i, Y=y_j).\\]\nThis is the product rule of probability with conditionals involved.\nLet us simplify the notation by dropping the \\(X=\\) and \\(Y=\\).\nThen we can write the marginal probability of x as a sum over the joint distribution of x and y where we sum over all possibilities of y,\n\\[p(x) = \\sum_y p(x,y) \\].\nWe can rewrite a joint distribution as a product of a conditional and marginal probability,\n\\[ p(x,y) = p(y\\mid x) p(x) \\]\nThe product rule is applied repeatedly to give expressions for the joint probability involving more than two variables. For example, the joint distribution over three variables can be factorized into a product of conditional probabilities:\n\\[ p(x,y,z) = p(x|y,z) \\, p(y,z) = p(x |y,z) \\, p(y|z) p(z) \\]\n\nBayes rule\nObserve that\n\\[ p(x,y) = p(y\\mid x) p(x) = P(x\\mid y)p(y).\\]\nGiven the product rule one can derive the Bayes rule, which plays a central role in a lot of the things we will be talking:\n\\[ p(y\\mid x) = \\frac{p(x\\mid y) \\, p(y) }{p(x)} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x,y')} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x\\mid y')p(y')}\\]\n\n\nIndependence\nTwo variables are said to be independent if their joint distribution factorizes into a product of two marginal probabilities:\n\\[ p(x,y) = p(x) \\, p(y) \\]\nAnother consequence of independence is that if \\(x\\) and \\(y\\) are independent, the conditional probability of \\(x\\) given \\(y\\) is just the probability of \\(x\\):\n\\[ p(x|y) = p(x) \\]\nIn other words, by conditioning on a particular \\(y\\), we have learned nothing about \\(x\\) because of independence. Two variables \\(x\\) and \\(y\\) and said to be conditionally independent of \\(z\\) if the following holds:\n\\[ p(x,y|z) = p(x|z) p(y|z) \\]\nTherefore, if we learn about z, x and y become independent. Another way to write that \\(x\\) and \\(y\\) are conditionally independent of \\(z\\) is\n\\[ p(x| z, y) = p(x|z) \\]\nIn other words, if we condition on \\(z\\), and now also learn about \\(y\\), this is not going to change the probability of \\(x\\). It is important to realize that conditional independence between \\(x\\) and \\(y\\) does not imply independence between \\(x\\) and \\(y\\).\n\n\nApplication of Bayes Theorem\n\nSally Clark, a lawyer who lost her first son at 11 weeks and her second at 8 weeks, was convicted in 1999. A prominent pediatrician, Sir Roy Meadow, had testified for the prosecution about Sudden Infant Death Syndrome, known as SIDS in the U.S. and cot death in Britain. Citing a government study, Meadow said the incidence of one SIDS death was one in 8,500 in a family like Clark’s–stable, affluent, nonsmoking, with a mother more than 26 years old.\n\n\nThen, despite the fact that some families are predisposed to SIDS, Meadow assumed erroneously that each sibling’s death occurred independently of the other. Multiplying 8,500 by 8,500, he calculated that the chance of two children dying in a family like Sally Clark’s was so rare–one in 73 million–that they must have been murdered.\n\n(from http://www.mcgrayne.com/disc.htm)\np(child 1 dying of sids) = 1/8500\n\nP(child 2 dying of sids) = 1/100\n\nFirst, we look at natural causes of sudden infant death. The chance of one random infant dying from SIDS was about 1 in 1,300 during this period in Britain. Meadow’s argument was flawed and produced a much slimmer chance of natural death. The estimated odds of a second SIDS death in the same family was much larger, perhaps one in 100, because family members can share a common environmental or genetic propensity for SIDS.\n\n\nSecond, we turn to the hypothesis that the babies were murdered. Only about 30 children out of 650,000 annual births in England, Scotland, and Wales were known to have been murdered by their mothers. The number of double murders must be much lower, estimated as 10 times less likely.\n\np(S2 = both children dying of sids) =  0.000007\np(notS2 = not both dying of sids) =  0.999993\n\nData: both children died unexpectedly\nSo now ask, whats:\np(data | S2) = 1\np(data | notS2) = ? both died but not SIDS. Murder? =  30/650000    × 1/10 = 0.000005\nWe want to calculate the “posterior probability”:\np(S2 | data) = P(data | S2) P(S2) /(P(data | S2) P(S2) + P(data|notS2)P(notS2))\n= 1*0.000007/(1*0.000007 + 0.000005*0.999993)\n=0.58\n58% chance of having died from SIDS!\nSally Clark spent 3 years in jail."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nopen\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "til/open.html",
    "href": "til/open.html",
    "title": "open",
    "section": "",
    "text": "MacOS has a great command open. You can use it to open any file in any folder from the terminal in its default app. For example:\nopen bla.pdf\nwill open a file in Preview.\nSometimes you want another app. Then you can use the -a flag. Like so:\nopen -a /Applications/Typora.app bla.md"
  },
  {
    "objectID": "collections/software/hamilton.html#why-choose-this-tool",
    "href": "collections/software/hamilton.html#why-choose-this-tool",
    "title": "Stitchfix Hamilton",
    "section": "Why choose this tool?",
    "text": "Why choose this tool?\nA scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/awk.html",
    "href": "collections/software/awk.html",
    "title": "Awk",
    "section": "",
    "text": "An old goody! For quick command line analysis of data.\n\nThe following examples were taken from the tldr page for awk:\nPrint the fifth column (a.k.a. field) in a space-separated file:\nawk '{print $5}' filename\nPrint the second column of the lines containing “foo” in a space-separated file:\nawk '/foo/ {print $2}' filename\nPrint the last column of each line in a file, using a comma (instead of space) as a field separator:\nawk -F ',' '{print $NF}' filename\nSum the values in the first column of a file and print the total:\nawk '{s+=$1} END {print s}' filename\nPrint every third line starting from the first line:\nawk 'NR%3==1' filename\nPrint different values based on conditions:\nawk '{if ($1 == \"foo\") print \"Exact match foo\"; else if ($1 ~ \"bar\") print \"Partial match bar\"; else print \"Baz\"}' filename\nPrint all lines where the 10th column value equals the specified value:\nawk '($10 == value)'\nPrint all the lines which the 10th column value is between a min and a max:\nawk '($10 &gt;= min_value && $10 &lt;= max_value)'"
  }
]