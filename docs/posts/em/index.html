<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-01-13">
<meta name="description" content="Introduces the Expectation-Maximization algorithm for maximum likelihood estimation with missing or latent variables. Starts with estimating bivariate normal parameters from incomplete data, builds the theoretical foundation using KL-divergence and the ELBO, and applies EM to Gaussian mixture models for unsupervised clustering.">

<title>The EM Algorithm – rahuldave</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c67a7d068a8370113d9027fc4e8bf30e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-e0d64750a3675fa668af59a9862b8111.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c67a7d068a8370113d9027fc4e8bf30e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d2fef15c612ec386ae0907ffd6f4ccdb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-a009389674a9596cea61ac77c12264b2.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-d2fef15c612ec386ae0907ffd6f4ccdb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Google Fonts: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code) -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Bitter:wght@400;500;600;700&amp;family=Source+Serif+4:opsz,wght@8..60,400;8..60,500;8..60,600&amp;family=IBM+Plex+Mono:wght@400;500;600&amp;display=swap" rel="stylesheet">

<!-- Bootstrap Icons for brand mark and UI elements -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="The EM Algorithm – rahuldave">
<meta property="og:description" content="Introduces the Expectation-Maximization algorithm for maximum likelihood estimation with missing or latent variables. Starts with estimating bivariate normal parameters from incomplete data, builds the theoretical foundation using KL-divergence and the ELBO, and applies EM to Gaussian mixture models for unsupervised clustering.">
<meta property="og:image" content="https://rahuldave.github.io/posts/em/assets/gform1.png">
<meta property="og:site_name" content="rahuldave">
<meta property="og:image:height" content="72">
<meta property="og:image:width" content="340">
<meta name="twitter:title" content="The EM Algorithm – rahuldave">
<meta name="twitter:description" content="Introduces the Expectation-Maximization algorithm for maximum likelihood estimation with missing or latent variables. Starts with estimating bivariate normal parameters from incomplete data, builds the theoretical foundation using KL-divergence and the ELBO, and applies EM to Gaussian mixture models for unsupervised clustering.">
<meta name="twitter:image" content="https://rahuldave.github.io/posts/em/assets/gform1.png">
<meta name="twitter:creator" content="@rahuldave">
<meta name="twitter:image-height" content="72">
<meta name="twitter:image-width" content="340">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">rahuldave</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../til.html"> 
<span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../collections.html"> 
<span class="menu-text">Collections</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-em-algorithm" id="toc-the-em-algorithm" class="nav-link active" data-scroll-target="#the-em-algorithm">The EM algorithm</a>
  <ul class="collapse">
  <li><a href="#a-toy-problem" id="toc-a-toy-problem" class="nav-link" data-scroll-target="#a-toy-problem">A toy problem</a></li>
  <li><a href="#the-em-algorithm-1" id="toc-the-em-algorithm-1" class="nav-link" data-scroll-target="#the-em-algorithm-1">The EM algorithm</a></li>
  <li><a href="#why-does-em-work" id="toc-why-does-em-work" class="nav-link" data-scroll-target="#why-does-em-work">Why does EM work?</a>
  <ul class="collapse">
  <li><a href="#e-step" id="toc-e-step" class="nav-link" data-scroll-target="#e-step">E-step</a></li>
  <li><a href="#now-the-m-step." id="toc-now-the-m-step." class="nav-link" data-scroll-target="#now-the-m-step.">Now the <strong>M-step</strong>.</a></li>
  <li><a href="#the-process" id="toc-the-process" class="nav-link" data-scroll-target="#the-process">The process</a></li>
  </ul></li>
  <li><a href="#the-em-algorithm-with-indices-laid-out" id="toc-the-em-algorithm-with-indices-laid-out" class="nav-link" data-scroll-target="#the-em-algorithm-with-indices-laid-out">The EM algorithm with indices laid out</a></li>
  <li><a href="#the-gaussian-mixture-model-using-em" id="toc-the-gaussian-mixture-model-using-em" class="nav-link" data-scroll-target="#the-gaussian-mixture-model-using-em">The Gaussian Mixture model using EM</a></li>
  <li><a href="#why-is-em-important" id="toc-why-is-em-important" class="nav-link" data-scroll-target="#why-is-em-important">Why is EM important?</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Theme toggle persistence script -->
<script>
(function() {
  // Check for saved theme preference, otherwise use system preference
  const savedTheme = localStorage.getItem('quarto-color-scheme');
  if (savedTheme) {
    document.documentElement.setAttribute('data-bs-theme', savedTheme);
  }

  // Listen for theme changes (Quarto's built-in toggle) and persist
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.attributeName === 'data-bs-theme') {
        const currentTheme = document.documentElement.getAttribute('data-bs-theme');
        if (currentTheme) {
          localStorage.setItem('quarto-color-scheme', currentTheme);
        }
      }
    });
  });

  // Start observing once DOM is ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
      observer.observe(document.documentElement, { attributes: true });
    });
  } else {
    observer.observe(document.documentElement, { attributes: true });
  }
})();
</script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The EM Algorithm</h1>
<p class="subtitle lead">Inferring parameters from incomplete data through iterative expectation and maximization.</p>
  <div class="quarto-categories">
    <div class="quarto-category">statistics</div>
    <div class="quarto-category">models</div>
    <div class="quarto-category">optimization</div>
  </div>
  </div>

<div>
  <div class="description">
    Introduces the Expectation-Maximization algorithm for maximum likelihood estimation with missing or latent variables. Starts with estimating bivariate normal parameters from incomplete data, builds the theoretical foundation using KL-divergence and the ELBO, and applies EM to Gaussian mixture models for unsupervised clustering.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 13, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="the-em-algorithm" class="level1">
<h1>The EM algorithm</h1>
<section id="keywords-maximum-likelihood-mixture-model-elbo-full-data-likelihood-x-likelihood-latent-variables-log-likelihood-training-set-normal-distribution-z-posterior" class="level5">
<h5 class="anchored" data-anchor-id="keywords-maximum-likelihood-mixture-model-elbo-full-data-likelihood-x-likelihood-latent-variables-log-likelihood-training-set-normal-distribution-z-posterior">Keywords: maximum likelihood, mixture model, ELBO, full-data likelihood, x-likelihood, latent variables, log-likelihood, training set, normal distribution, z-posterior</h5>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.cm <span class="im">as</span> cm</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.width'</span>, <span class="dv">500</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">100</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.notebook_repr_html'</span>, <span class="va">True</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">"whitegrid"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>sns.set_context(<span class="st">"poster"</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc3 <span class="im">as</span> pm</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><span class="math display">\[\newcommand{\isum}{\sum_{i}}\]</span> <span class="math display">\[\newcommand{\zsum}{\sum_{k=1}^{K}}\]</span> <span class="math display">\[\newcommand{\zsumi}{\sum_{\{z_i\}}}\]</span></p>
</section>
<section id="a-toy-problem" class="level2">
<h2 class="anchored" data-anchor-id="a-toy-problem">A toy problem</h2>
<p>This example is taken from Efron and Hastie, Computer Age Statistical Inference.</p>
<p>Assume we have data drawn from a bi-variate Normal</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/gform1.png" class="img-fluid figure-img"></p>
<figcaption>Bivariate normal model: the joint distribution of (x1, x2) with means, variances, and correlation parameter rho.</figcaption>
</figure>
</div>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>sig1<span class="op">=</span><span class="dv">1</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>sig2<span class="op">=</span><span class="fl">0.75</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>mu1<span class="op">=</span><span class="fl">1.85</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>mu2<span class="op">=</span><span class="dv">1</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>rho<span class="op">=</span><span class="fl">0.82</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>means<span class="op">=</span>np.array([mu1, mu2])</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> np.array([</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    [sig1<span class="op">**</span><span class="dv">2</span>, sig1<span class="op">*</span>sig2<span class="op">*</span>rho],</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    [sig2<span class="op">*</span>sig1<span class="op">*</span>rho, sig2<span class="op">**</span><span class="dv">2</span>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>means, cov</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>(array([ 1.85,  1.  ]), array([[ 1.    ,  0.615 ],
        [ 0.615 ,  0.5625]]))</code></pre>
</div>
</div>
<p>We plot the samples below as blue circles. Now say we lose the y-values of the last-20 pieces of data. We are left with a missing data or hidden data or latent-variables problem. We plot both datasets below, with the y-values of the lost points set to 0</p>
<div id="cell-7" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>samples<span class="op">=</span>np.random.multivariate_normal(means, cov, size<span class="op">=</span><span class="dv">40</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-8" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>samples_censored<span class="op">=</span>np.copy(samples)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>samples_censored[<span class="dv">20</span>:,<span class="dv">1</span>]<span class="op">=</span><span class="dv">0</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.plot(samples[:,<span class="dv">0</span>], samples[:,<span class="dv">1</span>], <span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.plot(samples_censored[:,<span class="dv">0</span>], samples_censored[:,<span class="dv">1</span>], <span class="st">'s'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We would use MLE if we had all the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/gform2.png" class="img-fluid figure-img"></p>
<figcaption>Maximum likelihood estimators for the bivariate normal parameters: sample means, standard deviations, and correlation.</figcaption>
</figure>
</div>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>mu1 <span class="op">=</span> <span class="kw">lambda</span> s: np.mean(s[:,<span class="dv">0</span>])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>mu2 <span class="op">=</span> <span class="kw">lambda</span> s: np.mean(s[:,<span class="dv">1</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>s1 <span class="op">=</span> <span class="kw">lambda</span> s: np.std(s[:,<span class="dv">0</span>])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>s2 <span class="op">=</span> <span class="kw">lambda</span> s: np.std(s[:,<span class="dv">1</span>])</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>rho <span class="op">=</span> <span class="kw">lambda</span> s: np.mean((s[:,<span class="dv">0</span>] <span class="op">-</span> mu1(s))<span class="op">*</span>(s[:,<span class="dv">1</span>] <span class="op">-</span> mu2(s)))<span class="op">/</span>(s1(s)<span class="op">*</span>s2(s))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>But we dont. So we shall follow an iterative process to find them.</p>
<div id="cell-12" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>mu1s<span class="op">=</span>[]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>mu2s<span class="op">=</span>[]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>s1s<span class="op">=</span>[]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>s2s<span class="op">=</span>[]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>rhos<span class="op">=</span>[]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Bur remember our data are missing in the y-direction. Assume 0 and go. Since we are using the MLE of the <strong>full-data</strong> likelihood, with this assumption we can use the MLE formulae. This is called M-step or maximization step since we used the MLE formulae.</p>
<div id="cell-14" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>mu1s.append(mu1(samples_censored))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>mu2s.append(mu2(samples_censored))</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>s1s.append(s1(samples_censored))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>s2s.append(s2(samples_censored))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>rhos.append(rho(samples_censored))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>mu1s,mu2s,s1s,s2s,rhos</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>([1.7547657491036741],
 [0.44394554402533365],
 [1.2718969990037354],
 [0.77180100755764103],
 [0.52348014627786521])</code></pre>
</div>
</div>
<p>Now having new estimates of our parameters due to our (fake) y-data, lets go in the other direction. Using these parameters let us calculate new y-values.</p>
<p>One way we might do this is to replace the old-missing-y values with the means of these fixing the parameters of the multi-variate normal and the non-missing data. In other words:</p>
<p><span class="math display">\[E_{p(z \vert \theta, x)}[z]\]</span></p>
<p>where we have used the notation <span class="math inline">\(z\)</span> to refer to the missing values.</p>
<p>This <strong>posterior</strong> distribution (in the sense of bayes theorem, not bayesian analysis) for the multi-variate gaussian is a gaussian..see <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions">wikipedia</a> for the formulae</p>
<p><span class="math display">\[\bar{y}(t+1) - \hat{\mu_2}(t) = \hat{\rho}(t)\frac{\hat{\sigma_2}(t)}{\hat{\sigma_1}(t)} \left( \bar{x} - \hat{\mu_1}(t) \right)\]</span></p>
<div id="cell-16" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ynew(x, mu1, mu2, s1, s2, rho):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu2 <span class="op">+</span> rho<span class="op">*</span>(s2<span class="op">/</span>s1)<span class="op">*</span>(x <span class="op">-</span> mu1)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-17" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>newys<span class="op">=</span>ynew(samples_censored[<span class="dv">20</span>:,<span class="dv">0</span>], mu1s[<span class="dv">0</span>], mu2s[<span class="dv">0</span>], s1s[<span class="dv">0</span>], s2s[<span class="dv">0</span>], rhos[<span class="dv">0</span>])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>newys</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>array([-0.25149771,  0.71551635,  0.81675223,  0.85921124,  0.08168773,
       -0.14712658,  0.7300238 ,  0.42993898,  0.51504132,  0.28024916,
       -0.06713977,  0.44793596,  0.54224183,  1.30632573,  0.23563191,
        0.5396655 ,  0.25878933,  0.36945243,  0.99094696,  0.53980901])</code></pre>
</div>
</div>
<p>This is called the E-step as it computes an expectation for us. Lets run this iteratively and see if we converge.</p>
<div id="cell-19" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">20</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    samples_censored[<span class="dv">20</span>:,<span class="dv">1</span>] <span class="op">=</span> newys</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#M-step</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    mu1s.append(mu1(samples_censored))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    mu2s.append(mu2(samples_censored))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    s1s.append(s1(samples_censored))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    s2s.append(s2(samples_censored))</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    rhos.append(rho(samples_censored))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#E-step</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    newys<span class="op">=</span>ynew(samples_censored[<span class="dv">20</span>:,<span class="dv">0</span>], mu1s[step], mu2s[step], s1s[step], s2s[step], rhos[step])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>df<span class="op">=</span>pd.DataFrame.from_dict(<span class="bu">dict</span>(mu1<span class="op">=</span>mu1s, mu2<span class="op">=</span>mu2s, s1<span class="op">=</span>s1s, s2<span class="op">=</span>s2s, rho<span class="op">=</span>rhos))</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>df</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mu1</th>
<th data-quarto-table-cell-role="th">mu2</th>
<th data-quarto-table-cell-role="th">rho</th>
<th data-quarto-table-cell-role="th">s1</th>
<th data-quarto-table-cell-role="th">s2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>1.754766</td>
<td>0.443946</td>
<td>0.523480</td>
<td>1.271897</td>
<td>0.771801</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>1.754766</td>
<td>0.673782</td>
<td>0.822228</td>
<td>1.271897</td>
<td>0.718523</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>1.754766</td>
<td>0.792335</td>
<td>0.904408</td>
<td>1.271897</td>
<td>0.749225</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>1.754766</td>
<td>0.853302</td>
<td>0.925737</td>
<td>1.271897</td>
<td>0.775802</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>1.754766</td>
<td>0.884575</td>
<td>0.932120</td>
<td>1.271897</td>
<td>0.790958</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">5</th>
<td>1.754766</td>
<td>0.900583</td>
<td>0.934331</td>
<td>1.271897</td>
<td>0.798740</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">6</th>
<td>1.754766</td>
<td>0.908762</td>
<td>0.935193</td>
<td>1.271897</td>
<td>0.802589</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">7</th>
<td>1.754766</td>
<td>0.912935</td>
<td>0.935558</td>
<td>1.271897</td>
<td>0.804467</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">8</th>
<td>1.754766</td>
<td>0.915062</td>
<td>0.935723</td>
<td>1.271897</td>
<td>0.805378</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">9</th>
<td>1.754766</td>
<td>0.916144</td>
<td>0.935799</td>
<td>1.271897</td>
<td>0.805821</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">10</th>
<td>1.754766</td>
<td>0.916695</td>
<td>0.935835</td>
<td>1.271897</td>
<td>0.806036</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">11</th>
<td>1.754766</td>
<td>0.916974</td>
<td>0.935853</td>
<td>1.271897</td>
<td>0.806141</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">12</th>
<td>1.754766</td>
<td>0.917116</td>
<td>0.935861</td>
<td>1.271897</td>
<td>0.806192</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">13</th>
<td>1.754766</td>
<td>0.917189</td>
<td>0.935866</td>
<td>1.271897</td>
<td>0.806218</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">14</th>
<td>1.754766</td>
<td>0.917225</td>
<td>0.935868</td>
<td>1.271897</td>
<td>0.806230</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">15</th>
<td>1.754766</td>
<td>0.917244</td>
<td>0.935869</td>
<td>1.271897</td>
<td>0.806236</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">16</th>
<td>1.754766</td>
<td>0.917253</td>
<td>0.935869</td>
<td>1.271897</td>
<td>0.806239</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">17</th>
<td>1.754766</td>
<td>0.917258</td>
<td>0.935869</td>
<td>1.271897</td>
<td>0.806241</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">18</th>
<td>1.754766</td>
<td>0.917260</td>
<td>0.935870</td>
<td>1.271897</td>
<td>0.806242</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">19</th>
<td>1.754766</td>
<td>0.917261</td>
<td>0.935870</td>
<td>1.271897</td>
<td>0.806242</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Voila. We converge to stable values of our parameters.</p>
<p>But they may not be the ones we seeded the samples with. The Em algorithm is only good upto finding local minima, and a finite sample size also means that the minimum found can be slightly different.</p>
</section>
<section id="the-em-algorithm-1" class="level2">
<h2 class="anchored" data-anchor-id="the-em-algorithm-1">The EM algorithm</h2>
<p>** Expectation-maximization (EM)** method is an iterative method for maximizing difficult likelihood (or posterior) problems. It was first introduced by Dempster, Laird, and Rubin (1977).</p>
<p>EM recognizes that if the data were fully observed, then ML/ MAP estimates would be easy to compute. It thus alternates between inferring the missing values given the parameters (E step), and then optimizing the parameters given the “filled in” data (M step). The idea is to find a lower-bound on the log-likelihood <span class="math inline">\(\ell\)</span> (E-step) and the optimize the lower-bound (M-step).</p>
<p><strong>We want to use EM wherever we dont know how to optimize simply the x-data likelihood.</strong></p>
<p>The EM algorithm is naturally useful for missing data, truncated data, censored data and grouped data problems. But it is also useful at places where you have clusters like mixture models, and you are working in an unsupervised or semi-supervised mode. Basically, any place you can set up a latent variable model or a data augmentation procedure, EM is useful.</p>
<p>There are applications in astronomical image analysis, genetics, engineering, etc</p>
<p>The basic idea is:</p>
<p><strong>calculate MLE estimates for the incomplete data problem by using the complete-data likelihood. To create complete data, augment the observed data with manufactured data</strong></p>
<p>Sorta like, just assign points to clusters to start with and iterate.</p>
<p>Then, at each iteration, replace the augmented data by its conditional expectation given current observed data and parameter estimates. (E-step) Maximize the full-data likelihood to do the M-step.</p>
</section>
<section id="why-does-em-work" class="level2">
<h2 class="anchored" data-anchor-id="why-does-em-work">Why does EM work?</h2>
<p>To understand why EM works we will start with our old friend the KL-divergence. All images in this section are stolen from Bishop</p>
<p>We cast the problem as a MLE problem, but in general any problem in which there is a <span class="math inline">\(\theta, x, z\)</span> triad will work.</p>
<p>We start with:</p>
<p><span class="math display">\[p(x) = \sum_z p(x, z)\]</span></p>
<p>Specifically, lets cast these as likelihoods</p>
<p><span class="math display">\[p(x \vert \theta) = \sum_z p(x, z \vert \theta)\]</span></p>
<p>where the <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> range over the multiple points in your data set.</p>
<p>Then <span class="math display">\[\ell( x \vert \theta) = log\, p(x \vert \theta) = log\, \sum_z p(x, z \vert \theta)\]</span></p>
<p>Assume <span class="math inline">\(z\)</span> has some normalized distribution:</p>
<p><span class="math display">\[z \sim q(z)\]</span>.</p>
<p>We wish to compute conditional expectations of the type:</p>
<p><span class="math display">\[E_{p(z \vert x, \theta)}[z]\]</span></p>
<p>but we dont know this “posterior”. Lets however say that we somehow know <span class="math inline">\(q\)</span> and thus can define our loss function as the KL-divergence between <span class="math inline">\(q\)</span> and the posterior <span class="math inline">\(p(z \vert x, \theta)\)</span>, henceforth abbreviated <span class="math inline">\(p\)</span>.</p>
<p>Then (notice new notation to be consistent with Bishop):</p>
<p><span class="math display">\[ \mathrm{KL}(q \vert\vert p) = D_{KL}(q, p) = E_q[log \frac{q}{p}] = -E_q[log \frac{p}{q}]\]</span></p>
<p><span class="math display">\[D_{KL}(q, p) = -E_q[log \frac{p(x, z \vert \theta)}{q\,p(x \vert \theta)}]\]</span></p>
<p>where we have used Bayes Theorem.</p>
<p>Now at first blush this does not seem to have bought us anything as we dont know the full data likelihood and we dont know how to optimize the x-data likelihood. But lets persevere:</p>
<p><span class="math display">\[D_{KL}(q, p) = - \left( E_q[log \frac{p(x, z \vert \theta)}{q}] - E_q[log\,p(x \vert \theta)] \right)\]</span></p>
<p>The second term does not depend on <span class="math inline">\(q\)</span>, and <span class="math inline">\(q\)</span> is normalized so the expectation just gives 1 and we can rewrite the equation in terns of the x-data log likelihoood thus:</p>
<p><span class="math display">\[log\,p(x \vert \theta) = E_q[log \frac{p(x, z \vert \theta)}{q}] + D_{KL}(q, p)\]</span></p>
<p>If we define the ELBO or Evidence Lower bound as:</p>
<p><span class="math display">\[\mathcal{L}(q, \theta) = E_q[log \frac{p(x, z \vert \theta)}{q}]\]</span></p>
<p>, then</p>
<p><span class="math inline">\(log\,p(x \vert \theta)\)</span> = ELBO + KL-divergence</p>
<p>a situation made clear in the diagram below where the ELBO goues upto the blue line and the divergence from the blue to the red. Be careful with the p’s, the one on the right is for the x-data and the one on the left for the latent-variable posterior.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/klsplitup.png" class="img-fluid figure-img"></p>
<figcaption>Decomposition of the log marginal likelihood into the ELBO and KL divergence between the variational distribution q and the true posterior p.&nbsp;From Bishop.</figcaption>
</figure>
</div>
<p>Now recall that the Kullback Liebler divergence is 0 only if the distributions as a function of <span class="math inline">\(z\)</span> are the same at every poiny; it is otherwise <strong>ALWAYS</strong> greater than 0. This tells us that the quantity <span class="math inline">\(\mathcal{L}(q, \theta)\)</span>, is <strong>ALWAYS</strong> smaller than or equal to the log-likelihood of <span class="math inline">\(p(x  \vert  \theta)\)</span>, as illustrated above. In other words, <span class="math inline">\(\mathcal{L}(q, \theta)\)</span> is a lower bound on the log-likelihood. This is why its called the ELBO, with the “evidence” aspect of it coming from Variational calculus (next lecture).</p>
<p>The ELBO itself is the expected value of the full-data log-likelihood minus the entropy of <span class="math inline">\(q\)</span>:</p>
<p><span class="math display">\[\mathcal{L}(q, \theta) = E_q[log \frac{p(x, z \vert \theta)}{q}] = E_q[log p(x, z \vert \theta)] - E_q[log\, q]\]</span></p>
<section id="e-step" class="level3">
<h3 class="anchored" data-anchor-id="e-step">E-step</h3>
<p>These observations set up the EM algorithm for us. If we choose, in the <strong>E-step</strong>, at some (possibly initial) value of the parameters <span class="math inline">\(\theta_{old}\)</span>,</p>
<p><span class="math display">\[q(z) = p(z  \vert  x, \theta_{old}),\]</span></p>
<p>we then set the Kullback Liebler divergence to 0, and thus <span class="math inline">\(\mathcal{L}(q, \theta)\)</span> to the log-likelihood at <span class="math inline">\(\theta_{old}\)</span>, and maximizing the lower bound.</p>
<p>Using this missing data posterior, conditioned on observed data, and <span class="math inline">\(\theta_{old}\)</span>, we compute the expectation of the missing data with respect to the posterior and use it later.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/klsplitestep.png" class="img-fluid figure-img"></p>
<figcaption>The E-step of EM: setting q equal to the posterior makes KL(q||p)=0, raising the ELBO to match the log likelihood. From Bishop.</figcaption>
</figure>
</div>
</section>
<section id="now-the-m-step." class="level3">
<h3 class="anchored" data-anchor-id="now-the-m-step.">Now the <strong>M-step</strong>.</h3>
<p>Since after the E-step, the lower bound touches the log-likelihood, any maximization of this ELBO from its current value with respect to <span class="math inline">\(\theta\)</span> will also “push up” on the likelihood itself. Thus M step guaranteedly modifies the parameters <span class="math inline">\(\theta\)</span> to increase (or keep same) the likelihood of the observed data.</p>
<p>Thus we hold now the distribution <span class="math inline">\(q(z)\)</span> fixed at the hidden variable posterior calculated at <span class="math inline">\(\theta_{old}\)</span>, and maximize <span class="math inline">\(\mathcal{L}(q, \theta)\)</span> with respect to <span class="math inline">\(\theta\)</span> to obtain new parameter values <span class="math inline">\(\theta_{new}\)</span>. This is a regular maximization.</p>
<p>The distribution <span class="math inline">\(q\)</span>, calculated as it is at <span class="math inline">\(\theta_{old}\)</span> will not in general equal the new posterior distribution <span class="math inline">\(p(z \vert x,\theta_{new})\)</span>, and hence there will be a nonzero KL divergence. Thus the increase in the log-likelihood will be greater than the increase in the lower bound <span class="math inline">\(\mathcal{L}\)</span>, as illustrated below.</p>
<p>The M in “M-step” and “EM” stands for “maximization”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/klsplitmstep.png" class="img-fluid figure-img"></p>
<figcaption>The M-step of EM: holding q fixed and maximizing the ELBO with respect to theta increases the log likelihood by at least as much. From Bishop.</figcaption>
</figure>
</div>
</section>
<section id="the-process" class="level3">
<h3 class="anchored" data-anchor-id="the-process">The process</h3>
<p>Note that since <span class="math inline">\(\mathcal{L}\)</span> is maximized with respect to <span class="math inline">\(\theta\)</span>, one can equivalently maximize the expectation of the full-data log likelihood <span class="math inline">\(\mathrm{E_q[\ell( x,z  \vert  \theta)]}\)</span> in the M-step since the difference is purely a function of <span class="math inline">\(q\)</span>. Furthermore, if the joint distribution <span class="math inline">\(p(x, z \vert  \theta)\)</span> is a member of the exponential family, the log-likelihood will have a particularly simple form and will lead to a much simpler maximization than that of the incomple-data log-likelihood <span class="math inline">\(p(x \vert \theta)\)</span>.</p>
<p>We now set <span class="math inline">\(\theta_{old} = \theta_{new}\)</span> and repeat the process. This <strong>EM algorithm</strong> is presented and illustrated below:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/emupdate.png" class="img-fluid figure-img"></p>
<figcaption>EM convergence: the red curve shows log p(X|theta), the green curve shows the ELBO, and each EM iteration moves from theta_old to theta_new, monotonically increasing the likelihood. From Bishop.</figcaption>
</figure>
</div>
<ol type="1">
<li>We start with the log-likelihood <span class="math inline">\(p(x  \vert  \theta)\)</span>(red curve) and the initial guess <span class="math inline">\(\theta_{old}\)</span> of the parameter values</li>
<li>Until convergence (the <span class="math inline">\(\theta\)</span> values dont change too much):
<ol type="1">
<li>E-step: Evaluate the hidden variable posterior <span class="math inline">\(q(z, \theta_{old}) = p(z  \vert  x, \theta_{old})\)</span> which gives rise to a lower bound function of <span class="math inline">\(\theta\)</span>: <span class="math inline">\(\mathcal{L}(q(z, \theta_{old}), \theta)\)</span>(blue curve) whose value equals the value of <span class="math inline">\(p(x  \vert  \theta)\)</span> at <span class="math inline">\(\theta_{old}\)</span>.</li>
<li>M-step: maximize the lower bound function with respect to <span class="math inline">\(\theta\)</span> to get <span class="math inline">\(\theta_{new}\)</span>.</li>
<li>Set <span class="math inline">\(\theta_{old} = \theta_{new}\)</span></li>
</ol></li>
</ol>
<p>One iteration more is illustrated above, where the subsequent E-step constructs a new lower-bound function that is tangential to the log-likelihood at <span class="math inline">\(\theta_{new}\)</span>, and whose value at <span class="math inline">\(\theta_{new}\)</span> is higher than the lower bound at <span class="math inline">\(\theta_{old}\)</span> from the previous step.</p>
<p>Thus</p>
<p><span class="math display">\[\ell(\theta_{t+1}) \ge \mathcal{L}(q(z,\theta_t), \theta_{t+1}) \ge \mathcal{L}(q(z,\theta_t), \theta_{t}) = \ell(\theta_t)\]</span></p>
<p>The first equality follows since <span class="math inline">\(\mathcal{L}\)</span> is a lower bound on <span class="math inline">\(\ell\)</span>, the second from the M-step’s maximization of <span class="math inline">\(\mathcal{L}\)</span>, and the last from the vanishing of the KL-divergence after the E-step. As a consequence, you <strong>must</strong> observe monotonic increase of the observed-data log likelihood <span class="math inline">\(\ell\)</span> across iterations. <strong>This is a powerful debugging tool for your code</strong>.</p>
<p>Note that as shown above, since each EM iteration can only improve the likelihood, you are guaranteeing convergence to a local maximum. Because it <strong>IS</strong> local , you must try some different initial values of <span class="math inline">\(\theta_{old}\)</span> and take the one that gives you the largest <span class="math inline">\(\ell\)</span>.</p>
</section>
</section>
<section id="the-em-algorithm-with-indices-laid-out" class="level2">
<h2 class="anchored" data-anchor-id="the-em-algorithm-with-indices-laid-out">The EM algorithm with indices laid out</h2>
<p>I often find it confusing as to what the indices are actually doing if I dont write them out explicitly. So lets visit the EM derivation once more, focussing on mixtures, and explicitly writing out indices. The derivation does not need mixtures, but I find it helpful to imagine that we are fitting such a model.</p>
<p>Suppose we have an estimation problem in which we have data consising of <span class="math inline">\(m\)</span> independent examples <span class="math inline">\(\{x_1,\ldots,x_m\}\)</span> . The goal is to fit the parameters of the model, where the log-likelihood is given by <span class="math display">\[\begin{eqnarray}
\ell(x  \vert  \theta)&amp;=&amp; \log \prod_{i=1}^{m} p(x_i \vert  \theta) =   \sum_{i=1}^{m} \log \,p(x_i \vert  \theta)  \\
   &amp;=&amp; \sum_{i=1}^{m} \log \zsumi \,p(x_i,z \vert  \theta)  \\
\end{eqnarray}\]</span></p>
<p>where the <span class="math inline">\(z\)</span> are the latent random variables. If <span class="math inline">\(z\)</span> were observed then the maximum likelihood estimation would be easy.</p>
<p>Indeed then, let us start with the full data log-likelihood,</p>
<p><span class="math display">\[\ell(x, z  \vert  \theta) = \sum_{i=1}^{m}  \log \,p(x_i, z_i  \vert  \theta),\]</span></p>
<p>which is the log-likelihood we’d calculate if we knew all the <span class="math inline">\(z_i\)</span>. But we do not know thse, so lets assume the <span class="math inline">\(\{z_i\}\)</span> have some normalized distribution <span class="math inline">\(q(z)\)</span>, and calculate the expected value of the full data log likelihood with respect to this distribution:</p>
<p><span class="math display">\[\begin{eqnarray}
\mathrm{E_q[\ell( x,z  \vert  \theta)]}  &amp;=&amp; \sum_i \zsumi q_{i}(z_i) \log \,p(x_i, z_i  \vert  \theta)\\
    &amp;=&amp; \sum_i \zsumi q_{i}(z_i) \log \,\frac{p(x_i, z_i  \vert  \theta)}{q_{i}(z_i)} +  \sum_i \zsumi q_{i}(z_i) \log \,q_{i}(z_i)
\end{eqnarray}\]</span></p>
<p>The second term only involves <span class="math inline">\(q\)</span> and is independent of <span class="math inline">\(\theta\)</span>. Looking only at the first term inside the i-summation:</p>
<p><span class="math display">\[\begin{eqnarray}
\mathcal{L}(i, q, \theta) &amp;=&amp;  \zsumi q_{i}(z_i) \log \,\frac{p(x_i, z_i  \vert  \theta)}{q_{i}(z_i)} \\
&amp;=&amp; \zsumi  q_i(z_i) \left( \log \frac{p(z_i \vert  x_i,  \theta)}{ q_i(z_i)} + \log p(x_i  \vert  \theta)\right)
\end{eqnarray}\]</span></p>
<p>we can see that, since <span class="math inline">\(\zsumi q_i(z_i) = 1\)</span>:</p>
<p><span class="math display">\[\begin{eqnarray}
\mathcal{L}(i, q, \theta) &amp;=&amp; \zsumi  q_i(z_i) \left( \log \frac{p(z_i \vert  x_i,  \theta)}{ q_i(z_i)} + \log p(x_i  \vert  \theta)\right)\\
    &amp;=&amp; -\mathrm{KL}\left(q_i  \vert  \vert  p_i \right) + \log p(x_i  \vert  \theta)\\
\end{eqnarray}\]</span></p>
<p>where <span class="math inline">\(\mathrm{KL}\)</span> is the Kullback-Leibler divergence between <span class="math inline">\(q(x)\)</span> and the hidden variable posterior distribution <span class="math inline">\(p(z \vert x,\theta)\)</span> at the poin <span class="math inline">\(i\)</span>.</p>
<p>Since the sum over the data-points of the second term is just the log-likelihood we desire, it can then can be written as:</p>
<p><span class="math display">\[\begin{eqnarray}
\ell(x  \vert  \theta) &amp;=&amp; \sum_i \left(\mathcal{L}(i, q, \theta) +\mathrm{KL}\left(q_i   \vert  \vert  p_i \right)\right)\\
&amp;=&amp; \mathcal{L}(q, \theta) + \mathrm{KL}\left(q  \vert  \vert  p \right)
\end{eqnarray}\]</span></p>
<p>where we are defining:</p>
<p><span class="math display">\[\mathrm{KL}(q  \vert  \vert  p) = \sum_i \mathrm{KL}\left(q_i   \vert  \vert  p_i \right)\]</span></p>
<p>as the sum of the KL-divergence at each data point, and <span class="math inline">\(\mathcal{L}(q, \theta)\)</span> as the sum of <span class="math inline">\(\mathcal{L}\)</span> at each data point.</p>
</section>
<section id="the-gaussian-mixture-model-using-em" class="level2">
<h2 class="anchored" data-anchor-id="the-gaussian-mixture-model-using-em">The Gaussian Mixture model using EM</h2>
<p>We dont know how to solve for the MLE of the unsupervised problem. The EM algorithm comes to the rescue. As described above here is the algorithm:</p>
<ul>
<li>Repeat until convergence</li>
<li>E-step: For each <span class="math inline">\(i,j\)</span> calculate</li>
</ul>
<p><span class="math display">\[ w_{i,j} = q_i(z_i=j)=p(z_i=j \vert  x_i, \lambda, \mu, \Sigma) \]</span></p>
<ul>
<li>M-step: We need to maximize, with respect to our parameters the</li>
</ul>
<p><span class="math display">\[
\begin{eqnarray}
\mathcal{L} &amp;=&amp; \sum_i \sum_{z_i} q_i(z_i) \log \frac{p(x_i,z_i  \vert \lambda, \mu, \Sigma)}{q_i(z_i)} \nonumber \\
\mathcal{L} &amp;=&amp; \sum_i \sum_{j=i}^{k}  q_i(z_i=j) \log \frac{p(x_i \vert z_i=j , \mu, \Sigma) p(z_i=j \vert \lambda)}{q_i(z_i=j)} \\
\mathcal{L} &amp; =&amp;  \sum_{i=1}^{m} \sum_{j=i}^{k} w_{i,j}  \log \left[   \frac{ \frac{1}{ (2\pi)^{n/2} \vert \Sigma_j \vert ^{1/2}} \exp \left(    -\frac{1}{2}(x_i-\mu_j)^T \Sigma_j^{-1} (x_i-\mu_j) \right)  \, \lambda_j   }{w_{i,j}}\right]
\end{eqnarray}
\]</span></p>
<p>Taking the derivatives yields the following updating formulas:</p>
<p><span class="math display">\[
\begin{eqnarray}
\lambda_j &amp;=&amp; \frac{1}{m} \sum_{i=1}^m w_{i,j} \nonumber \\
\mu_j&amp;=&amp; \frac{ \sum_{i=1}^m  w_{i,j} \, x_i}{ \sum_{i=1}^m  w_{i,j}} \nonumber \\
\Sigma_j &amp;=&amp; \frac{ \sum_{i=1}^m  w_{i,j} \, (x_i-\mu_j)(x_i-\mu_j)^T}{ \sum_{i=1}^m  w_{i,j}}
\end{eqnarray}
\]</span></p>
<p>To calculate the E-step we basically calculating the posterior of the <span class="math inline">\(z\)</span>’s given the <span class="math inline">\(x\)</span>’s and the current estimate of our parameters. We can use Bayes rule</p>
<p><span class="math display">\[ w_{i,j}= p(z_i=j \vert  x_i, \lambda, \mu, \Sigma) = \frac{p( x_i \vert  z_i=j,  \mu, \Sigma)\, p(z_i=j \vert \lambda)}{\sum_{l=1}^k p(x_i  \vert  z_i=l,  \mu, \Sigma) \, p(z_i=l \vert \lambda)} \]</span></p>
<p>Where <span class="math inline">\(p(x_i  \vert  z_i =j,  \mu, \Sigma)\)</span> is the density of the Gaussian with mean <span class="math inline">\(\mu_j\)</span> and covariance <span class="math inline">\(\Sigma_j\)</span> at <span class="math inline">\(x_i\)</span> and <span class="math inline">\(p(z_i=j \vert  \lambda)\)</span> is simply <span class="math inline">\(\lambda_j\)</span>. If we to compare these formulas in the M-step with the ones we found in GDA we can see that are very similar except that instead of using <span class="math inline">\(\delta\)</span> functions we use the <span class="math inline">\(w\)</span>’s. Thus the EM algorithm corresponds here to a weighted maximum likelihood and the weights are interpreted as the ‘probability’ of coming from that Gaussian instead of the deterministic <span class="math inline">\(\delta\)</span> functions. Thus we have achived a <strong>soft clustering</strong> (as opposed to k-means in the unsupervised case and classification in the supervised case).</p>
<div id="cell-32" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#In 1-D</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter values</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="op">=</span> [<span class="fl">0.6</span>, <span class="fl">0.6</span>]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>lambda_true <span class="op">=</span> <span class="fl">.4</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate from each distribution according to mixing proportion psi</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, lambda_true, n)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([np.random.normal(mu_true[i], sigma_true[i]) <span class="cf">for</span> i <span class="kw">in</span> z])</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>plt.hist(x, bins<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-33" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#from Bios366 lecture notes</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats.distributions <span class="im">import</span> norm</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Estep(x, mu, sigma, lam):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> lam <span class="op">*</span> norm.pdf(x, mu[<span class="dv">0</span>], sigma[<span class="dv">0</span>])</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> (<span class="fl">1.</span> <span class="op">-</span> lam) <span class="op">*</span> norm.pdf(x, mu[<span class="dv">1</span>], sigma[<span class="dv">1</span>])</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> b <span class="op">/</span> (a <span class="op">+</span> b)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-34" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Mstep(x, w):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    lam <span class="op">=</span> np.mean(<span class="fl">1.</span><span class="op">-</span>w) </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> [np.<span class="bu">sum</span>((<span class="dv">1</span><span class="op">-</span>w) <span class="op">*</span> x)<span class="op">/</span>np.<span class="bu">sum</span>(<span class="dv">1</span><span class="op">-</span>w), np.<span class="bu">sum</span>(w <span class="op">*</span> x)<span class="op">/</span>np.<span class="bu">sum</span>(w)]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> [np.sqrt(np.<span class="bu">sum</span>((<span class="dv">1</span><span class="op">-</span>w) <span class="op">*</span> (x <span class="op">-</span> mu[<span class="dv">0</span>])<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>np.<span class="bu">sum</span>(<span class="dv">1</span><span class="op">-</span>w)), </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>             np.sqrt(np.<span class="bu">sum</span>(w <span class="op">*</span> (x <span class="op">-</span> mu[<span class="dv">1</span>])<span class="op">**</span><span class="dv">2</span>)<span class="op">/</span>np.<span class="bu">sum</span>(w))]</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu, sigma, lam</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-35" class="cell" data-execution_count="52">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lambda_true, mu_true, sigma_true)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize values</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.random.normal(<span class="dv">4</span>, <span class="dv">10</span>, size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">5</span>, size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>lam <span class="op">=</span> np.random.random()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initials, mu:"</span>, mu)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initials, sigma:"</span>, sigma)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Initials, lam:"</span>, lam)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Stopping criterion</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>crit <span class="op">=</span> <span class="fl">1e-15</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Convergence flag</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>converged <span class="op">=</span> <span class="va">False</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Loop until converged</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>iterations<span class="op">=</span><span class="dv">1</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="kw">not</span> converged:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># E-step</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.isnan(mu[<span class="dv">0</span>]) <span class="kw">or</span> np.isnan(mu[<span class="dv">1</span>]) <span class="kw">or</span> np.isnan(sigma[<span class="dv">0</span>]) <span class="kw">or</span> np.isnan(sigma[<span class="dv">1</span>]):</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Singularity!"</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> Estep(x, mu, sigma, lam)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># M-step</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    mu_new, sigma_new, lam_new <span class="op">=</span> Mstep(x, w)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check convergence</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    converged <span class="op">=</span> ((np.<span class="bu">abs</span>(lam_new <span class="op">-</span> lam) <span class="op">&lt;</span> crit) </span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>                 <span class="op">&amp;</span> np.<span class="bu">all</span>(np.<span class="bu">abs</span>((np.array(mu_new) <span class="op">-</span> np.array(mu)) <span class="op">&lt;</span> crit))</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>                 <span class="op">&amp;</span> np.<span class="bu">all</span>(np.<span class="bu">abs</span>((np.array(sigma_new) <span class="op">-</span> np.array(sigma)) <span class="op">&lt;</span> crit)))</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    mu, sigma, lam <span class="op">=</span> mu_new, sigma_new, lam_new</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    iterations <span class="op">+=</span><span class="dv">1</span>           </span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Iterations"</span>, iterations)</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'A: N(</span><span class="sc">{0:.4f}</span><span class="st">, </span><span class="sc">{1:.4f}</span><span class="st">)</span><span class="ch">\n</span><span class="st">B: N(</span><span class="sc">{2:.4f}</span><span class="st">, </span><span class="sc">{3:.4f}</span><span class="st">)</span><span class="ch">\n</span><span class="st">lam: </span><span class="sc">{4:.4f}</span><span class="st">'</span>.<span class="bu">format</span>(</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>                        mu_new[<span class="dv">0</span>], sigma_new[<span class="dv">0</span>], mu_new[<span class="dv">1</span>], sigma_new[<span class="dv">1</span>], lam_new))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.4 [2, 5] [0.6, 0.6]
Initials, mu: [  0.72500527 -20.77035111]
Initials, sigma: [ 4.59386658  3.6262629 ]
Initials, lam: 0.6261111131564271
Iterations 95
A: N(5.0083, 0.6288)
B: N(2.0261, 0.5936)
lam: 0.4116</code></pre>
</div>
</div>
</section>
<section id="why-is-em-important" class="level2">
<h2 class="anchored" data-anchor-id="why-is-em-important">Why is EM important?</h2>
<p>We have motivated the EM algorithm using mixture models and missing data, but that is not its only place of use.</p>
<p>Since MLE’s can overfit, we often prefer to use MAP estimation. EM is a perfectly reasonable method for MAP estimation in mixture models; you just need to multiply in the prior.</p>
<p>Basically the EM algorithm has a similar setup to the data augmentation problem and can be used in any problem which has a similar structure. Suppose for example you have two parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\gamma\)</span> in a posterior estimation, with daya <span class="math inline">\(y\)</span>. Say that we’d like to estimate the posterior <span class="math inline">\(p(\phi  \vert  y)\)</span>. It may be relatively hard to estimate this, but suppose we can work with <span class="math inline">\(p(\phi  \vert  \gamma, y)\)</span> and <span class="math inline">\(p(\gamma  \vert  \phi, y)\)</span>. Then you can use the structure of the EM algorithm to estimate the marginal posterior of any one parameter. Start with:</p>
<p><span class="math display">\[log p(\phi  \vert  y) = log p(\gamma, \phi  \vert  y) - log p(\gamma  \vert  \phi, y)\]</span></p>
<p>Notice the similarity of this to the above expressions with <span class="math inline">\(\phi\)</span> as <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span> as <span class="math inline">\(\theta\)</span>, and <span class="math inline">\(\gamma\)</span> as <span class="math inline">\(z\)</span>. Thus the same derivations apply toany problem with this structure.</p>
<p>This structure can also be used in type-2 likelihood or emprical bayes estimation.</p>


</section>
</section>

</main> <!-- /main -->
<!-- Inject yin-yang brand icon into navbar -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Find the navbar brand
  const navbarBrand = document.querySelector('.navbar-brand');
  if (navbarBrand) {
    // Create the brand mark element
    const brandMark = document.createElement('span');
    brandMark.className = 'brand-mark';
    brandMark.innerHTML = '<i class="bi bi-yin-yang"></i>';

    // Insert at the beginning of navbar brand
    navbarBrand.insertBefore(brandMark, navbarBrand.firstChild);
  }
});
</script>
<div id="discuss-links" style="margin-top: 2.5rem; padding-top: 1.5rem; margin-bottom: 4rem; border-top: 1px solid var(--color-border, #ddd);">
  <p style="font-family: var(--font-mono, monospace); font-size: 0.8rem; text-transform: uppercase; letter-spacing: 0.05em; color: var(--color-text-muted, #666); margin-bottom: 0.75rem;">Discuss this post</p>
  <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
    <a id="discuss-twitter" href="#" target="_blank" rel="noopener" style="font-size: 0.9rem;">X / Twitter</a>
    <a id="discuss-bluesky" href="#" target="_blank" rel="noopener" style="font-size: 0.9rem;">Bluesky</a>
    <a id="discuss-linkedin" href="#" target="_blank" rel="noopener" style="font-size: 0.9rem;">LinkedIn</a>
  </div>
</div>
<script>
(function() {
  var p = window.location.pathname;
  var isContent = p.startsWith("/posts/") || p.startsWith("/til/") || p.startsWith("/collections/");
  var isListing = (p === "/posts/" || p === "/posts" || p === "/til/" || p === "/til" || p === "/collections/" || p === "/collections");
  if (!isContent || isListing) {
    document.getElementById("discuss-links").style.display = "none";
    return;
  }

  var url = encodeURIComponent(window.location.href);
  var title = encodeURIComponent(document.title);

  document.getElementById("discuss-twitter").href =
    "https://twitter.com/intent/tweet?text=" + title + "&url=" + url + "&via=rahuldave";
  document.getElementById("discuss-bluesky").href =
    "https://bsky.app/intent/compose?text=" + title + " " + encodeURIComponent(window.location.href);
  document.getElementById("discuss-linkedin").href =
    "https://www.linkedin.com/sharing/share-offsite/?url=" + url;
})();
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/rahuldave\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>