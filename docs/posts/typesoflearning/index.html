<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-26">
<meta name="description" content="Covers the three fundamental types of machine learning through the lens of Gaussian Mixture Models. Demonstrates how GDA works in supervised settings, contrasts with unsupervised clustering where labels are hidden, and introduces semi-supervised learning combining labeled and unlabeled data.">

<title>Mixture Models and Types of Learning – rahuldave</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c67a7d068a8370113d9027fc4e8bf30e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-e0d64750a3675fa668af59a9862b8111.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c67a7d068a8370113d9027fc4e8bf30e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d2fef15c612ec386ae0907ffd6f4ccdb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-a009389674a9596cea61ac77c12264b2.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-d2fef15c612ec386ae0907ffd6f4ccdb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<!-- Google Fonts: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code) -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Bitter:wght@400;500;600;700&amp;family=Source+Serif+4:opsz,wght@8..60,400;8..60,500;8..60,600&amp;family=IBM+Plex+Mono:wght@400;500;600&amp;display=swap" rel="stylesheet">

<!-- Bootstrap Icons for brand mark and UI elements -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Mixture Models and Types of Learning – rahuldave">
<meta property="og:description" content="Covers the three fundamental types of machine learning through the lens of Gaussian Mixture Models. Demonstrates how GDA works in supervised settings, contrasts with unsupervised clustering where labels are hidden, and introduces semi-supervised learning combining labeled and unlabeled data.">
<meta property="og:image" content="https://rahuldave.github.io/posts/typesoflearning/assets/zinfmonks.png">
<meta property="og:site_name" content="rahuldave">
<meta property="og:image:height" content="426">
<meta property="og:image:width" content="776">
<meta name="twitter:title" content="Mixture Models and Types of Learning – rahuldave">
<meta name="twitter:description" content="Covers the three fundamental types of machine learning through the lens of Gaussian Mixture Models. Demonstrates how GDA works in supervised settings, contrasts with unsupervised clustering where labels are hidden, and introduces semi-supervised learning combining labeled and unlabeled data.">
<meta name="twitter:image" content="https://rahuldave.github.io/posts/typesoflearning/assets/zinfmonks.png">
<meta name="twitter:creator" content="@rahuldave">
<meta name="twitter:image-height" content="426">
<meta name="twitter:image-width" content="776">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">rahuldave</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../til.html"> 
<span class="menu-text">TIL</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../collections.html"> 
<span class="menu-text">Collections</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#mixture-models-and-types-of-learning" id="toc-mixture-models-and-types-of-learning" class="nav-link active" data-scroll-target="#mixture-models-and-types-of-learning">Mixture Models, and types of learning</a>
  <ul class="collapse">
  <li><a href="#mixture-models" id="toc-mixture-models" class="nav-link" data-scroll-target="#mixture-models">Mixture models</a></li>
  <li><a href="#gaussian-mixture-models" id="toc-gaussian-mixture-models" class="nav-link" data-scroll-target="#gaussian-mixture-models">Gaussian Mixture Models</a></li>
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning">Supervised learning</a>
  <ul class="collapse">
  <li><a href="#gaussian-discriminant-analysis" id="toc-gaussian-discriminant-analysis" class="nav-link" data-scroll-target="#gaussian-discriminant-analysis">Gaussian Discriminant Analysis</a></li>
  </ul></li>
  <li><a href="#unsupervised-learning-mixture-of-gaussians" id="toc-unsupervised-learning-mixture-of-gaussians" class="nav-link" data-scroll-target="#unsupervised-learning-mixture-of-gaussians">Unsupervised learning: Mixture of Gaussians</a>
  <ul class="collapse">
  <li><a href="#concretely-formulating-the-problem" id="toc-concretely-formulating-the-problem" class="nav-link" data-scroll-target="#concretely-formulating-the-problem">Concretely formulating the problem</a></li>
  </ul></li>
  <li><a href="#semi-supervised" id="toc-semi-supervised" class="nav-link" data-scroll-target="#semi-supervised">Semi-supervised</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<!-- Theme toggle persistence script -->
<script>
(function() {
  // Check for saved theme preference, otherwise use system preference
  const savedTheme = localStorage.getItem('quarto-color-scheme');
  if (savedTheme) {
    document.documentElement.setAttribute('data-bs-theme', savedTheme);
  }

  // Listen for theme changes (Quarto's built-in toggle) and persist
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      if (mutation.attributeName === 'data-bs-theme') {
        const currentTheme = document.documentElement.getAttribute('data-bs-theme');
        if (currentTheme) {
          localStorage.setItem('quarto-color-scheme', currentTheme);
        }
      }
    });
  });

  // Start observing once DOM is ready
  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', function() {
      observer.observe(document.documentElement, { attributes: true });
    });
  } else {
    observer.observe(document.documentElement, { attributes: true });
  }
})();
</script>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Mixture Models and Types of Learning</h1>
<p class="subtitle lead">From generative classifiers to clustering: supervised, unsupervised, and semi-supervised approaches.</p>
  <div class="quarto-categories">
    <div class="quarto-category">classification</div>
    <div class="quarto-category">models</div>
    <div class="quarto-category">statistics</div>
  </div>
  </div>

<div>
  <div class="description">
    Covers the three fundamental types of machine learning through the lens of Gaussian Mixture Models. Demonstrates how GDA works in supervised settings, contrasts with unsupervised clustering where labels are hidden, and introduces semi-supervised learning combining labeled and unlabeled data.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 26, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="mixture-models-and-types-of-learning" class="level1">
<h1>Mixture Models, and types of learning</h1>
<section id="keywords-generative-model-supervised-learning-semi-supervised-learning-unsupervised-learning-mixture-model-gaussian-mixture-model-latent-variables" class="level5">
<h5 class="anchored" data-anchor-id="keywords-generative-model-supervised-learning-semi-supervised-learning-unsupervised-learning-mixture-model-gaussian-mixture-model-latent-variables">Keywords: generative model, supervised learning, semi-supervised learning, unsupervised learning, mixture model, gaussian mixture model, latent variables</h5>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.width'</span>, <span class="dv">500</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">100</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><span class="math display">\[\newcommand{\isum}{\sum_{i}}\]</span> <span class="math display">\[\newcommand{\zsum}{\sum_{k=1}^{K}}\]</span> <span class="math display">\[\newcommand{\zsumi}{\sum_{\{z_i\}}}\]</span></p>
</section>
<section id="mixture-models" class="level2">
<h2 class="anchored" data-anchor-id="mixture-models">Mixture models</h2>
<p>It is common to assume that observations are correlated due to some common “cause”. Hierarchical bayesian models are an example where we assume that information flows between observations through a tied-together set of higher level hyper-parameters.</p>
<p>We can also construct models with ‘hidden’ or ‘augmented’ variables, also known as latent variable models, which may or may not correlate with a cause. Since such models often have fewer parameters than observations, they are useful in modelling many problems.</p>
<p>An example of a hidden model is the mixture model. A distribution <span class="math inline">\(p(x \vert \{\theta_{k}\})\)</span> is a mixture of <span class="math inline">\(K\)</span> component distributions <span class="math inline">\(p_1, p_2,... p_K\)</span> if:</p>
<p><span class="math display">\[p(x \vert \{\theta_{k}\}) = \sum_k \lambda_k p_{k}(x \vert \theta_k)\]</span></p>
<p>with the <span class="math inline">\(\lambda_k\)</span> being mixing weights, <span class="math inline">\(\lambda_k &gt; 0\)</span>, <span class="math inline">\(\zsum \lambda_k = 1\)</span>.</p>
<p>The <span class="math inline">\(p_k\)</span>’s can be completely arbitrary, but we usually assume they are from the same family, like Gaussians with different centers and variances, or Poissons with different means.</p>
<p>We have already seen an example of this with the zero-inflated poisson distribution where</p>
<p>the likelihood of observing 0 is:</p>
<p><span class="math display">\[\cal{L}(y=0) = p + (1-p) e^{-\lambda},\]</span></p>
<p>and the Likelihood of a non-zero <span class="math inline">\(y\)</span> is:</p>
<p><span class="math display">\[\cal{L}(y \ne 0) = (1-p) \frac{\lambda^y e^{-\lambda}}{y!}\]</span></p>
<p>This model can be described by this diagram, taken from Mc-Elreath</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/zinfmonks.png" class="img-fluid figure-img"></p>
<figcaption>Zero-inflated monks model: a mixture process where monks either drink (observe y=0) or work (observe y&gt;0), with the resulting zero-inflated distribution of manuscripts completed. From McElreath.</figcaption>
</figure>
</div>
<p>The way to generate a new observation from such a distribution thus would be the following:</p>
<p><span class="math display">\[Z \sim Categorical(\lambda_1,\lambda_2,...,\lambda_K)\]</span></p>
<p>where <span class="math inline">\(Z\)</span> says which component X is drawn from (we could write this <span class="math inline">\(Z \sim Categorical(\bar{\lambda})\)</span>). Thus <span class="math inline">\(\lambda_j\)</span> is the probability that the hidden class variable <span class="math inline">\(z\)</span> is <span class="math inline">\(j\)</span>. Then:</p>
<p><span class="math display">\[X \sim p_{z}(x \vert \theta_z)\]</span></p>
<p>Thus we can see the general structure above:</p>
<p><span class="math display">\[p(x  \vert  \theta) = \sum_z p(z)p(x  \vert  z, \theta)\]</span></p>
<p>where <span class="math inline">\(\theta = \{ \theta_k \}\)</span> is the collection of distribution parameters.</p>
<p>Whats going on? Pick a bitmask according to the Categorical (the analog of the bernoulli, a multinomial with n=1). Where you find the 1, thats the distribution from which you make a draw.</p>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multinomial</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>multinomial.rvs(<span class="dv">1</span>,[<span class="fl">0.6</span>,<span class="fl">0.1</span>, <span class="fl">0.3</span>], size<span class="op">=</span><span class="dv">10</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>array([[1, 0, 0],
       [1, 0, 0],
       [0, 0, 1],
       [0, 0, 1],
       [0, 0, 1],
       [0, 0, 1],
       [0, 1, 0],
       [1, 0, 0],
       [1, 0, 0],
       [1, 0, 0]])</code></pre>
</div>
</div>
</section>
<section id="gaussian-mixture-models" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-mixture-models">Gaussian Mixture Models</h2>
<p>The Gaussian mixture model or GMM is the most widely used mixture distribution. In this model, each base distribution is a multivariate Gaussian with mean <span class="math inline">\(\mu_k\)</span> and covariance matrix <span class="math inline">\(\Sigma_k\)</span>. Thus the model has the form</p>
<p><span class="math display">\[p(x \vert  \{\theta_{k}\}) = \zsum \lambda_k N(x \vert \mu_k , \Sigma_k ) \]</span></p>
<p>Thus each mixture component can be thought of as represented by a different set of eliptical contours, and we add these to create our overall density.</p>
<div id="cell-8" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#In 1-D</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter values</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">0.5</span>])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>lambda_true <span class="op">=</span> np.array([<span class="fl">.4</span>, <span class="fl">.2</span>, <span class="fl">.4</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate from each distribution according to mixing proportion lambda</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> multinomial.rvs(<span class="dv">1</span>, lambda_true, size<span class="op">=</span>n)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-9" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.array([np.random.normal(mu_true[i.astype(<span class="st">'bool'</span>)][<span class="dv">0</span>], sigma_true[i.astype(<span class="st">'bool'</span>)][<span class="dv">0</span>]) <span class="cf">for</span> i <span class="kw">in</span> z])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-10" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>sns.distplot(x, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="supervised-learning" class="level2">
<h2 class="anchored" data-anchor-id="supervised-learning">Supervised learning</h2>
<p>In supervised learning, we have some training data of <span class="math inline">\(N\)</span> data points, each with <span class="math inline">\(m\)</span> features. The big idea is that on the training set, we have <strong>a label associated with each data point</strong>. Here the label is <span class="math inline">\(z\)</span>.</p>
<p>For a feature vector x, we use Bayes rule to express the posterior of the class-conditional as:</p>
<p><span class="math display">\[p(z = c \vert x, \theta) = \frac{p(z = c  \vert  \theta)p(x  \vert  z = c, \theta)}{ \sum_{c′} p(z = c′  \vert  \theta) p(x  \vert  z = c′, \theta)}\]</span></p>
<p>This is a <strong>generative classifier</strong>, since it specifies how to generate the data using the class-conditional density <span class="math inline">\(p(x \vert z = c, \theta)\)</span> and the class prior <span class="math inline">\(p(z = c\vert \theta)\)</span>.</p>
<p>Notice that even though we are talking about priors and posteriors here, I am carrying along <span class="math inline">\(\theta\)</span>. This is because the models here are just probabilistic models, and we havent chosen a frequentist or bayesian modelling paradigm yet. We are just specifying the model, and the priors and posteriors we are talking about here are simply those from bayes theorem.</p>
<p>An alternative approach is to directly fit the class posterior, <span class="math inline">\(p(z = c \vert x, \theta)\)</span>; this is known as a <strong>discriminative classifier</strong>. For example, a Gaussian Mixture model or Naive bayes is a generative classifier whose discriminative counterpart is the logistic regression.</p>
<p><strong>The supervised learning case is the one in which where hidden variables <span class="math inline">\(z\)</span> are known on the training set</strong>. So the supervized case is one in which the model is not a hidden variables model at all.</p>
<section id="gaussian-discriminant-analysis" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-discriminant-analysis">Gaussian Discriminant Analysis</h3>
<p>Suppose we have input data <span class="math inline">\(x\)</span> that are continuous-valued random variables, and <span class="math inline">\(z\)</span> labels. We call <span class="math inline">\(p(x,z \vert \theta)\)</span> the <strong>full-data likelihood</strong>. Since we have both <span class="math inline">\(z\)</span> and <span class="math inline">\(x\)</span> on our training set, this is the likelihood we will want to maximize.</p>
<p>Let us see how to obtain it.</p>
<p>Our model is (limiting ourselves to two gaussians for simplicity):</p>
<p><span class="math display">\[ Z \sim \rm{Bernoulli}(\lambda) \]</span> <span class="math display">\[ X \vert z=0 \sim {\cal N}(\mu_0, \Sigma_0) \]</span> <span class="math display">\[ X \vert z=1 \sim {\cal N}(\mu_1, \Sigma_1) \]</span></p>
<p>The distributions in details are <span class="math display">\[ p(z) = \lambda^z (1-\lambda)^{1-z}\]</span> <span class="math display">\[ p(x \vert z=0) = \frac{1}{(2\pi)^{n/2}  \vert  \Sigma \vert ^{1/2}} \exp \left( -\frac{1}{2}(x-\mu_0)^T \,\Sigma_0^{-1}(x-\mu_0) \right) \]</span> <span class="math display">\[ p(x \vert z=1) = \frac{1}{(2\pi)^{n/2}  \vert  \Sigma \vert ^{1/2}} \exp \left( -\frac{1}{2}(x-\mu_1)^T \,\Sigma_1^{-1}(x-\mu_1) \right) \]</span></p>
<p>where the parameters of the model <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\Sigma_0\)</span>, <span class="math inline">\(\Sigma_1\)</span>, <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span> are to be determined. From here on, for simplicity of exposition we will use one covariance matrix for both Gaussians, <span class="math inline">\(\Sigma\)</span>. This is called tieing the covariances and leads to a model in the literature called Linear Discriminant Analysis.</p>
<p>The full-data log-likelihood of the data is given</p>
<p><span class="math display">\[
\begin{eqnarray}
l(x,z \vert  \lambda,\mu_0, \mu_1, \Sigma) &amp;=&amp; \log \prod_{i=1}^{m} p(x_i,z_i \vert  \lambda, \mu_0, \mu_1, \Sigma) \nonumber \\
          &amp;=&amp; \sum_{i=1}^{m} \log \left[p(x_i \vert z_i,  \mu_0, \mu_1, \Sigma) \,p(z_i \vert  \lambda) \right]  \nonumber \\
          &amp;=&amp; \sum_{i=1}^{m} \log p(x_i \vert z_i,  \mu_0, \mu_1, \Sigma) + \sum_{i=1}^{m}  \log p(z_i \vert  \lambda)   \nonumber   \\      
    &amp;=&amp;  -\sum_{i=1}^{m} \log ((2\pi)^{n/2}  \vert  \Sigma \vert ^{1/2}) - \frac{1}{2} \sum_{i=1}^{m}  (x-\mu_{z_i})^T \,\Sigma^{-1}(x-\mu_{z_i})   \nonumber \\
        &amp; &amp; \quad \quad +\sum_{i=1}^{m} \left[ z_i \, \log \lambda + (1-z_i) \log(1-\lambda) \right]
\end{eqnarray}
\]</span></p>
<p>Taking derivatives with respect to <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\Sigma\)</span>, <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span> and setting them to zero we get</p>
<p><span class="math display">\[
\begin{eqnarray}
   \lambda &amp; = &amp;\frac{1}{m}  \sum_{i=1}^{m}  \delta_{z_i,1} \nonumber  \\
   \mu_0 &amp;=&amp; \frac{ \sum_{i=1}^{m}  \delta_{z_i,0} \, x_i  }{ \sum_{i=1}^{m}   \delta_{z_i,0}}\nonumber  \\
    \mu_1 &amp;=&amp; \frac{ \sum_{i=1}^{m}  \delta_{z_i,1} \, x_i  }{ \sum_{i=1}^{m}   \delta_{z_i,1}}\nonumber  \\
\Sigma &amp;=&amp;\frac{1}{m}   \sum_{i=1}^{m}  (x_i-\mu_{z_i})   (x_i-\mu_{z_i})^{T}
\end{eqnarray}
\]</span></p>
<p>This gives us the obvious result, namely <span class="math inline">\(\lambda\)</span> is nothing more but the fraction of objects with label <span class="math inline">\(z=1\)</span> and the total number of objects, <span class="math inline">\(\mu\)</span>’s are the mean within the class and <span class="math inline">\(\Sigma\)</span> is the the covariance matrix for each group. This analysis is called “Gaussian Discriminant Analysis” or GDA, here specifically LDA as we tied the covariance matrices.</p>
<p>Lets do this in code. First we simulate some data:</p>
<div id="cell-13" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#In 1-D</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># True parameter values</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>mu_true <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">5</span>])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.6</span>])</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>lambda_true <span class="op">=</span> np.array([<span class="fl">.4</span>, <span class="fl">.6</span>])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate from each distribution according to mixing proportion lambda_true</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> binom.rvs(<span class="dv">1</span>, lambda_true[<span class="dv">0</span>], size<span class="op">=</span>n)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.array([np.random.normal(mu_true[i], sigma_true[i]) <span class="cf">for</span> i <span class="kw">in</span> z])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>sns.distplot(x, bins<span class="op">=</span><span class="dv">100</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now we split into a training set and a test set.</p>
<div id="cell-15" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#the z's are the classes in the supervised learning</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#the 'feature' is the x position of the sample</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ztrain, ztest, xtrain, xtest <span class="op">=</span> train_test_split(z,x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-16" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ztrain.shape, xtrain.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>((7500,), (7500,))</code></pre>
</div>
</div>
<div id="cell-17" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.hist(xtrain, bins<span class="op">=</span><span class="dv">20</span>)<span class="op">;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-18" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>lambda_train<span class="op">=</span>np.mean(ztrain)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>mu0_train <span class="op">=</span> np.<span class="bu">sum</span>(xtrain[ztrain<span class="op">==</span><span class="dv">0</span>])<span class="op">/</span>(np.<span class="bu">sum</span>(ztrain<span class="op">==</span><span class="dv">0</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>mu1_train <span class="op">=</span> np.<span class="bu">sum</span>(xtrain[ztrain<span class="op">==</span><span class="dv">1</span>])<span class="op">/</span>(np.<span class="bu">sum</span>(ztrain<span class="op">==</span><span class="dv">1</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>xmus<span class="op">=</span>np.array([mu0_train <span class="cf">if</span> z<span class="op">==</span><span class="dv">0</span> <span class="cf">else</span> mu1_train <span class="cf">for</span> z <span class="kw">in</span> ztrain])</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>xdiffs <span class="op">=</span> xtrain <span class="op">-</span> xmus</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>sigma_train<span class="op">=</span>np.sqrt(np.dot(xdiffs, xdiffs)<span class="op">/</span>xtrain.shape[<span class="dv">0</span>])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lambda_train, mu0_train, mu1_train, sigma_train)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.407066666667 1.99814996529 5.0176232719 0.599407546657</code></pre>
</div>
</div>
<p>We can use the log likelihood at a given <code>x</code> as a classifier: assign the class ‘0’ or ‘1’ depending upon which probability <span class="math inline">\(p(x_j \vert \lambda, z, \Sigma)\)</span> is larger. Note that this is JUST the <span class="math inline">\(x\)</span> likelihood, because we want to compare probabilities for fixed <span class="math inline">\(z\)</span>s.</p>
<p><span class="math display">\[log\,p(x_j \vert \lambda, z, \Sigma) = -\sum_{i=1}^{m} \log ((2\pi)^{n/2}  \vert  \Sigma \vert ^{1/2}) - \frac{1}{2} \sum_{i=1}^{m}  (x-\mu_{z_i})^T \,\Sigma^{-1}(x-\mu_{z_i})  \]</span></p>
<p>The first term of the likelihood does not matter since it is independent of <span class="math inline">\(z\)</span>, therefore:</p>
<div id="cell-20" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loglikdiff(x):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    for0<span class="op">=</span> <span class="op">-</span> (x<span class="op">-</span>mu0_train)<span class="op">*</span>(x<span class="op">-</span>mu0_train)<span class="op">/</span>(<span class="fl">2.0</span><span class="op">*</span>sigma_train<span class="op">*</span>sigma_train) </span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    for0 <span class="op">=</span> for0 <span class="op">+</span> np.log(<span class="fl">1.</span><span class="op">-</span>lambda_train)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    for1 <span class="op">=</span> <span class="op">-</span> (x<span class="op">-</span>mu1_train)<span class="op">*</span>(x<span class="op">-</span>mu1_train)<span class="op">/</span>(<span class="fl">2.0</span><span class="op">*</span>sigma_train<span class="op">*</span>sigma_train) </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    for1 <span class="op">=</span> for1 <span class="op">+</span> np.log(lambda_train)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span><span class="op">*</span>(for1 <span class="op">-</span> for0 <span class="op">&gt;=</span> <span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="cell-21" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> np.array([loglikdiff(test_x) <span class="cf">for</span> test_x <span class="kw">in</span> xtest])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"correct classification rate"</span>, np.mean(ztest <span class="op">==</span> pred))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>correct classification rate 0.9944</code></pre>
</div>
</div>
</section>
</section>
<section id="unsupervised-learning-mixture-of-gaussians" class="level2">
<h2 class="anchored" data-anchor-id="unsupervised-learning-mixture-of-gaussians">Unsupervised learning: Mixture of Gaussians</h2>
<p>In unsupervised learning, we do not know the class labels. We wish to generate these labels automatically from the data. An example of an unsupervised model is clustering. In the context of mixture models we then do not know what the components of the mixture model are, i.e.&nbsp;what the parameters of the components and their admixture (<span class="math inline">\(\lambda\)</span>s) are. Indeed, we might not even know how many components we have!!</p>
<p>In this case, to carry out the clustering, we first fit the mixture model, and then compute <span class="math inline">\(p(z_i = k  \vert  x_i, \theta)\)</span>, which represents the posterior probability that point i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be computed as before using Bayes rule as follows:</p>
<p><span class="math display">\[p(z_k = c \vert x_i, \theta) = \frac{p(z_k = c  \vert  \theta)p(x_i  \vert  z_k = c, \theta)}
{ \sum_{_c′} p(z_k = c′  \vert  \theta) p(x_i  \vert  z_k = c′, \theta)}\]</span></p>
<p>This is called soft clustering. K-means is a hard-clustering analog where you associate a data point with a cluster rather than simply computing probabilities for the association.</p>
<p>The process is identical to the computations performed before in the supervised learning, except at training time: here we never observe <span class="math inline">\(z_k\)</span> for any samples, whereas before with the generative GDA classifier, we did observe <span class="math inline">\(z_k\)</span> on the training set.</p>
<p>How many clusters? The best number will generalize best to future data, something we can use cross-validation or other techniques to find.</p>
<p>Put differently, <strong>unsupervised learning is a density estimation problem for <span class="math inline">\(p(x)\)</span></strong>.</p>
<p><span class="math display">\[p(x) = \sum_c p(x \vert z) p(z).\]</span></p>
<p>In other words we discover the marginal p(x) through the generative model <span class="math inline">\(p(x \vert z)\)</span>. This is also very useful in discovering <strong>outliers</strong> in our data.</p>
<section id="concretely-formulating-the-problem" class="level3">
<h3 class="anchored" data-anchor-id="concretely-formulating-the-problem">Concretely formulating the problem</h3>
<p>So let us turn our attention to the case where we do not know the labels <span class="math inline">\(z\)</span>.</p>
<p>Suppose we are given a data set <span class="math inline">\(\{x_1,\ldots, x_m\}\)</span> but not given the labels <span class="math inline">\(z\)</span>. The model consists of <span class="math inline">\(k\)</span> Gaussians. In other words our model assumes that each <span class="math inline">\(x_i\)</span> was generated by randomly choosing <span class="math inline">\(z_i\)</span> from <span class="math inline">\(\{1, \ldots, k\}\)</span>, and then <span class="math inline">\(x_i\)</span> is drawn from one of the <span class="math inline">\(k\)</span> Gaussians depending on <span class="math inline">\(z_i\)</span>.</p>
<p>We wish to compute either the maximum likelihood estimate for this model, <span class="math inline">\(p(\{x_{i}\} \vert \theta)\)</span>. The goal is to model the joint distribution <span class="math inline">\(p(\{x_i\}, \{z_i\})=p(\{x_i\} \vert \{z_i\}) \, p(\{z_i\})\)</span> where <span class="math inline">\(z_i \sim \rm{Categorical}(\lambda)\)</span>, and <span class="math inline">\(\lambda = \{\lambda_j\}\)</span>.</p>
<p>As in our definition of mixture models <span class="math inline">\(\lambda_j\ge0\)</span> and</p>
<p><span class="math display">\[ \sum_j^k \lambda_i = 1 \]</span></p>
<p>The parameters <span class="math inline">\(\lambda_j\)</span> produce <span class="math inline">\(p(z_i=j)\)</span> and then <span class="math inline">\(x_i \vert z_i=j \sim {\cal N}(\mu_j, \Sigma_j)\)</span>.</p>
<p>The parameters of our problem are <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>. But because the labels <span class="math inline">\(z\)</span> are hidden to us, we no longer have the full-data likelihood. So we estimate our parameters by minimizing the <span class="math inline">\(x\)</span>-data log-likelihood:</p>
<p><span class="math display">\[
\begin{eqnarray}
l(x \vert  \lambda, \mu, \Sigma) &amp;=&amp; \sum_{i=1}^{m} \log p(x_i \vert  \lambda,  \mu ,\Sigma)   \nonumber \\
     &amp;=&amp; \sum_{i=1}^{m} \log \sum_z p(x_i \vert  z_i,  \mu , \Sigma) \, p(z_i \vert  \lambda)  
\end{eqnarray}
\]</span></p>
<p>However, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we’ll find that it is not possible to find the maximum likelihood estimates Futhermore, we have to enforce constraints such as mixing weights summing to 1, covariance matrices being positive definite, etc.</p>
<p>For all of these reasons, its simpler, but not always faster to use an iterative algorithm called the EM algorithm to get the local maximum likelihood or MAP estimate. We shall learn this algorithm soon. But we can also set this problem up as a bayesian problem with reasonable priors on the parameters and try to use MCMC.</p>
</section>
</section>
<section id="semi-supervised" class="level2">
<h2 class="anchored" data-anchor-id="semi-supervised">Semi-supervised</h2>
<p>In unsupervized learning we are given samples from some unknown data distribution with density <span class="math inline">\(p(x)\)</span>. Our goal is to estimate this density or a known functional (like the risk) thereof. Supervized learning can be treated as estimating <span class="math inline">\(p(x,c)\)</span> or a known functional of it. But there is usually no need to estimate the input distribution so estimating the complete density is wasteful, and we usually focus on estimating <span class="math inline">\(p(c \vert x)\)</span> discriminatively or generatively(via <span class="math inline">\(p(x \vert c) p(c)\)</span>. Here <span class="math inline">\(c\)</span> or <span class="math inline">\(y\)</span> or <span class="math inline">\(z\)</span> are the classes we are trying to estimate. In the unsupervized case we often estimate <span class="math inline">\(\sum_z p(x \vert z) p(z) = p(x)\)</span> with latent (hidden) <span class="math inline">\(z\)</span>, which you may or may not wish to identify with classes.</p>
<p>Semi-supervised learning is the situation in which we have some labels, but typically very few labels: not enough to form a good training set.</p>
<p>In semi-supervized learning we combine the two worlds. We write a joint likelihood for the supervised and unsupervised parts:</p>
<p><span class="math display">\[ l(\{x_i\},\{x_j\},\{z_i\} \vert \theta, \lambda) = \sum_i log \, p(x_i, z_i \vert \lambda, \theta) +  \sum_j log \, p(x_j \vert \lambda, \theta) = \sum_i log \, p(z_i \vert \lambda) p(x_i \vert z_,\theta) + \sum_j log \, \sum_z p(z_j \vert \lambda) p(x_j \vert z_j,\theta) \]</span></p>
<p>Here <span class="math inline">\(i\)</span> ranges over the data points where we have labels, and <span class="math inline">\(j\)</span> over the data points where we dont.</p>
<p>In a traditional classification-based machine learning scenario we might still split the data into a training and validation set. But the basic idea in semi-supervised learning is that there is structure in <span class="math inline">\(p(x)\)</span> which might help us divine the conditionals, so what we would want to do, is include in the training set unlabelled points. The game then is that if there is geometric structure in <span class="math inline">\(p(x)\)</span>, some kind of cluster based foliation, then we can explot this.</p>


</section>
</section>

</main> <!-- /main -->
<!-- Inject yin-yang brand icon into navbar -->
<script>
document.addEventListener('DOMContentLoaded', function() {
  // Find the navbar brand
  const navbarBrand = document.querySelector('.navbar-brand');
  if (navbarBrand) {
    // Create the brand mark element
    const brandMark = document.createElement('span');
    brandMark.className = 'brand-mark';
    brandMark.innerHTML = '<i class="bi bi-yin-yang"></i>';

    // Insert at the beginning of navbar brand
    navbarBrand.insertBefore(brandMark, navbarBrand.firstChild);
  }
});
</script>
<div id="discuss-links" style="margin-top: 2.5rem; padding-top: 1.5rem; margin-bottom: 4rem; border-top: 1px solid var(--color-border, #ddd);">
  <p style="font-family: var(--font-mono, monospace); font-size: 0.8rem; text-transform: uppercase; letter-spacing: 0.05em; color: var(--color-text-muted, #666); margin-bottom: 0.75rem;">Discuss this post</p>
  <div style="display: flex; gap: 1rem; flex-wrap: wrap;">
    <a id="discuss-twitter" href="#" target="_blank" rel="noopener" style="font-size: 0.9rem;">X / Twitter</a>
    <a id="discuss-bluesky" href="#" target="_blank" rel="noopener" style="font-size: 0.9rem;">Bluesky</a>
    <a id="discuss-linkedin" href="#" target="_blank" rel="noopener" style="font-size: 0.9rem;">LinkedIn</a>
  </div>
</div>
<script>
(function() {
  var p = window.location.pathname;
  var isContent = p.startsWith("/posts/") || p.startsWith("/til/") || p.startsWith("/collections/");
  var isListing = (p === "/posts/" || p === "/posts" || p === "/til/" || p === "/til" || p === "/collections/" || p === "/collections");
  if (!isContent || isListing) {
    document.getElementById("discuss-links").style.display = "none";
    return;
  }

  var url = encodeURIComponent(window.location.href);
  var title = encodeURIComponent(document.title);

  document.getElementById("discuss-twitter").href =
    "https://twitter.com/intent/tweet?text=" + title + "&url=" + url + "&via=rahuldave";
  document.getElementById("discuss-bluesky").href =
    "https://bsky.app/intent/compose?text=" + title + " " + encodeURIComponent(window.location.href);
  document.getElementById("discuss-linkedin").href =
    "https://www.linkedin.com/sharing/share-offsite/?url=" + url;
})();
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/rahuldave\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>