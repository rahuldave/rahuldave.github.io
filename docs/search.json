[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "Claude Session Context",
    "section": "",
    "text": "This is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io.\n\n\n\n\nAM207 course wiki at ~/Attic/Projects/AM207/2018fall_wiki/wiki/ — lectures, notebooks, markdown notes\nLecture index at ~/Attic/Projects/AM207/2018fall_wiki/lectures/index.md (replace .html with .md for source files)\nEach lecture .md links to wiki notes; check for .ipynb (primary source) before falling back to .md\nUse /import-wiki-notes skill to import notes as blog posts\nHelper scripts in _scripts/:\n\nimport_notebook.py — creates posts/&lt;name&gt;/index.ipynb from wiki .ipynb (adds frontmatter, fixes paths, copies images). Run with --help.\nupdate_captions.py — updates image captions in notebook markdown cells. Used by /caption-images skill.\n\nAfter importing, run /caption-images to add captions with citations to embedded images\n\n\n\n\n\nLecture 1 (Intro & Probability): DONE\n\nboxloop — already existed, skipped\nprobability — notebook, imported to posts/probability/\ndistributions — markdown only, imported to posts/distributions.md\ndistrib-example — notebook, imported to posts/distrib-example/\n\nLecture 2 (Probability, Sampling, Laws, Monte Carlo): DONE\n\ndistributions — already imported (Lecture 1), skipped\nexpectations — notebook, imported to posts/expectations/\nsamplingclt — notebook, imported to posts/samplingclt/\nbasicmontecarlo — notebook, imported to posts/basicmontecarlo/\nmontecarlointegrals — notebook, imported to posts/montecarlointegrals/\n\nLecture 3 (From Monte Carlo to Frequentist Stats): DONE\n\nExpectations — already imported (Lecture 2), skipped\nSamplingCLT — already imported (Lecture 2), skipped\nbasicmontecarlo — already imported (Lecture 2), skipped\nmontecarlointegrals — already imported (Lecture 2), skipped\nfrequentist — markdown only, imported to posts/frequentist.md\nMLE — notebook, imported to posts/MLE/\n\nLecture 4 (MLE, Sampling, and Learning): DONE\n\nnoiseless_learning — notebook, imported to posts/noiseless_learning/\nnoisylearning — notebook, imported to posts/noisylearning/\nMLE — already imported (Lecture 3), skipped\ntestingtraining — notebook, imported to posts/testingtraining/\nvalidation — notebook, imported to posts/validation/\nregularization — notebook, imported to posts/regularization/\n\nLecture 5 (Regression, AIC, Info. Theory): DONE\n\ndoseplacebo — notebook, imported to posts/doseplacebo/\nnoiseless_learning — already imported (Lecture 4), skipped\nnoisylearning — already imported (Lecture 4), skipped\ntestingtraining — already imported (Lecture 4), skipped\nvalidation — already imported (Lecture 4), skipped\nregularization — already imported (Lecture 4), skipped\njensens — notebook, imported to posts/jensens/\nDivergence — notebook, imported to posts/divergence/\nunderstandingaic — notebook, imported to posts/understandingaic/\n\nLecture 6 (Risk and Information): DONE\n\njensens — already imported (Lecture 5), skipped\nDivergence — already imported (Lecture 5), skipped\nunderstandingaic — already imported (Lecture 5), skipped\nEntropy — notebook, imported to posts/entropy/\n\nLecture 7 (From Entropy to Bayes): DONE\n\nDivergence — already imported (Lecture 5), skipped\nEntropy — already imported (Lecture 6), skipped\nbayes_withsampling — notebook, imported to posts/bayes_withsampling/\nglobemodel — notebook, imported to posts/globemodel/\nsufstatexch — notebook, imported to posts/sufstatexch/\n\nLecture 8 (Bayes and Sampling): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nglobemodel — already imported (Lecture 7), skipped\nsufstatexch — already imported (Lecture 7), skipped\nglobemodellab — notebook, imported to posts/globemodellab/\nnormalmodel — notebook, imported to posts/normalmodel/\nlightspeed — source not found in wiki, skipped\ninversetransform — notebook, imported to posts/inversetransform/\nrejectionsampling — notebook, imported to posts/rejectionsampling/\nimportancesampling — notebook, imported to posts/importancesampling/\n\nLecture 9 (Bayes and Sampling, cont.): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nnormalmodel — already imported (Lecture 8), skipped\nsufstatexch — already imported (Lecture 7), skipped\nlightspeed — source not found in wiki, skipped\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nbayesianregression — notebook, imported to posts/bayesianregression/\nnormalreg — notebook, imported to posts/normalreg/ (includes Howell1.csv data)\n\nLecture 10 (Sampling and Gradient Descent): DONE\n\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nimportancesampling — already imported (Lecture 8), skipped\ngradientdescent — notebook, imported to posts/gradientdescent/ (SGD gif replaced with link)\nLogisticBP — notebook, imported to posts/logisticbp/\n\nLectures 11–26: NOT YET IMPORTED\n\n\n\n\n\nCanonical categories are in _categories.txt (root of project), one per line, sorted alphabetically\nCurrent categories: bayesian, data, elections, information-theory, integration, interactive, macos, models, montecarlo, optimization, orchestration, pipeline, probability, regression, sampling, statistics, visualization\nAll categories must be lowercase\nWhen importing, map source keywords to existing categories; propose new ones for user approval\nThe /import-wiki-notes skill enforces this workflow (step 7c)\n\n\n\n\n\nProfile photo: assets/profile.jpg (sourced from GitHub avatar)\nComprehensive bio context: ~/Context/rahul-dave-profile.md\nContact form via Formspree (ID: mykjwlyk) — forwards to rahuldave@univ.ai\nLinks: GitHub, X/Twitter, Bluesky (rahuldave.bsky.social), Google Scholar, LinkedIn\n\n\n\n\n\nincludes/discuss-links.html — “Discuss this post” links (Twitter, Bluesky, LinkedIn) on posts, til, and collections pages only\nUses IIFE (not DOMContentLoaded) because include-after-body runs after DOM is ready\nLinks are constructed at runtime from window.location.href — will use production domain automatically\nOpen Graph and Twitter Card metadata enabled in _quarto.yml\n\n\n\n\n\nAll listing pages sort by date desc: index.qmd, posts.qmd, til.qmd, collections.qmd\n\n\n\n\n\ndesigns/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — fork of depth, modernized with animations/glassmorphism\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nColorBrewer Blues palette (#eff3ff → #08306b)\nDark/light mode support\n\n\n\n--cb-blue-50: #eff3ff   --cb-blue-100: #c6dbef   --cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6  --cb-blue-400: #4292c6   --cb-blue-500: #2171b5\n--cb-blue-600: #08519c  --cb-blue-700: #084594   --cb-blue-800: #08306b\n\n\n\n\n\n\n\n\n_quarto.yml — project config: website type, navbar, SCSS theme (light/dark), TOC, margin references, Open Graph, Twitter Cards\nThemes: styles/modern-light.scss, styles/modern-dark.scss\nIncludes: includes/fonts.html, includes/brand-icon.html, includes/theme-toggle.html, includes/discuss-links.html\n\n\n\n\nNotebooks get their own folder with index.ipynb (or index.qmd for JS demos):\nposts/\n  probability/\n    index.ipynb        # URL becomes /posts/probability/\n    assets/            # images, data files referenced by THIS notebook only\n      venn.png\n      bishop-prob.png\n  votingforcongress/\n    index.ipynb\n    assets/\n      sep7.png\n  earth-demo/\n    index.qmd          # Three.js demo using .qmd format\n    assets/\n      earth-card.png   # Card thumbnail for listing (no content images in post)\nMarkdown posts stay as flat files, images go in shared posts/images/ or posts/data/:\nposts/\n  boxloop.md           # URL becomes /posts/boxloop\n  distributions.md\n  images/              # shared images for flat .md posts\n    2tosscdf.png\n  data/                # shared data for flat .md posts (if needed)\n\n\n\nNotebooks must have a raw cell (cell_type: “raw”) as the first cell with YAML:\n---\ntitle: \"Post Title\"\nsubtitle: \"Catchy one-liner for listing cards.\"\ndescription: \"Two-sentence summary for the post page.\"\ncategories:\n    - probability\n    - statistics\ndate: 2025-01-08\n---\n\nsubtitle appears on listing cards\ndescription is the longer summary\ncategories are used for filtering (must be from _categories.txt)\ndate controls sort order\nimage — optional, for posts without content images (e.g. image: assets/earth-card.png)\n\nFor markdown posts, the same YAML goes in the standard frontmatter block at the top.\n\n\n\nFor posts with no embedded images (e.g. interactive Three.js demos), use the browser agent to screenshot the rendered page, crop to the key visual, save to assets/, and add image: to frontmatter. See skill step 9 for details.\n\n\n\nUse .qmd format (not .ipynb) for JavaScript-heavy interactive content: - Load external JS via format: html: include-in-header: in frontmatter - Use Quarto’s fenced div syntax ::: {.classname} for layout containers - Inline &lt;script&gt; blocks at the end of the .qmd file - Read CSS custom properties (--cb-blue-*, --interactive-*) for theme integration - Example: posts/earth-demo/index.qmd\n\n\n\nmake preview                # Live dev server with hot reload\nmake render                 # Build full site to _site/\nmake build                  # Render + rsync _site/ to docs/ for GitHub Pages\nquarto render posts/probability/index.ipynb  # Render a single post\n\nrender produces _site/; build adds an rsync step to sync changed files into docs/\nCommit and push are intentionally separate from build\n\n\n\n\n\nNotebook posts: assets/filename.png (relative to the notebook’s folder)\nMarkdown posts: images/filename.png (relative to posts/)\nSite-wide assets already in /assets/ (e.g. lawoflargenumbers images) use /assets/... absolute paths\nProfile photo: assets/profile.jpg (JPEG, not PNG — watch for GitHub avatar format mismatch)\n\n\n\n\n\nAll categories must be lowercase — no Statistics, use statistics; no MonteCarlo, use montecarlo\nCanonical list in _categories.txt at project root\nThis prevents duplicate tags in Quarto listing filters\n\n\n\n\n\ntitle — post title\nsubtitle — catchy one-liner shown on listing cards\ndescription — 2-sentence content summary\ncategories — lowercase tags for filtering (from _categories.txt)\ndate — controls sort order (YYYY-MM-DD)\nimage — card thumbnail path (optional, for posts without content images)\n\n\n\n\n\nposts/ — main blog posts (shown on index page)\ntil/ — Today I Learned (shown only on TIL page, NOT on index)\ncollections/software/ — software tools (shown only on Collections page, NOT on index)\nindex.qmd listing contents should only include posts/ and posts/**/\n\n\n\n\n\nAfter adding/moving/renaming files, restart quarto preview — the live server caches resource IDs and will show “Bad resource ID” for changed files\nListing .qmd files must not have a stray --- after the YAML block (causes parse errors)\nListing contents paths should NOT have a leading / (use til/*.qmd not /til/*.qmd)\ninclude-after-body scripts run after DOMContentLoaded — use IIFE, not event listeners\nGitHub avatar downloads are JPEG even with .png URL — always check with file command and use correct extension"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "Claude Session Context",
    "section": "",
    "text": "This is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io."
  },
  {
    "objectID": "CLAUDE.html#content-source",
    "href": "CLAUDE.html#content-source",
    "title": "Claude Session Context",
    "section": "",
    "text": "AM207 course wiki at ~/Attic/Projects/AM207/2018fall_wiki/wiki/ — lectures, notebooks, markdown notes\nLecture index at ~/Attic/Projects/AM207/2018fall_wiki/lectures/index.md (replace .html with .md for source files)\nEach lecture .md links to wiki notes; check for .ipynb (primary source) before falling back to .md\nUse /import-wiki-notes skill to import notes as blog posts\nHelper scripts in _scripts/:\n\nimport_notebook.py — creates posts/&lt;name&gt;/index.ipynb from wiki .ipynb (adds frontmatter, fixes paths, copies images). Run with --help.\nupdate_captions.py — updates image captions in notebook markdown cells. Used by /caption-images skill.\n\nAfter importing, run /caption-images to add captions with citations to embedded images"
  },
  {
    "objectID": "CLAUDE.html#content-import-status-from-am207-wiki",
    "href": "CLAUDE.html#content-import-status-from-am207-wiki",
    "title": "Claude Session Context",
    "section": "",
    "text": "Lecture 1 (Intro & Probability): DONE\n\nboxloop — already existed, skipped\nprobability — notebook, imported to posts/probability/\ndistributions — markdown only, imported to posts/distributions.md\ndistrib-example — notebook, imported to posts/distrib-example/\n\nLecture 2 (Probability, Sampling, Laws, Monte Carlo): DONE\n\ndistributions — already imported (Lecture 1), skipped\nexpectations — notebook, imported to posts/expectations/\nsamplingclt — notebook, imported to posts/samplingclt/\nbasicmontecarlo — notebook, imported to posts/basicmontecarlo/\nmontecarlointegrals — notebook, imported to posts/montecarlointegrals/\n\nLecture 3 (From Monte Carlo to Frequentist Stats): DONE\n\nExpectations — already imported (Lecture 2), skipped\nSamplingCLT — already imported (Lecture 2), skipped\nbasicmontecarlo — already imported (Lecture 2), skipped\nmontecarlointegrals — already imported (Lecture 2), skipped\nfrequentist — markdown only, imported to posts/frequentist.md\nMLE — notebook, imported to posts/MLE/\n\nLecture 4 (MLE, Sampling, and Learning): DONE\n\nnoiseless_learning — notebook, imported to posts/noiseless_learning/\nnoisylearning — notebook, imported to posts/noisylearning/\nMLE — already imported (Lecture 3), skipped\ntestingtraining — notebook, imported to posts/testingtraining/\nvalidation — notebook, imported to posts/validation/\nregularization — notebook, imported to posts/regularization/\n\nLecture 5 (Regression, AIC, Info. Theory): DONE\n\ndoseplacebo — notebook, imported to posts/doseplacebo/\nnoiseless_learning — already imported (Lecture 4), skipped\nnoisylearning — already imported (Lecture 4), skipped\ntestingtraining — already imported (Lecture 4), skipped\nvalidation — already imported (Lecture 4), skipped\nregularization — already imported (Lecture 4), skipped\njensens — notebook, imported to posts/jensens/\nDivergence — notebook, imported to posts/divergence/\nunderstandingaic — notebook, imported to posts/understandingaic/\n\nLecture 6 (Risk and Information): DONE\n\njensens — already imported (Lecture 5), skipped\nDivergence — already imported (Lecture 5), skipped\nunderstandingaic — already imported (Lecture 5), skipped\nEntropy — notebook, imported to posts/entropy/\n\nLecture 7 (From Entropy to Bayes): DONE\n\nDivergence — already imported (Lecture 5), skipped\nEntropy — already imported (Lecture 6), skipped\nbayes_withsampling — notebook, imported to posts/bayes_withsampling/\nglobemodel — notebook, imported to posts/globemodel/\nsufstatexch — notebook, imported to posts/sufstatexch/\n\nLecture 8 (Bayes and Sampling): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nglobemodel — already imported (Lecture 7), skipped\nsufstatexch — already imported (Lecture 7), skipped\nglobemodellab — notebook, imported to posts/globemodellab/\nnormalmodel — notebook, imported to posts/normalmodel/\nlightspeed — source not found in wiki, skipped\ninversetransform — notebook, imported to posts/inversetransform/\nrejectionsampling — notebook, imported to posts/rejectionsampling/\nimportancesampling — notebook, imported to posts/importancesampling/\n\nLecture 9 (Bayes and Sampling, cont.): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nnormalmodel — already imported (Lecture 8), skipped\nsufstatexch — already imported (Lecture 7), skipped\nlightspeed — source not found in wiki, skipped\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nbayesianregression — notebook, imported to posts/bayesianregression/\nnormalreg — notebook, imported to posts/normalreg/ (includes Howell1.csv data)\n\nLecture 10 (Sampling and Gradient Descent): DONE\n\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nimportancesampling — already imported (Lecture 8), skipped\ngradientdescent — notebook, imported to posts/gradientdescent/ (SGD gif replaced with link)\nLogisticBP — notebook, imported to posts/logisticbp/\n\nLectures 11–26: NOT YET IMPORTED"
  },
  {
    "objectID": "CLAUDE.html#category-system",
    "href": "CLAUDE.html#category-system",
    "title": "Claude Session Context",
    "section": "",
    "text": "Canonical categories are in _categories.txt (root of project), one per line, sorted alphabetically\nCurrent categories: bayesian, data, elections, information-theory, integration, interactive, macos, models, montecarlo, optimization, orchestration, pipeline, probability, regression, sampling, statistics, visualization\nAll categories must be lowercase\nWhen importing, map source keywords to existing categories; propose new ones for user approval\nThe /import-wiki-notes skill enforces this workflow (step 7c)"
  },
  {
    "objectID": "CLAUDE.html#about-page",
    "href": "CLAUDE.html#about-page",
    "title": "Claude Session Context",
    "section": "",
    "text": "Profile photo: assets/profile.jpg (sourced from GitHub avatar)\nComprehensive bio context: ~/Context/rahul-dave-profile.md\nContact form via Formspree (ID: mykjwlyk) — forwards to rahuldave@univ.ai\nLinks: GitHub, X/Twitter, Bluesky (rahuldave.bsky.social), Google Scholar, LinkedIn"
  },
  {
    "objectID": "CLAUDE.html#social-discussion-links",
    "href": "CLAUDE.html#social-discussion-links",
    "title": "Claude Session Context",
    "section": "",
    "text": "includes/discuss-links.html — “Discuss this post” links (Twitter, Bluesky, LinkedIn) on posts, til, and collections pages only\nUses IIFE (not DOMContentLoaded) because include-after-body runs after DOM is ready\nLinks are constructed at runtime from window.location.href — will use production domain automatically\nOpen Graph and Twitter Card metadata enabled in _quarto.yml"
  },
  {
    "objectID": "CLAUDE.html#listing-sort-order",
    "href": "CLAUDE.html#listing-sort-order",
    "title": "Claude Session Context",
    "section": "",
    "text": "All listing pages sort by date desc: index.qmd, posts.qmd, til.qmd, collections.qmd"
  },
  {
    "objectID": "CLAUDE.html#design",
    "href": "CLAUDE.html#design",
    "title": "Claude Session Context",
    "section": "",
    "text": "designs/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — fork of depth, modernized with animations/glassmorphism\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nColorBrewer Blues palette (#eff3ff → #08306b)\nDark/light mode support\n\n\n\n--cb-blue-50: #eff3ff   --cb-blue-100: #c6dbef   --cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6  --cb-blue-400: #4292c6   --cb-blue-500: #2171b5\n--cb-blue-600: #08519c  --cb-blue-700: #084594   --cb-blue-800: #08306b"
  },
  {
    "objectID": "CLAUDE.html#quarto-site-structure",
    "href": "CLAUDE.html#quarto-site-structure",
    "title": "Claude Session Context",
    "section": "",
    "text": "_quarto.yml — project config: website type, navbar, SCSS theme (light/dark), TOC, margin references, Open Graph, Twitter Cards\nThemes: styles/modern-light.scss, styles/modern-dark.scss\nIncludes: includes/fonts.html, includes/brand-icon.html, includes/theme-toggle.html, includes/discuss-links.html\n\n\n\n\nNotebooks get their own folder with index.ipynb (or index.qmd for JS demos):\nposts/\n  probability/\n    index.ipynb        # URL becomes /posts/probability/\n    assets/            # images, data files referenced by THIS notebook only\n      venn.png\n      bishop-prob.png\n  votingforcongress/\n    index.ipynb\n    assets/\n      sep7.png\n  earth-demo/\n    index.qmd          # Three.js demo using .qmd format\n    assets/\n      earth-card.png   # Card thumbnail for listing (no content images in post)\nMarkdown posts stay as flat files, images go in shared posts/images/ or posts/data/:\nposts/\n  boxloop.md           # URL becomes /posts/boxloop\n  distributions.md\n  images/              # shared images for flat .md posts\n    2tosscdf.png\n  data/                # shared data for flat .md posts (if needed)\n\n\n\nNotebooks must have a raw cell (cell_type: “raw”) as the first cell with YAML:\n---\ntitle: \"Post Title\"\nsubtitle: \"Catchy one-liner for listing cards.\"\ndescription: \"Two-sentence summary for the post page.\"\ncategories:\n    - probability\n    - statistics\ndate: 2025-01-08\n---\n\nsubtitle appears on listing cards\ndescription is the longer summary\ncategories are used for filtering (must be from _categories.txt)\ndate controls sort order\nimage — optional, for posts without content images (e.g. image: assets/earth-card.png)\n\nFor markdown posts, the same YAML goes in the standard frontmatter block at the top.\n\n\n\nFor posts with no embedded images (e.g. interactive Three.js demos), use the browser agent to screenshot the rendered page, crop to the key visual, save to assets/, and add image: to frontmatter. See skill step 9 for details.\n\n\n\nUse .qmd format (not .ipynb) for JavaScript-heavy interactive content: - Load external JS via format: html: include-in-header: in frontmatter - Use Quarto’s fenced div syntax ::: {.classname} for layout containers - Inline &lt;script&gt; blocks at the end of the .qmd file - Read CSS custom properties (--cb-blue-*, --interactive-*) for theme integration - Example: posts/earth-demo/index.qmd\n\n\n\nmake preview                # Live dev server with hot reload\nmake render                 # Build full site to _site/\nmake build                  # Render + rsync _site/ to docs/ for GitHub Pages\nquarto render posts/probability/index.ipynb  # Render a single post\n\nrender produces _site/; build adds an rsync step to sync changed files into docs/\nCommit and push are intentionally separate from build\n\n\n\n\n\nNotebook posts: assets/filename.png (relative to the notebook’s folder)\nMarkdown posts: images/filename.png (relative to posts/)\nSite-wide assets already in /assets/ (e.g. lawoflargenumbers images) use /assets/... absolute paths\nProfile photo: assets/profile.jpg (JPEG, not PNG — watch for GitHub avatar format mismatch)\n\n\n\n\n\nAll categories must be lowercase — no Statistics, use statistics; no MonteCarlo, use montecarlo\nCanonical list in _categories.txt at project root\nThis prevents duplicate tags in Quarto listing filters\n\n\n\n\n\ntitle — post title\nsubtitle — catchy one-liner shown on listing cards\ndescription — 2-sentence content summary\ncategories — lowercase tags for filtering (from _categories.txt)\ndate — controls sort order (YYYY-MM-DD)\nimage — card thumbnail path (optional, for posts without content images)\n\n\n\n\n\nposts/ — main blog posts (shown on index page)\ntil/ — Today I Learned (shown only on TIL page, NOT on index)\ncollections/software/ — software tools (shown only on Collections page, NOT on index)\nindex.qmd listing contents should only include posts/ and posts/**/\n\n\n\n\n\nAfter adding/moving/renaming files, restart quarto preview — the live server caches resource IDs and will show “Bad resource ID” for changed files\nListing .qmd files must not have a stray --- after the YAML block (causes parse errors)\nListing contents paths should NOT have a leading / (use til/*.qmd not /til/*.qmd)\ninclude-after-body scripts run after DOMContentLoaded — use IIFE, not event listeners\nGitHub avatar downloads are JPEG even with .png URL — always check with file command and use correct extension"
  },
  {
    "objectID": "collections/software/prefect.html",
    "href": "collections/software/prefect.html",
    "title": "Prefect-2.0",
    "section": "",
    "text": "Prefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#why-choose-this-tool",
    "href": "collections/software/prefect.html#why-choose-this-tool",
    "title": "Prefect-2.0",
    "section": "",
    "text": "Prefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#more-about-the-tool",
    "href": "collections/software/prefect.html#more-about-the-tool",
    "title": "Prefect-2.0",
    "section": "More about the tool",
    "text": "More about the tool\nPrefect is organized around the notion of fllows. Flows can have subflows, and both of these can have tasks, but tasks cannot have sub-tasks. Flows have implementation as processes or as docker containers.\n\nflows can be run adhoc\nflows can be scheduled\nother DAG based software such as DVC pipelines, hamilton, and dbt can be run as prefect processes\nprefect does not seem to support event based activation of pipelines, although the ability to create deployments in python can enable us to create some such flow\nprefect is well integrated with dask, which we can then use for hyper-parameter optimizations on our cluster or other such distributed computations"
  },
  {
    "objectID": "collections/software/prefect.html#how-to-install",
    "href": "collections/software/prefect.html#how-to-install",
    "title": "Prefect-2.0",
    "section": "How to install",
    "text": "How to install\npip install -U prefect\nThe prefect orion UI will need proxying out of a cluster."
  },
  {
    "objectID": "collections/software/prefect.html#alternatives",
    "href": "collections/software/prefect.html#alternatives",
    "title": "Prefect-2.0",
    "section": "Alternatives",
    "text": "Alternatives\nSeveral alternatives exist. The old airflow and luigi are still around."
  },
  {
    "objectID": "collections/mysoft/deebase.html",
    "href": "collections/mysoft/deebase.html",
    "title": "DeeBase",
    "section": "",
    "text": "DeeBase is an async SQLAlchemy-based database library with a fastlite-inspired API that makes async database operations simple across multiple backends."
  },
  {
    "objectID": "collections/mysoft/deebase.html#example",
    "href": "collections/mysoft/deebase.html#example",
    "title": "DeeBase",
    "section": "Example",
    "text": "Example\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom deebase import Database, ForeignKey, Text\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    email: str\n    status: str = \"active\"\n\n@dataclass\nclass Post:\n    id: int\n    author_id: ForeignKey[int, \"user\"]\n    title: str\n    content: Text\n    created_at: datetime\n\ndb = Database(\"sqlite+aiosqlite:///blog.db\")\nusers = await db.create(User, pk='id')\nposts = await db.create(Post, pk='id')\n\nalice = await users.insert(User(id=None, name=\"Alice\", email=\"alice@example.com\"))\npost = await posts.insert(Post(\n    id=None, author_id=alice.id, title=\"Hello\",\n    content=\"First post!\", created_at=datetime.now()\n))\n\n# FK navigation\nauthor = await posts.fk.author_id(post)\nprint(author.name)  # \"Alice\"\n\n# Query\nall_posts = await posts()\npost = await posts[1]"
  },
  {
    "objectID": "collections/mysoft/deebase.html#features",
    "href": "collections/mysoft/deebase.html#features",
    "title": "DeeBase",
    "section": "Features",
    "text": "Features\n\nAsync/await first — built on SQLAlchemy 2.0+ with aiosqlite and asyncpg drivers\nErgonomic API — await users[1], await users(), await users.lookup(email=\"...\")\nType safety — optional @dataclass support with IDE autocomplete; or start with plain classes and call .dataclass() later\nRich type system — str, Text, int, float, bool, bytes, dict (JSON), datetime, date, time, Optional[T]\nForeign keys and defaults — ForeignKey[int, \"user\"] type annotation; SQL defaults from class field defaults\nFK navigation — await posts.fk.author_id(post) to fetch parent; await users.get_children(user, \"post\", \"author_id\") for children\nIndexes — simple, composite, unique, and named indexes via Index or create_index()\nFull-text search — BM25-ranked search via SQLite FTS5 and PostgreSQL pg_textsearch with automatic index sync\nViews — db.create_view() for JOINs and CTEs with full query API\nxtra() filtering — create scoped table views that auto-apply filters on all operations including inserts\nTransactions — atomic multi-operation commits with rollback\nDatabase reflection — await db.reflect() to work with existing databases; access tables via db.t.tablename\nCode generation — create_mod_from_tables() exports schemas as Python dataclasses\n6 exception types — NotFoundError, IntegrityError, ValidationError, SchemaError, ConnectionError, InvalidOperationError\nCLI — deebase init, deebase table create, deebase migrate, deebase sql, deebase data, and more\nMigrations — schema migrations with up/down support and version tracking\nFastAPI integration — create_crud_router() auto-generates CRUD endpoints with Swagger docs and FK validation\nAdmin interface — Django-like admin UI at /admin/ via deebase api serve --admin\nValidation layer — shared validation for CLI, admin, and API"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Why Do We Have Seasons?\n\n\nEarth’s 23.5° tilt is the secret — not distance from the Sun.\n\n\n\nvisualization\n\ninteractive\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\nTaming complexity by penalizing parameters.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nValidation and Cross-Validation\n\n\nWhy one split is never enough.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Bounds and the Test Set\n\n\nHow to honestly evaluate what your model has learned.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning With Noise\n\n\nBias, variance, and the tradeoff that haunts every model.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 2, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Without Noise\n\n\nWhat happens when you fit a model to perfect data.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nFind the parameters that make your data most probable.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Statistics\n\n\nFixed parameters, random data — the frequentist creed.\n\n\n\nstatistics\n\nprobability\n\n\n\n\nFeb 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Integration\n\n\nWhen calculus is hard, sample instead.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\nprobability\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBasic Monte Carlo\n\n\nLet randomness do the heavy lifting.\n\n\n\nmontecarlo\n\nprobability\n\nsampling\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nSampling and the Central Limit Theorem\n\n\nWhy everything looks normal in the limit.\n\n\n\nprobability\n\nstatistics\n\nmontecarlo\n\nsampling\n\n\n\n\nFeb 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nExpectations and the Law of Large Numbers\n\n\nWhat you expect is what you get — eventually.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression and Backpropagation\n\n\nFrom MLE classification to computing gradients through layers.\n\n\n\noptimization\n\nmodels\n\nstatistics\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent and SGD\n\n\nOptimization by following the slope downhill.\n\n\n\noptimization\n\nregression\n\nmodels\n\n\n\n\nJan 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom the Normal Model to Regression\n\n\nBuilding a Bayesian regression from Kung San census data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\nsampling\n\n\n\n\nJan 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Regression\n\n\nPutting priors on regression coefficients and updating with data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\nSample where it matters most to compute integrals efficiently.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nRejection Sampling\n\n\nAccept or reject: a simple algorithm for hard distributions.\n\n\n\nsampling\n\nmontecarlo\n\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Inverse Transform\n\n\nTurning uniform random numbers into any distribution you want.\n\n\n\nsampling\n\nprobability\n\n\n\n\nJan 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Normal Model\n\n\nConjugate priors for the workhorse of statistics.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nJan 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nLab: The Beta-Binomial Globe Model\n\n\nGrid approximation, quadratic approximation, and MCMC in practice.\n\n\n\nbayesian\n\nsampling\n\nmodels\n\n\n\n\nJan 17, 2025\n\n\n\n\n\n\n\n\n\n\n\nSufficient Statistics and Exchangeability\n\n\nWhen less data tells you everything, and order doesn’t matter.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions Example: Elections\n\n\nSimulating a presidential election with coin flips.\n\n\n\nprobability\n\nstatistics\n\nelections\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Beta-Binomial Globe Model\n\n\nConjugate priors, Bayesian updating, and decision theory on a globe toss.\n\n\n\nbayesian\n\nprobability\n\nmodels\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nTreating parameters as random variables changes everything.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\nsampling\n\n\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nEntropy and Maximum Entropy\n\n\nQuantifying uncertainty and the distributions it favors.\n\n\n\ninformation-theory\n\nprobability\n\nstatistics\n\n\n\n\nJan 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\nThe shapes that randomness takes.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding AIC\n\n\nAn information-theoretic shortcut for model selection.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nDivergence and Deviance\n\n\nMeasuring how far your model is from the truth.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nConvexity and Jensen’s Inequality\n\n\nWhy the average of a function isn’t the function of the average.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Significance and Size of Effects\n\n\nWhen a drug works, how much does it matter?\n\n\n\nstatistics\n\nregression\n\n\n\n\nJan 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nFrom coin flips to Bayes’ theorem.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\nBuild, compute, critique, repeat.\n\n\n\nmodels\n\nprobability\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\nDoes the president’s party always lose seats in congress?\n\n\n\nelections\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\nFlip enough coins and the truth emerges.\n\n\n\nstatistics\n\nmontecarlo\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\nDon’t make your audience think.\n\n\n\nvisualization\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "collections.html",
    "href": "collections.html",
    "title": "Collections",
    "section": "",
    "text": "Software I’ve Written\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\nDeeBase\n\n\n \n\n\n\n\n\n\nJan 15, 2025\n\n\nHooksett\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\nSoftware I Like\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\nStitchfix Hamilton\n\n\n \n\n\n\n\n\n\nDec 3, 2023\n\n\nAwk\n\n\n \n\n\n\n\n\n\nDec 3, 2022\n\n\nPrefect-2.0\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nRahul’s Lair\n",
    "section": "",
    "text": "Data, Stats, ML and AI\n\n\n\nA collection of notebooks, tutorials, and deep dives into statistical inference, machine learning, AI, and the art of communicating with data."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "\nRahul’s Lair\n",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nWhy Do We Have Seasons?\n\n\nEarth’s 23.5° tilt is the secret — not distance from the Sun.\n\n\n\nvisualization\n\ninteractive\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\nTaming complexity by penalizing parameters.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nValidation and Cross-Validation\n\n\nWhy one split is never enough.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Bounds and the Test Set\n\n\nHow to honestly evaluate what your model has learned.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning With Noise\n\n\nBias, variance, and the tradeoff that haunts every model.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nMar 2, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Without Noise\n\n\nWhat happens when you fit a model to perfect data.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nFeb 28, 2025\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nFind the parameters that make your data most probable.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Statistics\n\n\nFixed parameters, random data — the frequentist creed.\n\n\n\nstatistics\n\nprobability\n\n\n\n\nFeb 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Integration\n\n\nWhen calculus is hard, sample instead.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\nprobability\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBasic Monte Carlo\n\n\nLet randomness do the heavy lifting.\n\n\n\nmontecarlo\n\nprobability\n\nsampling\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nSampling and the Central Limit Theorem\n\n\nWhy everything looks normal in the limit.\n\n\n\nprobability\n\nstatistics\n\nmontecarlo\n\nsampling\n\n\n\n\nFeb 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nExpectations and the Law of Large Numbers\n\n\nWhat you expect is what you get — eventually.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression and Backpropagation\n\n\nFrom MLE classification to computing gradients through layers.\n\n\n\noptimization\n\nmodels\n\nstatistics\n\n\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent and SGD\n\n\nOptimization by following the slope downhill.\n\n\n\noptimization\n\nregression\n\nmodels\n\n\n\n\nJan 24, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom the Normal Model to Regression\n\n\nBuilding a Bayesian regression from Kung San census data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\nsampling\n\n\n\n\nJan 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Regression\n\n\nPutting priors on regression coefficients and updating with data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\nSample where it matters most to compute integrals efficiently.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\n\n\n\nJan 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nRejection Sampling\n\n\nAccept or reject: a simple algorithm for hard distributions.\n\n\n\nsampling\n\nmontecarlo\n\n\n\n\nJan 20, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Inverse Transform\n\n\nTurning uniform random numbers into any distribution you want.\n\n\n\nsampling\n\nprobability\n\n\n\n\nJan 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Normal Model\n\n\nConjugate priors for the workhorse of statistics.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nJan 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nLab: The Beta-Binomial Globe Model\n\n\nGrid approximation, quadratic approximation, and MCMC in practice.\n\n\n\nbayesian\n\nsampling\n\nmodels\n\n\n\n\nJan 17, 2025\n\n\n\n\n\n\n\n\n\n\n\nSufficient Statistics and Exchangeability\n\n\nWhen less data tells you everything, and order doesn’t matter.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nJan 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions Example: Elections\n\n\nSimulating a presidential election with coin flips.\n\n\n\nprobability\n\nstatistics\n\nelections\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Beta-Binomial Globe Model\n\n\nConjugate priors, Bayesian updating, and decision theory on a globe toss.\n\n\n\nbayesian\n\nprobability\n\nmodels\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nTreating parameters as random variables changes everything.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\nsampling\n\n\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nEntropy and Maximum Entropy\n\n\nQuantifying uncertainty and the distributions it favors.\n\n\n\ninformation-theory\n\nprobability\n\nstatistics\n\n\n\n\nJan 13, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\nThe shapes that randomness takes.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding AIC\n\n\nAn information-theoretic shortcut for model selection.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nDivergence and Deviance\n\n\nMeasuring how far your model is from the truth.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\nConvexity and Jensen’s Inequality\n\n\nWhy the average of a function isn’t the function of the average.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 10, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Significance and Size of Effects\n\n\nWhen a drug works, how much does it matter?\n\n\n\nstatistics\n\nregression\n\n\n\n\nJan 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nFrom coin flips to Bayes’ theorem.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\nBuild, compute, critique, repeat.\n\n\n\nmodels\n\nprobability\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\nDoes the president’s party always lose seats in congress?\n\n\n\nelections\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\nFlip enough coins and the truth emerges.\n\n\n\nstatistics\n\nmontecarlo\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\nDon’t make your audience think.\n\n\n\nvisualization\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rejectionsampling/index.html",
    "href": "posts/rejectionsampling/index.html",
    "title": "Rejection Sampling",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(\"Setup Finished\")\n\nSetup Finished"
  },
  {
    "objectID": "posts/rejectionsampling/index.html#basic-rejection-sampling",
    "href": "posts/rejectionsampling/index.html#basic-rejection-sampling",
    "title": "Rejection Sampling",
    "section": "Basic Rejection Sampling",
    "text": "Basic Rejection Sampling\nThe basic idea, come up with by von Neumann is:\nIf you have a function you are trying to sample from, whose functional form is well known, basically accept the sample by generating a uniform random number at any \\(x\\) and accepting it if the value is below the value of the function at that \\(x\\).\nThis is illustrated in the diagram below:\n\n\n\nBasic rejection sampling: points drawn uniformly in the bounding box are accepted if they fall under the target density curve, rejected otherwise.\n\n\n\nThe process\n\nDraw \\(x\\) uniformly from \\([x_{min},\\, x_{max}]\\)\nDraw \\(y\\) uniformly from [0,\\(y_{max}\\)]\nif \\(y\\) &lt; f(\\(x\\)), accept the sample\notherwise reject it\nrepeat\n\nThis works as more samples will be accepted in the regions of \\(x\\)-space where the function \\(f\\) is higher: indeed they will be accepted in the ratio of the height of the function at any given \\(x\\) to \\(y_{max}\\).\nThe reason this all works is the frequentist interpretation of probability in each \\(x\\) sliver As we have more samples the accept-to-total ratio reflects the probablity mass in that sliver better.\n\n\nExample\nThe following code produces samples that follow the distribution \\(P(x)=e^{-x}\\) for \\(x=[0,10]\\) and generates a histogram of the sampled distribution.\n\n\nP = lambda x: np.exp(-x)\n\n# domain limits\nxmin = 0 # the lower limit of our domain\nxmax = 10 # the upper limit of our domain\n\n# range limit (supremum) for y\nymax = 1\n#you might have to do an optimization to find this.\n\nN = 10000 # the total of samples we wish to generate\naccepted = 0 # the number of accepted samples\nsamples = np.zeros(N)\ncount = 0 # the total count of proposals\n\n# generation loop\nwhile (accepted &lt; N):\n    \n    # pick a uniform number on [xmin, xmax) (e.g. 0...10)\n    x = np.random.uniform(xmin, xmax)\n    \n    # pick a uniform number on [0, ymax)\n    y = np.random.uniform(0,ymax)\n    \n    # Do the accept/reject comparison\n    if y &lt; P(x):\n        samples[accepted] = x\n        accepted += 1\n    \n    count +=1\n    \nprint(\"Count\",count, \"Accepted\", accepted)\n\n# get the histogram info\nhinfo = np.histogram(samples,30)\n\n# plot the histogram\nplt.hist(samples,bins=30, label=u'Samples');\n\n# plot our (normalized) function\nxvals=np.linspace(xmin, xmax, 1000)\nplt.plot(xvals, hinfo[0][0]*P(xvals), 'r', label=u'P(x)')\n\n# turn on the legend\nplt.legend();\n\nCount 99359 Accepted 10000\n\n\n\n\n\n\n\n\n\nNotice that \\(y_{max}\\) was just assumed here. In general we might have to do a maximization. This has a cost. We want to keep this cost low, or we might be spending some time there. If the optimization is complex, it might be cheaper to do something else…"
  },
  {
    "objectID": "posts/rejectionsampling/index.html#rejection-sampling-with-steroids",
    "href": "posts/rejectionsampling/index.html#rejection-sampling-with-steroids",
    "title": "Rejection Sampling",
    "section": "Rejection Sampling with Steroids",
    "text": "Rejection Sampling with Steroids\nThe simple rejection sampling method has fundamental problems.\nFor our simple example, it’s quite easy to determine the supremum. In practice, while you may know how to quickly (i.e. constant time) evaluate your function everywhere on the domain of interest, finding a bound very close to the supremum may not be a feasible calculation. In addition, even if you find a tight bound for the supremum, basic rejection sampling will still be very inefficient as you will reject many samples (especially in low density regions).\nFurthermore, if you support is infinitely large, you are not going to be able to reject from an infinitely long box in finite time!\nThis is a hard problem to solve and we will need other techniques to address this problem of low acceptance probability.\nHowever, it is possible to do a more efficient job while still taking advantage of the simplicity of rejection sampling. Our modified technique will introduce a proposal density \\(g(x)\\). This notion of a proposal density is one used in many sampling techniques, in different ways, but its importance will always lie in figuring ways to increase the acceptance rate.\nThe proposal density will have the following characteristics:\n\n\\(g(x)\\) is easy to sample from and (calculate the pdf)\nSome \\(M\\) between 1 and \\(\\infty\\) exists so that \\(M \\, g(x) &gt; f(x)\\) in your entire domain of interest\nideally \\(g(x)\\) will be somewhat close to \\(f\\) so that you’ll sample more in high density regions and much less in low density regions\n\nIts probably obvious that an optimal value for M is the supremum over your domain of interest of \\(f/g\\). At that \\(x\\) you will accept stuff with probability 1. In general you want \\(M\\) as close to 1 as possible, since the probability of acceptance is \\(1/M\\).\nYou can see that this is the case by finding the proportion of samples from \\(g(x)\\) that are accepted at each \\(x\\) and then averaging over \\(x\\):\n\\[\\int dx g(x) prop(x) =  \\int dx g(x) \\frac{f(x)}{Mg(x)} = \\frac{1}{M}\\int dx f(x) = \\frac{1}{M}\\]\nOnce you’ve picked a proposal distribution g, your modified rejection sampling technique is as follows:\n\nDraw \\(x\\) from your proposal distribution \\(g(x)\\)\nDraw \\(y\\) uniformly from [0,1]\nif \\(y\\) &lt; f(\\(x\\))/\\(M g(x)\\), accept the sample\notherwise reject it\nrepeat\n\nThe entire process is illustrated in the diagram below:\n\n\n\nRejection sampling with an envelope: using a scaled proposal distribution M·g(x) that bounds f(x) improves acceptance rates over a uniform bounding box.\n\n\nExample\nThe following code produces samples that follow the distribution \\(P(x)=e^{-x}\\) for \\(x=[0,10]\\) and generates a histogram of the sampled distribution.\n\n\np = lambda x: np.exp(-x)  # our distribution\ng = lambda x: 1/(x+1)  # our proposal pdf (we're thus choosing M to be 1)\ninvCDFg = lambda x: np.log(x +1) # generates our proposal using inverse sampling\n\n# domain limits\nxmin = 0 # the lower limit of our domain\nxmax = 10 # the upper limit of our domain\n\n# range limits for inverse sampling\numin = invCDFg(xmin)\numax = invCDFg(xmax)\n\nN = 10000 # the total of samples we wish to generate\naccepted = 0 # the number of accepted samples\nsamples = np.zeros(N)\ncount = 0 # the total count of proposals\n\n# generation loop\nwhile (accepted &lt; N):\n    \n    # Sample from g using inverse sampling\n    u = np.random.uniform(umin, umax)\n    xproposal = np.exp(u) - 1\n    \n    # pick a uniform number on [0, 1)\n    y = np.random.uniform(0,1)\n    \n    # Do the accept/reject comparison\n    if y &lt; p(xproposal)/g(xproposal):\n        samples[accepted] = xproposal\n        accepted += 1\n    \n    count +=1\n    \nprint(\"Count\", count, \"Accepted\", accepted)\n\n# get the histogram info\nhinfo = np.histogram(samples,50)\n\n# plot the histogram\nplt.hist(samples,bins=50, label=u'Samples');\n\n# plot our (normalized) function\nxvals=np.linspace(xmin, xmax, 1000)\nplt.plot(xvals, hinfo[0][0]*p(xvals), 'r', label=u'p(x)')\nplt.plot(xvals, hinfo[0][0]*g(xvals), 'k', label=u'g(x)')\n\n\n\n# turn on the legend\nplt.legend();\n\nCount 23692 Accepted 10000"
  },
  {
    "objectID": "posts/validation/index.html",
    "href": "posts/validation/index.html",
    "title": "Validation and Cross-Validation",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/validation/index.html#revisiting-the-model",
    "href": "posts/validation/index.html#revisiting-the-model",
    "title": "Validation and Cross-Validation",
    "section": "Revisiting the model",
    "text": "Revisiting the model\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\n\ndffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\n\nx=dffull.rfrac.values\nf=dffull.promney.values\n\n\ndf = pd.read_csv(\"data/noisysample.csv\")\ndf.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\nfrom sklearn.cross_validation import train_test_split\ndatasize=df.shape[0]\n#split dataset using the index, as we have x,f, and y that we want to split.\nitrain,itest = train_test_split(range(30),train_size=24, test_size=6)\nxtrain= df.x[itrain].values\nftrain = df.f[itrain].values\nytrain = df.y[itrain].values\nxtest= df.x[itest].values\nftest = df.f[itest].values\nytest = df.y[itest].values"
  },
  {
    "objectID": "posts/validation/index.html#validation",
    "href": "posts/validation/index.html#validation",
    "title": "Validation and Cross-Validation",
    "section": "Validation",
    "text": "Validation\nA separate validation set is needed because what we have done in picking a given polynomial degree \\(d\\) as the best hypothesis is that we have used the test set as a training set. How?\nOur process used the training set to fit for the parameters(values of the coefficients) of the polynomial of given degree \\(d\\) based on minimizing the traing set error (empirical risk minimization). We then calculated the error on the test set at that \\(d\\). If we go further and choose the best \\(d\\) based on minimizing the test set error, we have then “fit for” \\(d\\) on the test set. We will thus call \\(d\\) a hyperparameter of the model.\nIn this case, the test-set error will underestimate the true out-of-sample error. Furthermore, we have contaminated the test set by fitting for \\(d\\) on it; it is no longer a true test set.\nThus, we must introduce a new validation set on which the complexity parameter \\(d\\) is fit, and leave out a test set which we can use to estimate the true out-of-sample performance of our learner. The place of this set in the scheme of things is shown below:\n\n\n\nSplitting dataset D into training, validation, and test sets\n\n\nWe have split the old training set into a training set and a validation set, holding the old test aside for FINAL testing AFTER we have “fit” for complexity \\(d\\). Obviously we have decreased the size of the data available for training further, but this is a price we must pay for obtaining a good estimate of the out-of-sample risk \\(\\cal{E_{out}}\\) (also denoted as risk \\(R_{out}\\)) through the test risk \\(\\cal{E_{test}}\\) (\\(R_{test}\\)).\n\n\n\nValidation workflow: train, validate, choose hypothesis, retrain, test\n\n\nThe validation process is illustrated in these two figures. We first loop over all the hypothesis sets that we wish to consider: in our case this is a loop over the complexity parameter \\(d\\), the degree of the polynomials we will try and fit. Then for each degree \\(d\\), we obtain a best fit model \\(g^-_d\\) where the “minus” superscript indicates that we fit our model on the new training set which is obtained by removing (“minusing”) a validation chunk (often the same size as the test chunk) from the old training set. We then “test” this model on the validation chunk, obtaining the validation error for the best-fit polynomial coefficients and for degree \\(d\\). We move on to the next degree \\(d\\) and repeat the process, just like before. We compare all the validation set errors, just like we did with the test errors earlier, and pick the degree \\(d_*\\) which minimizes this validation set error.\n\n\n\nLooping over hypothesis sets with validation to select the best model\n\n\nHaving picked the hyperparameter \\(d_\\*\\), we retrain using the hypothesis set \\(\\cal{H_\\*}\\) on the entire old training-set to find the parameters of the polynomial of order \\(d_\\*\\) and the corresponding best fit hypothesis \\(g_\\*\\). Note that we left the minus off the \\(g\\) to indicate that it was trained on the entire old traing set. We now compute the test error on the test set as an estimate of the test risk \\(\\cal{E_{test}}\\).\nThus the validation set is the set on which the hyperparameter is fit. This method of splitting the data \\(\\cal{D}\\) is called the train-validate-test split.\n\nProperties of the validation set\nFirst assume that the validation set is acting like a test set. then, for the same reasons as in the case of a test set, the validation risk or error is an unbiased estimate of the out of sample risk. Secondly, the Hoeffding bound for a validation set is then for the same reason identical to that of the test set.\nMore often though the validation set is used in a model selection process. Here we wish to choose the complexity parameter \\(d\\), something we wrongly already attempted to do on our previous test set.\nNotice that the process of validation consists of fixing \\(d\\) and finding the best fit \\(g^\\*\\) on the training set. We then calculate as many risks as our parameter grid on the validation set with the different fit hypothesis, and choose the \\(d, g^\\*\\) combination with the lowest validation set risk. Now, \\(R_{val}(g^{-\\*}, d^\\*)\\) also has an optimistic bias, and its Hoeffding bound must now take into account the grid-size as the effecting size of the hypothesis space. This size from hyperparameters is typically a smaller size than that from parameters.\nWe finally now retrain on the entire train+validation set using the appropriate \\((g^{-\\*}, d^\\*)\\) combination. This works as training a given model with more data typically reduces the risk even further. (One can show this using learning curves but thats out of our scope).\n\n\nWorking it out\nWe carry out this process for one training/validation split below. Note the smaller size of the new training set. We hold the test set at the same size.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\ndef make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n\n\n#we split the training set down further\nintrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\nxntrain= df.x[intrain].values\nfntrain = df.f[intrain].values\nyntrain = df.y[intrain].values\nxnvalid= df.x[invalid].values\nfnvalid = df.f[invalid].values\nynvalid = df.y[invalid].values\n\ndegrees=range(21)\nerror_train=np.empty(len(degrees))\nerror_valid=np.empty(len(degrees))\ntrainvalidlists=make_features(xntrain, xnvalid, degrees)\n\n#we now train on the smaller training set\nfor d in degrees:#for increasing polynomial degrees 0,1,2...\n    #Create polynomials from x\n    Xntrain = trainvalidlists[d]['train']\n    Xnvalid = trainvalidlists[d]['test']\n    #fit a model linear in polynomial coefficients on the new smaller training set\n    est = LinearRegression()\n    est.fit(Xntrain, yntrain)\n    #predict on new training and validation sets and calculate mean squared error\n    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n\n#calculate the degree at which validation error is minimized\nmindeg = np.argmin(error_valid)\n#need to remake polynomial features on the whole training set\nttlist=make_features(xtrain, xtest, degrees)\nfeatures_at_mindeg = ttlist[mindeg]['train']\ntest_features_at_mindeg = ttlist[mindeg]['test']\n#fit on whole training set now. Put MSE in variable err.\n#your code here\nclf = LinearRegression()\nclf.fit(features_at_mindeg, ytrain) # fit\n#predict on the test set now and calculate error\npred = clf.predict(test_features_at_mindeg)\nerr = mean_squared_error(ytest, pred)\n\n\nplt.plot(degrees, error_train, marker='o', label='train (in-sample)')\nplt.plot(degrees, error_valid, marker='o', label='validation')\nplt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper left')\nplt.yscale(\"log\")\nprint(mindeg)\n\n2\n\n\n\n\n\n\n\n\n\nLets do this again, choosing a new random split between training and validation data:\n\nintrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\nxntrain= df.x[intrain].values\nfntrain = df.f[intrain].values\nyntrain = df.y[intrain].values\nxnvalid= df.x[invalid].values\nfnvalid = df.f[invalid].values\nynvalid = df.y[invalid].values\n\ndegrees=range(21)\nerror_train=np.empty(len(degrees))\nerror_valid=np.empty(len(degrees))\ntrainvalidlists=make_features(xntrain, xnvalid, degrees)\n\nfor d in degrees:#for increasing polynomial degrees 0,1,2...\n    #Create polynomials from x\n    Xntrain = trainvalidlists[d]['train']\n    Xnvalid = trainvalidlists[d]['test']\n    #fit a model linear in polynomial coefficients on the training set\n    est = LinearRegression()\n    est.fit(Xntrain, yntrain)\n    #calculate mean squared error\n    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n\nmindeg = np.argmin(error_valid)\nttlist=make_features(xtrain, xtest, degrees)\nfeatures_at_mindeg = ttlist[mindeg]['train']\ntest_features_at_mindeg = ttlist[mindeg]['test']\nclf = LinearRegression()\nclf.fit(features_at_mindeg, ytrain) # fit\npred = clf.predict(test_features_at_mindeg)\nerr = mean_squared_error(ytest, pred)\n\n\nplt.plot(degrees, error_train, marker='o', label='train (in-sample)')\nplt.plot(degrees, error_valid, marker='o', label='validation')\nplt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='lower left')\nplt.yscale(\"log\")\nprint(mindeg)\n\n2\n\n\n\n\n\n\n\n\n\nThis time the validation error minimizing polynomial degree might change! What happened?"
  },
  {
    "objectID": "posts/validation/index.html#cross-validation",
    "href": "posts/validation/index.html#cross-validation",
    "title": "Validation and Cross-Validation",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nThe problem\n\nSince we are dealing with small data sizes here, you should worry that a given split exposes us to the peculiarity of the data set that got randomly chosen for us. This naturally leads us to want to choose multiple such random splits and somehow average over this process to find the “best” validation minimizing polynomial degree or complexity \\(d\\).\nThe multiple splits process also allows us to get an estimate of how consistent our prediction error is: in other words, just like in the hair example, it gives us a distribution. So far we have been channeling the hair through the bootstrap, but choosing multiple splits is another way to get different training samples..\nFurthermore the validation set that we left out has two competing demands on it. The larger the set is, the better is our estimate of the out-of-sample error. So we’d like to hold out as much as possible. But the smaller the validation set is, the more data we have to train ourmodel on. Thus we can fit a better, more expressive model. We want to balance these two desires, and additionally, not be exposed to any peculiarities that might randomly arise in any single train-validate split of the old training set.\n\n\n\nThe Idea\nTo deal with this we engage in a process called cross-validation, which is illustrated in the figure below, for a given hypothesis set \\(\\cal{H}_a\\) with complexity parameter \\(d=a\\) (the polynomial degree). We do the train/validate split, not once but multiple times.\nIn the figure below we create 4-folds from the training set part of our data set \\(\\cal{D}\\). By this we mean that we divide our set roughly into 4 equal parts. As illustrated below, this can be done in 4 different ways, or folds. In each fold we train a model on 3 of the parts. The model so trained is denoted as \\(g^-_{Fi}\\), for example \\(g^-_{F3}\\) . The minus sign in the superscript once again indicates that we are training on a reduced set. The \\(F3\\) indicates that this model was trained on the third fold. Note that the model trained on each fold will be different!\nFor each fold, after training the model, we calculate the risk or error on the remaining one validation part. We then add the validation errors together from the different folds, and divide by the number of folds to calculate an average error. Note again that this average error is an average over different models \\(g^-_{Fi}\\). We use this error as the validation error for \\(d=a\\) in the validation process described earlier.\n\n\n\nK-fold cross-validation: rotating the validation fold across the dataset\n\n\nNote that the number of folds is equal to the number of splits in the data. For example, if we have 5 splits, there will be 5 folds. To illustrate cross-validation consider below fits in \\(\\cal{H}_0\\) and \\(\\cal{H}_1\\) (means and straight lines) to a sine curve, with only 3 data points.\nWe have described cross-validation here from the perspective of sensibly fitting for the complexity hyperparameter \\(d\\). But we can use it just like a pure validation set as well, just making sure we arent getting strange results due to a wierdly sampled validation set. In that case, (it can also shown that) cross-validation error is an unbiased estimate of the out of sample-error.\nNotice that just like the bootstraps we do in frequentist inference, cross-validation is a re-sampling method. Indeed, a question might be, why not use bootstrap instead. See http://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio , and note that the so-called “out-of-bag” errors from “bagging” in random forests utilizes the bootstrap.\n\n\nThe entire description of K-fold Cross-validation\nWe put thogether this scheme to calculate the error for a given polynomial degree \\(d\\) with the method we used earlier to choose a model given the validation-set risk as a function of \\(d\\):\n\ncreate n_folds partitions of the training data.\nWe then train on n_folds -1 of these partitions, and test on the remaining partition. There are n_folds such combinations of partitions (or folds), and thus we obtain n_fold risks.\nWe average the error or risk of all such combinations to obtain, for each value of \\(d\\), \\(R_{dCV}\\).\nWe move on to the next value of \\(d\\), and repeat 3\nand then find the optimal value of d that minimizes risk \\(d=*\\).\nWe finally use that value to make the final fit in \\(\\cal{H}_*\\) on the entire old training set.\n\n\n\n\nCross-validation over multiple hypothesis sets, then retrain and test\n\n\nLet us now do 4-fold cross-validation on our Romney votes data set. We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split in 4 ways into 4 folds, train on 3 folds, and calculate the validation error on the ramining one. We then average the erros over the four folds to get a cross-validation error for that \\(d\\). Then we did what we did before: find the hypothesis space \\(\\cal{H_*}\\) with the lowest cross-validation error, and refit it using the entire training set. We can then use the test set to estimate \\(E_{out}\\).\n\nfrom sklearn.cross_validation import KFold\nn_folds=4\ndegrees=range(21)\nresults=[]\nfor d in degrees:\n    hypothesisresults=[]\n    for train, test in KFold(24, n_folds): # split data into train/test groups, 4 times\n        tvlist=make_features(xtrain[train], xtrain[test], degrees)\n        clf = LinearRegression()\n        clf.fit(tvlist[d]['train'], ytrain[train]) # fit\n        hypothesisresults.append(mean_squared_error(ytrain[test], clf.predict(tvlist[d]['test']))) # evaluate score function on held-out data\n    results.append((np.mean(hypothesisresults), np.min(hypothesisresults), np.max(hypothesisresults), np.std(hypothesisresults))) # average\n\n\nmindeg = np.argmin([r[0] for r in results])\nttlist=make_features(xtrain, xtest, degrees)\n#fit on whole training set now.\nclf = LinearRegression()\nclf.fit(ttlist[mindeg]['train'], ytrain) # fit\npred = clf.predict(ttlist[mindeg]['test'])\nerr = mean_squared_error(pred, ytest)\nerrtr=mean_squared_error(ytrain, clf.predict(ttlist[mindeg]['train']))\nerrout=0.8*errtr+0.2*err\nc0=sns.color_palette()[0]\nc1=sns.color_palette()[1]\n#plt.errorbar(degrees, [r[0] for r in results], yerr=[r[1] for r in results], marker='o', label='CV error', alpha=0.5)\nplt.plot(degrees, [r[0] for r in results], marker='o', label='CV error', alpha=0.9)\nplt.fill_between(degrees, [r[1] for r in results], [r[2] for r in results], color=c0, alpha=0.2)\n\n\nplt.plot([mindeg], [err], 'o',  label='test set error')\nplt.plot([mindeg], [errout], 'o',  label='full sample error')\n\n\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper right')\nplt.yscale(\"log\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\nWe see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well.\nSo now we have an average out of sample error, matched to the in-sample error, and error bars telling is that the entire order 1-8 polynomial region (roughly) is trustable…\n\n\nWhat does Cross Validation do?\nOne can think about the validation process as one that estimates \\(R_{out}\\) directly, on the validation set. It’s critical use is in the model selection process. Once you do that you can estimate \\(R_{out}\\) using the test set as usual, but now you have also got the benefit of a robust average and error bars.\nOne key subtlety to remember about cross-validation is that in the risk averaging process, you are actually averaging over different \\(g^-\\) models, with different parameters. You arrive at the least risk for the hyperparameter and then refit on the entire training set, which will likely give you slightly different parameters as well."
  },
  {
    "objectID": "posts/importancesampling/index.html",
    "href": "posts/importancesampling/index.html",
    "title": "Importance Sampling",
    "section": "",
    "text": "Importance sampling is directly a method to calculate integrals or expectations, which is one of our main goals at the end of things.\nThe basic idea behind importance sampling is that we want to draw more samples where \\(h(x)\\), a function whose integral or expectation we desire, is large. In the case we are doing an expectation, it would indeed be even better to draw more samples where \\(h(x)f(x)\\) is large, where \\(f(x)\\) is the pdf we are calculating the integral with respect to.\nWhy is this important? Often, in the computation of an expectation or other integral, the integrand has a very small value on a dominant fraction of the whole integration volume. If the points are chosen evenly in the integration volume, the small minority of the points close to the ‘peak’ give the dominant contribution to the integral.\nFor example lets look at the expectation\n\\[ E_f[h] = \\int_V f(x) h(x) dx. \\]\nChoose a distribution \\(g(x)\\), which is close to the function \\(f(x)\\), but which is simple enough so that it is possible to generate random \\(x\\)-values from this distribution. The integral can now be re-written as:\n\\[ E_f[h] = \\int h(x) g(x) \\frac{f(x)}{g(x)} dx \\]\nTherefore if we choose random numbers \\(x_i\\) from distribution \\(g(x)\\), we obtain\n\\[ E_f[h] = \\lim_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{x_{i}\\sim g(.)} h(x_i)\\frac{f(x_i)}{g(x_i)} \\]\nUsually you might have written:\n\\[E_f[h] = \\lim_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{x_{i}\\sim f(.)} h(x_i) \\]\nbut now we have a reweighting with \\(w(x_i) =  \\frac{f(x_i)}{g(x_i)}\\) and the samples are drawn from \\(g(x)\\):\n\\[ E_f[h] = \\lim_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{x_{i}\\sim g(.)} w(x_i) h(x_i) \\]\nUnlike rejection sampling we have used all samples!\nNow remember that the variance of our montecarlo estimate is given to us by\n\\[\\hat{V} = \\frac{V_f[h(x)]}{N}\\]\nwhere \\(N\\) is the sample size.\nWith importance sampling this formula has now changed to\n\\[\\hat{V} = \\frac{V_g[w(x)h(x)]}{N}\\]\nOur game here now is to try and minimize \\(V_g[w(x)h(x)]\\).\nAs a somewhat absurd notion, this variance would be sent to zero, if:\n\\[w(x)h(x) = C \\implies f(x) h(x) = C g(x),\\]\nwhich leads to (since g(x) is a density we must normalize)\n\\[g(x) = \\frac{f(x)h(x)}{\\int f(x) h(x) dx} = \\frac{f(x)h(x)}{E_f[h(x)]}\\]\nThe expectation was what we were trying to estimate in the first place so our tautological absurdity seems to grow..\nBut, ignoring the denominator, this formula tells us that to achieve low variance, we must have \\(g(x)\\) large where the product \\(f(x)h(x)\\) is large. After all, maximizing the latter in some fashion was our original intuition.\nOr to put it another way, \\(\\frac{g(x)}{f(x)}\\) ought to be large where \\(h(x)\\) is large. This means that, as we said earlier, choose more samples near the peak.\nSo now we have the ingredients of our method. We have a \\(f\\) that we might or might not know. We have a pdf \\(g\\) which we choose to be higher than \\(f\\) at the points where \\(h\\) has peaks. Now what we are left to do is to sample from \\(g\\), and this will give us an oversampling at the place \\(h\\) has peaks, and thus we must correct this there by multiplying by weights \\(w  = \\frac{f}{g} \\lt 1\\) in thse places.\nBe careful to choose \\(g(x)\\) appropriately, it should have thicker tails than f, or the ratio \\(f/g\\) will be too big and count contribute too much in the tails.\nAll of these considerations may be seen in the diagram below:\n\n\n\nImportance sampling: a proposal g(x) concentrates samples where the integrand f(x)·h(x) is large, reweighting by f/g to correct the bias.\n\n\nAnother way of seeing this whole thing is that we will draw the sample from a proposal distribution and re-weight the integral appropriately so that the expectation with respect to the correct distribution is used. And since \\(f/g\\) is flatter than \\(f\\), the variance of \\(h \\times f/g\\) is smaller that the variance of \\(h \\times f\\) and therefore the error will be smaller for all \\(N\\).\n\nExample: Calculate $_{0}^{} (x) , x , dx $\nThe function has a shape that is similar to Gaussian and therefore we choose here a Gaussian as importance sampling distribution.\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nmu = 2;\nsig =.7;\n\nf = lambda x: np.sin(x)*x\ninfun = lambda x: np.sin(x)-x*np.cos(x)\np = lambda x: (1/np.sqrt(2*np.pi*sig**2))*np.exp(-(x-mu)**2/(2.0*sig**2))\nnormfun = lambda x:  norm.cdf(x-mu, scale=sig)\n\n\nplt.figure(figsize=(18,8))  # set the figure size\n\n\n# range of integration\nxmax =np.pi \nxmin =0\n\n# Number of draws \nN =1000\n\n# Just want to plot the function\nx=np.linspace(xmin, xmax, 1000)\nplt.subplot(1,2,1)\nplt.plot(x, f(x), 'b', label=u'Original  $x\\sin(x)$')\nplt.plot( x, p(x), 'r', label=u'Importance Sampling Function: Normal')\nplt.plot(x, np.ones(1000)/np.pi,'k')\nxis = mu + sig*np.random.randn(N,1);\nplt.plot(xis, 1/(np.pi*p(xis)),'.', alpha=0.1)\nplt.xlim([0, np.pi])\nplt.ylim([0,2])\nplt.xlabel('x')\nplt.legend()\n# =============================================\n# EXACT SOLUTION \n# =============================================\nIexact = infun(xmax)-infun(xmin)\nprint(\"Exact solution is: \", Iexact)\n\n# ============================================\n# VANILLA MONTE CARLO \n# ============================================\nIvmc = np.zeros(1000)\nfor k in np.arange(0,1000):\n    x = np.random.uniform(low=xmin, high=xmax, size=N)\n    Ivmc[k] = (xmax-xmin)*np.mean( f(x))\n\nprint(\"Mean basic MC estimate: \", np.mean(Ivmc))\nprint(\"Standard deviation of our estimates: \", np.std(Ivmc))\n\n# ============================================\n# IMPORTANCE SAMPLING \n# ============================================\n# CHOOSE Gaussian so it similar to the original functions\n\nIis = np.zeros(1000)\nfor k in np.arange(0,1000):\n    # DRAW FROM THE GAUSSIAN mean =2 std = sqrt(0.4) \n    xis = mu + sig*np.random.randn(N,1);\n    #hist(x)\n    xis = xis[ (xis&lt;xmax) & (xis&gt;xmin)] ;\n\n    # normalization for gaussian from 0..pi\n    normal = normfun(np.pi)-normfun(0);\n\n\n    Iis[k] =np.mean(f(xis)/p(xis))*normal;\n\nprint(\"Mean importance sampling MC estimate: \", np.mean(Iis))\nprint(\"Standard deviation of our estimates: \", np.std(Iis))\nplt.subplot(1,2,2)\nplt.hist(Iis,30, histtype='step', label=u'Importance Sampling');\nplt.hist(Ivmc, 30, color='r',histtype='step', label=u'Vanilla MC');\n \nplt.legend()\n \n \n \n\nExact solution is:  3.14159265359\nMean basic MC estimate:  3.14068341144\nStandard deviation of our estimates:  0.0617743877206\nMean importance sampling MC estimate:  3.14197268362\nStandard deviation of our estimates:  0.0161935244302"
  },
  {
    "objectID": "posts/vizasstory.html",
    "href": "posts/vizasstory.html",
    "title": "Visualization As Story",
    "section": "",
    "text": "There is this pretty famous book by Steve Krug, called “Dont Make Me Think”. Its a call to respect conventions for web elements, such as shopping carts (a cart should be on the upper right), so that the web experience is obvious to users.\n\n\n\nIn visualization, as in web development, your audience does not want to spend cognitive effort on things you could just show them, by convention, or by explicit writing. So, just point out the key facts and insights.\nFor example, in this great article in the financial times https://www.ft.com/content/0f11b219-0f1b-420e-8188-6651d1e749ff?hcb=1, the main point “Vaccines have made Covid-19 far less lethal” is written up-front.\n\n\n\nThe implications are made clear in the second sentence, comparing vaccinated 80 year-olds to un-vaccinated 50 year-olds. This implication is illustrated in the visualization as well, with a horizontal black line, and a caption.\nInstead of point markers, downwards pointing arrows are used on lines to reinforce the notion of lower risk. Captions and annotations are used to point out key insights. Extraneous frames and tick marks are removed.\nThis is an example of framing. It grabs the audience and leads it through the insights you want to share.\n\n\n\nThere’s been a lot of worry about breakthrough vaccination, especially with the news about the Provincetown cluster. Here is another visualization from the same article, telling us why the large number of breakthrough infections are to be expected.\n\n\n\nIt walks us through the entire calculation visually. And does it in two scenarios: high vaccination rates and low vaccination rates. We can ourselves see the larger hospitalization numbers in the low-vaccination scenario.\nThe visualization and explanation could have been framed in terms of base rates and conditional probabilities, but by illustrating the concepts with an example, they are made accessible to everyone. And the framing drives home the story: go get your shot!\nRead more on how to make good visualizations using R in this book by @khealy . If you are a pythonista, learn how to make good plots in @matplotlib using https://end-to-end-machine-learning.teachable.com/p/navigating-matplotlib-tutorial-how-to/ by @_brohrer_ ."
  },
  {
    "objectID": "posts/seasons/index.html",
    "href": "posts/seasons/index.html",
    "title": "Why Do We Have Seasons?",
    "section": "",
    "text": "Seasons aren’t caused by Earth’s distance from the Sun. The secret is Earth’s 23.5° axial tilt. As Earth orbits, different hemispheres tilt toward the Sun, receiving more direct sunlight — and that’s what makes it warm.\nUse the buttons below to jump between seasons and see how the sun’s rays strike Earth differently. The golden rays show parallel sunlight beams; the markers on Earth’s surface show where light is concentrated (direct, small footprint = hot) versus spread out (angled, large footprint = cool)."
  },
  {
    "objectID": "posts/seasons/index.html#how-it-works",
    "href": "posts/seasons/index.html#how-it-works",
    "title": "Why Do We Have Seasons?",
    "section": "How It Works",
    "text": "How It Works\nDirect sunlight concentrates energy in a small area — that’s summer. Angled sunlight spreads the same energy over a larger area — that’s winter. It’s like a flashlight: shine it straight down and you get a bright spot; tilt it and the light spreads out and dims.\nA mind-blowing fact: Earth is actually 3 million miles closer to the Sun during Northern Hemisphere winter! But 3 million out of 93 million is only ~3%. The angle of sunlight matters far more than distance.\nFor a deeper exploration with orbits, speed controls, and camera modes, see the full Earth & Sun Explorer."
  },
  {
    "objectID": "posts/samplingclt/index.html",
    "href": "posts/samplingclt/index.html",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/samplingclt/index.html#samples-from-a-population-of-coin-flips",
    "href": "posts/samplingclt/index.html#samples-from-a-population-of-coin-flips",
    "title": "Sampling and the Central Limit Theorem",
    "section": "Samples from a population of coin flips",
    "text": "Samples from a population of coin flips\nLets do some more coin flips; this time we’ll do them in many replications. We’ll establish some terminology at first.\nWe will do a large set of replications M, in each of which we will do many coin flips N. We’ll call the result of each coin flip an observation, and a single replication a sample of observations. Thus the number of samples is M, and the sample size is N. These samples have been chosen from a population of size \\(n &gt;&gt; N\\).\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\n\ndef make_throws(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.mean(start, axis=1)\n\nWe show the mean over the observations, or sample mean, for a sample size of 10, with 20 replications. There are thus 20 means.\n\nmake_throws(number_of_samples=20, sample_size=10)\n\narray([ 0.5,  0.8,  0.5,  1. ,  0.7,  0.7,  0.6,  0.6,  0.7,  1. ,  0.7,\n        0.4,  0.4,  0.4,  0.4,  0.7,  0.7,  0.5,  0.5,  0.5])\n\n\nLet us now do 200 replications, each of which has a sample size of 1000 flips, and store the 200 means for each sample size from 1 to 1000 in sample_means.\n\nsample_sizes=np.arange(1,1001,1)\nsample_means = [make_throws(number_of_samples=200, sample_size=i) for i in sample_sizes]\n\nLets formalize what we are up to. Lets call the N random variables in the \\(m^{th}\\) sample \\(x_{m1},x_{m2},...,x_{mN}\\) and lets define the sample mean\n\\[\\bar{x_m}(N) = \\frac{1}{N}\\, \\sum_{i=1}^{N} x_{mi} \\]\nNow imagine the size of the sample becoming large, asymptoting to the size of an infinite or very large population (ie the sample becomes the population). Then you would expect the sample mean to approach the mean of the population distribution. This is just a restatement of the law of large numbers.\nOf course, if you drew many different samples of a size N (which is not infinite), the sample means \\(\\bar{x_1}\\), \\(\\bar{x_2}\\), etc would all be a bit different from each other. But the law of large numbers intuitively indicates that as the sample size gets very large and becomes an infinite population size, these slightly differeing means would all come together and converge to the population (or distribution) mean.\nTo see this lets define, instead, the mean or expectation of the sample means over the set of samples or replications, at a sample size N:\n\\[E_{\\{R\\}}(\\bar{x}) = \\frac{1}{M} \\,\\sum_{m=1}^{M} \\bar{x_m}(N) ,\\] where \\(\\{R\\}\\) is the set of M replications, and calculate and plot this quantity.\n\nmean_of_sample_means = [np.mean(means) for means in sample_means]\n\n\nplt.plot(sample_sizes, mean_of_sample_means);\nplt.ylim([0.480,0.520]);\n\n\n\n\n\n\n\n\nNot surprisingly, the mean of the sample means converges to the distribution mean as the sample size N gets very large."
  },
  {
    "objectID": "posts/samplingclt/index.html#the-notion-of-a-sampling-distribution",
    "href": "posts/samplingclt/index.html#the-notion-of-a-sampling-distribution",
    "title": "Sampling and the Central Limit Theorem",
    "section": "The notion of a Sampling Distribution",
    "text": "The notion of a Sampling Distribution\nIn data science, we are always interested in understanding the world from incomplete data, in other words from a sample or a few samples of a population at large. Our experience with the world tells us that even if we are able to repeat an experiment or process, we will get more or less different answers the next time. If all of the answers were very different each time, we would never be able to make any predictions.\nBut some kind of answers differ only a little, especially as we get to larger sample sizes. So the important question then becomes one of the distribution of these quantities from sample to sample, also known as a sampling distribution.\nSince, in the real world, we see only one sample, this distribution helps us do inference, or figure the uncertainty of the estimates of quantities we are interested in. If we can somehow cook up samples just somewhat different from the one we were given, we can calculate quantities of interest, such as the mean on each one of these samples. By seeing how these means vary from one sample to the other, we can say how typical the mean in the sample we were given is, and whats the uncertainty range of this quantity. This is why the mean of the sample means is an interesting quantity; it characterizes the sampling distribution of the mean, or the distribution of sample means.\nWe can see this mathematically by writing the mean or expectation value of the sample means thus:\n\\[E_{\\{R\\}}(N\\,\\bar{x}) = E_{\\{R\\}}(x_1 + x_2 + ... + x_N) = E_{\\{R\\}}(x_1) + E_{\\{R\\}}(x_2) + ... + E_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the expectations in the right hand side can be replaced by the population mean using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nE_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\mu\\\\\nE_{\\{R\\}}(\\bar{x}) &=& \\mu\n\\end{eqnarray}\\]\nwhich tells us that in the limit of a large number of replications the expectation value of the sampling means converges to the population mean. This limit gives us the true sampling distribution, as opposed to what we might estimate from our finite set of replicates. (Thus there our \\(E_{\\{R\\}}\\) would be replaced by some \\(E_{fs}\\) where by \\(fs\\) we wish to indicate the pmf or density of the sampling distribution).\n\nThe sampling distribution as a function of sample size\nWe can see what the estimated sampling distribution of the mean looks like at different sample sizes.\n\nsample_means_at_size_10=sample_means[9]\nsample_means_at_size_100=sample_means[99]\nsample_means_at_size_1000=sample_means[999]\n\n\nplt.hist(sample_means_at_size_10, bins=np.arange(0,1,0.01), alpha=0.5);\nplt.hist(sample_means_at_size_100, bins=np.arange(0,1,0.01), alpha=0.4);\nplt.hist(sample_means_at_size_1000, bins=np.arange(0,1,0.01), alpha=0.3);\n\n\n\n\n\n\n\n\nThe distribution is much tighter at large sample sizes, and that you can have way low and way large means at small sample sizes. Indeed there are means as small as 0.1 at a sample size of 10, and as small as 0.3 at a sample size of 100.\nLets plot the distribution of the mean as a function of sample size.\n\nfor i in sample_sizes:\n    if i %50 ==0 and i &lt; 1000:\n        plt.scatter([i]*200, sample_means[i], alpha=0.05);\nplt.xlim([0,1000])\nplt.ylim([0.25,0.75]);\n\n\n\n\n\n\n\n\n\n\nThe variation of the sample mean\nLet the underlying distribution from which we have drawn our samples have, additionally to a well defined mean \\(\\mu\\), a well defined variance \\(\\sigma^2\\).\nThen, as before:\n\\[V_{\\{R\\}}(N\\,\\bar{x}) = V_{\\{R\\}}(x_1 + x_2 + ... + x_N) = V_{\\{R\\}}(x_1) + V_{\\{R\\}}(x_2) + ... + V_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the variances in the right hand side can be replaced by the population variance using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nV_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\sigma^2\\\\\nV(\\bar{x}) &=& \\frac{\\sigma^2}{N}\n\\end{eqnarray}\\]\nThis simple formula is called De-Moivre’s formula, and explains the tell-tale triangular plot we saw above, with lots of variation at low sample sizes turning into a tight distribution at large sample size(N).\nThe square root of \\(V\\), or the standard deviation of the sampling distribution of the mean (in other words, the distribution of sample means) is also called the Standard Error.\nWe can obtain the standard deviation of the sampling distribution of the mean at different sample sizes and plot it against the sample size, to confirm the \\(1/\\sqrt(N)\\) behaviour.\n\nstd_of_sample_means = [np.std(means) for means in sample_means]\n\n\nplt.plot(np.log10(sample_sizes), np.log10(std_of_sample_means));\n\n\n\n\n\n\n\n\nLet us plot again the distribution of sample means at a large sample size, \\(N=1000\\). What distribution is this?\n\nplt.hist(sample_means_at_size_1000, bins=np.arange(0.4,0.6,0.002));\n\n\n\n\n\n\n\n\nLets step back and try and think about what this all means. As an example, say I have a weight-watchers’ study of 1000 people, whose average weight is 150 lbs with standard deviation of 30lbs. If I was to randomly choose many samples of 100 people each, the mean weights of those samples would cluster around 150lbs with a standard error of 30/\\(\\sqrt{100}\\) = 3lbs. Now if i gave you a different sample of 100 people with an average weight of 170lbs, this weight would be more than 6 standard errors beyond the population mean, 1 and would thus be very unlikely to be from the weight watchers group.\n1 this example is motivated by the crazy bus example in Charles Whelan’s excellent Naked Statistics Book\n\nThe Gaussian Distribution\nWe saw in the last section that the sampling distribution of the mean itself has a mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\). This distribution is called the Gaussian or Normal Distribution, and is probably the most important distribution in all of statistics.\nThe probability density of the normal distribution is given as:\n\\[ N(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{(x-\\mu)^2}{2s^2} } .\\]\nThe expected value of the Gaussian distribution is \\(E[X]=\\mu\\) and the variance is \\(Var[X]=s^2\\).\n\nnorm =  sp.stats.norm\nx = np.linspace(-5,5, num=200)\n\n\nfig = plt.figure(figsize=(12,6))\nfor mu, sigma, c in zip([0.5]*3, [0.2, 0.5, 0.8], sns.color_palette()[:3]):\n    plt.plot(x, norm.pdf(x, mu, sigma), lw=2, \n             c=c, label = r\"$\\mu = {0:.1f}, s={1:.1f}$\".format(mu, sigma))\n    plt.fill_between(x, norm.pdf(x, mu, sigma), color=c, alpha = .4)\n    \n    \nplt.xlim([-5,5])\nplt.legend(loc=0)\nplt.ylabel(\"PDF at $x$\")\nplt.xlabel(\"$x$\");\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/samplingclt/index.html#the-central-limit-theorem",
    "href": "posts/samplingclt/index.html#the-central-limit-theorem",
    "title": "Sampling and the Central Limit Theorem",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nThe reason for the distribution’s importance is the Central Limit Theorem(CLT). The theorem is stated as thus, very similar to the law of large numbers:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nIn other words:\n\\[s^2 = \\frac{\\sigma^2}{N}.\\]\nThis is true, regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nStrictly speaking, under some conditions called Lyapunov conditions, the variables \\(x_i\\) dont have to be identically distributed, as long as \\(\\mu\\) is the mean of the means and \\(\\sigma^2\\) is the sum of the individual variances. This has major consequences, for the importance of this theorem.\nMany random variables can be thought of as having come from the sum of a large number of small and independent effects. For example human height or weight can be thought of as the sum as a large number of genetic and environmental factors, which add to increase or decrease height or weight respectively. Or think of a measurement of a height. There are lots of ways things could go wrong: frayed tapes, stretched tapes, smudged marks, bad lining up of the eye, etc. These are all independent and have no systematic error in one direction or the other.\nThen the sum of these factors, as long as there are a large number of them, will be distributed as a gaussian.[this has nothing to do with the sampling distribution of the mean but is part of the origin story of the gaussian distribution]\nAs a rule of thumb, the CLT starts holding at \\(N \\sim 30\\).\n\nWhat does this all mean?\nThe sample mean, or mean of the random variables \\(x_{mi}\\) in the sample \\(m\\), has a sampling distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\), as shown before. Now for large sample sizes we can go further and use the CLT theorem to say that this distribution is the normal distribution,\n\\[S_N \\sim N(\\mu, \\frac{\\sigma^2}{N})\\].\nThe preciseness of saying that we have a gaussian is a huge gain in our expository power. For example, for the case of the weight-watchers program above, a separation of 20lbs is more than 3 standard errors away, which corresponds to being way in the tail of a gaussian distribution. Because we can now quantify the area under the curve, we can say that 99.7% of the sample means lie within 9lbs of 150. Thus you can way easily reject the possibility that the new sample is from the weight-watchers program with 99.7% confidence.\nIndeed, the CLT allows us to take the reduction in variance we get from large samples, and make statements in different cases that are quite strong:\n\nif we know a lot about the population, and randomly sampled 100 points from it, the sample mean would be with 99.7% confidence within \\(0.3\\sigma\\) of the population mean. And thus, if \\(\\sigma\\) is small, the sample mean is quite representative of the population mean.\nThe reverse: if we have a well sampled 100 data points, we could make strong statements about the population as a whole. This is indeed how election polling and other sampling works.\nwe can infer, as we just did, if a sample is consistent with a population\nby the same token, you can compare two samples and infer if they are from the same population."
  },
  {
    "objectID": "posts/samplingclt/index.html#the-sampling-distribution-of-the-variance",
    "href": "posts/samplingclt/index.html#the-sampling-distribution-of-the-variance",
    "title": "Sampling and the Central Limit Theorem",
    "section": "The sampling distribution of the Variance",
    "text": "The sampling distribution of the Variance\nAt this point you might be curious about what the sampling distribution of the variance looks like, and what can we surmise from it about the variance of the entire sample. We can do this, just like we did for the means. We’ll stick with a high number of replicates and plot the mean of the sample variances as well as the truish sampling distribution of the variances at a sample size of 100.\n\ndef make_throws_var(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.var(start, axis=1)\nsample_vars_1000_replicates = [make_throws_var(number_of_samples=1000, sample_size=i) for i in sample_sizes]\nmean_of_sample_vars_1000 = [np.mean(vars) for vars in sample_vars_1000_replicates]\nplt.plot(sample_sizes, mean_of_sample_vars_1000);\nplt.xscale(\"log\");\n\n\n\n\n\n\n\n\nThe “mean sample variance” asymptotes to the true variance of 0.25 by a sample size of 100.\nHow well does the sample variance estimate the true variance?\nIf \\(V_m\\) denotes the variance of a sample,\n\\[ N\\,V_m = \\sum_{i=1}^{N} (x_{mi} - \\bar{x_m})^2 = \\sum_{i=1}^{N}(x_{mi} - \\mu)^2 - N\\,(\\bar{x_m} - \\mu)^2. \\]\nThen \\[E_{\\{R\\}}(N\\,V_m) = E_{\\{R\\}}(\\sum_{i=1}^{N}(x_{mi} - \\mu)^2) - E_{\\{R\\}}(N\\,(\\bar{x_m} - \\mu)^2)\\] In the asymptotic limit of a very large number of replicates, we can then write \\[E(N\\,V) = N\\,\\sigma^2 - \\sigma^2, \\] and thus we have \\[E(V) = \\frac{N-1}{N} \\,\\sigma^2\\].\nIn other words, the expected value of the sample variance is LESS than the actual variance. This should not be surprising: consider for example a sample of size 1 from the population. There is zero variance! More generally, whenever you sample a population, you tend to pick the more likely members of the population, and so the variance in the sample is less than the variance in the population.\nAn interesting application of this idea, as Shalizi points out in http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/, is that the loss of variability due to sampling of genes is indeed the origin of genetic drift. More prosaically, the fact that the above graph of expected sample variance against sample size asymptotes to 0.25 is as \\(\\frac{N-1}{N}\\) if very close to 1 at large N.\nOr put another way, you ought to correct your sample variances by a factor of \\(\\frac{n}{n-1}\\) to estimate the population variance, which itself works as the sampling distribution of the sample variance is rather tight, as seen below.\nThat is, defining the sample variance with \\(n-1\\) in the denominator instead of \\(n\\) gives you an unbiased eatimator of the true variance. This is why, for example, Pandas will do this by default for series and dataframes. (numpy wont, so beware!).\n\nplt.hist(sample_vars_1000_replicates[99], bins=np.arange(0.2,0.26,0.001), alpha=0.2, normed=True);"
  },
  {
    "objectID": "posts/inversetransform/index.html",
    "href": "posts/inversetransform/index.html",
    "title": "The Inverse Transform",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(\"Setup Finished\")\n\nSetup Finished"
  },
  {
    "objectID": "posts/inversetransform/index.html#the-idea",
    "href": "posts/inversetransform/index.html#the-idea",
    "title": "The Inverse Transform",
    "section": "The idea",
    "text": "The idea\nThe basic idea behind the inverse transform method is to transform uniform samples into samples from a different distribution. That is, by somehow drawing from a uniform distribution, we make it possible to draw from the other distribution in question.\nAt first glance this seems to be a quixotic quest, but the key observation is this: the CDF of a distribution is a function that ranges from 0 to 1. Now assume you use \\[Uniform(0,1)\\] to generate a random number, say 0.63. Now map this number on the range (or y-axis) to a x using the CDF curve to generate a sample. This process is illustrated below:\n\n\n\nThe inverse transform method: uniform samples (left axis of CDF, right panel) are mapped through the inverse CDF to produce samples from the target distribution (left panel).\n\n\nThe right hand side image is the CDF while the left hand side is the pdf we want to sample from.\nNotice that we randomly choose some samples from a uniform on the right hand side image and these correspond to x’s for the samples from the CDF. On the left hand side we can see on the pdf the samples that these correspond to. If you sample from the uniform you will get more samples in the steep part of the cdf as the steep part of the cdf covers a good part of the probability values between 0 and 1. And thus you will get more samples in the higher parts of the pdf than elsewhere.\nClearly, for all this to work, we must be able to invert the cdf function, so that we can invert a uniform sample to get an \\(x\\)."
  },
  {
    "objectID": "posts/inversetransform/index.html#let-us-formalize-this",
    "href": "posts/inversetransform/index.html#let-us-formalize-this",
    "title": "The Inverse Transform",
    "section": "Let us formalize this:",
    "text": "Let us formalize this:\nThis is the process:\n\nget a uniform sample \\(u\\) from \\(Unif(0,1)\\)\nsolve for \\(x\\) yielding a new equation \\(x=F^{-1}(u)\\) where \\(F\\) is the CDF of the distribution we desire.\nrepeat.\n\nWhy does this work?\nFirst note that:\n$F^{-1}(u) = $ smallest x such that \\(F(x) &gt;=u\\)\nWhat distribution does random variable \\(y = F^{-1}(u)\\) follow?\nThe CDF of y is \\(p(y &lt;= x)\\). Since F is monotonic, we can without loss of generality write:\n\\[p(y &lt;= x) = p(F(y) &lt;= F(x)) = p(u &lt;= F(x)) = F(x)\\]\nThus we get the CDF and hence the pdf that we want to sample from!\n\nExample: Draw from the distribution \\(f(x) \\sim \\exp{(-x)}\\)\nFor example, lets assume we would like to generate random numbers that follow the exponential distribution \\(f(x) = \\frac{1}{\\lambda} e^{-x/\\lambda}\\) for \\(x\\ge0\\) and \\(f(x)=0\\) otherwise. Following the recipe from above\n\\[ u = \\int_{0}^{x} \\frac{1}{\\lambda} e^{-x'/\\lambda} dx'  = 1- e^{-x/\\lambda} \\]\nSolving for \\(x\\) \\[ x = - \\lambda \\ln (1-u) \\]\nNow we want the exponential with \\(\\lambda = 1\\). The following code will produce numbers that follow this \\(\\exp{(-x)}\\) distribution. The figure generated by code below shows the resulting histogram of the generated numbers compared to the actual \\(\\exp{(-x)}\\).\n\n# probability distribution we're trying to calculate\np = lambda x: np.exp(-x)\n\n# CDF of p\nCDF = lambda x: 1-np.exp(-x)\n\n# invert the CDF\ninvCDF = lambda r: -np.log(1-r)\n\n# domain limits\nxmin = 0 # the lower limit of our domain\nxmax = 6 # the upper limit of our domain\n\n# range limits\nrmin = CDF(xmin)\nrmax = CDF(xmax)\n\nN = 10000 # the total of samples we wish to generate\n\n# generate uniform samples in our range then invert the CDF\n# to get samples of our target distribution\nR = np.random.uniform(rmin, rmax, N)\nX = invCDF(R)\n\n# get the histogram info\nhinfo = np.histogram(X,100)\n\n# plot the histogram\nplt.hist(X,bins=100, label=u'Samples');\n\n# plot our (normalized) function\nxvals=np.linspace(xmin, xmax, 1000)\nplt.plot(xvals, hinfo[0][0]*p(xvals), 'r', label=u'p(x)')\n\n# turn on the legend\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nBox-Muller algorithm\nIn many cases the integral to calculate the CDF may not be easy to calculate analytically and we need to come with clever algorithms. For example there is no closed form formula for the integral of the normal distribution $ I= _{-}^{x} e{-x’2/2}dx’ $.\nThe Box Muller method is a brilliant trick to overcome this by producing two independent standard normals from two independent uniforms.\nThe idea is this:\nConsider (without loss of generality) the product of two independent normals N(0,1):\n\\[ X \\sim N(0,1), Y \\sim N(0,1) \\implies X,Y \\sim N(0,1)N(0,1)\\]\nThe pdf then is:\n\\[f_{XY}(x,y)  =  \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\times \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2} = \\frac{1}{2\\pi} \\times e^{-r^2/2}\\]\nwhere \\(r^2 = x^2 + y^2\\).\nIf you think of this in terms of polar co-ordinates \\(r\\) and \\(\\theta\\), we have\n\\[\\Theta \\sim Unif(0, 2\\pi),  S = R^2 \\sim Exp(1/2)\\]\nFrom the inverse method for the exponential above:\n\\[ s = r^2 = -2 ln(1-u) \\]\nwhere u is a sample from \\(U \\sim Unif(0,1)\\). Now if \\(U \\sim Unif(0,1)\\), then \\(1-U \\sim Unif(0,1)\\).\nThus we can write:\n\\[r = \\sqrt{-2\\,ln(u_1)}, \\theta = 2\\pi\\, u_2\\]\nwhere \\(u_1\\) and \\(u_2\\) are both drawn from a \\(Unif(0,1)\\)s.\nNow we can use:\n\\[x = r\\,cos\\theta, y = r\\,sin\\theta\\]\nto generate samples for the normally distributed random variables \\(x\\) and \\(Y\\).\nWe’ve hand-waved around a bit here in this derivation, so let us ask, what is the pdf in polar co-ordinates? Lets make a few observations.\n\nclearly, no matter what co-ordinates we use \\(\\int dF =1\\). In other words, no matter how we add slivers, the probabilities in these slivers must add to 1.\none can think of cartesian co-ordinate slivers being histogram skyscrapers on a regular grid. In polar co-ordinates the slivers are arranged radially\nWe have in terms of the pdfs:\n\n\\[\\int dx dy f(x,y) = \\int dr d\\theta f2(r, \\theta) = \\int dr d\\theta f2r(r)\\, f2t(\\theta)\\]\nAnd we have seen:\nfpolar() =Unif(0, 2)\nWe might be tempted to think that \\(f2r(r) = e^{-r^2/2}\\). But this is not correct on two counts. First, its not even dimensionally right. Secondly, then you transform the \\(dxdy\\) to polar , you get \\(rdrd\\theta\\).\nWhat this means is that :\n\\[f2r(r) = re^{-r^2/2}\\]\nThis is called the Raleigh distribution.\nAnd now you can see how the transformation \\(s=r^2\\) gives us an exponential in s. And this is why we could take \\(R^2 \\sim Exp(1/2)\\) without much ado..the exponential happily normalizes out to 1 dur to the \\(r\\) multiplying the exponential in the pdf above.\nMore generally, if \\(z=g(x)\\) so that \\(x=g^{-1}(z)\\), let us define the Jacobian \\(J(z)\\) of the transformation \\(x=g^{-1}(z)\\) as the partial derivatives matrix of the transformation.\nThen:\n\\[f_Z(z) = f_X(g^{-1}(z)) \\times det(J(z))\\]\nWe can work this out with \\(z\\) the polar co-ordinates and \\(g^{-1}\\) as \\(x=r\\,cos(\\theta)\\) and \\(y=r\\,sin(\\theta)\\), with \\(g\\) as \\(r=\\sqrt{x^2 + y^2}\\), \\(tan(\\theta) = y/x\\).\n\\[ J =  \\binom{cos(\\theta)\\:sin(\\theta)}{-r sin(\\theta)\\:r cos(\\theta)}\\]\nwhose determinant is \\(r\\), and thus\n\\[f_{R, \\Theta}(r, \\theta) = f_{X,Y}(r cos(\\theta), r sin(\\theta)) \\times r =  \\frac{1}{\\sqrt{2\\pi}} e^{-(r cos(\\theta))^2/2} \\times \\frac{1}{\\sqrt{2\\pi}} e^{-(r sin(\\theta))^2/2} = \\frac{1}{2\\pi} \\times e^{-r^2/2} \\times r\\]."
  },
  {
    "objectID": "posts/divergence/index.html",
    "href": "posts/divergence/index.html",
    "title": "Divergence and Deviance",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/divergence/index.html#the-problem-of-learning",
    "href": "posts/divergence/index.html#the-problem-of-learning",
    "title": "Divergence and Deviance",
    "section": "The problem of learning",
    "text": "The problem of learning\nWe’ve seen cross-validation as a way of minimizing a loss (cost, error, or risk) on the training set, and then obtaining the final model on our validation set, with the possible fitting of a hyperparameter.\nWhat we have done here is, to choose a particular model from a hypothesis set, based on a cost minimization criterion.\nThe basic idea in doing that was to find the out-of-sample, or population loss. Since we showed that we can bound it to within the validation (and test) loss using Hoeffding’s inequality, we can use the latter losses as proxy.\nAnd we have wanted to avoid overfitting, which is, as McElreath calls it, the tendency of the model to get over-excited by the training sample.\nWe also seen regularization in this context. In this case we choose a more complex model than we would have otherwise, but use cross-validation on a changed cost function which then bounds the set of admissible functions from the more complex model.\nWe also saw these issues in the context of supervized learning, where we were trying to solve a classification or regression problem.\nIn the realm of probabilistic models, both supervized learning and unsupervized learning boil down to probability density estimation. For supervized learning we want to find \\(p(y\\vert x)\\) or \\(p(x,y)\\) and in unsupervized learning, we wish to find \\(p(x)\\).\nIn these cases, the problem could be cast in the following form: suppose nature has a true “population” distribution \\(p(x)\\). As usual I am given a sample, and make my effort learning a distribution from this sample, \\(q(x)\\). Our question then is: how good did i do? And what additional uncertainty did I introduce by using \\(q\\) instead of \\(p\\)?"
  },
  {
    "objectID": "posts/divergence/index.html#information-theory-kl-divergence",
    "href": "posts/divergence/index.html#information-theory-kl-divergence",
    "title": "Divergence and Deviance",
    "section": "Information Theory: KL Divergence",
    "text": "Information Theory: KL Divergence\nIn other words, if \\(p\\) is nature’s distribution, we want to know how far we are from “perfect accuracy” by using \\(q\\). In other words we need to develop a distance scale for distances between distributions.\nThis scale is called the Kullback-Leibler (KL) Divergence, introduced in 1951. It is defined thus:\n\\[\\renewcommand{\\kld}{D_{KL}}\\]\n\\[\\kld(p, q) = E_p[log(p) - log(q)] = E_p[log(p/q)] = \\sum_i p_i log(\\frac{p_i}{q_i}) \\,\\,or\\, \\int dP log(\\frac{p}{q})\\]\nThe distance between a distribution and itself is clearly \\(\\kld(p,p) = 0\\).\nWe can use Jensen’s inequality for expectations on a convex function \\(f(x)\\),\n\\[ E[f(X)] \\ge f(E[X]) \\]\nto show that \\(\\kld(p,q) \\ge 0\\) with equality iff (if and only if) \\(q=p\\).\n\\[\\kld(p,q) = E_p[log(p/q)] = E_p[-log(q/p)] \\ge -\\log \\left( E_p[q/p] \\right) = -\\log(\\int dQ) = 0\\]\nwhere we have used the fact that \\(-log(x)\\) is a convex function, and that \\(q(x)\\) normalizes to a distribution. Infact, since \\(-\\log(x)\\) is strictly convex, the equality only happens if \\(q(x) = p(x)\\) for ALL x.\nThus we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x). In frequentist statistics, the KL-divergence is related to the maximum likelihood, in Bayesian statistics the KL divergence can be used as a measure of the information gain in moving from a prior to posterior (with a common goal in Bayesian experimental design to maximise the expected KL divergence between the prior and the posterior). The divergence is also used to understand mutual information in clustering, and in variational bayesian inference.\n\nA simple example\nConsider a Bernoulli distribution with probability parameter \\(p=0.3\\). This is a discrete distribution, defined at 0 and 1. Consider using another Bernoulli with parameter \\(q\\) to approximate it. You can see that the divergence is 0 for \\(q=0.3\\) and always higher for any other \\(q\\).\n\np=0.3\n\ndef kld(p,q):\n    return p*np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n\nqs=np.linspace(0,1,100)\nplt.plot(qs, [kld(0.3,q) for q in qs]);\n\n\n\n\n\n\n\n\n\n\nRelationship to Entropy\nIf one defines the Cross-Entropy:\n\\[H(p, q) = - E_p[log(q)]\\]\nThen one can write:\n\\[\\kld(p, q) = H(p,q) - H(p) \\]\nSo one can think of the KL-Divergence as the additional entropy introduced by using \\(q\\) instead of \\(p\\).\nNotice that \\(H(p,q)\\) and \\(\\kld(p, q)\\) is not symmetric. This is by design, and indeed is important. The interpretation is that if you use a unusual , low entropy distribution to approximate a usual one, you will be more surprised than if you used a high entropy, many choices one to approximate an unusual one. An example from McElreath provides some intuition: if you went to Mars from Earth you would be less suprised than the other way: Martians have only seen very dry..we’ve seen it all.\nA corollary here is that if we use a high entropy distribution to aproximate the true one, we will incur lesser error."
  },
  {
    "objectID": "posts/divergence/index.html#likelihoods-and-model-comparison",
    "href": "posts/divergence/index.html#likelihoods-and-model-comparison",
    "title": "Divergence and Deviance",
    "section": "Likelihoods and model comparison",
    "text": "Likelihoods and model comparison\nWhen we minimize risk or maximize likelihood, we do it by taking a sum of risks on a point wise basis, or by multiplying likelihood distributions on a point wise basis.\nWe have not really justified that yet, but we do it because its (a) intuitive and (b) we have an intuitive justification at the back of our mind of using the law of large numbers on a sample.\nThat is, we approximate the true population distribution \\(p\\) by a sample-based empirical distribution:\n\\[\\hat{p} = \\frac{1}{N}\\sum_i \\delta (x - x_i),\\]\nwhere we have used the dirac delta function. This is just another way of replacing population integrals by sample sums or averages.\nThe point here is that we dont know \\(p\\), or else why would be doing this in the first place?\n\nMaximum Likelihood justification\n\\[\\kld(p, q) = E_p[log(p/q)] = \\frac{1}{N}\\sum_i (log(p_i) - log(q_i)\\]\nThus minimizing the KL-divergence involves maximizing \\(\\sum_i log(q_i)\\) which is exactly the log likelihood. Hence we can justify the maximum likelihood principle.\n\n\nComparing Models\nBy the same token we can use the KL-Divergences of two different models to do model comparison:\n\\[\\kld(p, q) -\\kld(p, r) = H(p, q) - H(p, r) = E_p[log(r) - log(q)] = E_p[log(\\frac{r}{q})]\\]\nIn the sample approximation we have:\n\\[\\kld(p, q) -\\kld(p, r) = \\frac{1}{N} \\sum_i log(\\frac{r_i}{q_i}) = \\frac{1}{N} log(\\frac{\\prod_i r_i}{\\prod_i q_i}) =  \\frac{1}{N}log(\\frac{\\cal{L}_r}{\\cal{L}_q})\\]\nThis ratio inside the brackets on the right is the likelihood ratio and is used to test goodness of fit. You can read more about it in Wasserman."
  },
  {
    "objectID": "posts/divergence/index.html#from-divergence-to-deviance",
    "href": "posts/divergence/index.html#from-divergence-to-deviance",
    "title": "Divergence and Deviance",
    "section": "From Divergence to Deviance",
    "text": "From Divergence to Deviance\nIf you look at the expression above, you notice that to compare a model with distribution \\(r\\) to one with distribution \\(q\\), you only need the sample averages of the logarithm of \\(r\\) and \\(q\\):\n\\[\\kld(p, q) -\\kld(p, r) = \\langle log(r) \\rangle - \\langle log(q) \\rangle\\]\nwhere the angled brackets mean sample average. If we define the deviance:\n\\[D(q) = -2 \\sum_i log(q_i)\\],\nthen\n\\[\\kld(p, q) -\\kld(p, r) = \\frac{2}{N} (D(q) - D(r))\\]\nso that we can use the deviance’s for model comparison instead. Indeed, this is what we will do, starting in the frequentist realm and moving onto the bayesian realm.\nNotice that deviance is just a negative log likelihood, or risk.\n(Notice that even though we used likelihoods in the last section, I have been vague about the word distribution here. In Bayesian stats we use the posterior averaged likelihood distribution (posterior predictive) instead to do such comparisons.)\n\nBut we are still in-sample\nWe spent a lot of time in machine learning figuring out how to learn out of sample. However, all the machinery developed here has made no mention of it. When we use the empirical distribution and sample quantities here we are working with our training sample.\nClearly we can calculate deviance on the validation and test samples as well to remedy this issue. And the results will be similar to what we found with machine learning, with the training deviance decreasing with complexity and the testing deviance increasing. McElreath has a plot of this for data generated from a gaussian with standard deviation 1 and means:\n\\[\\mu_i = 0.15 x_{1,i} - 0.4 x_{2,i}\\]\nThe deviances in-sample and out-of sample, at 10,000 simulations for each model type, for two sample sizes are shown below.\n\n\n\nIn-sample vs. out-of-sample deviance as model complexity increases, for N=20 and N=100. From McElreath, Statistical Rethinking.\n\n\nNotice:\n\nthe best fit model may not be the original generating model. Remember that the choice of fit depends on the amount of data you have and the less data you have, the less parameters you should use\non average, out of sample deviance must be larger than in-sample deviance, through an individual pair may have that order reversed because of sample peculiarity.\n\nNow when one plots the mean deviances together, we see an interesting phenomenon:\n\n\n\nAIC approximation (dashed) to out-of-sample deviance: the gap between in-sample and out-of-sample deviance grows roughly as twice the number of parameters. From McElreath, Statistical Rethinking.\n\n\nThe test set deviances are \\(2*p\\) above the training set ones, approximately, where \\(p\\) is the number of parameters in the model.\nThis observation leads to an estimate of the out-of-sample deviance by what is called an information criterion, the Akake Information Criterion, or AIC:\n\\[AIC = D_{train} + 2p\\]\nwhich does carry as an assumption the notion that the likelihood is approximately multivariate gaussian, which as we have seen will be true near its peak.\nThis is just a penalized log-likelihood or risk if we choose to identify our distribution with the likelihood, and at higher numbers of parameters, increases the out-of-sample deviance, making them less desirable. In a sense, this penalization is a simple form of regularization on our model.\nWe wont derive the AIC here, but if you are interested, see http://www.stat.cmu.edu/~larry/=stat705/Lecture16.pdf\nWhy would we want to use such information criteria? Cross validation can be expensive, especially with multiple hyper-parameters. We will have more to say about informatiom criterion when we figure how to do model selection in the bayesian context."
  },
  {
    "objectID": "posts/normalmodel/index.html",
    "href": "posts/normalmodel/index.html",
    "title": "The Normal Model",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pylab as plt \nimport seaborn as sn\n\nfrom scipy.stats import norm\nA random variable \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Thus its density is given by :\n\\[ p(y \\vert \\mu, \\sigma^2) =  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{y-\\mu}{2 \\sigma})^2} \\]\nSuppose our model is \\(\\{y_1, \\ldots, y_n \\vert \\mu, \\sigma^2 \\} \\sim N(\\mu, \\sigma^2)\\) then the likelihood is\n\\[\np(y_1, \\ldots, y_n \\vert \\mu, \\sigma^2) =\n\\prod_{i=1}^{n} p(y_i \\vert \\mu, \\sigma^2)=\\prod_{i=1}^{n}  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{(y_i-\\mu)^2}{2\\sigma^2})} =\n\\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}}   \\exp \\left\\{  - \\frac{1}{2}  \\sum_i \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right\\}\n\\]\nWe can now write the posterior for this model thus:\n\\[ p( \\mu, \\sigma^2 \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{ - \\frac{1}{2\\sigma^2} \\sum (y_i - \\mu)^2 } \\, p(\\mu, \\sigma^2)\\]\nLets see the posterior of \\(\\mu\\) assuming we know \\(\\sigma^2\\)."
  },
  {
    "objectID": "posts/normalmodel/index.html#normal-model-for-fixed-sigma",
    "href": "posts/normalmodel/index.html#normal-model-for-fixed-sigma",
    "title": "The Normal Model",
    "section": "Normal Model for fixed \\(\\sigma\\)",
    "text": "Normal Model for fixed \\(\\sigma\\)\nNow we wish to condition on a known \\(\\sigma^2\\). The prior probability distribution for it can then be written as:\n\\[p(\\sigma^2) = \\delta(\\sigma^2 -\\sigma_0^2)\\]\n(which does integrate to 1).\nNow, keeping in mind that \\(p(\\mu, \\sigma^2) = p(\\mu \\vert \\sigma^2) p(\\sigma^2)\\) and carrying out the integral over \\(\\sigma^2\\) which because of the delta distribution means that we must just substitute \\(\\sigma_0^2\\) in, we get:\n\\[ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2 = \\sigma_0^2)  \\propto p(\\mu \\vert \\sigma^2=\\sigma_0^2) \\,e^{ - \\frac{1}{2\\sigma_0^2} \\sum (y_i - \\mu)^2 }\\]\nwhere I have dropped the \\(\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\) factor as there is no stochasticity in it (its fixed).\nSay we have the prior\n\\[ p(\\mu \\vert \\sigma^2) = \\exp \\left\\{ -\\frac{1}{2 \\tau^2} (\\hat{\\mu}-\\mu)^2 \\right\\} \\]\nthen it can be shown that the posterior is\n\\[  p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2) \\propto \\exp \\left\\{ -\\frac{a}{2} (\\mu-b/a)^2 \\right\\} \\] where \\[ a = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma_0^2} , \\;\\;\\;\\;\\; b = \\frac{\\hat{\\mu}}{\\tau^2} + \\frac{\\sum y_i}{\\sigma_0^2} \\] This is a normal density curve with \\(1/\\sqrt{a}\\) playing the role of the standard deviation and \\(b/a\\) playing the role of the mean. Re-writing this,\n\\[ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{\\mu-b/a}{1/\\sqrt(a)}\\right)^2 \\right\\} \\]\nThe conjugate of the normal is the normal itself.\nDefine $= ^2 / ^2 $ to be the variance of the sample model in units of variance of our prior belief (prior distribution) then the posterior mean is\n\\[\\mu_p = \\frac{b}{a} = \\frac{ \\kappa}{\\kappa + n }  \\hat{\\mu} + \\frac{n}{\\kappa + n} \\bar{y} \\]\nwhich is a weighted average of prior mean and sampling mean. The variance is\n\\[ \\sigma_p^2 = \\frac{1}{1/\\tau^2+n/\\sigma^2} \\] or better\n\\[ \\frac{1}{\\sigma_p^2} = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}. \\]\nYou can see that as \\(n\\) increases, the data dominates the prior and the posterior mean approaches the data mean, with the posterior distribution narrowing…"
  },
  {
    "objectID": "posts/normalmodel/index.html#example-of-the-normal-model-for-fixed-sigma",
    "href": "posts/normalmodel/index.html#example-of-the-normal-model-for-fixed-sigma",
    "title": "The Normal Model",
    "section": "Example of the normal model for fixed \\(\\sigma\\)",
    "text": "Example of the normal model for fixed \\(\\sigma\\)\nWe have data on the wing length in millimeters of a nine members of a particular species of moth. We wish to make inferences from those measurements on the population mean \\(\\mu\\). Other studies show the wing length to be around 19 mm. We also know that the length must be positive. We can choose a prior that is normal and most of the density is above zero (\\(\\mu=19.5,\\tau=10\\)). This is only a marginally informative prior.\nMany bayesians would prefer you choose relatively uninformative (and thus weakly regularizing) priors. This keeps the posterior in-line (it really does help a sampler remain in important regions), but does not add too much information into the problem.\nThe measurements were: 16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8 giving \\(\\bar{y}=18.14\\).\n\nY = [16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8]\n#Data Quantities\nsig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\nmu_data = np.mean(Y)\nn = len(Y)\nprint(\"sigma\", sig, \"mu\", mu_data, \"n\", n)\n\nsigma 1.33092374864 mu 18.1444444444 n 9\n\n\n\n# Prior mean\nmu_prior = 19.5\n# prior std\ntau = 10 \n\n\nkappa = sig**2 / tau**2\nsig_post =np.sqrt(1./( 1./tau**2 + n/sig**2));\n# posterior mean\nmu_post = kappa / (kappa + n) *mu_prior + n/(kappa+n)* mu_data\nprint(\"mu post\", mu_post, \"sig_post\", sig_post)\n\nmu post 18.1471071751 sig_post 0.443205311006\n\n\n\n#samples\nN = 15000\ntheta_prior = np.random.normal(loc=mu_prior, scale=tau, size=N);\ntheta_post = np.random.normal(loc=mu_post, scale=sig_post, size=N);\n\n\nplt.hist(theta_post, bins=30, alpha=0.9, label=\"posterior\");\nplt.hist(theta_prior, bins=30, alpha=0.2, label=\"prior\");\n#plt.xlim([10, 30])\nplt.xlabel(\"wing length (mm)\")\nplt.ylabel(\"Number of samples\")\nplt.legend();\n\n\n\n\n\n\n\n\nIn the case that we dont know \\(\\sigma^2\\) or wont estimate it the way we did above, it turns out that a conjugate prior for the precision (inverse variance) is a gamma distribution. Interested folks can see Murphy’s detailed document here. but you can always just use our MH machinery to draw from any vaguely informative prior for the variance ( a gamma for the precision or even for the variance)."
  },
  {
    "objectID": "posts/entropy/index.html",
    "href": "posts/entropy/index.html",
    "title": "Entropy and Maximum Entropy",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/entropy/index.html#information-and-entropy",
    "href": "posts/entropy/index.html#information-and-entropy",
    "title": "Entropy and Maximum Entropy",
    "section": "Information and Entropy",
    "text": "Information and Entropy\nImagine tossing a coin. If I knew the exact physics of the coin, the initial conditions of the tossing, I could predict deterministically how the coin would land. But usually, without knowing anything, I say by symmetry or by “long-run” experince that the odds of getting heads are 50%.\nThis reflects my minimal knowledge about this system. Without knowing anything else about the universe…the physics, the humidity, the weighting of the coin, etc..assuming a probability of 0.5 of getting heads is the most conservative thing I could do.\nYou can think of this as a situation with minimal information content.\nIt is a very interesting situation, however, when you think about this from the perspective of the multiple ways you could get half the coin tosses in a long run of coin tosses come up heads. There is only one way in which you can get all coin tosses come up heads. There are \\(n \\choose n/2\\) ways on the other hand in which you can get half heads! You can think of this situation being one of more states or events being consistent with a probability of heads of 0.5 than with a probability of 1.\nBy the same token, an election with a win probability of 0.99 isnt that interesting. A lot of information went into getting this probability presumably: polls, economic modelling, etc. But to get such certainty implies a greater determinism-to-randomness ratio in the process.\nOne can think of information as the reduction in uncertainty from learning an outcome\nClearly we need a measure of uncertainty so that we can quantify how much it has decreased.\n\nDesiderata for a measure of uncertainty\n\nmust be continuous so that there are no jumps\nmust be additive across events or states, and must increase as the number of events/states increases"
  },
  {
    "objectID": "posts/entropy/index.html#entropy-measures-uncertainty",
    "href": "posts/entropy/index.html#entropy-measures-uncertainty",
    "title": "Entropy and Maximum Entropy",
    "section": "Entropy measures uncertainty",
    "text": "Entropy measures uncertainty\nA function that satisfies these desiderata is the information entropy:\n\\[H(p) = - E_p[log(p)] = - \\int p(x) log(p(x))dx \\,\\,\\,OR\\, - \\sum_i p_i log(p_i) \\]\nThus the entropy is the average log probability of an event…\n\nExample of the coin toss or Bernoulli variable\n\\[H(p) = - E_p[log(p)] = - p*log(p) - (1-p)*log(1-p)\\]\nFor \\(p=0\\) or \\(p=1\\) we must use L’Hospital’s rule: if we have the division of two limits as \\(0/0\\) or \\(\\infty/\\infty\\) then differentiate both the numerator and denominator and try again:\n\\[\\lim_{p \\to 0}  \\frac{log(p)}{1/p} =  \\lim_{p \\to 0}  \\frac{1/p}{-1/p^2} = 0\\]\n\nimport math\np = np.linspace(0,1,100)\ndef h(p):\n    if p==1.:\n        ent = 0\n    elif p==0.:\n        ent = 0\n    else:\n        ent = - (p*math.log(p) + (1-p)* math.log(1-p))\n    return ent\nplt.plot(p, [h(pr) for pr in p]);\nplt.axvline(0.5, 0, 1,'r')\n\n\n\n\n\n\n\n\nThus you can see there is maximal uncertainty at 0.5."
  },
  {
    "objectID": "posts/entropy/index.html#thermodynamic-notion-of-entropy",
    "href": "posts/entropy/index.html#thermodynamic-notion-of-entropy",
    "title": "Entropy and Maximum Entropy",
    "section": "Thermodynamic notion of Entropy",
    "text": "Thermodynamic notion of Entropy\nImagine dividing \\(N\\) objects amongst \\(M\\) bins. One can think of this as stone tossing, where we toss N stones and see in which bin they land up. There is a distribution for this, \\(\\{p_i\\}\\), of-course, so lets see what it is.\nThere are \\(N\\) ways to fill the first bin, \\(N-1\\) ways to fill the second, \\(N-2\\) ways to fill the third, and so on…thus \\(N!\\) ways. Since we dont distinguish the arrangement of objects in each bin we must divide bu the factorial of the bin amounts. If we then assume a uniform chance of landing in each bucket, then we just get the nultinomial distribution:\n\\[P(n_1, n_2, ..., n_M) = \\frac{N!}{\\prod_{i} n_i!} \\prod_i (\\frac{1}{M})^{n_i} = \\frac{N!}{\\prod_{i} n_i!} \\left(\\frac{1}{M}\\right)^N\\]\n\\[ W =  \\frac{N!}{\\prod_{i} n_i!} \\]\nis called the multiplicity and the entropy is then defined as:\n\\[H = \\frac{1}{N} log(W)\\] which is:\n\\[\\frac{1}{N}log(P(n_i, n_2, ...,n_M))\\]\nwith a constant term removed.\n\\[H = \\frac{1}{N} log(N!) - \\frac{1}{N} \\sum_i log(n_i!)\\].\nUsing Stirling’s approximation \\(log(N!) \\sim Nlog(N) -N\\) as \\(N \\to \\infty\\) and where the fractions \\(n_i/N\\) are held fixed:\n\\[ H =  \\frac{1}{N}\\left( N log(N) - N - \\sum_i (n_i log(n_i) - n_i)\\right)\\]\n\\[ = log(N) -1 -\\frac{1}{N} \\sum_i (Np_i log(Np_i) - Np_i) = log(N) -1 - \\sum_i \\left(p_i(log(N) + log(p_i)) - p_i\\right)\\]\nThus\n\\[H = -\\sum_i p_i log(p_i)\\]\nIf the probabilities of landing in each bucket are not equal, ie not uniform, then we can show:\n\\[\\frac{1}{N}log(P(n_i, n_2, ...,n_M)) = -\\sum_i p_i log(\\frac{p_i}{q_i})\\]\nThis definition has origins in statistical mechanics. Entropy was first introduced in thermodynamics and then later interpreted as a measure of disorder: how many events or states can a system constrained to have a given enrgy have. A physicist calls a particular arrangement \\(\\{n_i\\} = (m_1, n_2, n_3,...,n_M)\\) a microstate and the overall distribution of \\(\\{p_i\\}\\), here the multinomial , a macrostate, with \\(W\\) calledthe weight."
  },
  {
    "objectID": "posts/entropy/index.html#maximum-entropy-maxent",
    "href": "posts/entropy/index.html#maximum-entropy-maxent",
    "title": "Entropy and Maximum Entropy",
    "section": "Maximum Entropy (maxent)",
    "text": "Maximum Entropy (maxent)\nMaximum entropy is the notion of finding distributions consistent with constraints and the current state of our knowledge . In other words, what would be the least surprising distribution? The one with the least additional assumptions?\nWe can maximize\n\\[H = -\\sum_i p_i log(p_i)\\]\nin the case of the ball and bin model above, by considering the langrange-multiplier enhanced, constraint enforcing entropy\n\\[H = -\\sum_i p_i log(p_i) + \\lambda \\left( \\sum_i p(x_i) - 1 \\right)\\]\n\\[\\frac{\\partial H}{\\partial p_j} = 0 \\implies -(1+log(p_j)) + \\lambda = 0\\]\nThis means that the \\(p_j\\)’s are all equal and thus must be \\(\\frac{1}{M}\\): thus the distribution with all \\(p\\)s equal maximizes entropy.\nThe distribution that can happen in the most ways is the one with the highest entropy, as we can see above.\n\nNormal as maxent\nThe origin story of the gaussian itself is that many small effects add up to produce them. It is exactly the “many” aspect os these that makes the gaussian a maxent distribution. For every sequence that produces an unbalanced outcome(like a long string of heads), there are many more ways of producing a balanced outcome. In otherwords, there are so many microstates of the system that can produce the “peak” macrostates.\nThis is a plot from McElreath of a bunch of generalized normal distributions. with same mean and variance. The Gaussuan has the highest entropy, as we shall prove below.\n\n\n\nGeneralized normal distributions (left) and their entropy as a function of shape parameter (right). The Gaussian (shape=2) has maximum entropy among distributions with fixed variance.\n\n\nIf you think about entropy increasing as we make a distribution flatter, you realize that the shape must come about because finite and equal variance puts a limit on how wide the distribution can be.\n\\[\\renewcommand{kld}{D_{KL}}\\]\nFor a gaussian\n\\[p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(x - \\mu)^2/2\\sigma^2}\\]\n\\[H(p) = E_p[log(p)] = E_p[-\\frac{1}{2}log(2\\pi\\sigma^2) - (x - \\mu)^2/2\\sigma^2]\\]\n\\[ =  -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}E_p[(x - \\mu)^2] = -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2} = \\frac{1}{2}log(2\\pi e \\sigma^2)\\]\nNo other distribution \\(q\\) can have higher entropy than this, provided they share the same variance and mean.\nTo see this consider (note change in order, we are considering \\(\\kld(q, p)\\):\n\\[\\kld(q, p) = E_q[log(q/p)] = H(q,p) - H(q)\\]\n\\[H(q,p) = E_q[log(p)] = E_q[-\\frac{1}{2}log(2\\pi\\sigma^2) - (x - \\mu)^2/2\\sigma^2] \\\\= -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}E_q[(x - \\mu)^2]\\]\nThe second expectation here is the variance \\(\\s\nigma^2\\) on the assumption that \\(E_q[x] = \\mu\\).\nThus\n\\[H(q,p) =  -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2} =  -\\frac{1}{2}log(2\\pi e \\sigma^2) = H(p)\\]\nNow as we have shown \\(\\kld(q,p) &gt;=0\\). This means that \\(H(q,p) - H(q) &gt;= 0\\). Which then means that \\(H(p) - H(q) &gt;= 0\\) or \\(H(p) &gt;= H(q)\\). This means that the Gaussian has the highest entropy of any distribution with the same mean and variance.\nSee http://www.math.uconn.edu/~kconrad/blurbs/analysis/entropypost.pdf for details on maxent for distributions.\n\n\nBinomial as Maxent\nInformation entropy increases as a probability distribution becomes more even. We saw that with the thermodynamic idea of entropy and the multinomial distribution.\nConsider the situation when:\n\nonly two outcomes (unordered) are possible.\nthe process generating the outcomes is invariant in time, ie the expected value remains constant (over temporal or other subsequences)\n\nThen it turns out that for these constraints, the maximum entropy distribution is the binomial. The binomial basically spreads probability out as evenly and conservatively as possible, making sure that outcomes that have many more ways they can happen have more probability mass. Basically the binomial figures the number of ways any possible sequence of data can be realized, which is what entropy does. Thus it turns out that likelihoods derived by such counting turn out to be maximum entropy likelihoods.\n\\[H(q,p) &gt;= H(q) \\implies -E_q[log(p)] &gt;= -E_q[log(q)]\\]\nFor binomial parameter \\(\\lambda/n\\):\n\\[ H(q, p) = - \\sum_i q_i log(p_i) = -\\sum_i q_i \\left(log \\left(\\frac{\\lambda}{n}\\right)^{x_i}  + log \\left(\\frac{n-\\lambda}{n}\\right)^{n - x_i} \\right)\\]\n\\[ =  - \\sum_i q_i \\left( x_i log\\left(\\frac{\\lambda}{n}\\right) + (n - x_i) log \\left(\\frac{n-\\lambda}{n}\\right)\\right)\\]\n\\[ =  - \\sum_i q_i  \\left( x_i log \\left(\\frac{\\lambda}{n-\\lambda}\\right)  + n log \\left(\\frac{n-\\lambda}{\\lambda}\\right) \\right)\\]\n\\[ H (q, p) =  - n log \\left(\\frac{n-\\lambda}{\\lambda}\\right) -  log\\left(\\frac{\\lambda}{n-\\lambda}\\right)E_q[x]\\]\nNow, if \\(E_q[x] = \\lambda\\), our invariant expectation, we have \\(H(q,p) = H(p)\\) as we get the same formula if we substitute \\(q=p\\) to get the entropy of the binomial. In other words, \\(H(p) &gt;= H(q)\\) and we have shown the binomial has maximum entropy amongst discrete distributions with two outcomes and fixed expectations."
  },
  {
    "objectID": "posts/entropy/index.html#the-importance-of-maxent",
    "href": "posts/entropy/index.html#the-importance-of-maxent",
    "title": "Entropy and Maximum Entropy",
    "section": "The importance of maxent",
    "text": "The importance of maxent\nThe most common distributions used as likelihoods (and priors) in modeling are those in the exponential family. The exponential family can be defined as having pmf or pdf:\n\\[p(x|\\theta) =  \\frac{1}{Z(\\theta)} h(x) e^{\\theta^T\\phi(x)}\\]\nWhere \\(Z(\\theta)\\), also called the partition function, is the normalization.\nFor example, the univariate Gaussian Distribution can be obtained with:\n\\[\n\\begin{eqnarray}\n\\theta &=& \\begin{pmatrix}\\mu/\\sigma^2 \\\\-1/2\\sigma^2\\end{pmatrix}\\\\\n\\phi(x) &=&  \\begin{pmatrix}x \\\\x^2\\end{pmatrix}\\\\\nZ(\\mu, \\sigma^2) &=& \\sigma\\sqrt{2\\pi} e^{\\mu^2/2\\sigma^2}\\\\\nh(x) &=& 1\n\\end{eqnarray}\n\\]\nEach member of the exponential family turns out to be a maximum entropy distribution subject to different constraints. These distributions are then used as likelihoods.\n\n\n\nRelationships among members of the exponential family: Gamma, Normal, Binomial, and Poisson distributions arise as limiting cases of the Exponential distribution.\n\n\nFor example, the gamma distribution, which we shall see later, is maximum entropy amongst all distributions with the same mean and same average logarithm. The poisson distribution, used for low event rates, is maxent under similar conditions as the binomial as it is a special case of the binomial. The exponential distribution is maxent among all non-negative continuous distributions with the same average inter-event displacement. (In our births example, the inter-birth time).\nWe’ll talk more about these distributions when we encounter them, and when we talk about generalized linear models.\nBut here is the critical point. We will often choose a maximum entropy distribution as a likelihood . Information entropy ennumerates the number of ways a distribution can arise, after having fixed some assumptions. Thus, in choosing a MAXENT distribution as a likelihood, we choose a distribution that once the constraints has been met, does not contain any additional assumptions. It is thus the most conservative distribution we could choose consistent with our constraints."
  },
  {
    "objectID": "posts/frequentist.html",
    "href": "posts/frequentist.html",
    "title": "Frequentist Statistics",
    "section": "",
    "text": "\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]"
  },
  {
    "objectID": "posts/frequentist.html#what-is-data",
    "href": "posts/frequentist.html#what-is-data",
    "title": "Frequentist Statistics",
    "section": "What is data?",
    "text": "What is data?\nWhat is data? Frequentist statistics is one answer to this philosophical question. It treats data as a sample from an existing population.\nThis notion is probably clearest to you from elections, where some companies like Zogby or CNN take polls. The sample in these polls maybe a 1000 people, but they “represent” the electoral population at large. We attempt to draw inferences about how the population will vote based on these samples.\nWe model the sample we have. This model typically involves having some kind of distribution, some kind of algorithm, some kind of story that characterizes the data. These descriptions have, usually, some parameters which need estimation.\nFrequentist analysis considers these parameters as fixed and data as varying (stochastic), with our data as one possible sample from the population.\n\nThe Data story\nData analysis involves coming up with a story of how the data came to be. This may be a causal story, or a descriptive one (correlational, associative). The critical point is this:\nThe story must be sufficient to specify an algorithm to simulate new data. This is the model we have been talking about: a formal probability model. And once we have pinned it down from our existing sample, using a method such as Maximum Likelihood Estimation talked about below, we can use it as a generating mechanism.\nConsider, for example, tossing a globe in the air and catching it. When you catch it mark whats under your right index finger: W for water, L for land.\nLets say you toss the globe 10 times and get something like WLWWWLWlWW. We wish to analyze this experiment to figure how much of the earth is covered in water (according to the globe, at any rate!).\nLet us say that our model is:\n\nThe true proportion of water is \\(p\\).\nWe use this as a Bernoulli probability for each globe toss, where \\(p\\) is thus the probability that you get a W. This assumption is one of being Identically Distributed.\nEach globe toss is Independent of the other.\n\nAssumptions 2 and 3 taken together are called IID, or Independent and Identially Distributed Data."
  },
  {
    "objectID": "posts/frequentist.html#a-probabilistic-model",
    "href": "posts/frequentist.html#a-probabilistic-model",
    "title": "Frequentist Statistics",
    "section": "A probabilistic model",
    "text": "A probabilistic model\n(from the data story)\nThe components of the model depend upon what kind of statistical analysis we are doing. For Frequentist analysis, the components are:\n\nThe likelihood, or the plausibility of the data under the model\nand the parameters which go into this plausibility.\n\nFor our example, we begin by enumerating the events. These are W and L. There’s nothing else.\nThen we consider N such tosses and ask the question, how often would we see Ws.\nThis given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k \\mid n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\nBinomial distribution for various values of p\n\n\nNow we use a method to fit our model and find the parameter \\(p\\), or rather, the estimate \\(\\hat{p}\\) that we can obtain from our sample. Once we have that, we can use the Binomial distribution to generate new samples.\nNote that there is a problem with this, in that we dont know the true value \\(p^*\\) of the globe-toss model (speaking in the frequentist paradigm). Thus we are generating new samples from our estimate, rather than our true value."
  },
  {
    "objectID": "posts/frequentist.html#maximum-likelihood-estimation",
    "href": "posts/frequentist.html#maximum-likelihood-estimation",
    "title": "Frequentist Statistics",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nOne of the techniques used to estimate parameters in frequentist statistics, from the data in a given sample, is maximum likelihood estimation. Briefly, the idea behind it is:\nThe likelihood for IID data \\(x_1,...,x_n\\), is the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i | \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nAn image can explain this better. We want to choose the distribution that maximizes the product of the vertical lines. Here the blue does better, but it is not clear if the blue is the best.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nOften it is easier and numerically more stable to maximize the log likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nNotice that the definition here is a bit different from that for the question of the globe tosses above: there the data was \\(k\\), the number of W tosses and not the exact order, and so we formulate the question in that form.\nSo dont follow the formula blindly, but think of (a) what is the data, and (b) what is the data generating mechanism!\n\nMLE for binomial.\nThere:\n\\[P(X = k \\mid n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nSo:\n\\[\\ell = log({n\\choose k}) + k log(p) + (n-k) log(1-p)\\]\nDifferentiating with respect to \\(p\\) and setting to 0 yields:\n\\[\\frac{d\\ell}{dp} = \\frac{k}{p}  - \\frac{n -k}{1-p} = 0\\]\nwhich gives us:\n\\[p_{MLE} = \\frac{k}{n}\\]\nwhich you might have intuitively expected."
  },
  {
    "objectID": "posts/frequentist.html#point-estimates",
    "href": "posts/frequentist.html#point-estimates",
    "title": "Frequentist Statistics",
    "section": "Point estimates",
    "text": "Point estimates\nIn frequentist statistics, the data we have in hand, is viewed as a sample from a population. So if we want to calculate some quantity of the population, like say the mean, we estimate it on the sample.\nThis is because we’ve been given only one sample. Ideally we’d want to see the population, but we have no such luck.\nThe parameter estimate is computed by applying an estimator \\(F\\) to the sample data \\(D\\), so \\(\\est{\\mu} = F(D)\\).\nThe parameter is viewed as fixed and the data as random, which is the exact opposite of the Bayesian approach which you will learn later in this class.\nIf you assume that your model describes the true generating process for the data, then there is some true \\(\\trueval{\\mu}\\) . We dont know this. The best we can do to start with is to estimate the \\(\\est{\\mu}\\) from the data set we have.\n\nFrom single to multiple estimates\nNow, imagine that I let you peek at the entire population in this way: I gave you some M data sets drawn from the population, and you can now find \\(\\mu\\) on each such dataset, of which the one we have here is one. So, we’d have M estimates of the \\(\\mu\\).\nThus if we had many replications of this data set: that is, an ensemble of data sets, for example, we can compute other \\(\\est{\\mu}\\), and begin to construct what is called the sampling distribution of \\(\\mu\\).\nBut we dont."
  },
  {
    "objectID": "posts/frequentist.html#sampling-distribution-of-the-parameter",
    "href": "posts/frequentist.html#sampling-distribution-of-the-parameter",
    "title": "Frequentist Statistics",
    "section": "Sampling Distribution of the parameter",
    "text": "Sampling Distribution of the parameter\nWhat you are doing is sampling M Data Sets \\(D_i\\) from the true population. We will now calculate M \\(\\est{\\mu}_i\\), one for each dataset. As we let \\(M \\rightarrow \\infty\\), the distribution induced on \\(\\est{\\mu}\\) is the sampling distribution of the estimator.\nOur estimation could be of anything, even for example the \\(\\lambda\\) we were tying to find with MLE (F would be the MLE estimation process).\nWe can use the sampling distribution to put confidence intervals on the estimation of the parameters, for example."
  },
  {
    "objectID": "posts/frequentist.html#bootstrap",
    "href": "posts/frequentist.html#bootstrap",
    "title": "Frequentist Statistics",
    "section": "Bootstrap",
    "text": "Bootstrap\nBootstrap tries to approximate our sampling distribution. If we knew the true parameters of the population, we could generate M fake datasets. Then we could compute the parameter (or another estimator) on each one of these, to get a empirical sampling distribution of the parameter or estimator.\n\nParametric Bootstrap\nBut we dont have the true parameter. So we generate these samples, using the parameter we calculated. This is the parametric bootstrap. The process is illustrated in the diagram below, taken from Shalizi:\n\n\n\nThe parametric bootstrap process\n\n\nThere are 3 sources of error with respect to the sampling distribution that come from the bootstrap:\n\nsimulation error: the number of samples M is finite. This can be made arbitrarily small by making M large\nstatistical error: resampling from an estimated parameter is not the “true” data generating process. Often though, the distribution of an estimator from the samples around the truth is more invariant, so subtraction is a good choice in reducing the sampling error\nspecification error: the model isnt quite good.\n\n\n\nNon-parametric bootstrap\nTo address specification error, alternatively, we sample with replacement the X from our original sample D, generating many fake datasets, and then compute the distribution on the parameters as before. This is the non parametric bootstrap. We want to sample with replacement, for if we do so, more typical values will be represented more often in the multiple datasets we create.\nHere we are using the empirical distribution, since it comes without any model preconceptions. This process may be illustrated so:\n\n\n\nThe non-parametric bootstrap process"
  },
  {
    "objectID": "posts/sufstatexch/index.html",
    "href": "posts/sufstatexch/index.html",
    "title": "Sufficient Statistics and Exchangeability",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pylab as plt \nimport seaborn as sn\n\nfrom scipy.stats import norm\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/sufstatexch/index.html#sufficient-statistics-and-the-exponential-family",
    "href": "posts/sufstatexch/index.html#sufficient-statistics-and-the-exponential-family",
    "title": "Sufficient Statistics and Exchangeability",
    "section": "Sufficient Statistics and the Exponential Family",
    "text": "Sufficient Statistics and the Exponential Family\nProbability distributions that belong to an exponential family have natural conjugate prior distributions. The form of the exponential family is:\n\\[p(y_i \\vert \\theta) = f(y_i)g(\\theta) e^{\\phi(\\theta)^{T} u(y_i)}.\\]\nThus the likelihood corresponding to n i.i.d. points is:\n\\[ p(y \\vert \\theta) = \\left(\\prod_{i=1}^{n} f(y_i) \\right) g(\\theta)^n \\,\\, \\exp\\left(\\phi(\\theta)\\sum_{i=1}^{n} u(y_i)\\right)\\]\nNow notice that the product of y -dependent stuff in front is irrelavant as far as sampling goes: it does not interact with \\(\\theta\\) in any way! If I wanted the actual value of the likelihood it would be important to model it well. But if all I want is to use this expression in a samples generator, I dont care. This kind of observation will bve critical for us as we sample from ever more complex models: indeed isolating such dependencies is at the cornerstone of the gibbs method.\nThus one can say, that for all n and y, this has a fixed form as a functio of \\(\\theta\\):\n\\[ p(y \\vert \\theta)  \\propto g(\\theta)^n \\, e^{\\phi (\\theta)^T   t(y)}\\]\nwhere \\(t(y) = \\sum_{i=1}^{n} u(y_i)\\) is said to be a sufficient statistic for \\(\\theta\\) , because the likelihood for theta “depends” on y only through \\(t(y)\\).\nIn general the exponential families are the only classes of distributions that have natural conjugate prior distributions, since, apart from some special cases, they are the only distributions having a fixed number of sufficient statistics for all \\(n\\).\nThis family includes exponential, poisson, gamma, beta, pareto, binomial, gaussian…."
  },
  {
    "objectID": "posts/sufstatexch/index.html#an-example-with-poissons-and-gammas",
    "href": "posts/sufstatexch/index.html#an-example-with-poissons-and-gammas",
    "title": "Sufficient Statistics and Exchangeability",
    "section": "An example with Poissons and Gammas",
    "text": "An example with Poissons and Gammas\nConsider some data gathered in the 1990s on educational attainment. The data consists of 155 women who were 40 years old. We are interested in the birth rate of women with a college degree and women without. We are told that 111 women without college degrees have 217 children, while 44 women with college degrees have 66 children.\nLet \\(Y_{1,1}, \\ldots, Y_{n_1,1}\\) denote the number of children for the \\(n_1\\) women without college degrees and \\(Y_{1,2}, \\ldots, Y_{n_2,2}\\) be the data for \\(n_2\\) women with college degrees.\n\nExchangeability\nLets assume that the number of children of a women in any one of these classes can me modelled as coming from ONE birth rate (we dont know anything about their individual situations so we treat each woman as interchangeable or exchageable with another within the same class). This is the basis for the IID assumption that we generally use.\nAnother way to think about it, is that the in-class likelihood for these women is invariant to a permutation of variables. If we assume a Poisson likelihood (low counts) for the number of births for each woman, we have, for each woman:\n\\[Y_{i,1} \\sim Poisson(\\theta_{1}),  Y_{i,2} \\sim Poisson(\\theta_{2})\\]\nThen, the likelihood for the first population is:\n\\[ p(Y_{1,1}, \\ldots, Y_{n_1,1}  \\vert  \\theta_1)  = \\prod_{i=1}^{n_1} p(Y_{i,1} \\vert \\theta_1) =  \\prod_{i=1}^{n_1}  \\frac{1}{Y_{i,1} !} \\theta_1^{Y_{i,1}} e^{-\\theta_1}\n= c(Y_{1,1}, \\ldots, Y_{n_1,1}) \\,\\, (n_{1}\\theta_{1})^{\\sum Y_{i,1}} e^{-n_1 \\theta_1}\n  \\sim Poisson(n_1 \\theta_1) \\]\nand similarly\n\\[ Y_{1,2}, \\ldots, Y_{n_1,2}  \\vert  \\theta_2 \\sim Poisson(n_2\\theta_2) \\]\n** The distributions are still poisson **\n\n\nObtaining the Posterior\nThe posterior is a simple product of two sub-posteriors:\n\\[p(\\theta_1 \\vert  Y_{1,1}, \\ldots, Y_{n_1,1} )* p(\\theta_2 \\vert  Y_{1,2}, \\ldots, Y_{n_2,2} ) ,\\] which, given independent priors on\n\\(\\theta_1\\) and \\(\\theta_2\\), is:\n\\[c_1(n_1, y_1, \\ldots, y_{n_1}) \\,\\, (n_{1}\\theta_{1})^{\\sum Y_{i,1}} e^{-n_1 \\theta_1}\\, p(\\theta_1) \\times c_2(n_2, y_1, \\ldots, y_{n_2}) \\,\\, (n_{2}\\theta_{2})^{\\sum Y_{i,2}} e^{-n_2 \\theta_2}  \\, p(\\theta_2) \\]\nThe quantity \\(\\sum Y_i\\) contains all the information about \\(\\theta\\) and thus \\(\\sum Y_i\\) is sufficient statistics. Indeed all you need is the total number of children in each class of mom as far as making any inferences about the \\(\\theta_{1 or 2}\\) are concerned.\nSo as long as we dont need the exact value of the likelihood, we are go ob treating the likelihood as a\nFor our example we have \\(n_1 =111\\), \\(\\sum_i^{n_1} Y_{i,1} =217\\) and \\(n_2=44\\), \\(\\sum_i^{n_2} Y_{i,2} =66\\).\n\n\nCongugate Priors\nLets now choose priors. A class of priors is said to be conjugate for a sampling distribution \\(p(y_1, \\ldots, y_n \\vert  \\theta)\\) if the posterior is also in the class.\nFor the Poisson :\n\\[ p(Y_1, \\ldots, y_n \\vert  \\theta)  \\sim  \\theta^{\\sum Y_i} e^{-n \\theta} \\]\nKeeping the same functional form means our conjugate class has to include terms like \\(\\theta^{c_1} e^{-c_2 \\theta}\\).\nThis is a known family known as Gamma distributions. In the shape-rate parametrization (see wikipedia)\n\\[p(\\theta) =  \\rm{Gamma}(\\theta, a, b) = \\frac{b^a}{\\Gamma(a)} \\theta^{a-1} e^{-b \\theta} \\]\nIf \\(p(\\theta) =  \\rm{Gamma}(\\theta, a, b)\\) and $ p(Y_1 , Y_n ) \\rm{Gamma}(_{1,2}, a=2, b=1) $. The mean and variance of gamma distributions are known\n\\[ E[\\theta] = a/b, var[\\theta] = a/b^2 .\\]\nSo the mean of the gamma is roughly a notion of your belief of prior kids to moms. Here we say 2.\n\nfrom scipy.stats import gamma\nxxx=np.linspace(0,10,100)\nplt.plot(xxx, gamma.pdf(xxx, 3, scale=1), label=\"3 kids 1 mom\");\nplt.plot(xxx, gamma.pdf(xxx, 2, scale=1), label=\"2 kids 1 mom\");\nplt.plot(xxx, gamma.pdf(xxx, 1, scale=1), label=\"1 kid 1 mom\");\nplt.plot(xxx, gamma.pdf(xxx, 1, scale=1/3), label=\"1 kids 3 moms\");\nplt.legend();\n\n\n\n\n\n\n\n\n\n\nOur Posteriors\n\\[ p(\\theta_1 \\vert n_1 = 111,  \\sum_i^{n_1} Y_{i,1}=217 ) \\sim  \\rm{Gamma}(\\theta_1, 2+217, 1+111) =  \\rm{Gamma}(\\theta_1, 219, 112) \\]\n\\[ p(\\theta_2 \\vert n_2 = 44,  \\sum_i^{n_2} Y_{i,2}=66 ) \\sim  \\rm{Gamma}(\\theta_2, 2+66, 1+44) =  \\rm{Gamma}(\\theta_2, 68, 45) \\]\nThe mean of our posterior is then a ratio of posterior kids to moms:\n\\[ E[\\theta] = (a + \\sum y_i)/(b + N), var[\\theta] = (a + \\sum y_i)/(b + N)^2 .\\]\nIn this case 219/112 and 68/45 which is not very sensitive to our prior as you might expect.\n\n219/112, 68/45\n\n(1.9553571428571428, 1.511111111111111)\n\n\nWe can calculate and plot the posterior predictives. We do that here for \\(\\theta_1\\) and \\(\\theta_2\\). We also show (lack of) sensitivity to the prior by considering a wierd prior with a=20, b=2.\n\nfrom scipy.stats import gamma\na = 2 # Gamma prior, a,b values \nb = 1 \n\nn1 = 111\nsy1 = 217  # sum of y1\nn2 = 44 \nsy2=66     #sum of y2\nN=1000\n\n\n# ACTUAL VALUES \n# posterior mean \n(a+sy1)/(b+n1) \n(a+sy2)/(b+n2)\n\n# EXACT POSTERIORS\n\n\ntheta1=gamma.rvs(a+sy1, scale=1.0/( b+n1), size=N)\nq=plt.hist(theta1, 50,linewidth=1.5,normed=True, histtype='step',   label=u'posterior for theta1')\ntheta2 = gamma.rvs(a+sy2,scale= 1./(b+n2), size=N)\nq=plt.hist(theta2, 50, linewidth=1,histtype='step', alpha=1.0,normed=True,   label=u'posterior for theta2') \n\n\n\nth_prior = gamma.rvs(2.0, 1.0, size=N);\nplt.hist(th_prior, 50,linewidth=1, histtype='step',alpha=1.0, normed=True,   label=u'prior') \n\n#just for theta1, try a wierd pri\nth_priorwierd = gamma.rvs(20.0, 1.0, size=N);\ntheta1wierd=gamma.rvs(20+sy1, scale=1.0/( 2+n1), size=N)\nplt.hist(th_priorwierd, 50,linewidth=1, histtype='step',alpha=1.0, normed=True,   label=u'prior 20,2') \nplt.hist(theta1wierd, 50, linewidth=1,histtype='step', alpha=1.0,normed=True,   label=u'posterior for theta1 with 20,2') \n\n\n\n\n#plt.xlim( [0,8])\nplt.xlabel('\\theta')\n\n\n\n# ## MONTE CARLO APPROACH - REJECTION METHOD \n\n# a =2.0 \n# b = 1.0 \n# prior = lambda theta:  gamma.pdf(theta, a ,b)\n# pdf_s1 = lambda theta: prior(theta)*  poisson.pmf(sy1, n1*theta)\n# pdf_s2 = lambda theta:  prior(theta)* poisson.pmf(sy2, n2*theta)\n\n\nplt.xlim([0,8])\nplt.legend()\n\n\n## Finally we can do inference as we wish\n\n\n\n\n\n\n\n\nThe mean birth-rates can be calculated from the samples, as can the variances, which are also given us by the formulae from above:\n\\[ E[\\theta] = (a + \\sum y_i)/(b + N), var[\\theta] = (a + \\sum y_i)/(b + N)^2 .\\]\n\nnp.mean(theta1), np.var(theta1)\n\n(1.9516881521791478, 0.018527204185785785)\n\n\n\nnp.mean(theta2), np.var(theta2)\n\n(1.5037252100213609, 0.034220717257786061)\n\n\nIts easy to get the posterior birth-rate difference from the samples\n\nnp.mean(theta1 - theta2)\n\n(0.43687373794997003, -0.43687373794997003)\n\n\n\n\nPosterior predictives\nRemember that the posterior predictive is the following integral\n\\[p(y^{*} \\vert D) = \\int d\\theta p(y^{*} \\vert \\theta) p(\\theta \\vert D)\\]\nFrom the perspective of sampling, all we have to do is to first draw the thetas from the posterior, then draw y’s from the likelihood, and histogram the likelihood. This is the same logic as marginal posteriors, with the addition of the fact that we must draw y from the likelihood once we drew \\(\\theta\\). You might think that we have to draw multiple \\(y\\)s at a theta, but this is already taken care of for us because of the nature of sampling. We already have multiple \\(\\theta\\)a in a bin.\n\nfrom scipy.stats import poisson\npostpred1 = poisson.rvs(theta1)\npostpred2 = poisson.rvs(theta2)\n\n\nplt.hist(postpred1, alpha=0.4, align=\"left\");\nplt.hist(postpred2, alpha=0.4, align=\"left\");\n\n\n\n\n\n\n\n\nIt turns out that the distribution characterizing the posterior predictive is a negative binomial (see wikipedia, this requires some manipulations of gamma functions which we shall not reproduce here). The mean of the posterior predictive distribution is the same as that of the posterior\n\\[ E[y^*] = \\frac{(a + \\sum y_i)}{(b + N)}, var[y^*] = \\frac{(a + \\sum y_i)}{(b + N)^2} (N + b + 1) .\\]\n\nnp.mean(postpred1), np.var(postpred1)\n\n(1.976, 1.8554239999999997)\n\n\n\nnp.mean(postpred2), np.var(postpred2)\n\n(1.502, 1.5719960000000002)\n\n\nNotice that the error on the posterior predictive is much larger, even with the same means. The reason for this is that in the posterior predictive, you are smearing out the posterior error. At each point in the posterior, there is the smearing associated with the sampling distribution for \\(y^* \\sim \\theta\\), and thus the posterior predictive is conservative.\n\nnp.mean(postpred1 - postpred2)\n\n0.47399999999999998\n\n\nWhy bother with the posterior predictive?\n\nyou might want to make predictions\nmodel checking: is the model kosher?\n\nThere are multiple ways of accomplishing the latter (all of which we shall see).\n\nFuture observations could be compared to the posterior predictive.\ncross-validation could be used for this purpose to calculate a prediction error\njust plotting posterior predictives can be useful since sometimes a visual inspection of simulation data gives away the fact that it looks nothing like actual data."
  },
  {
    "objectID": "posts/jensens/index.html",
    "href": "posts/jensens/index.html",
    "title": "Convexity and Jensen’s Inequality",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/jensens/index.html#convexity",
    "href": "posts/jensens/index.html#convexity",
    "title": "Convexity and Jensen’s Inequality",
    "section": "Convexity",
    "text": "Convexity\nLet \\(f\\) be a function with domain the set of real numbers. If the second derivative is greater than zero for all \\(x\\in R\\) this function is convex.\nConsider the case of two random variables \\(x_1\\) and \\(x_2\\), as seen in the diagram below:\n\n\n\nJensen’s inequality: for a convex function, the weighted average of function values always lies above the function of the weighted average.\n\n\nDefnition Let f be a real valued function defined on an interval \\(I = [a, b]\\). \\(f\\) is said to be convex on I if \\(\\forall x_1, x_2 \\in I, \\lambda \\in [0, 1]\\),\n\\[\\begin{equation}\nf(\\lambda x_1 + (1 - \\lambda)\\,x_2) \\le \\lambda f(x_1) + (1- \\lambda)\\,f(x_2).\n\\end{equation}\\]\n\\(f\\) is said to be strictly convex if the inequality is strict. Intuitively, this definition states that the function falls below the straight line (the secant) from points \\((x_1, f(x_1))\\) to \\((x_2, f(x_2))\\). In other words, the equality is satisfied only for \\(\\lambda = 0\\) and \\(\\lambda = 1\\)."
  },
  {
    "objectID": "posts/jensens/index.html#jensens-inequality",
    "href": "posts/jensens/index.html#jensens-inequality",
    "title": "Convexity and Jensen’s Inequality",
    "section": "Jensen’s Inequality",
    "text": "Jensen’s Inequality\nLet \\(f\\) be a convex function defined on an interval \\(I\\). If \\(x_1,x_2,\\dots,x_n \\in I {\\rm and} \\lambda_1, \\lambda_2,\\ldots,\\lambda_n \\ge  0\\) with \\(\\sum^n_{i=1} \\lambda_i = 1\\),\n\\[\\begin{equation}\nf \\left( \\sum_{i=1}^n \\lambda_i \\,  x_i \\right) \\le  \\sum_{i=1}^n \\lambda_i f(x_i)\n\\end{equation}\\]\nProof: For \\(n = 1\\) this is trivial. The case \\(n = 2\\) corresponds to the definition of convexity (see above). To show that this is true for all natural numbers, we proceed by induction. Assume the theorem is true for some \\(n\\) then,\n\\[\n\\begin{eqnarray}\nf \\left( \\sum_{i=1}^{n+1} \\lambda_i \\,  x_i \\right) &=& f\\left( \\lambda_{n+1} x_{n+1} + \\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n        &=&  f\\left( \\lambda_{n+1} x_{n+1} + \\frac{(1-\\lambda_{n+1})}{(1-\\lambda_{n+1})}\\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n        & \\le & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1}) f \\left( \\frac{1}{(1-\\lambda_{n+1})} \\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n        & = & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1}) f \\left( \\sum_{i=1}^n \\frac{\\lambda_i}{(1-\\lambda_{n+1})} \\,  x_i  \\right)  \\nonumber \\\\\n        & \\le & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1})  \\sum_{i=1}^n \\frac{\\lambda_i}{(1-\\lambda_{n+1})} \\,  f(x_i)  \\nonumber \\\\\n        & =&  \\lambda_{n+1} f(x_{n+1}) + \\sum_{i=1}^n \\lambda_i f(x_i) \\nonumber \\\\\n        & =&  \\sum_{i=1}^{n+1}  \\lambda_i f(x_i)\n\\end{eqnarray}\n\\]\nBy interpreting the \\(\\lambda_i\\) as the probability distribution over a discrete variable \\(x\\) taking the values \\(\\{x_i\\}\\):\n\\[f(\\mathrm{E}[x]) \\le \\mathrm{E}[f(x)]\\]\nTheorem. Let \\(f\\) be a convex function, and \\(X\\) be a random variable, then\n\\[ E[f(X)] \\ge f(E[X]) \\]\nFurthermore, if \\(f\\) is stricly convex (i.e. \\(f''(x)&gt;0\\)), then \\(E[f(x)]=f(E[X])\\) only if \\(X=E[X]\\) with probability 1 (\\(X\\) is constant)."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html",
    "href": "posts/basicmontecarlo/index.html",
    "title": "Basic Monte Carlo",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('talk')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#monte-carlo",
    "href": "posts/basicmontecarlo/index.html#monte-carlo",
    "title": "Basic Monte Carlo",
    "section": "Monte Carlo",
    "text": "Monte Carlo\nThe basic idea of a Monte Carlo Algorithm is to use randomness to solve what is often a deterministic problem. In this course, we’ll study their application in 3 different places: optimization, integration, and obtaining draws from a probability distribution. These uses are often intertwined: optimization is needed to find modes of distributions and integration to find expectations.\nWikipedia has a facinating bit of history on the subject, from which I quote:\n\nThe first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a Canfield solitaire laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than “abstract thinking” might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to John von Neumann, and we began to plan actual calculations. –Stanislaw Ulam\n\n\nBeing secret, the work of von Neumann and Ulam required a code name.[citation needed] A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name Monte Carlo, which refers to the Monte Carlo Casino in Monaco where Ulam’s uncle would borrow money from relatives to gamble."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#estimate-the-area-of-a-unit-circle",
    "href": "posts/basicmontecarlo/index.html#estimate-the-area-of-a-unit-circle",
    "title": "Basic Monte Carlo",
    "section": "Estimate the area of a unit circle",
    "text": "Estimate the area of a unit circle\nTo understand how randomness can be brought to bear upon solving deterministic problems, consider a very simple example: the value of \\(\\pi\\). If you could uniformly generate random numbers on a square, you could ask, how many of these numbers would fall inside a unit circle embedded in and touching the midpoints of the sides of the square. This ratio would be\n\\[\\frac{\\pi \\times 1^2}{2^2} = \\frac{\\pi}{4}.\\]\n\n#area of the bounding box\nbox_area = 4.0    \n\n#number of samples\nN_total = 10000.0 \n\n#drawing random points uniform between -1 and 1\nX = np.random.uniform(low=-1, high=1, size=N_total)  \nY = np.random.uniform(low=-1, high=1, size=N_total)   \n\n# calculate the distance of the points from the center \ndistance = np.sqrt(X**2+Y**2);  \n \n# check if point is inside the circle    \nis_point_inside = distance&lt;1.0\n\n# sum up the hits inside the circle\nN_inside=np.sum(is_point_inside)\n\n# estimate the circle area\ncircle_area = box_area * N_inside/N_total\n\n# some nice visualization\nplt.scatter(X,Y, c=is_point_inside, s=5.0, edgecolors='none', cmap=plt.cm.Paired)  \nplt.axis('equal')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# text output\nprint(\"Area of the circle = \", circle_area)\nprint(\"pi = \", np.pi)\n\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n\n\nArea of the circle =  3.1344\npi =  3.141592653589793\n\n\n\n\n\n\n\n\n\nIntuitively, one might expect our estimate of \\(\\pi\\) to get better as we draw more and more samples: we are covering the areas with samples much better when we do that.\nLets try to think about the mathematics in the intuition which tells us that we can calculate \\(\\pi\\) in this way.\nThe area of the circle C can be obtained by computing a double integral like so:\n\\[A = \\int_x \\int_y I_{\\in C}(x, y) dx dy = \\int \\int_{\\in C} dx dy \\]\nwhere \\(I_{\\in C} (x, y) = 1\\) if \\(x,y \\in C\\) and \\(I_{\\in C}(x) = 0\\) if \\(x,y \\notin C\\).\nThis is basically adding up all the small area elements inside the circle.\nRemember from the LOTUS:\n\\[E_f[I_{\\in C} (X,Y)] = \\int I_{\\in C} (X,Y) dF(X,Y) = \\int_{\\in C} dF(X,Y) = \\int \\int_{\\in C} f_{X,Y} (x,y) dx dy = p(X,Y \\in C)\\]\nand then we can use the law of large numbers to calculate this expectation and thus this probability.\nThe relationship of the expression all the way on the left to that all the way on the right is simply the law of large numbers we saw before. This is a distribution independent statement.\nBut the critical thing to notice is that:\n\\[\\int \\int_{\\in C} f_{X,Y} (x,y) dx dy  =  \\frac{1}{V} \\int \\int_{\\in C}  dx dy = E_f[I_{\\in C} (X,Y)]\\]\nonce we choose a uniform distribution. Here \\(V\\) is the support, the normalizing factor..here 4. The expectation from the law of large numbers comes from a sequence of identically distributed bernoullis (independent of \\(f\\) which here is uniform). All we have to do, is just like before in the law, count the frequency of samples inside."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#hit-or-miss-method",
    "href": "posts/basicmontecarlo/index.html#hit-or-miss-method",
    "title": "Basic Monte Carlo",
    "section": "Hit or miss method",
    "text": "Hit or miss method\nThis simple scenario of inside-or-outside can be used as a general (but poor, as missing increases exponentially with dimension) way to use the generation of samples to carry out integration\n\n\n\nBounding box for hit-or-miss Monte Carlo integration\n\n\nYou basically generate samples from a uniform distribution with support on the rectangle and see how many fall below \\(y(x)\\) at a specific x.\nThis is the basic idea behind rejection sampling"
  },
  {
    "objectID": "posts/lawoflargenumbers.html",
    "href": "posts/lawoflargenumbers.html",
    "title": "The LLN",
    "section": "",
    "text": "Suppose that you toss a fair coin and catch it to see if you got heads or tails. Then you have this intuition that while you might get a streak of several heads in a row, in the long run the heads and tails are balanced.\nThis is actually an example of a famous law: the Law of Large numbers (LLN), which states that if you have a random variable X with a mean, the average value of X over a sample of size N converges i.e. gets close and closer to this mean as N becomes larger and larger.\n\n\n\nThe LLN was first proved by Jakob Bernoulli in Ars Conjectandi, published posthumously by his nephew Niklaus Bernoulli, who appropriated entire passages of it for his treatise on law. It is the basis of much of modern statistics, including the Monte-Carlo method.\nLets parse the law. A random variable is one that can take multiple values, each with some probability. So if X represents the flip of a coin, it will take values Heads and Tails with some probability. We’ll assign Heads the value 1 and Tails the value 0.\nThe probabilities attatched to the values a random variable takes is called a distribution, or probability mass function (pmf). For a fair coin, the “Bernoulli” Distribution attaches the probabilities 0.5 to value 1 and 0.5 to value 0. These probabilities must add to 1.\n\n\n\nAn unfair coin thats more likely to land on heads might have a distribution where 0 has attached probability 0.4 and 1 has attached probability 0.6. In this case the mean µ of the distribution is 0.4 x 0 + 0.6 x 1 = 0.6.\n\n\n\nThis mean does not need to be one of the allowed values of the distribution (here 0 and 1). The mean here simply indicates whats more likely: 0.6 means that heads is more likely than tails. What is the mean in the case of the fair coin?\nNow let us simulate the case of the fair coin. We’ll toss a sample of N coins, or 1 coin N times, using the magic of numpy. We’ll find the average of these N tosses. This is the fraction of heads! We’ll plot this sample average against the sample size N.\n\n\n\nWe find that these sample averages are quite close to 0.5. And, as we increase the sample size N, these sample averages become super close to 0.5. Indeed, as N becomes infinite, the sample averages approach the mean µ=0.5. This is the Law of Large Numbers.\n\n\n\nThe LLN can be tautologically used to define the probability of a fair coin showing heads as the asymptotic (infinite N) sampling average. This is the frequentist definition of “sampling probability”, the population frequency µ.\nBut we might also treat the mean µ as an intrinsic fraction of heads, a “parameter” of the Bernoulli distribution. Where does it come from in the first place? The value µ can be thought of as an “inferential probability” derived from symmetry and lack of knowledge.\n\n\n\nIf you have a coin (2 sides, 2 possibilities), and no additional information about the coin and toss physics (thus fair), you would guess fraction µ=0.5 for heads. The LLN then says that sampling probabilities converge to this “inferential probability”.\n\nwh"
  },
  {
    "objectID": "posts/normalreg/index.html",
    "href": "posts/normalreg/index.html",
    "title": "From the Normal Model to Regression",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nThe example we use here is described in McElreath’s book, and our discussion mostly follows the one there, in sections 4.3 and 4.4."
  },
  {
    "objectID": "posts/normalreg/index.html#howells-data",
    "href": "posts/normalreg/index.html#howells-data",
    "title": "From the Normal Model to Regression",
    "section": "Howell’s data",
    "text": "Howell’s data\nThese are census data for the Dobe area !Kung San (https://en.wikipedia.org/wiki/%C7%83Kung_people). Nancy Howell conducted detailed quantitative studies of this Kalahari foraging population in the 1960s.\n\ndf = pd.read_csv('assets/Howell1.csv', sep=';', header=0)\ndf.head()\n\n\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n0\n151.765\n47.825606\n63.0\n1\n\n\n1\n139.700\n36.485807\n63.0\n0\n\n\n2\n136.525\n31.864838\n65.0\n0\n\n\n3\n156.845\n53.041915\n41.0\n1\n\n\n4\n145.415\n41.276872\n51.0\n0\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n539\n145.415\n31.127751\n17.0\n1\n\n\n540\n162.560\n52.163080\n31.0\n1\n\n\n541\n156.210\n54.062496\n21.0\n0\n\n\n542\n71.120\n8.051258\n0.0\n1\n\n\n543\n158.750\n52.531624\n68.0\n1\n\n\n\n\n\n\n\n\nplt.hist(df.height, bins=30);\n\n\n\n\n\n\n\n\nWe get rid of the kids and only look at the heights of the adults.\n\ndf2 = df[df.age &gt;= 18]\nplt.hist(df2.height, bins=30);"
  },
  {
    "objectID": "posts/normalreg/index.html#model-for-heights",
    "href": "posts/normalreg/index.html#model-for-heights",
    "title": "From the Normal Model to Regression",
    "section": "Model for heights",
    "text": "Model for heights\nWe will now get relatively formal in specifying our models.\nWe will use a Normal model, \\(h \\sim N(\\mu, \\sigma)\\), and assume that the priors are independent. That is \\(p(\\mu, \\sigma) = p(\\mu \\vert \\sigma) p(\\sigma) = p(\\mu)p(\\sigma)\\).\nOur model is:\n\\[\nh \\sim N(\\mu, \\sigma)\\\\\n\\mu \\sim Normal(148, 20)\\\\\n\\sigma = Std. dev.\n\\]\n\nfrom scipy.stats import norm\nY = df2.height.values\n#Data Quantities\nsig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\nmu_data = np.mean(Y)\nn = len(Y)\nprint(\"sigma\", sig, \"mu\", mu_data, \"n\", n)\n\nsigma 7.73132668454 mu 154.597092614 n 352\n\n\n\nplt.hist(Y, bins=30, alpha=0.5);\n\n\n\n\n\n\n\n\n\n# Prior mean\nmu_prior = 148\n# prior std\ntau = 20\n\n\nkappa = sig**2 / tau**2\nsig_post =np.sqrt(1./( 1./tau**2 + n/sig**2));\n# posterior mean\nmu_post = kappa / (kappa + n) *mu_prior + n/(kappa+n)* mu_data\nprint(\"mu post\", mu_post, \"sig_post\", sig_post)\n\nmu post 154.594293158 sig_post 0.41199365493\n\n\n\n#samples\nN = 15000\ntheta_prior = np.random.normal(loc=mu_prior, scale=tau, size=N);\ntheta_post = np.random.normal(loc=mu_post, scale=sig_post, size=N);\n\n\nplt.hist(theta_post, bins=30, alpha=0.9, label=\"posterior\");\nplt.hist(theta_prior, bins=30, alpha=0.2, label=\"prior\");\n#plt.xlim([10, 30])\nplt.legend();\n\n\n\n\n\n\n\n\n\nY_postpred = np.random.normal(loc=mu_post, scale=np.sqrt(sig_post**2 + sig**2), size=N);\n\n\nY_postpred_sample = np.random.normal(loc=theta_post, scale=sig);\n\n\nplt.hist(Y_postpred, bins=100, alpha=0.2);\nplt.hist(Y_postpred_sample, bins=100, alpha=0.2);\nplt.hist(np.random.choice(Y, replace=True, size=N), bins=100, alpha=0.5);"
  },
  {
    "objectID": "posts/normalreg/index.html#regression-adding-a-predictor",
    "href": "posts/normalreg/index.html#regression-adding-a-predictor",
    "title": "From the Normal Model to Regression",
    "section": "Regression: adding a predictor",
    "text": "Regression: adding a predictor\n\nplt.plot(df2.height, df2.weight, '.');\n\n\n\n\n\n\n\n\nSo lets write our model out now:\n\\[\nh \\sim N(\\mu, \\sigma)\\\\\n\\mu = intercept + slope \\times weight\\\\\nintercept \\sim N(150, 100)\\\\\nslope \\sim N(0, 10)\\\\\n\\sigma = std. dev,\n\\]\nWhy should you not use a uniform prior on a slope?\n\nminweight = df2.weight.min()\nmaxweight = df2.weight.max()\nminheight = df2.height.min()\nmaxheight = df2.height.max()\n\n\nfrom scipy.stats import norm\nfrom scipy.stats import multivariate_normal\ndef cplot(f, ax=None, lims=None):\n    if not ax:\n        plt.figure()\n        ax=plt.gca()\n    if lims:\n        xx,yy=np.mgrid[lims[0]:lims[1]:lims[2], lims[3]:lims[4]:lims[5]]\n    else:\n        xx,yy=np.mgrid[0:300:1,-15:15:.1]\n    pos = np.empty(xx.shape + (2,))\n    pos[:, :, 0] = xx\n    pos[:, :, 1] = yy\n    ax.contourf(xx, yy, f(pos))\n    #data = [x, y]\n    return ax\ndef plotSampleLines(mu, sigma, numberOfLines, dataPoints=None, ax=None):\n    #Plot the specified number of lines of the form y = w0 + w1*x in [-1,1]x[-1,1] by\n    # drawing w0, w1 from a bivariate normal distribution with specified values\n    # for mu = mean and sigma = covariance Matrix. Also plot the data points as\n    # blue circles. \n    #print \"datap\",dataPoints\n    if not ax:\n        plt.figure()\n        ax=plt.gca()\n    for i in range(numberOfLines):\n        w = np.random.multivariate_normal(mu,sigma)\n        func = lambda x: w[0] + w[1]*x\n        xx=np.array([minweight, maxweight])\n        ax.plot(xx,func(xx),'r', alpha=0.05)\n    if dataPoints:\n        ax.scatter(dataPoints[0],dataPoints[1], alpha=0.4, s=10)\n    #ax.set_xlim([minweight,maxweight])\n    #ax.set_ylim([minheight,maxheight])\n\n\n\npriorMean = np.array([150, 0])\npriorPrecision=2.0\npriorCovariance = np.array([[100*100, 0],[0, 10*10]])\npriorPDF = lambda w: multivariate_normal.pdf(w,mean=priorMean,cov=priorCovariance)\npriorPDF([1,2])\n\n5.1409768989960456e-05\n\n\n\ncplot(priorPDF);\n\n\n\n\n\n\n\n\n\nplotSampleLines(priorMean,priorCovariance,50)\n\n\n\n\n\n\n\n\n\nlikelihoodPrecision = 1./(sig*sig)\n\n\nPosterior\nWe can now continue with the standard Bayesian formalism\n\\[\n\\begin{eqnarray}\np(\\bf w| \\bf y,X) &\\propto& p(\\bf y | X, \\bf w) \\, p(\\bf w) \\nonumber \\\\\n                       &\\propto& \\exp{ \\left(- \\frac{1}{2 \\sigma_n^2}(\\bf y-X^T \\bf w)^T(\\bf y - X^T \\bf w) \\right)}\n                        \\exp{\\left( -\\frac{1}{2} \\bf w^T \\Sigma^{-1} \\bf w \\right)}  \\nonumber \\\\\n\\end{eqnarray}\n\\]\nIn the next step we `complete the square’ and obtain\n\\[\\begin{equation}\np(\\bf w| \\bf y,X)  \\propto  \\exp \\left( -\\frac{1}{2} (\\bf w - \\bar{\\bf w})^T  (\\frac{1}{\\sigma_n^2} X X^T + \\Sigma^{-1})(\\bf w - \\bar{\\bf w} )  \\right)\n\\end{equation}\\]\nThis is a Gaussian with inverse-covariance\n\\[A= \\sigma_n^{-2}XX^T +\\Sigma^{-1}\\]\nwhere the new mean is\n\\[\\bar{\\bf w} = A^{-1}\\Sigma^{-1}{\\bf w_0} + \\sigma_n^{-2}( A^{-1} X^T \\bf y )\\]\nTo make predictions for a test case we average over all possible parameter predictive distribution values, weighted by their posterior probability. This is in contrast to non Bayesian schemes, where a single parameter is typically chosen by some criterion.\n\n# Given the mean = priorMu and covarianceMatrix = priorSigma of a prior\n# Gaussian distribution over regression parameters; observed data, x\n# and y; and the likelihood precision, generate the posterior\n# distribution, postW via Bayesian updating and return the updated values\n# for mu and sigma. xtrain is a design matrix whose first column is the all\n# ones vector.\ndef update(x,y,likelihoodPrecision,priorMu,priorCovariance): \n    postCovInv  = np.linalg.inv(priorCovariance) + likelihoodPrecision*np.dot(x.T,x)\n    postCovariance = np.linalg.inv(postCovInv)\n    postMu = np.dot(np.dot(postCovariance,np.linalg.inv(priorCovariance)),priorMu) + likelihoodPrecision*np.dot(postCovariance,np.dot(x.T,y))\n    postW = lambda w: multivariate_normal.pdf(w,postMu,postCovariance)\n    return postW, postMu, postCovariance\n\n\ndesign = np.concatenate([np.ones(n).reshape(-1,1), df2.weight.values.reshape(-1,1)], axis=1)\nresponse = df2.height.values\n\n\n# For each iteration plot  the\n# posterior over the first i data points and sample lines whose\n# parameters are drawn from the corresponding posterior. \nfig, axes=plt.subplots(figsize=(12,6), nrows=1, ncols=2);\nmu = priorMean\ncov = priorCovariance\npostW,mu,cov = update(design,response,likelihoodPrecision,mu,cov)\ncplot(postW, axes[0], lims=[107, 122, 0.1, 0.7, 1.1, 0.01])\nplotSampleLines(mu, cov,50, (df2.weight.values,df2.height.values), axes[1])\n\n\n\n\n\n\n\n\n\n\nLets get the posteriors “at each point”\n\nweightgrid = np.arange(-20, 100)\ntest_design = np.concatenate([np.ones(len(weightgrid)).reshape(-1,1), weightgrid.reshape(-1,1)], axis=1)\n\n\nw = np.random.multivariate_normal(mu,cov, 1000) #1000 samples\nw[:,0].shape\n\n(1000,)\n\n\n\nsns.distplot(w[:,0] + w[:,1] * 55) # the weight=55 posterior\n\n\n\n\n\n\n\n\n\nmu_pred = np.zeros((len(weightgrid), 1000))\nfor i, weight in enumerate(weightgrid):\n    mu_pred[i, :] = w[:,0] + w[:,1] * weight\n\npost_means = np.mean(mu_pred, axis=1)\npost_stds = np.std(mu_pred, axis=1)\n\n(120,)\n\n\n\nwith sns.plotting_context('poster'):\n    plt.scatter(df2.weight, df2.height, c='b', alpha=0.9, s=10)\n    plt.plot(weightgrid, post_means, 'r')\n    #plt.fill_between(weightgrid, mu_hpd[:,0], mu_hpd[:,1], color='r', alpha=0.5)\n    plt.fill_between(weightgrid, post_means - 1.96*post_stds, ppmeans + 1.96*post_stds, color='red', alpha=0.4)\n\n\n    plt.xlabel('weight')\n    plt.ylabel('height')\n\n\n\n\n\n\n\n\n\nOops, what happened here? Our correlations in parameters are huge! But the regression lines do make some sense. Lets look at the posterior predictive.\n\n\nPosterior Predictive Distribution\nThus the predictive distribution at some \\(x^{*}\\) is given by averaging the output of all possible linear models w.r.t. the posterior\n\\[\n\\begin{eqnarray}\np(y^{*} | x^{*}, {\\bf x,y}) &=& \\int p({\\bf y}^{*}| {\\bf x}^{*}, {\\bf w} ) p(\\bf w| X, y)dw \\nonumber \\\\\n                                    &=& {\\cal N} \\left(y \\vert \\bar{\\bf w}^{T}x^{*}, \\sigma_n^2 + x^{*^T}A^{-1}x^{*} \\right),\n\\end{eqnarray}\n\\]\nwhich is again Gaussian, with a mean given by the posterior mean multiplied by the test input and the variance is a quadratic form of the test input with the posterior covariance matrix, showing that the predictive uncertainties grow with the magnitude of the test input, as one would expect for a linear model.\n\nppmeans = np.empty(len(weightgrid))\nppsigs = np.empty(len(weightgrid))\nt2 = np.empty(len(weightgrid))\n\n\n\nfor i, tp in enumerate(test_design):\n    ppmeans[i] = mu @ tp\n    ppsigs[i] = np.sqrt(sig*sig + tp@cov@tp)\n    t2[i] = np.sqrt(tp@cov@tp)\n\n\nweightgrid[75]\n\n55\n\n\n\nplt.hist(w[:,0] + w[:,1] * 55, alpha=0.8)\nplt.hist(norm.rvs(ppmeans[75], ppsigs[75], 1000), alpha=0.5)\n\n(array([   6.,   18.,   62.,  125.,  263.,  239.,  176.,   92.,   16.,    3.]),\n array([ 136.32163732,  141.63972751,  146.9578177 ,  152.27590789,\n         157.59399808,  162.91208827,  168.23017846,  173.54826866,\n         178.86635885,  184.18444904,  189.50253923]),\n &lt;a list of 10 Patch objects&gt;)\n\n\n\n\n\n\n\n\n\n\nwith sns.plotting_context('poster'):\n    plt.scatter(df2.weight, df2.height, c='b', alpha=0.9, s=10)\n    plt.plot(weightgrid, ppmeans, 'r')\n    #plt.fill_between(weightgrid, mu_hpd[:,0], mu_hpd[:,1], color='r', alpha=0.5)\n    plt.fill_between(weightgrid, ppmeans - 1.96*ppsigs, ppmeans + 1.96*ppsigs, color='green', alpha=0.4)\n\n\n    plt.xlabel('weight')\n    plt.ylabel('height')\n\n\n\n\n\n\n\n\nHowever, by including the \\(\\mu\\) as a deterministic in our traces we only get to see the traces at existing data points. If we want the traces on a grid of weights, we’ll have to explivitly plug in the intercept and slope traces in the regression formula"
  },
  {
    "objectID": "posts/globemodel/index.html",
    "href": "posts/globemodel/index.html",
    "title": "The Beta-Binomial Globe Model",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/globemodel/index.html#formulation-of-the-problem",
    "href": "posts/globemodel/index.html#formulation-of-the-problem",
    "title": "The Beta-Binomial Globe Model",
    "section": "Formulation of the problem",
    "text": "Formulation of the problem\nThis problem, taken from McElreath’s book, involves a seal (or a well trained human) tossing a globe, catching it on the nose, and noting down if the globe came down on water or land.\nThe seal tells us that the first 9 samples were:\nWLWWWLWLW.\nWe wish to understand the evolution of belief in the fraction of water on earth as the seal tosses the globe.\nSuppose \\(\\theta\\) is the true fraction of water covering the globe. Our data story if that \\(\\theta\\) then is the probability of the nose landing on water, with each throw or toss of the globe being independent.\nNow we build a probabilistic model for the problem, which we shall use to guide a process of Bayesian updating of the model as data comes in.\n\\[\\cal{L} = p(n,k|\\theta) = Binom(n,k, \\theta)=\\frac{n!}{k! (n-k)! } \\, \\theta^k \\, (1-\\theta)^{(n-k)} \\]\nSince our seal hasnt really seen any water or land, (strange, I know), it assigns equal probabilities, ie uniform probability to any value of \\(\\theta\\).\nThis is our prior information\nFor reasons of conjugacy we choose as prior the beta distribution, with \\(Beta(1,1)\\) being the uniform prior."
  },
  {
    "objectID": "posts/globemodel/index.html#choosing-a-prior-and-posterior",
    "href": "posts/globemodel/index.html#choosing-a-prior-and-posterior",
    "title": "The Beta-Binomial Globe Model",
    "section": "Choosing a prior and posterior",
    "text": "Choosing a prior and posterior\nThe mean of \\(Beta(\\alpha, \\beta)\\) is \\(\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\) while the variance is\n\\[V=\\mu (1- \\mu)/(\\alpha + \\beta + 1)\\]\n\nfrom scipy.stats import beta\nx=np.linspace(0., 1., 100)\nplt.plot(x, beta.pdf(x, 1, 1));\nplt.plot(x, beta.pdf(x, 1, 9));\nplt.plot(x, beta.pdf(x, 1.2, 9));\nplt.plot(x, beta.pdf(x, 2, 18));\n\n\n\n\n\n\n\n\nWe shall choose \\(\\alpha=1\\) and \\(\\beta=1\\) to be uniform.\n\\[ p(\\theta) = {\\rm Beta}(\\theta,\\alpha, \\beta) = \\frac{\\theta^{\\alpha-1} (1-x)^{\\beta-1} }{B(\\alpha, \\beta)} \\] where \\(B(\\alpha, \\beta)\\) is independent of \\(\\theta\\) and it is the normalization factor.\nFrom Bayes theorem, the posterior for \\(\\theta\\) is\n\\[ p(\\theta|D) \\propto  p(\\theta) \\, p(n,k|\\theta)  =  Binom(n,k, \\theta) \\,  {\\rm Beta}(\\theta,\\alpha, \\beta)  \\]\nwhich can be shown to be\n\\[{\\rm Beta}(\\theta, \\alpha+k, \\beta+n-k)\\]\n\nfrom scipy.stats import beta, binom\n\nplt.figure(figsize=( 15, 18))\n\nprior_params = np.array( [1.,1.] )  # FLAT \n\nx = np.linspace(0.00, 1, 125)\ndatastring = \"WLWWWLWLW\"\ndata=[]\nfor c in datastring:\n    data.append(1*(c=='W'))\ndata=np.array(data)\nprint(data)\nchoices=['Land','Water']\n\n\nfor i,v in enumerate(data):\n    plt.subplot(9,1,i+1)\n    prior_pdf = beta.pdf( x, *prior_params)\n    if v==1:\n        water = [1,0]\n    else:\n        water = [0,1]\n    posterior_params = prior_params + np.array( water )    # posteriors beta parameters\n    posterior_pdf = beta.pdf( x, *posterior_params)  # the posterior \n    prior_params = posterior_params\n    plt.plot( x,prior_pdf, label = r\"prior for this step\", lw =1, color =\"#348ABD\" )\n    plt.plot( x, posterior_pdf, label = \"posterior for this step\", lw= 3, color =\"#A60628\" )\n    plt.fill_between( x, 0, prior_pdf, color =\"#348ABD\", alpha = 0.15) \n    plt.fill_between( x, 0, posterior_pdf, color =\"#A60628\", alpha = 0.15) \n    \n    plt.legend(title = \"N=%d, %s\"%(i, choices[v]));\n    #plt.ylim( 0, 10)#\n\n[1 0 1 1 1 0 1 0 1]"
  },
  {
    "objectID": "posts/globemodel/index.html#interrogating-the-posterior",
    "href": "posts/globemodel/index.html#interrogating-the-posterior",
    "title": "The Beta-Binomial Globe Model",
    "section": "Interrogating the posterior",
    "text": "Interrogating the posterior\nSince we can sample from the posterior now after 9 observations, lets do so!\n\nsamples = beta.rvs(*posterior_params, size=10000)\nplt.hist(samples, bins=50, normed=True);\nsns.kdeplot(samples);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\nNow we can calculate all sorts of stuff.\nThe probability that the amount of water is less than 50%\n\nnp.mean(samples &lt; 0.5)\n\n0.17299999999999999\n\n\nThe probability by which we get 80% of the samples.\n\nnp.percentile(samples, 80)\n\n0.76255263476156399\n\n\nYou might try and find a credible interval. This, unlike the wierd definition of confidence intervals, is exactly what you think it is, the amount of probability mass between certain percentages, like the middle 80%\n\nnp.percentile(samples, [10, 90])\n\narray([ 0.44604094,  0.81516349])\n\n\nYou can make various point estimates: mean, median\n\nnp.mean(samples), np.median(samples), np.percentile(samples, 50) #last 2 are same\n\n(0.63787343440335842, 0.6473143052303143, 0.6473143052303143)\n\n\nA particularly important and useful point estimate is the MAP, or “maximum a-posteriori” estimate, the value of the parameter at which the pdf (num-samples) reach a maximum.\n\nsampleshisto = np.histogram(samples, bins=50)\n\n(array([  2,   3,   4,   7,  13,   9,  23,  27,  37,  53,  58,  57,  74,\n         94, 124, 152, 170, 216, 215, 224, 224, 269, 310, 308, 341, 335,\n        371, 405, 434, 419, 455, 474, 407, 427, 425, 380, 360, 332, 307,\n        297, 262, 202, 194, 152, 138,  90,  55,  35,  23,   7]),\n array([ 0.1684931 ,  0.18443135,  0.20036959,  0.21630783,  0.23224608,\n         0.24818432,  0.26412256,  0.2800608 ,  0.29599905,  0.31193729,\n         0.32787553,  0.34381378,  0.35975202,  0.37569026,  0.39162851,\n         0.40756675,  0.42350499,  0.43944324,  0.45538148,  0.47131972,\n         0.48725797,  0.50319621,  0.51913445,  0.5350727 ,  0.55101094,\n         0.56694918,  0.58288743,  0.59882567,  0.61476391,  0.63070215,\n         0.6466404 ,  0.66257864,  0.67851688,  0.69445513,  0.71039337,\n         0.72633161,  0.74226986,  0.7582081 ,  0.77414634,  0.79008459,\n         0.80602283,  0.82196107,  0.83789932,  0.85383756,  0.8697758 ,\n         0.88571405,  0.90165229,  0.91759053,  0.93352878,  0.94946702,\n         0.96540526]))\n\n\n\nmaxcountindex = np.argmax(sampleshisto[0])\nmapvalue = sampleshisto[1][maxcountindex]\nprint(maxcountindex, mapvalue)\n\n31 0.662578641304\n\n\nA principled way to get these point estimates is a loss function. This is the subject of decision theory, and we shall come to it soon. Different losses correspond to different well known point estimates, as we shall see.\nBut as a quick idea of this, consider the squared error decision loss:\n\\[R(t) = E_{p(\\theta \\vert D)}[(\\theta -t)^2] = \\int d\\theta  (\\theta -t)^2  p(\\theta \\vert D)\\]\n\\[\\frac{dR(t)}{dt} = 0 \\implies  \\int  d\\theta -2(\\theta -t)p(\\theta \\vert D) = 0\\]\nor\n\\[ t= \\int d\\theta \\theta\\,p(\\theta \\vert D) \\]\nor the mean of the posterior.\nWe can see this with some quick computation:\n\nmse = [np.mean((xi-samples)**2) for xi in x]\nplt.plot(x, mse);\nprint(\"Mean\",np.mean(samples));\n\nMean 0.634941511888"
  },
  {
    "objectID": "posts/globemodel/index.html#obtaining-the-posterior-predictive",
    "href": "posts/globemodel/index.html#obtaining-the-posterior-predictive",
    "title": "The Beta-Binomial Globe Model",
    "section": "Obtaining the posterior predictive",
    "text": "Obtaining the posterior predictive\nIts easy to sample from any one probability to get the sampling distribution at a particular \\(\\theta\\)\n\npoint3samps = np.random.binomial( len(data), 0.3, size=10000);\npoint7samps = np.random.binomial( len(data), 0.7, size=10000);\nplt.hist(point3samps, lw=3, alpha=0.5, histtype=\"stepfilled\", bins=np.arange(11));\nplt.hist(point7samps, lw=3, alpha=0.3,histtype=\"stepfilled\", bins=np.arange(11));\n\n\n\n\n\n\n\n\nThe posterior predictive:\n\\[p(y^{*} \\vert D) = \\int d\\theta p(y^{*} \\vert \\theta) p(\\theta \\vert D)\\]\nseems to be a complex integral. But if you parse it, its not so complex. This diagram from McElreath helps:\n\n\n\nThe posterior predictive distribution as a mixture: each parameter value implies a sampling distribution, weighted by the posterior probability, producing the marginal prediction. From McElreath, Statistical Rethinking.\n\n\nA similar risk-minimization holds for the posterior-predictive so that\n\\[y_{min mse} = \\int  dy \\, y \\, p(y \\vert D)\\]\nwhich is indeed what we would use in a regression scenario…\n\nPlug-in Approximation\nAlso, often, people will use the plug-in approximation by putting the posterior mean or MAP value\n\\[p(\\theta \\vert D) = \\delta(\\theta - \\theta_{MAP})\\]\nand then simply drawing the posterior predictive from :\n\\[p(y^{*} \\vert D) = p(y^{*} \\vert \\theta_{MAP})\\]\n(the same thing could be done for \\(\\theta_{mean}\\)).\n\npluginpreds = np.random.binomial( len(data), mapvalue, size = len(samples))\n\n\nplt.hist(pluginpreds, bins=np.arange(11));\n\n\n\n\n\n\n\n\nThis approximation is just sampling from the likelihood(sampling distribution), at a posterior-obtained value of \\(\\theta\\). It might be useful if the posterior is an expensive MCMC and the MAP is easier to find by optimization, and can be used in conjunction with quadratic (gaussian) approximations to the posterior, as we will see in variational inference. But for now we have all the samples, and it would be inane not to use them…\n\n\nThe posterior predictive from sampling\nBut really from the perspective of sampling, all we have to do is to first draw the thetas from the posterior, then draw y’s from the likelihood, and histogram the likelihood. This is the same logic as marginal posteriors, with the addition of the fact that we must draw y from the likelihood once we drew \\(\\theta\\). You might think that we have to draw multiple \\(y\\)s at a theta, but this is already taken care of for us because of the nature of sampling. We already have multiple \\(\\theta\\)a in a bin.\n\npostpred = np.random.binomial( len(data), samples);\n\n\npostpred\n\narray([5, 5, 7, ..., 7, 5, 8])\n\n\n\nsamples.shape, postpred.shape\n\n((10000,), (10000,))\n\n\n\nplt.hist(postpred, bins=np.arange(11), alpha=0.5, align=\"left\", label=\"predictive\")\nplt.hist(pluginpreds, bins=np.arange(11), alpha=0.2, align=\"left\", label=\"plug-in (MAP)\")\nplt.title('Posterior predictive')\nplt.xlabel('k')\nplt.legend()\n\n\n\n\n\n\n\n\nYou can interrogate the posterior-predictive, or simulated samples in other ways, asking about the longest run of water tosses, or the number of times the water/land switched. This is left as an exercise. In particular, you will find that the number of switches is not consistent with what you see in our data. This might lead you to question our model…always a good thing..but note that we have very little data as yet to go on"
  },
  {
    "objectID": "posts/montecarlointegrals/index.html",
    "href": "posts/montecarlointegrals/index.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#the-basic-idea",
    "href": "posts/montecarlointegrals/index.html#the-basic-idea",
    "title": "Monte Carlo Integration",
    "section": "The basic idea",
    "text": "The basic idea\nLet us formalize the basic idea behind Monte Carlo Integration in 1-D.\nConsider the definite integral:\n\\[ I = \\int_{a}^{b} f(x) \\, dx \\]\nConsider:\n\\[ J = \\int_{a}^{b} f(x) U_{ab}(x) \\, dx \\]\nIf \\(V\\) is the support of the uniform distribution on a to b then the pdf \\[ U_{ab}(x) = \\frac{1}{V} = \\frac{1}{b-a}\\]\nThen from LOTUS and the law of large numbers:\n\\[J = \\frac{1}{V}  \\int_{a}^{b} f(x) \\, dx  =  \\frac{I}{V} = E_{U}[f] = \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nor\n\\[I  = V  \\times \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nPractically speaking, our estimate will only be as exact as the number of samples we draw, but more on this soon..\n\nExample.\n**Calculate the integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx. $**\nWe know from calculus that the anti-derivative is \\[ x^3/3 + 4\\sin(x) -4x\\cos(x). \\]\nTo solve this using MC, we simply draw \\(N\\) random numbers from 2 to 3 and then take the average of all the values \\(f(x)=x^2 + 4 \\, x \\,\\sin(x)\\) and normalized over the volume; this case the volume is 1 (3-2=1).\n\ndef f(x):\n    return x**2 + 4*x*np.sin(x) \n\ndef intf(x): \n    return x**3/3.0+4.0*np.sin(x) - 4.0*x*np.cos(x) \n\n\na = 2;    \nb = 3; \n\n# use N draws \nN= 10000\n\nX = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \nY =f(X)   # CALCULATE THE f(x) \nV = b-a\nImc= V * np.sum(Y)/ N;\n\nexactval=intf(b)-intf(a)\n\nprint(\"Monte Carlo estimation=\",Imc, \"Exact number=\", intf(b)-intf(a))\n\nMonte Carlo estimation= 11.8120823531 Exact number= 11.8113589251\n\n\n\n\nMutlidimensional integral.\nThat is nice but how about a multidimensional case?\nLet us calculate the two dimensional integral \\(I=\\int \\int f(x, y) dx dy\\) where \\(f(x,y) = x^2 +y^2\\) over the region deﬁned by the condition \\(x^2 +y^2 ≤ 1\\)\nIn other words we are talking about a uniform distribution on the unit circle\n\nfmd = lambda x,y: x*x + y*y\n\n\n# use N draws \nN= 8000\nX= np.random.uniform(low=-1, high=1, size=N) \nY= np.random.uniform(low=-1, high=1, size=N) \nZ=fmd(X, Y)   # CALCULATE THE f(x) \n\nR = X**2 + Y**2\nV = np.pi*1.0*1.0\nN = np.sum(R&lt;1)\nsumsamples = np.sum(Z[R&lt;1])\n\nprint(\"I=\",V*sumsamples/N, \"actual\", np.pi/2.0) #actual value (change to polar to calculate)\n\nI= 1.56308724855 actual 1.5707963267948966"
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#monte-carlo-as-a-function-of-number-of-samples",
    "href": "posts/montecarlointegrals/index.html#monte-carlo-as-a-function-of-number-of-samples",
    "title": "Monte Carlo Integration",
    "section": "Monte-Carlo as a function of number of samples",
    "text": "Monte-Carlo as a function of number of samples\nHow does the accuracy depends on the number of points(samples)? Lets try the same 1-D integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx $ as a function of the number of points.\n\nImc=np.zeros(1000)\nNa = np.linspace(0,1000,1000)\n\nexactval= intf(b)-intf(a)\n\nfor N in np.arange(0,1000):\n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[N]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.plot(Na[10:],np.sqrt((Imc[10:]-exactval)**2), alpha=0.7)\nplt.plot(Na[10:], 1/np.sqrt(Na[10:]), 'r')\nplt.xlabel(\"N\")\nplt.ylabel(\"sqrt((Imc-ExactValue)$^2$)\")\n\n# \n\n\n\n\n\n\n\n\nObviously this depends on the number of \\(N\\) as \\(1/\\sqrt{N}\\)."
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#errors-in-mc",
    "href": "posts/montecarlointegrals/index.html#errors-in-mc",
    "title": "Monte Carlo Integration",
    "section": "Errors in MC",
    "text": "Errors in MC\nMonte Carlo methods yield approximate answers whose accuracy depends on the number of draws. So far, we have used our knowledge of the exact value to determine that the error in the Monte Carlo method approaches zero as approximately \\(1/\\sqrt{N}\\) for large \\(N\\), where \\(N\\) is the number of trials.\nBut in the usual case, the exact answer is unknown. Why do this otherwise?\nSo, lets repeat the same evaluation \\(m\\) times and check the variance of the estimate.\n\n# multiple MC estimations\nm=1000\nN=10000\nImc=np.zeros(m)\n\n\nfor i in np.arange(m):\n    \n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[i]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.hist(Imc, bins=30)\nplt.xlabel(\"Imc\")\nprint(np.mean(Imc), np.std(Imc))\n\n11.8114651823 0.00398497853806\n\n\n\n\n\n\n\n\n\nThis looks like our telltale Normal distribution.\nThis is not surprising\n\nEstimating the error in MC integration using the CLT.\nWe know from the CLT that if \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\), and that if \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\).\nThen,\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nThis is true regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nThe sums\n\\[S_n(f) = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\]\nare exactly what we want to calculate for Monte-Carlo Integration(due to the LOTUS) and correspond to the random variable f(X) where X is uniformly distributed on the support.\nWhatever the original variance of f(X) might be, we can see that the variance of the sampling distribution of the mean goes down as \\(1/n\\) and thus the standard error goes down as \\(1/\\sqrt{n}\\) as we discovered when we compared it to the exact value as well.\nWhy is this important?\n\n\nComparing to standard integration techniques\nWhat if we changed the dimensionality of the integral? The formula for \\(S_n\\) does not change, we just replace \\(g(x_i)\\) by \\(g(x_i, y_i, z_i...)\\). Thus, the CLT still holds and the error still scales as \\(\\frac{1}{\\sqrt{n}}\\).\nOn the other hand, if we divide the \\(a, b\\)-interval into \\(N\\) steps and use some regular integration routine, what is the error? Consider the midpoint rule as illustrated in this diagram from Wikipedia:\n\n\n\nRectangle rule for numerical integration\n\n\nThe basic idea is that the function value at the midpoint of the interval is used as the height of the approximating rectangle. In general, the differing methods consist of choosing different \\(x_i\\) below..with left being at the left end, right being at the right end. \\[I(est) = \\sum_i f(x_i)\\Delta x_i = \\frac{b-a}{n} \\sum_i f(x_i)\\]\nThe error on the estimation of the integral can be shown to decrease as \\(\\frac{1}{n^2}\\). The basic reason for this can be understood on a taylor series expansion of the function to second order. When you integrate on the sub-interval, the linear term vanishes while the quadratic term becomes cubic in \\(\\Delta x\\). So the local error goes as \\(\\frac{1}{n^3}\\) and thus the global as \\(\\frac{1}{n^2}\\).\nMonte-Carlo if clearly not competitive with the midpoint method in 1-D. Its actually not even competitive with left or right rectangle methods.\nThe trapeziod rule uses a line between the sub-interval points while the Simpsons rule uses a quadratic.\nThese integrations can be generalized to multiple dimensions, and the rule for these\n\nleft or right rule: \\(\\propto 1/n\\)\nMidpoint rule: \\(\\propto 1/n^2\\)\nTrapezoid: \\(\\propto 1/n^2\\)\nSimpson: \\(\\propto 1/n^4\\)\n\nwhere \\(n=N^{1/d}\\). MC becomes better than the Simpson method only in 8 dimensions.."
  },
  {
    "objectID": "posts/boxloop.html",
    "href": "posts/boxloop.html",
    "title": "Box’s Loop",
    "section": "",
    "text": "In the 1960’s, the great statistician Box, along with his collaborators, formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box’s loop by Blei et. al., 1, and illustrated in the diagram (taken from the above linked paper) below:\n1 Blei, David M. “Build, compute, critique, repeat: Data analysis with latent variable models.” Annual Review of Statistics and Its Application 1 (2014): 203-232.\n\n\nBox’s loop: Build, Infer, Criticize, Apply\n\n\nBox himself focussed on the scientific method, but the loop is applicable at large to other examples of probabilistic modelling, such as the building of an information retrieval or recommendation system, exploratory data analysis, etc, etc\nWe:\n\nfirst build a model. This is as much as an art as a science if we are of the philosophical bent that we desire explainability. We bring in domain experts.\nWe compute a model using the observed data.\nWe then critique our model, studying how they succeed or fail and how they predict future data or on held out sets.\nIf we are satisfied with the performance of our model we apply it in the context of a predictive or explanatory system. If we are not, we go back to 1.\n\nIf we are Bayesians, we compute the posterior distribution (the distribution of the parameters conditioned on the data) of the (hidden) parameters of the model. Here we assume that the data is fixed and our stochasticity is in the parameters.\nIf we are Frequentists, we assume our data is a sample from a population and compute the parameters of our models abd confidence intervals for those parameters. Here we assume that the data is stochastic as in we could get multiple different samplkes, but that the parameter is fixed and given.\nWe could have mis-specified our model. It might be too simple or too complex. If so we go back to (1) and try again with another model specification."
  },
  {
    "objectID": "posts/distrib-example/index.html",
    "href": "posts/distrib-example/index.html",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\nIn the last section, we made a simple simulation of a coin-toss on the computer from a fair-coin model which associated equal probability with heads and tails. Let us consider another model here, a table of probabilities that PredictWise made on October 2, 2012 for the US presidential elections. PredictWise aggregated polling data and, for each state, estimated the probability that the Obama or Romney would win. Here are those estimated probabilities:\npredictwise = pd.read_csv('assets/predictwise.csv').set_index('States')\npredictwise.head()\n\n\n\n\n\n\n\nObama\nRomney\nVotes\n\n\nStates\n\n\n\n\n\n\n\nAlabama\n0.000\n1.000\n9\n\n\nAlaska\n0.000\n1.000\n3\n\n\nArizona\n0.062\n0.938\n11\n\n\nArkansas\n0.000\n1.000\n6\n\n\nCalifornia\n1.000\n0.000\n55\nSay you toss a coin and have a model which says that the probability of heads is 0.5 (you have figured this out from symmetry, or physics, or something). Still, there will be sequences of flips in which more or less than half the flips are heads. These fluctuations induce a distribution on the number of heads (say k) in N coin tosses (this is a binomial distribution).\nSimilarly, here, if the probability of Romney winning in Arizona is 0.938, it means that if somehow, there were 10000 replications (as if we were running the election in 10000 parallel universes) with an election each, Romney would win in 9380 of those Arizonas on the average across the replications. And there would be some replications with Romney winning more, and some with less. We can run these simulated universes or replications on a computer though not in real life."
  },
  {
    "objectID": "posts/distrib-example/index.html#simulating-a-simple-election-model",
    "href": "posts/distrib-example/index.html#simulating-a-simple-election-model",
    "title": "Distributions Example: Elections",
    "section": "Simulating a simple election model",
    "text": "Simulating a simple election model\nTo do this, we will assume that the outcome in each state is the result of an independent coin flip whose probability of coming up Obama is given by the Predictwise state-wise win probabilities. Lets write a function simulate_election that uses this predictive model to simulate the outcome of the election given a table of probabilities.\n\nBernoulli Random Variables (in scipy.stats)\nThe Bernoulli Distribution represents the distribution for coin flips. Let the random variable X represent such a coin flip, where X=1 is heads, and X=0 is tails. Let us further say that the probability of heads is p (p=0.5 is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p),\\]\nwhich is to be read as X has distribution Bernoulli(p). The probability distribution function (pdf) or probability mass function associated with the Bernoulli distribution is\n\\[\\begin{eqnarray}\nP(X = 1) &=& p \\\\\nP(X = 0) &=& 1 - p\n\\end{eqnarray}\\]\nfor p in the range 0 to 1. The pdf, or the probability that random variable \\(X=x\\) may thus be written as\n\\[P(X=x) = p^x(1-p)^{1-x}\\]\nfor x in the set {0,1}.\nThe Predictwise probability of Obama winning in each state is a Bernoulli Parameter. You can think of it as a different loaded coin being tossed in each state, and thus there is a bernoulli distribution for each state\nNote: some of the code, and ALL of the visual style for the distribution plots below was shamelessly stolen from https://gist.github.com/mattions/6113437/ .\n\nfrom scipy.stats import bernoulli\n#bernoulli random variable\nbrv=bernoulli(p=0.3)\nprint(brv.rvs(size=20))\nevent_space=[0,1]\nplt.figure(figsize=(12,8))\ncolors=sns.color_palette()\nfor i, p in enumerate([0.1, 0.2, 0.5, 0.7]):\n    ax = plt.subplot(1, 4, i+1)\n    plt.bar(event_space, bernoulli.pmf(event_space, p), label=p, color=colors[i], alpha=0.5)\n    plt.plot(event_space, bernoulli.cdf(event_space, p), color=colors[i], alpha=0.5)\n\n    ax.xaxis.set_ticks(event_space)\n   \n    plt.ylim((0,1))\n    plt.legend(loc=0)\n    if i == 0:\n        plt.ylabel(\"PDF at $k$\")\nplt.tight_layout()\n\n[1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0]\n\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\n\n\nRunning the simulation using the Uniform distribution\nIn the code below, each column simulates a single outcome from the 50 states + DC by choosing a random number between 0 and 1. Obama wins that simulation if the random number is \\(&lt;\\) the win probability. If he wins that simulation, we add in the electoral votes for that state, otherwise we dont. We do this n_sim times and return a list of total Obama electoral votes in each simulation.\n\ndef simulate_election(model, n_sim):\n    simulations = np.random.uniform(size=(51, n_sim))\n    obama_votes = (simulations &lt; model.Obama.values.reshape(-1, 1)) * model.Votes.values.reshape(-1, 1)\n    #summing over rows gives the total electoral votes for each simulation\n    return obama_votes.sum(axis=0)\n\nThe first thing to pick up on here is that np.random.uniform gives you a random number between 0 and 1, uniformly. In other words, the number is equally likely to be between 0 and 0.1, 0.1 and 0.2, and so on. This is a very intuitive idea, but it is formalized by the notion of the Uniform Distribution.\nWe then say:\n\\[X \\sim Uniform([0,1),\\]\nwhich is to be read as X has distribution Uniform([0,1]). The probability distribution function (pdf) associated with the Uniform distribution is\n\\[\\begin{eqnarray}\nP(X = x) &=& 1 \\, for \\, x \\in [0,1] \\\\\nP(X = x) &=& 0 \\, for \\, x \\notin [0,1]\n\\end{eqnarray}\\]\nWhat assigning the vote to Obama when the random variable drawn from the Uniform distribution is less than the Predictwise probability of Obama winning (which is a Bernoulli Parameter) does for us is this: if we have a large number of simulations and \\(p_{Obama}=0.7\\) , then 70% of the time, the random numbes drawn will be below 0.7. And then, assigning those as Obama wins will hew to the frequentist notion of probability of the Obama win. But remember, of course, that in 30% of the simulations, Obama wont win, and this will induce fluctuations and a distribution on the total number of electoral college votes that Obama gets. And this is what we see in the histogram below.\nThe following code takes the necessary probabilities for the Predictwise data, and runs 10000 simulations. If you think of this in terms of our coins, think of it as having 51 biased coins, one for each state, and tossing them 10,000 times each.\nWe use the results to compute the number of simulations, according to this predictive model, that Obama wins the election (i.e., the probability that he receives 269 or more electoral college votes)\n\nresult = simulate_election(predictwise, 10000)\nprint((result &gt;= 269).sum())\n\n9955\n\n\n\nresult\n\narray([303, 326, 329, ..., 332, 281, 324])\n\n\nThere are roughly only 50 simulations in which Romney wins the election!"
  },
  {
    "objectID": "posts/distrib-example/index.html#displaying-the-prediction",
    "href": "posts/distrib-example/index.html#displaying-the-prediction",
    "title": "Distributions Example: Elections",
    "section": "Displaying the prediction",
    "text": "Displaying the prediction\nNow, lets visualize the simulation. We will build a histogram from the result of simulate_election. We will normalize the histogram by dividing the frequency of a vote tally by the number of simulations. We’ll overplot the “victory threshold” of 269 votes as a vertical black line and the result (Obama winning 332 votes) as a vertical red line.\nWe also compute the number of votes at the 5th and 95th quantiles, which we call the spread, and display it (this is an estimate of the outcome’s uncertainty). By 5th quantile we mean that if we ordered the number of votes Obama gets in each simulation in increasing order, the 5th quantile is the number below which 5% of the simulations lie.\nWe also display the probability of an Obama victory\n\ndef plot_simulation(simulation):    \n    plt.hist(simulation, bins=np.arange(200, 538, 1), \n             label='simulations', align='left', normed=True)\n    plt.axvline(332, 0, .5, color='r', label='Actual Outcome')\n    plt.axvline(269, 0, .5, color='k', label='Victory Threshold')\n    p05 = np.percentile(simulation, 5.)\n    p95 = np.percentile(simulation, 95.)\n    iq = int(p95 - p05)\n    pwin = ((simulation &gt;= 269).mean() * 100)\n    plt.title(\"Chance of Obama Victory: %0.2f%%, Spread: %d votes\" % (pwin, iq))\n    plt.legend(frameon=False, loc='upper left')\n    plt.xlabel(\"Obama Electoral College Votes\")\n    plt.ylabel(\"Probability\")\n    sns.despine()\n\n\nplot_simulation(result)\n\n\n\n\n\n\n\n\nThe model created by combining the probabilities we obtained from Predictwise with the simulation of a biased coin flip corresponding to the win probability in each states leads us to obtain a histogram of election outcomes. We are plotting the probabilities of a prediction, so we call this distribution over outcomes the predictive distribution. Simulating from our model and plotting a histogram allows us to visualize this predictive distribution. In general, such a set of probabilities is called a probability mass function."
  },
  {
    "objectID": "posts/distrib-example/index.html#empirical-distribution",
    "href": "posts/distrib-example/index.html#empirical-distribution",
    "title": "Distributions Example: Elections",
    "section": "Empirical Distribution",
    "text": "Empirical Distribution\nThis is an empirical Probability Mass Function.\nLets summarize: the way the mass function arose here that we did ran 10,000 tosses (for each state), and depending on the value, assigned the state to Obama or Romney, and then summed up the electoral votes over the states.\nThere is a second, very useful question, we can ask of any such probability mass or probability density: what is the probability that a random variable is less than some value. In other words: \\(P(X &lt; x)\\). This is also a probability distribution and is called the Cumulative Distribution Function, or CDF (sometimes just called the distribution, as opposed to the density, or mass function). Its obtained by “summing” the probability density function for all \\(X\\) less than \\(x\\).\n\nCDF = lambda x: np.float(np.sum(result &lt; x))/result.shape[0]\nfor votes in [200, 300, 320, 340, 360, 400, 500]:\n    print(\"Obama Win CDF at votes=\", votes, \" is \", CDF(votes))\n\nObama Win CDF at votes= 200  is  0.0\nObama Win CDF at votes= 300  is  0.1447\nObama Win CDF at votes= 320  is  0.4439\nObama Win CDF at votes= 340  is  0.839\nObama Win CDF at votes= 360  is  0.9979\nObama Win CDF at votes= 400  is  1.0\nObama Win CDF at votes= 500  is  1.0\n\n\n\nvotelist=np.arange(0, 540, 5)\nplt.plot(votelist, [CDF(v) for v in votelist], '.-');\nplt.xlim([200,400])\nplt.ylim([-0.1,1.1])\nplt.xlabel(\"votes for Obama\")\nplt.ylabel(\"probability of Obama win\");"
  },
  {
    "objectID": "posts/distrib-example/index.html#binomial-distribution",
    "href": "posts/distrib-example/index.html#binomial-distribution",
    "title": "Distributions Example: Elections",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nLet us consider a population of coinflips, n of them to be precise, \\(x_1,x_2,...,x_n\\). The distribution of coin flips is the binomial distribution. By this we mean that each coin flip represents a bernoulli random variable (or comes from a bernoulli distribution) with \\(p=0.5\\).\nAt this point, you might want to ask the question, what is the probability of obtaining \\(k\\) heads in \\(n\\) flips of the coin. We have seen this before, when we flipped 2 coins. What happens when when we flip 3?\n(This diagram is taken from the Feynman Lectures on Physics, volume 1. The chapter on probability is http://www.feynmanlectures.caltech.edu/I_06.html) \nWe draw a possibilities diagram like we did with the 2 coin flips, and see that there are different probabilities associated with the events of 0, 1,2, and 3 heads with 1 and 2 heads being the most likely. The probability of each of these events is given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k; n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\n\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\n\n\n\n\n\n\nApplying the CLT to elections: Binomial distribution in the large n, large k limit\nConsider the binomial distribution Binomial(n,k, p) in the limit of large n. The number of successes k in n trials can be regarded as the sum of n IID Bernoulli variables with values 1 or 0. Call these \\(x_i\\).\nThen:\n\\[S_n = \\frac{1}{n} \\sum_i x_i .\\]\nThe CLT tells us then that for large n, we have:\n\\[S_n \\sim N(p, \\frac{p(1-p)}{n}),\\]\nsince the mean of a Bernoulli is \\(p\\), and its variance \\(p*(1-p)\\).\nThis means that we can replace the binomial distribution at large n by a gaussian where k is now a continuous variable, and whose mean is the mean of the binomial \\(np\\) (\\(nS_n\\), since the binomial distribution is on the sum, not on the average) and whose variance is \\(np(1-p)\\).\nThe accuracy of this approximation depends on the variance. A large variance makes for a broad distribution spanning many discrete k, thus justifying the transition from a discrete to a continuous distribution.\nThis approximation is used a lot in studying elections. For example, suppose I told you that I’d polled 1000 people in Ohio and found that 600 would vote Democratic, and 400 republican. Imagine that this 1000 is a “sample” drawn from the voting “population” of Ohio. Assume then that these are 1000 independent bernoulli trials with p=600/1000 = 0.6. Then we can say that, from the CLT, the mean of the sampling distribution of the mean of the bernoulli or is 0.6 (equivalently the binomial’s mean is 600), with a variance of \\(0.6*0.4/1000 = 0.00024\\) (equivalently the binomials variance is 240). Thus the standard deviation is 0.015 for a mean of 0.6, or 1.5% on a mean of 60% voting Democratic. This 1.5% if part of what pollsters quote as the margin of error of a candidates winning; they often include other factors such as errors in polling methodology.\n\n\nGallup Party Affiliation Poll\nEarlier we had used the Predictwise probabilities from Octover 12th to create a predictive model for the elections. This time we will try to estimate our own win probabilities to plug into our predictive model.\nWe will start with a simple forecast model. We will try to predict the outcome of the election based the estimated proportion of people in each state who identify with one one political party or the other.\nGallup measures the political leaning of each state, based on asking random people which party they identify or affiliate with. Here’s the data they collected from January-June of 2012:\n\ngallup_2012=pd.read_csv(\"assets/g12.csv\").set_index('State')\ngallup_2012[\"Unknown\"] = 100 - gallup_2012.Democrat - gallup_2012.Republican\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\n\n\nState\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n\n\n\n\n\n\n\nEach row lists a state, the percent of surveyed individuals who identify as Democrat/Republican, the percent whose identification is unknown or who haven’t made an affiliation yet, the margin between Democrats and Republicans (Dem_Adv: the percentage identifying as Democrats minus the percentage identifying as Republicans), and the number N of people surveyed.\nThe most obvious source of error in the Gallup data is the finite sample size – Gallup did not poll everybody in America, and thus the party affilitions are subject to sampling errors. How much uncertainty does this introduce? Lets estimate the sampling error using the definition of the standard error (we use N-1 rather than N; see the sample error section in the page on the CLT).\n\ngallup_2012[\"SE_percentage\"]=100.0*np.sqrt((gallup_2012.Democrat/100.)*((100. - gallup_2012.Democrat)/100.)/(gallup_2012.N -1))\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\nSE_percentage\n\n\nState\n\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n0.849059\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n2.395543\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n0.744384\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n1.082971\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n0.392658\n\n\n\n\n\n\n\nOn their webpage discussing these data, Gallup notes that the sampling error for the states is between 3 and 6%, with it being 3% for most states. This is more than what we find, so lets go with what Gallup says.\nWe now use Gallup’s estimate of 3% to build a Gallup model with some uncertainty. We will, using the CLT, assume that the sampling distribution of the Obama win percentage is a gaussian with mean the democrat percentage and standard error the sampling error of 3%.\nWe’ll build the model in the function uncertain_gallup_model, and return a forecast where the probability of an Obama victory is given by the probability that a sample from the Dem_Adv Gaussian is positive.\nTo do this we simply need to find the area under the curve of a Gaussian that is on the positive side of the x-axis. The probability that a sample from a Gaussian with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) exceeds a threhold \\(z\\) can be found using the the Cumulative Distribution Function of a Gaussian:\n\\[\nCDF(z) = \\frac1{2}\\left(1 + {\\mathrm erf}\\left(\\frac{z - \\mu}{\\sqrt{2 \\sigma^2}}\\right)\\right)\n\\]\n\nfrom scipy.special import erf\ndef uncertain_gallup_model(gallup):\n    sigma = 3\n    prob =  .5 * (1 + erf(gallup.Dem_Adv / np.sqrt(2 * sigma**2)))\n    return pd.DataFrame(dict(Obama=prob), index=gallup.index)\n\n\nmodel = uncertain_gallup_model(gallup_2012)\nmodel = model.join(predictwise.Votes)\n\n\nprediction = simulate_election(model, 10000)\nplot_simulation(prediction)\n\n\n\n\n\n\n\n\n\n\nMultiple Pollsters\nIf one has results from multiple pollsters, one can now treat them as independent samples from the voting population. Now we use the CLT again. Then the average from these samples will approach the average in the population, with the sample means distributed normally around it. So we can average the averages of the samples to get the population mean, and estimate the variance around this population mean as well."
  },
  {
    "objectID": "posts/distributions.html",
    "href": "posts/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Remember that a Random Variable is a mapping $ X: $ that assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\) in a sample space \\(\\Omega\\). The definitions below are taken from Larry Wasserman’s All of Statistics."
  },
  {
    "objectID": "posts/distributions.html#cumulative-distribution-function",
    "href": "posts/distributions.html#cumulative-distribution-function",
    "title": "Distributions",
    "section": "Cumulative distribution Function",
    "text": "Cumulative distribution Function\nThe cumulative distribution function, or the CDF, is a function\n\\[F_X : \\mathbb{R} → [0, 1] \\],\ndefined by\n\\[F_X (x) = p(X \\le x).\\]\nA note on notation: \\(X\\) is a random variable while \\(x\\) is a particular value of the random variable.\nLet \\(X\\) be the random variable representing the number of heads in two coin tosses. Then \\(x\\) can take on values 0, 1 and 2. The CDF for this random variable can be drawn thus (taken from All of Stats):\n\n\n\nCDF of the 2-coin-toss distribution\n\n\nNotice that this function is right-continuous and defined for all \\(x\\), even if $x $does not take real values in-between the integers."
  },
  {
    "objectID": "posts/distributions.html#probability-mass-and-distribution-function",
    "href": "posts/distributions.html#probability-mass-and-distribution-function",
    "title": "Distributions",
    "section": "Probability Mass and Distribution Function",
    "text": "Probability Mass and Distribution Function\n\\(X\\) is called a discrete random variable if it takes countably many values \\(\\{x_1, x_2,…\\}\\). We define the probability function or the probability mass function (pmf) for X by:\n\\[f_X(x) = p(X=x)\\]\n\\(f_X\\) is a probability.\nThe pmf for the number of heads in two coin tosses (taken from All of Stats) looks like this:\n\n\n\nPMF of the 2-coin-toss distribution\n\n\nOn the other hand, a random variable is called a continuous random variable if there exists a function \\(f_X\\) such that \\(f_X (x) \\ge 0\\) for all x, \\(\\int_{-\\infty}^{\\infty} f_X (x) dx = 1\\) and for every a ≤ b,\n\\[p(a &lt; X &lt; b) = \\int_{a}^{b} f_X (x) dx\\]\nThe function \\(f_X\\) is called the probability density function (pdf). We have the CDF:\n\\[F_X (x) = \\int_{-\\infty}^{x}f_X (t) dt \\]\nand \\(f_X (x) = \\frac{d F_X (x)}{dx}\\) at all points x at which \\(F_X\\) is differentiable.\nContinuous variables are confusing. Note:\n\n\\(p(X=x) = 0\\) for every \\(x\\). You cant think of \\(f_X(x)\\) as \\(p(X=x)\\). This holds only for discretes. You can only get probabilities from a pdf by integrating, if only over a very small paty of the space.\nA pdf can be bigger than 1 unlike a probability mass function, since probability masses represent actual probabilities.\n\n\nA continuous example: the Uniform Distribution\nSuppose that X has pdf \\[\nf_X (x) =\n\\begin{cases}\n1 & \\text{for } 0 \\leq x\\leq 1\\\\\n    0             & \\text{otherwise.}\n\\end{cases}\n\\] A random variable with this density is said to have a Uniform (0,1) distribution. This is meant to capture the idea of choosing a point at random between 0 and 1. The cdf is given by: \\[\nF_X (x) =\n\\begin{cases}\n0 & x \\le 0\\\\\nx & 0 \\leq x \\leq 1\\\\\n1 & x &gt; 1.\n\\end{cases}\n\\] and can be visualized as so (again from All of Stats):\n\n\n\nCDF of the uniform distribution\n\n\n\n\nA discrete example: the Bernoulli Distribution\nThe Bernoulli Distribution represents the distribution a coin flip. Let the random variable \\(X\\) represent such a coin flip, where \\(X=1\\) is heads, and \\(X=0\\) is tails. Let us further say that the probability of heads is \\(p\\) (\\(p=0.5\\) is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p)\\]\nwhich is to be read as \\(X\\) has distribution \\(Bernoulli(p)\\). The pmf or probability function associated with the Bernoulli distribution is \\[\nf(x) =\n\\begin{cases}\n1 - p & x = 0\\\\\np & x = 1.\n\\end{cases}\n\\]\nfor p in the range 0 to 1. This pmf may be written as\n\\[f(x) = p^x (1-p)^{1-x}\\]\nfor x in the set {0,1}.\n\\(p\\) is called a parameter of the Bernoulli distribution."
  },
  {
    "objectID": "posts/distributions.html#conditional-and-marginal-distributions",
    "href": "posts/distributions.html#conditional-and-marginal-distributions",
    "title": "Distributions",
    "section": "Conditional and Marginal Distributions",
    "text": "Conditional and Marginal Distributions\nMarginal mass functions are defined in analog to probabilities. Thus:\n\\[f_X(x) = p(X=x) =  \\sum_y f(x, y);\\,\\, f_Y(y) = p(Y=y) = \\sum_x f(x,y).\\]\nSimilarly, marginal densities are defined using integrals:\n\\[f_X(x) = \\int dy f(x,y);\\,\\, f_Y(y) = \\int dx f(x,y).\\]\nNotice there is no interpretation of the marginal densities in the continuous case as probabilities. An example here if \\(f(x,y) = e^{-(x+y)}\\) defined on the positive quadrant. The marginal is an exponential defined on the positive part of the line.\nConditional mass function is similarly, just a conditional probability. So:\n\\[f_{X \\mid Y}(x \\mid y) = p(X=x \\mid Y=y) = \\frac{p(X=x, Y=y)}{p(Y=y)} = \\frac{f_{XY}(x,y)}{f_Y(y)}\\]\nThe similar formula for continuous densities might be suspected to a bit more complex, because we are conditioning on the event \\(Y=y\\) which strictly speaking has 0 probability. But it can be proved that the same formula holds for densities with some additional requirements and interpretation:\n\\[f_{X \\mid Y}(x \\mid y)  = \\frac{f_{XY}(x,y)}{f_Y(y)},\\]\nwhere we must assume that \\(f_Y(y) &gt; 0\\). Then we have the interpretation that for some event A:\n\\[p(X \\in A \\mid Y=y) = \\int_{x \\in A} f_{X \\mid Y}(x,y) dx.\\]\nAn example of this is the uniform distribution on the unit square. Suppose then that \\(y=0.3\\). Then the conditional density is a uniform density on the line between 0 and 1 at \\(y=0.3\\)."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', False)\nimport seaborn as sns\nsns.set_style(\"white\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/probability/index.html#what-is-probability",
    "href": "posts/probability/index.html#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability?\nSuppose you were to flip a coin. Then you expect not to be able to say whether the next toss would yield a heads or a tails. You might tell a friend that the odds of getting a heads is equal to to the odds of getting a tails, and that both are \\(1/2\\).\nThis intuitive notion of odds is a probability. It comes about because of our physical model of the world: say that because of our faith in the U.S. Mint, we might be willing to, without having seen any tosses, say that the coin is fair. In other words, there are two choices, both of which are equally likely.\n\nProbability from Symmetry\nConsider another example. If we were tossing a ‘fair’ six-sided dice, we may thus equivalently say that the odds of the dice falling on any one of its sides is \\(1/6\\). Indeed if there are \\(C\\) different equally likely possibilities, we’d expect that the probability of any one particular outcome would be \\(1/C\\).\nThe examples of the coin as well as the dice illustrate the notion of probability springing from symmetry. Here we think of probability of of the number 4 on the dice as the ratio:\n\\[\\frac{Number\\: of\\: cases\\: for\\: number\\: 4}{number\\: of\\: possibilities} = \\frac{1}{6},\\] assuming equally likely possibilities.\nIn other words, the symmetry refers to the notion that when there are multiple ways for an event to happen, and that then we have an intuitive model of fairness between these ways that tells us that none of these are any more likely than the other.\n\n\nProbability from a model\nThus one might think of symmetry as providing a model. There are also other kinds of models.\nThink of an event like an election, say a presidential election. You cant exactly run multiple trials of the election: its a one-off event. But you still want to talk about the likelyhood of a candidate winning. However people do make models of elections, based on inputs such as race, age, income, sampling polls, etc. They assign likeyhoods of candidates winning and run large numbers of simulations of the election, making predictions based on that. Forecasters like Nate Silver, Sam Wang, And Drew Linzer, made incredibly successfull predictions of the 2012 elections.\nOr consider what a weather forecaster means when he or she says there is a 90% chance of rain today. Presumably, this conclusion has been made from many computer simulations which take in the weather conditions known in the past, and propagated using physics to the current day. The simulations give different results based on the uncertainty in the measurement of past weather, and the inability of the physics to capture the phenomenon exactly (all physics is some approximation to the natural world). But 90% of these simulations show rain.\nIn all of these cases, there is either a model (a fair coin, an election forecasting model, a weather differential equation), or an experiment ( a large number of coin tosses) that is used to estimate a probability, or the odds, of an event \\(E\\) occuring.\n\nCombining models and observations\nIn all of these cases, probability is something we speak of, for observations we are to make in the future. And it is something we assign, based on the model or belief of the world we have, or on the basis of past observations that we have made, or that we might even imagine that we would make.\nConsider some additional examples. You might ask the probability of the Yankees winning the next baseball game against the Red Sox. Or you might ask for the probability of a launch failure for the next missile protecting Tel-Aviv. These are not academic questions: lots of betting money and lives depend upon them respectively. In both cases there is some past data, and some other inputs such as say, weather conditions, which might be used to construct a model, which is then used to predict the fate of the next game or launch.\nThey key takeaway is this: for some reasons, and possibly using some data, we have constructed a model of the universe. In other words, we have combined prior beliefs and past frequencies respectively. This notion of such combination is yet another notion of probability, called the Bayesian notion of probability. And we can now use this model to make predictions, such us the future odds of a particular event happening.\n\n\n\nProbability from frequency\nConsider doing a large number of coin flips. You would do, or imagine doing, a large number of flips or trials \\(N\\), and finding the number of times you got heads \\(N_H\\). Then the probability of getting heads would be \\[\\frac{N_H}{N}.\\]\nThis is the notion of probability as a relative frequency: if there are multiple ways an event like the tossing of a coin can happen, lets look at multiple trials of the event and see the fraction of times one or other of these ways happened.\nThis jibes with our general notion of probability from symmetry: indeed you can think of it as an experimental verificaltion of a symmetry based model.\n\nSimulating the results of the model\nWe dont have a coin right now. So let us simulate this process on a computer. To do this we will use a form of the random number generator built into numpy. In particular, we will use the function np.random.choice, which will with equal probability for all items pick an item from a list (thus if the list is of size 6, it will pick one of the six list items each time, with a probability 1/6).\n\ndef throw_a_coin(N):\n    return np.random.choice(['H','T'], size=N)\nthrows=throw_a_coin(40)\nprint(\"Throws:\",\" \".join(throws))\nprint(\"Number of Heads:\", np.sum(throws=='H'))\nprint(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/40.)\n\nThrows: T H T H T H H H H H T H H H T H T H T H T H H H T H H H H T T T T H T T H T T T\nNumber of Heads: 22\np1 = Number of Heads/Total Throws: 0.55\n\n\nNotice that you do not necessarily get 20 heads.\nNow say that we run the entire process again, a second replication to obtain a second sample. Then we ask the same question: what is the fraction of heads we get this time? Lets call the odds of heads in sample 2, then, \\(p_2\\):\n\ndef make_throws(N):\n    throws=throw_a_coin(N)\n    if N &lt;= 100:\n        print(\"Throws:\",\" \".join(throws))\n    else:\n        print(\"First 100 Throws:\",\" \".join(throws[:100]))\n    print(\"Number of Heads:\", np.sum(throws=='H'))\n    print(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/N)\nmake_throws(40)\n\nThrows: H T H T H H H H T H H H T T T H T H H H H T H H T T H H T H T H T H H H T T H H\nNumber of Heads: 25\np1 = Number of Heads/Total Throws: 0.625\n\n\nLet’s do many more trials\n\nmake_throws(1000)\n\nFirst 100 Throws: H H H T H H T T T H T H H H H T T T H H H H H H T H T T T H T H T T T H H T H T H H H H T H T H T T H H T T T T T T H T H H T H T H T H T H T T H T H H H T H T T T H H T H T H T H T H H H H T T T T H\nNumber of Heads: 521\np1 = Number of Heads/Total Throws: 0.521\n\n\nAnd even more:\n\nmake_throws(10000)\n\nFirst 100 Throws: H T T T H T T T H T T T T H T T H T H H H H T H T H T T H T H T H H H H T T H H H H T H H H H T H T T T H T H H T T T H T H T H T T T H T T T T H T H H H H H T T H H H T T H H H H H H T H T H T T T H\nNumber of Heads: 5047\np1 = Number of Heads/Total Throws: 0.5047\n\n\nAs you can see, the larger number of trials we do, the closer we seem to get to half the tosses showing up heads. Lets see this more systematically:\n\ntrials=np.arange(0, 40000, 1000)\nplt.plot(trials, [np.sum(throw_a_coin(j)=='H')/np.float(j) for j in trials], 'o-', alpha=0.2);\nplt.axhline(0.5, 0, 1, color='r');\nplt.xlabel('number of trials');\nplt.ylabel('probability of heads from simulation');\nplt.title('frequentist probability of heads');\n\n\n\n\n\n\n\n\nThus, the true odds fluctuate about their long-run value of 0.5, in accordance with the model of a fair coin (which we encoded in our simulation by having np.random.choice choose between two possibilities with equal probability), with the fluctuations becoming much smaller (we shall talk a lot more about this later in the book). These fluctations are what give rise to probability distributions.\nEach finite length run is called a sample, which has been obtained from the generative model of our fair coin. Its called generative as we can use the model to generate, using simulation, a set of samples we can play with to understand a model. Such simulation from a model is a key technique which we will come back to again and again in learning from data."
  },
  {
    "objectID": "posts/probability/index.html#the-rules-of-probability",
    "href": "posts/probability/index.html#the-rules-of-probability",
    "title": "Probability",
    "section": "The rules of probability",
    "text": "The rules of probability\nWe have seen multiple notions of probability so far. One might assign probabilities based on symmetry, for eg, 2 sides of a fair coin, or six sides of a fair dice. One might assign probabilities based on doing an experiment. such as the long run number of heads in many coin flips. One might assign probabilities based on beliefs; and one might even assign probabilities to events that have no chance of repeating, such as the 2012 presidential election, or the probability of rain between 2pm and 6pm today.\nThus, the very definition of probability seems to be wishy-washy and subjective. Thus you might wonder how you might work with such probabilities. For this, we turn to the rules of probability.\nThe rules dont care where our probabilities come from, as to how we estimated them, as long as they behave in intuitively sensible ways.\nConsider an example:\nE is the event of getting a heads in a first coin toss, and F is the same for a second coin toss. Here \\(\\Omega\\), the set of all possibilities that can happen when you toss two coins is \\(\\{HH, HT, TH, TT\\}\\). Since E only specifies that the first toss is heads, \\(E=\\{HT, HH\\}\\). Similarly \\(F= {HH, TH}\\) The set of all events that are not E then is \\(\\tilde{E} = {TH, TT}\\).\nThese sets, along with some others are captured in the venn diagram below:\n\n\n\nVenn diagram for the 2-coin-toss sample space\n\n\n\nThe Multiply/And/Intersection Formula for independent events\nIf E and F are independent events, the probability of both events happening together \\(P(EF)\\) or \\(P(E \\cap F)\\) (read as E and F or E intersection F, respectively) is the multiplication of the individual probabilities.\n\\[ P(EF) = P(E) P(F) .\\]\nIf you made the two independent coin tosses in our example, and you had a fair coin, the probability of both coming up heads is \\((1/2)*(1/2) = 1/4\\). This makes intuitive sense: half the time the first coin comes up heads, and then 1/2 the time the second coin comes up heads, so its 1/4 of the times that both come up heads.\n\n\nThe Plus/Or/Union Formula\nWe can now ask the question, what is \\(P(E+F)\\), the odds of E alone, F alone, or both together. Translated into English, we are asking, whats the probability that only the first toss was heads, or only the second toss was heads, or that both came up heads? Or in other words, what are the odds of at least one heads? The answer to this question is given by the rule:\n\\[P(E+F) = P(E) + P(F) - P(EF),\\]\nthe “plus” formula, where E+F, read as E or F (also \\(E \\cup F\\), reads as E union F) means “E alone, F alone, or both together”. This rule is a hard one to understand and has a lot of notation, so lets examine it in some detail.\nThere are four ways that these two tosses can arrange themselves, as illustrated by this diagram, adapted from the probability chapter in Feynman’s lectures on Physics..you should read it!.\n\n\n\nTree diagram for 2 coin flips\n\n\nWe can have a HH, HT, TH, or TT. In three out of 4 of these cases, either the first toss was heads, or the second was heads. Thus \\(P(E+F)=3/4\\).\nThe formula says, add the odds that “the first toss was a heads, without worrying about the second one (1/2), to the probability that the second toss was a heads, without worrying about the first one” (1/2). Since this double counts the situation where both are heads; subtract that (1/4):\n\\[\\begin{eqnarray}\nP(E+F) \\, & = &\\, P(E) + P(F) - P(EF)\\\\\n\\frac{3}{4} \\, & = &\\, \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4}\n\\end{eqnarray}\\]\nArmed with these two formulas, we can tackle the world of conditional and marginal probabilities, and Bayes theorem!\n\n\nFormally summarizing the rules\nIf \\(X\\) and \\(Y\\) are two events and \\(p(X)\\) is the probability of the event \\(X\\) to happen. $X^- $ is the complement of \\(X\\), the event which is all the occurrences which are not in \\(X\\). \\(X+Y\\) is the union of \\(X\\) and \\(Y\\); \\(X,Y\\) is the intersection of \\(X\\) and \\(Y\\). (Both \\(X+Y\\) and \\(X,Y\\) are also events.)\n\n\nThe very fundamental rules of probability:\n\n\\(p(X) &gt;=0\\); probability must be non-negative\n\\(0 ≤ p(X) ≤ 1 \\;\\) \\(X\\) has probability range from 0 to 1.\n\\(p(X)+p(X^-)=1 \\;\\) \\(X\\) must either happen or not happen. These last two aximoms can be thought of as saying that the probabilities if all events put tohether must sum to 1.\n\\(p(X+Y)=p(X)+p(Y)−p(X,Y) \\;\\) \\(X\\) can happen and \\(Y\\) can happen but we must subtract the cases that are happening together so we do not over-count."
  },
  {
    "objectID": "posts/probability/index.html#random-variables",
    "href": "posts/probability/index.html#random-variables",
    "title": "Probability",
    "section": "Random Variables",
    "text": "Random Variables\nTo link the notion of events such as \\(E\\) and collections of events, or probability spaces \\(\\Omega\\) to data, we must introduce the concept of random variables. The following definition is taken from Larry Wasserman’s All of Stats.\nDefinition. A random variable is a mapping\n\\[ X: \\Omega \\rightarrow \\mathbb{R}\\]\nthat assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\). \\(\\Omega\\) is the sample space. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. Subsets of \\(\\Omega\\) are called Events. Say \\(\\omega = HHTTTTHTT\\) then \\(X(\\omega) = 3\\) if defined as number of heads in the sequence \\(\\omega\\).\nWe will assign a real number P(A) to every event A, called the probability of A. We also call P a probability distribution or a probability measure. To qualify as a probability, P must satisfy the three axioms (non-negative, \\(P(\\Omega)=1\\), disjoint probs add)."
  },
  {
    "objectID": "posts/probability/index.html#marginals-and-conditionals-and-bayes-theorem",
    "href": "posts/probability/index.html#marginals-and-conditionals-and-bayes-theorem",
    "title": "Probability",
    "section": "Marginals and conditionals, and Bayes Theorem",
    "text": "Marginals and conditionals, and Bayes Theorem\nThe diagram below taken from Bishop may be used to illustrate the concepts of conditionals and marginals. Consider two random variables, \\(X\\), which takes the values \\({x_i}\\) where \\(i = 1,...,M\\), and \\(Y\\), which takes the values \\({y_j}\\) where \\(j = 1,...,L\\). The number of instances for which \\(X = x_i\\) and \\(Y = y_j\\) is \\(n_{ij}\\). The number of points in column i where \\(X=x_i\\) is \\(c_i\\), and for the row where \\(Y = y_j\\) is \\(r_j\\).\n\n\n\nJoint probability table notation\n\n\nThen the joint probability of having \\(p(X = x_i, Y= y_j)\\) is in the asymptotic limit of large numbers in the frequency sense of probability \\(n_{ij}/N\\) where is the total number of instances. The \\(X\\) marginal, \\(p(X=x_i)\\) can be obtained by summing instances in all the cells in the i’th column:\n\\[p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)\\]\nLets consider next only those instances for which \\(X=x_i\\). This means that we are limiting our analysis to the ith row. Then, we write the conditional probability of \\(Y = y_j\\) given \\(X = x_i\\) as \\(p(Y = y_j \\mid X = x_i)\\). This is the asymptotic fraction of these instances where \\(Y = y_j\\) and is obtained by dividing the instances in the cell by those in the comumn as\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i}.\\]\nA little algebraic rearrangement gives:\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i} = \\frac{n_{ij}}{N} / \\frac{c_i}{N},\\]\nor:\n\\[p(Y = y_j \\mid X = x_i) \\times p(X=x_i) =  p(X=x_i, Y=y_j).\\]\nThis is the product rule of probability with conditionals involved.\nLet us simplify the notation by dropping the \\(X=\\) and \\(Y=\\).\nThen we can write the marginal probability of x as a sum over the joint distribution of x and y where we sum over all possibilities of y,\n\\[p(x) = \\sum_y p(x,y) \\].\nWe can rewrite a joint distribution as a product of a conditional and marginal probability,\n\\[ p(x,y) = p(y\\mid x) p(x) \\]\nThe product rule is applied repeatedly to give expressions for the joint probability involving more than two variables. For example, the joint distribution over three variables can be factorized into a product of conditional probabilities:\n\\[ p(x,y,z) = p(x|y,z) \\, p(y,z) = p(x |y,z) \\, p(y|z) p(z) \\]\n\nBayes rule\nObserve that\n\\[ p(x,y) = p(y\\mid x) p(x) = P(x\\mid y)p(y).\\]\nGiven the product rule one can derive the Bayes rule, which plays a central role in a lot of the things we will be talking:\n\\[ p(y\\mid x) = \\frac{p(x\\mid y) \\, p(y) }{p(x)} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x,y')} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x\\mid y')p(y')}\\]\n\n\nIndependence\nTwo variables are said to be independent if their joint distribution factorizes into a product of two marginal probabilities:\n\\[ p(x,y) = p(x) \\, p(y) \\]\nAnother consequence of independence is that if \\(x\\) and \\(y\\) are independent, the conditional probability of \\(x\\) given \\(y\\) is just the probability of \\(x\\):\n\\[ p(x|y) = p(x) \\]\nIn other words, by conditioning on a particular \\(y\\), we have learned nothing about \\(x\\) because of independence. Two variables \\(x\\) and \\(y\\) and said to be conditionally independent of \\(z\\) if the following holds:\n\\[ p(x,y|z) = p(x|z) p(y|z) \\]\nTherefore, if we learn about z, x and y become independent. Another way to write that \\(x\\) and \\(y\\) are conditionally independent of \\(z\\) is\n\\[ p(x| z, y) = p(x|z) \\]\nIn other words, if we condition on \\(z\\), and now also learn about \\(y\\), this is not going to change the probability of \\(x\\). It is important to realize that conditional independence between \\(x\\) and \\(y\\) does not imply independence between \\(x\\) and \\(y\\).\n\n\nApplication of Bayes Theorem\n\nSally Clark, a lawyer who lost her first son at 11 weeks and her second at 8 weeks, was convicted in 1999. A prominent pediatrician, Sir Roy Meadow, had testified for the prosecution about Sudden Infant Death Syndrome, known as SIDS in the U.S. and cot death in Britain. Citing a government study, Meadow said the incidence of one SIDS death was one in 8,500 in a family like Clark’s–stable, affluent, nonsmoking, with a mother more than 26 years old.\n\n\nThen, despite the fact that some families are predisposed to SIDS, Meadow assumed erroneously that each sibling’s death occurred independently of the other. Multiplying 8,500 by 8,500, he calculated that the chance of two children dying in a family like Sally Clark’s was so rare–one in 73 million–that they must have been murdered.\n\n(from http://www.mcgrayne.com/disc.htm)\np(child 1 dying of sids) = 1/8500\n\nP(child 2 dying of sids) = 1/100\n\nFirst, we look at natural causes of sudden infant death. The chance of one random infant dying from SIDS was about 1 in 1,300 during this period in Britain. Meadow’s argument was flawed and produced a much slimmer chance of natural death. The estimated odds of a second SIDS death in the same family was much larger, perhaps one in 100, because family members can share a common environmental or genetic propensity for SIDS.\n\n\nSecond, we turn to the hypothesis that the babies were murdered. Only about 30 children out of 650,000 annual births in England, Scotland, and Wales were known to have been murdered by their mothers. The number of double murders must be much lower, estimated as 10 times less likely.\n\np(S2 = both children dying of sids) =  0.000007\np(notS2 = not both dying of sids) =  0.999993\n\nData: both children died unexpectedly\nSo now ask, whats:\np(data | S2) = 1\np(data | notS2) = ? both died but not SIDS. Murder? =  30/650000    × 1/10 = 0.000005\nWe want to calculate the “posterior probability”:\np(S2 | data) = P(data | S2) P(S2) /(P(data | S2) P(S2) + P(data|notS2)P(notS2))\n= 1*0.000007/(1*0.000007 + 0.000005*0.999993)\n=0.58\n58% chance of having died from SIDS!\nSally Clark spent 3 years in jail."
  },
  {
    "objectID": "posts/noisylearning/index.html",
    "href": "posts/noisylearning/index.html",
    "title": "Learning With Noise",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/noisylearning/index.html#revisiting-our-model",
    "href": "posts/noisylearning/index.html#revisiting-our-model",
    "title": "Learning With Noise",
    "section": "Revisiting our model",
    "text": "Revisiting our model\nLet us revisit our model from noiseless_learning.\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\nLet us assume that we have a “population” of 200 counties \\(x\\):\n\ndf=pd.read_csv(\"data/religion.csv\")\ndf.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\nLets suppose now that the Lord came by and told us that the points in the plot below captures \\(f(x)\\) exactly.\n\nx=df.rfrac.values\nf=df.promney.values\nplt.plot(x,f,'.', alpha=0.3)\n\n\n\n\n\n\n\n\nNotice that our sampling of \\(x\\) is not quite uniform: there are more points around \\(x\\) of 0.7.\nNow, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample \\(\\cal{D}\\) of 30 data points. Such data is called in-sample data. Contrastingly, the entire population of data points is also called out-of-sample data.\n\ndfsample = pd.read_csv(\"data/noisysample.csv\")\nindexes=dfsample.i.values\n\n\ndfpop = pd.read_csv(\"data/noisypopulation.csv\")\ndfpop.head()\n\n\n\n\n\n\n\nf\nx\ny\n\n\n\n\n0\n0.047790\n0.00\n0.011307\n\n\n1\n0.051199\n0.01\n0.010000\n\n\n2\n0.054799\n0.02\n0.007237\n\n\n3\n0.058596\n0.03\n0.000056\n\n\n4\n0.062597\n0.04\n0.010000\n\n\n\n\n\n\n\n\nsamplex = x[indexes]\nsamplef = f[indexes]\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\naxes[1].plot(x,f, 'r.', alpha=0.2, label=\"population\");\naxes[1].plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n\n\n\n\n\n\n\n\nStatement of the learning problem.\nLet us restate the learning problem from noiseless learning.\nIf we find a hypothesis \\(g\\) that minimizes the cost or risk over the training set (our sample), this hypothesis might do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nIn other words, we hope the empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small."
  },
  {
    "objectID": "posts/noisylearning/index.html#why-go-out-of-sample",
    "href": "posts/noisylearning/index.html#why-go-out-of-sample",
    "title": "Learning With Noise",
    "section": "Why go out-of-sample",
    "text": "Why go out-of-sample\nClearly we want tolearn something about a population, a population we dont have access to. So far we have stayed within the constraints of our sample and just like we did in the case of monte-carlo, might want to draw all our conclusions from this sample.\nThis is a bad idea.\nYou probably noticed that I used weasel words like “might” and “hope” in the last section when saying that representative sampling in both training and test sets combined with ERM is what we need to learn a model. Let me give you a very simple counterexample: a prefect memorizer.\nSuppose I construct a model which memorizes all the data points in the training set. Then its emprical risk is zero by definition, but it has no way of predicting anything on a test set. Thus it might as well choose the value at a new point randomly, and will perform very poorly. The process of interpolating a curve from points is precisely this memorization\n\nStochastic Noise\nWe saw before that \\(g_{20}\\) did a very good job in capturing the curves of the population. However, note that the data obtained from \\(f\\), our target, was still quite smooth. Most real-world data sets are not smooth at all, because of various effects such as measurement errors, other co-variates, and so on. Such stochastic noise plays havoc with our fits, as we shall see soon.\nStochastic noise bedevils almost every data set known to humans, and happens for many different reasons.\nConsider for example two customers of a bank with identical credit histories and salaries. One defaults on their mortgage, and the other does not. In this case we have identical \\(x = (credit, salary)\\) for these two customers, but different \\(y\\), which is a variable that is 1 if the customer defaulted and 0 otherwise. The true \\(y\\) here might be a function of other co-variates, such as marital strife, sickness of parents, etc. But, as the bank, we might not have this information. So we get different \\(y\\) for different customers at the information \\(x\\) that we possess.\nA similar thing might be happen in the election example, where we have modelled the probability of voting for romney as a function of religiousness of the county. There are many other variables we might not have measured, such as the majority race in that county. But, we have not measured this information. Thus, in counties with high religiousness fraction \\(x\\) we might have more noise than in others. Consider for example two counties, one with \\(x=0.8\\) fraction of self-identified religious people in the county, and another with \\(x=0.82\\). Based on historical trends, if the first county was mostly white, the fraction of those claiming they would vote for Romney might be larger than in a second, mostly black county. Thus you might have two very \\(y\\)’s next to each other on our graphs.\nIt gets worse. When pollsters estimate the number of people voting for a particular candidate, they only use finite samples of voters. So there is a 4-6% polling error in any of these estimates. This “sampling noise” adds to the noisiness of the \\(y\\)’s.\nIndeed, we wish to estimate a function \\(f(x)\\) so that the values \\(y_i\\) come from the function \\(f\\). Since we are trying to estimate f with data from only some counties, and furthermore, our estimates of the population behaviour in these counties will be noisy, our estimate wont be the “god given” or “real” f, but rather some noisy estimate of it.\n\ndfsample.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\ndfsample.shape\n\n(30, 4)\n\n\n\nx = dfpop.x.values\nf = dfpop.f.values\ny = dfpop.y.values\n\n\nplt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\nplt.plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"in-sample y (observed)\");\nplt.plot(x, y, '.', alpha=0.6, label=\"population y\");\nplt.xlabel('$x$');\nplt.ylabel('$p_R$')\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nIn the figure above, one can see the scatter of the \\(y\\) population about the curve of \\(f\\). The errors of the 30 observation points (“in-sample”) are shown as squares. One can see that observations next to each other can now be fairly different, as we descibed above."
  },
  {
    "objectID": "posts/noisylearning/index.html#fitting-a-noisy-model",
    "href": "posts/noisylearning/index.html#fitting-a-noisy-model",
    "title": "Learning With Noise",
    "section": "Fitting a noisy model",
    "text": "Fitting a noisy model\nLet us now try and fit the noisy data we simulated above, both using straight lines (\\(\\cal{H_1}\\)), and 20th order polynomials(\\(\\cal{H_{20}}\\)).\nWhat we have done is introduced a noisy target \\(y\\), so that\n\\[y = f(x) + \\epsilon\\,,\\]\nwhere \\(\\epsilon\\) is a random noise term that represents the stochastic noise.\n\nDescribing things probabilistically\nAnother way to think about a noisy \\(y\\) is to imagine that our data is generated from a joint probability distribution \\(P(x,y)\\). In our earlier case with no stochastic noise, once you knew \\(x\\), if I were to give you \\(f(x)\\), you could give me \\(y\\) exactly. This is now not possible because of the noise \\(\\epsilon\\): we dont know exactly how much noise we have at any given \\(x\\). Thus we need to model \\(y\\) at a given \\(x\\), \\(P(y \\mid x)\\), as well using a probability distribution. Since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) .\\]\n\n\n\nThe noisy learning framework: target distribution P(y|x) replaces deterministic f(x)\n\n\nNow the entire learning problem can be cast as a problem in probability density estimation: if we can estimate \\(P(x,y)\\) and take actions based on that estimate thanks to our risk or error functional, we are done.\nWe now fit in both \\(\\cal{H_1}\\) and \\(\\cal{H_{20}}\\) to find the best fit straight line and best fit 20th order polynomial respectively.\n\ng1 = np.poly1d(np.polyfit(x[indexes],f[indexes],1))\ng20 = np.poly1d(np.polyfit(x[indexes],f[indexes],20))\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\ng1noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],1))\ng20noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],20))\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'r-', alpha=0.6, label=\"f\");\naxes[1].plot(x,f, 'r-', alpha=0.6, label=\"f\");\naxes[0].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\naxes[1].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\naxes[0].plot(x,g1(x),  'b--', alpha=0.6, label=\"$g_1 (no noise)$\");\naxes[0].plot(x,g1noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_1 (noisy)$\");\naxes[1].plot(x,g20(x),  'b--', alpha=0.6, label=\"$g_10 (no noise)$\");\naxes[1].plot(x,g20noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_{10}$ (noisy)\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n\n\n\n\n\n\n\nThe results are (to put it mildly) very interesting.\nLets look at the figure on the left first. The noise changes the best fit line by a little but not by much. The best fit line still does a very poor job of capturing the variation in the data.\nThe best fit 20th order polynomial, in the presence of noise, is very interesting. It tries to follow all the curves of the observations..in other words, it tries to fit the noise. This is a disaster, as you can see if you plot the population (out-of-sample) points on the plot as well:\n\nplt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\nplt.plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\nplt.plot(x,y,  '.', alpha=0.6, label=\"population y\");\nplt.plot(x,g20noisy(x), 'b:', alpha=0.6, label=\"$g_{10}$ (noisy)\");\nplt.ylim([0,1])\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nWhoa. The best-fit 20th order polynomial does a reasonable job fitting the in-sample data, and is even well behaved in the middle where we have a lot of in-sample data points. But at places with less in-sample data points, the polynomial wiggles maniacally.\nThis fitting to the noise is a danger you will encounter again and again in learning. Its called overfitting. So, \\(\\cal{H_{20}}\\) which seemed to be such a good candidate hypothesis space in the absence of noise, ceases to be one. The take away lesson from this is that we must further ensure that our model does not fit the noise.\nLets make a plot similar to the one we made for the deterministic noise earlier, and compare the error in the new \\(g_1\\) and \\(g_{20}\\) fits on the noisy data.\n\nplt.plot(x, ((g1noisy(x)-f)**2), lw=3, label=\"$g_1$\")\nplt.plot(x, ((g20noisy(x)-f)**2), lw=3,label=\"$g_{20}$\");\nplt.plot(x, [1]*x.shape[0], \"k:\", label=\"noise\", alpha=0.2);\nfor i in indexes[:-1]:\n    plt.axvline(x[i], 0, 1, color='r', alpha=0.1)\nplt.axvline(x[indexes[-1]], 0, 1, color='r', alpha=0.1, label=\"samples\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"population error\")\nplt.yscale(\"log\")\nplt.legend(loc=4);\nplt.title(\"Noisy Data\");\n\n\n\n\n\n\n\n\n\\(g_1\\) now, for the most part, has a lower error! So you’d be better off by having chosen a set of models with much more bias (the straight lines, \\(\\cal{H}_1\\)) than a more complex model set (\\(\\cal{H}_{20}\\)) in the case of noisy data.\n\n\nThe Variance of your model\nThis tendency of a more complex model to overfit, by having enough freedom to fit the noise, is described by something called high variance. What is variance?\nVariance, simply put, is the “error-bar” or spread in models that would be learnt by training on different data sets \\(\\cal{D_1}, \\cal{D_2},...\\) drawn from the population. Now, this seems like a circular concept, as in real-life, you do not have access to the population. But since we simulated our data here anyways, we do, and so let us see what happens if we choose different 30 points randomly from our population of 200, and fit models in both \\(\\cal{H_1}\\) and \\(\\cal{H_{20}}\\) to them. We do this on 200 sets of randomly chosen (from the population) data sets of 30 points each and plot the best fit models in noth hypothesis spaces for all 200 sets.\n\ndef gen(degree, nsims, size, x, out):\n    outpoly=[]\n    for i in range(nsims):\n        indexes=np.sort(np.random.choice(x.shape[0], size=size, replace=False))\n        pc=np.polyfit(x[indexes], out[indexes], degree)\n        p=np.poly1d(pc)\n        outpoly.append(p)\n    return outpoly\n\n\npolys1 = gen(1, 200, 30,x, y);\npolys20 = gen(20, 200, 30,x, y);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\naxes[1].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\naxes[0].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\naxes[1].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\naxes[0].plot(x, y, '.', alpha=0.6, label=\"population y\");\naxes[1].plot(x, y, '.', alpha=0.6, label=\"population y\");\nc=sns.color_palette()[2]\nfor i,p in enumerate(polys1[:-1]):\n    axes[0].plot(x,p(x), alpha=0.05, c=c)\naxes[0].plot(x,polys1[-1](x), alpha=0.05, c=c,label=\"$g_1$ from different samples\")\nfor i,p in enumerate(polys20[:-1]):\n    axes[1].plot(x,p(x), alpha=0.05, c=c)\naxes[1].plot(x,polys20[-1](x), alpha=0.05, c=c, label=\"$g_{10}$ from different samples\")\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\nOn the left panel, you see the 200 best fit straight lines, each a fit on a different 30 point training sets from the 200 point population. The best-fit lines bunch together, even if they dont quite capture \\(f\\) (the thick red line) or the data (squares) terribly well.\nOn the right panel, we see the same with best-fit models chosen from \\(\\cal{H}_{20}\\). It is a diaster. While most of the models still band around the central trend of the real curve \\(f\\) and data \\(y\\) (and you still see the waves corresponding to all too wiggly 20th order polynomials), a substantial amount of models veer off into all kinds of noisy hair all over the plot. This is variance: the the predictions at any given \\(x\\) are all over the place.\nThe variance can be seen in a different way by plotting the coefficients of the polynomial fit. Below we plot the coefficients of the fit in \\(\\cal{H}_1\\). The variance is barely 0.2 about the mean for both co-efficients.\n\npdict1={}\npdict20={}\nfor i in reversed(range(2)):\n    pdict1[i]=[]\n    for j, p in enumerate(polys1):\n        pdict1[i].append(p.c[i])\nfor i in reversed(range(21)):\n    pdict20[i]=[]\n    for j, p in enumerate(polys20):\n        pdict20[i].append(p.c[i]) \ndf1=pd.DataFrame(pdict1)\ndf20=pd.DataFrame(pdict20)\nfig = plt.figure(figsize=(14, 5)) \nfrom matplotlib import gridspec\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 10]) \naxes = [plt.subplot(gs[0]), plt.subplot(gs[1])]\naxes[0].set_ylabel(\"value\")\naxes[0].set_xlabel(\"coeffs\")\naxes[1].set_xlabel(\"coeffs\")\nplt.tight_layout();\nsns.violinplot(df1, ax=axes[0]);\nsns.violinplot(df20, ax=axes[1]);\naxes[0].set_yscale(\"symlog\");\naxes[1].set_yscale(\"symlog\");\naxes[0].set_ylim([-1e12, 1e12]);\naxes[1].set_ylim([-1e12, 1e12]);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/seaborn/categorical.py:1791: UserWarning: The violinplot API has been changed. Attempting to adjust your arguments for the new API (which might not work). Please update your code. See the version 0.6 release notes for more info.\n  warnings.warn(msg, UserWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\nIn the right panel we plot the coefficients of the fit in \\(\\cal{H}_{20}\\). This is why we use the word “variance”: the spread in the values of the middle coefficients about their means (dashed lines) is of the order \\(10^{10}\\) (the vertical height of the bulges), with huge outliers!! The 20th order polynomial fits are a disaster!"
  },
  {
    "objectID": "posts/noisylearning/index.html#bias-and-variance",
    "href": "posts/noisylearning/index.html#bias-and-variance",
    "title": "Learning With Noise",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nWe have so far informally described two different concepts: bias and variance. Bias is deterministic error, the kind of error you get when your model is not expressive enough to describe the data. Variance describes the opposite problem, where it is too expressive.\nEvery model has some bias and some variance. Clearly, you dont want either to dominate, which is something we’ll worry about soon.\nLet us mathematically understand what bias and variance are, so that we can use these terms more precisely from now onwards. Follow carefully to see how our calculation mimics the code/process we used above.\n\\[\\renewcommand{\\gcald}{g_{\\cal D}}\\] \\[\\renewcommand{\\ecald}{E_{\\cal{D}}}\\]\nWe had from noiseless learning:\n\\[R_{out}(h) =  E_{p(x)}[(h(x) - f(x))^2] = \\int dx p(x)  (h(x) - f(x))^2 .\\]\nIn the presence of noise \\(\\epsilon\\) which we shall assume to be 0-mean, variance \\(\\sigma^2\\) noise, we have \\(y = f(x) + \\epsilon\\) and the above formula becomes:\n\\[R_{out}(h) =  E_{p(x)}[(h(x) - y)^2] = \\int dx p(x)  (h(x) - f(x) - \\epsilon)^2 .\\]\nNow let us use Empirical Risk minimization to fit on our training set. We come up with a best fit hypothesis \\(h = \\gcald\\), where \\(\\cal{D}\\) is our training sample.\n\\[R_{out}(\\gcald) =  E_{p(x)}[(\\gcald(x) - f(x) - \\epsilon)^2] \\]\nLet us compute the expectation of this quantity with respect to the sampling distribution obtained by choosing different samples from the population. Note that we cant really do this if we have been only given one training set, but in this document, we have had access to the population and can thus experiment.\nDefine:\n\\[\\langle  R \\rangle = E_{\\cal{D}} [R_{out}(\\gcald)] =  E_{\\cal{D}}E_{p(x)}[(\\gcald(x) - f(x) - \\epsilon)^2] \\]\n\\[\n\\begin{eqnarray*}\n=& E_{p(x)}\\ecald[(\\gcald(x) - f(x) - \\epsilon)^2]\\\\\n=& E_{p(x)}[\\ecald[\\gcald^2] +  f^2 + \\epsilon^2 - 2\\,f\\,\\ecald[\\gcald]]\n\\end{eqnarray*}\n\\]\nDefine:\n\\[ \\bar{g} = \\ecald[\\gcald] = (1/M)\\sum_{\\cal{D}} \\gcald\\]\nas the average “g” over all the fits (M of them) on the different samples, so that we can write, adding and subtracting \\(\\bar{g}^2\\):\n\\[\\langle  R \\rangle =  E_{p(x)}[\\ecald[\\gcald^2] - \\bar{g}^2 +  f^2 - 2\\,f\\,\\bar{g} + \\bar{g}^2 + \\epsilon^2 ] = E_{p(x)}[\\ecald[(\\gcald - \\bar{g})^2]  +  (f - \\bar{g})^2 + \\epsilon^2 ]\\]\nThus:\n\\[\\langle  R \\rangle =  E_{p(x)}[\\ecald[(\\gcald - \\bar{g})^2]] + E_{p(x)}[(f - \\bar{g})^2] + \\sigma^2\\]\nThe first term here is called the variance, and captures the squared error of the various fit g’s from the average g, or in other words, the hairiness. The second term is called the bias, and tells us, how far the average g is from the original f this data came from. Finally the third term is the stochastic noise, the minimum error that this model will always have.\nNote that if we set the stochastic noise to 0 we get back the noiseless model we started out with. So even in a noiseless model, we do have bias and variance. This is because we still have sampling noise in such a model, and this is one of the sources of variance."
  },
  {
    "objectID": "posts/noisylearning/index.html#so-far",
    "href": "posts/noisylearning/index.html#so-far",
    "title": "Learning With Noise",
    "section": "So far?",
    "text": "So far?\n\nyou have learnt the basic formulation of the learning problem, the concept of a hypothesis space, and a strategy using minimization of distance (called cost or risk) to find the best fit model for the target function from this hypothesis space.\nYou learned the effect of noise on this fit, and the issues that crop up in learning target functions from data, chiefly the problem of overfitting to this noise.\n\nThe process of learning has two parts:\n\nFit for a model by minimizing the in-sample risk\nHope that the in-sample risk approximates the out-of-sample risk well.\n\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nWell, we are scientists. Just hoping does not befit us. But we only have a sample. What are we to do? We can model the in-sample risk and out-of-sample risk. And we can use a test set to estimate our out of sample risk, as we see here."
  },
  {
    "objectID": "posts/votingforcongress/index.html",
    "href": "posts/votingforcongress/index.html",
    "title": "Some Data Analysis about Congress",
    "section": "",
    "text": "PredictWise congressional voting visualization\n\n\n\nimport pandas as pd\n\n\ntbl = pd.read_html(\"https://www.presidency.ucsb.edu/statistics/data/seats-congress-gainedlost-the-presidents-party-mid-term-elections\")\n\n\ndf = tbl[0]\ndf.columns = df.columns.to_flat_index()\ndf\n\n\n\n\n\n\n\n\n(Unnamed: 0_level_0, Year)\n(Unnamed: 1_level_0, Lame Duck?)\n(Unnamed: 2_level_0, President)\n(Unnamed: 3_level_0, President'sParty)\n(President's Job Approval Percentage (Gallup) As of:, Early Aug)\n(President's Job Approval Percentage (Gallup) As of:, Late Aug)\n(President's Job Approval Percentage (Gallup) As of:, Early Sep)\n(President's Job Approval Percentage (Gallup) As of:, Late Sep)\n(President's Job Approval Percentage (Gallup) As of:, Early Oct)\n(President's Job Approval Percentage (Gallup) As of:, Late Oct)\n(President's Party, House Seatsto Defend)\n(President's Party, Senate Seatsto Defend)\n(Seat Change, President's Party, House Seats)\n(Seat Change, President's Party, Senate Seats)\n\n\n\n\n0\n1934\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n--\n313\n14\n+9\n+9\n\n\n1\n1938\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n60\n334\n27\n-81\n-7\n\n\n2\n1942\nNaN\nFranklin D. Roosevelt\nD\n74\n--\n74\n--\n--\n--\n267\n25\n-46\n-9\n\n\n3\n1946\nNaN\nHarry S. Truman\nD\n--\n--\n33\n--\n--\n27\n244\n21\n-45\n-12\n\n\n4\n1950\nLD*\nHarry S. Truman\nD\nnd\n43\n35\n35\n43\n41\n263\n21\n-29\n-6\n\n\n5\n1954\nNaN\nDwight D. Eisenhower\nR\n67\n62\n--\n66\n62\n--\n221\n11\n-18\n-1\n\n\n6\n1958\nLD\nDwight D. Eisenhower\nR\n58\n56\n56\n54\n57\n--\n203\n20\n-48\n-13\n\n\n7\n1962\nNaN\nJohn F. Kennedy\nD\n--\n67\n--\n63\n--\n61\n264\n18\n-4\n+3\n\n\n8\n1966\n†\nLyndon B. Johnson\nD\n51\n47\n--\n--\n44\n44\n295\n21\n-47\n-4\n\n\n9\n1970\nNaN\nRichard Nixon\nR\n55\n55\n57\n51\n58\n--\n192\n7\n-12\n+2\n\n\n10\n1974\n±\nGerald R. Ford (Nixon)\nR\n71\n--\n66\n50\n53\n--\n192\n15\n-48\n-5\n\n\n11\n1978\nNaN\nJimmy Carter\nD\n43\n43\n48\n--\n49\n45\n292\n14\n-15\n-3\n\n\n12\n1982\nNaN\nRonald Reagan\nR\n41\n42\n--\n42\n--\n42\n192\n12\n-26\n+1\n\n\n13\n1986\nLD\nRonald Reagan\nR\n--\n64\n--\n63\n64\n--\n181\n22\n-5\n-8\n\n\n14\n1990\nNaN\nGeorge Bush\nR\n75\n73\n54\n--\n--\n57\n175\n17\n-8\n-1\n\n\n15\n1994\nNaN\nWilliam J. Clinton\nD\n43\n40\n40\n44\n43\n48\n258\n17\n-52\n-8\n\n\n16\n1998\nLD\nWilliam J. Clinton\nD\n65\n62\n63\n66\n65\n65\n207\n18\n+5\n0\n\n\n17\n2002\nNaN\nGeorge W. Bush\nR\n--\n66\n66\n66\n68\n67\n220\n20\n+8\n+2\n\n\n18\n2006\nLD\nGeorge W. Bush\nR\n37\n42\n39\n44\n37\n37\n233\n15\n-30\n-6\n\n\n19\n2010\nNaN\nBarack Obama\nD\n44\n44\n45\n45\n45\n45\n257\n15\n-63\n-6\n\n\n20\n2014\nLD\nBarack Obama\nD\n42\n42\n41\n43\n42\n41\n201\n20\n-13\n-9\n\n\n21\n2018\nNaN\nDonald J. Trump\nR\n41\n41\n39\n41\n44\n44\n241\n9\n-40\n+2\n\n\n22\n2022\nNaN\nJoseph R. Biden\nD\n38\n44\n44\n42\n42\nNaN\n222\n14\nTBD\nTBD"
  },
  {
    "objectID": "posts/regularization/index.html",
    "href": "posts/regularization/index.html",
    "title": "Regularization",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/regularization/index.html#revisiting-the-model",
    "href": "posts/regularization/index.html#revisiting-the-model",
    "title": "Regularization",
    "section": "Revisiting the model",
    "text": "Revisiting the model\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\n\ndffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\n\nx=dffull.rfrac.values\nf=dffull.promney.values\n\n\ndf = pd.read_csv(\"data/noisysample.csv\")\ndf.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\nfrom sklearn.cross_validation import train_test_split\ndatasize=df.shape[0]\n#split dataset using the index, as we have x,f, and y that we want to split.\nitrain,itest = train_test_split(range(30),train_size=24, test_size=6)\nxtrain= df.x[itrain].values\nftrain = df.f[itrain].values\nytrain = df.y[itrain].values\nxtest= df.x[itest].values\nftest = df.f[itest].values\nytest = df.y[itest].values\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\ndef make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n\n\ndegrees=range(21)\ntraintestlists=make_features(xtrain, xtest, degrees)"
  },
  {
    "objectID": "posts/regularization/index.html#constraining-parameters-by-penalty",
    "href": "posts/regularization/index.html#constraining-parameters-by-penalty",
    "title": "Regularization",
    "section": "Constraining parameters by penalty",
    "text": "Constraining parameters by penalty\nUpto now we have focussed on finding the polynomial with the right degree of complexity \\(d^*\\) given the data that we have.\nLet us now ask a different question: if we are going to fit the data with an expressive model such as 20th order polynomials, how can we regularize or smooth or restrict the choices of the kinds of 20th order polynomials that we allow in our fits. In other words, we are again trying to bring down the complexity of the hypothesis space, but by a different tack: a tack which prefers smooth polynomials over wiggly ones.\nThat is, if we want to fit with a 20th order polynomial, ok, lets fit with it, but lets reduce the size of, or limit the functions in \\(\\cal{H}_{20}\\) that we allow.\nIn a sense we have already done this, havent we? When we compared \\(\\cal{H_{1}}\\) over \\(\\cal{H_{20}}\\), we imposed a hard constraint by setting all polynomial co-efficients for terms higher than \\(x\\) to 0. In other words you can think of a line as a 20th degree polynomial with many 0 coefficients. Why not have the math machinery do this for you than do it by hand, and use the data to figure out how to make the cut.\nWe do this by a soft constraint by setting:\n\\[\\sum_{i=0}^j a_i^2 &lt; C.\\]\nThis setting is called the Ridge.\nThis ensures that the coefficients dont get too high, which makes sure we dont get wildly behaving pilynomials with high coefficients. If we set\n\\[\\sum_{i=0}^j | a_i | &lt; C,\\]\nit can be shown that some coefficients will be set to exactly 0. This is called the Lasso.\nIt turns out that we can do this by adding a term to the empirical risk that we minimize on the training data for \\(\\cal{H}_j\\) (seeing why is beyond the scope here but google on lagrange multipliers and the dual problem):\n\\[\\cal{R}(h_j) =  \\sum_{y_i \\in \\cal{D}} (y_i - h_j(x_i))^2 +\\alpha \\sum_{i=0}^j a_i^2.\\]\nThis new risk takes the empirical risk and adds a “penalty term” to it to minimize overfitting. The term is proportional to the sum of the squares of the coefficients and is positive, so it will keep their values down\nThe entire structure is similar to what we did to find the optimal \\(d=*\\), with \\(\\alpha\\) being the analog of \\(d\\). And thus we can use the same validation and cross-validation technology that we developed to estimate \\(d\\).\nThis technique is called regularization or shrinkage as it takes the coefficients \\(a_i\\) towards smaller sizes. As you have seen earlier, for polynomials this corresponds to choosing smooth functions over wiggly functions. When \\(\\alpha=0\\) we have the regular polynomial regression problem, and if we are using 20th order polynomials we will wildly overfit. We are in the high variance zone. The problem with a non-zero \\(\\alpha\\) is called ridge regression. As \\(\\alpha\\) increases, the importance of the penalty term increases at the expense of the ERM term, and we are pushed to increase the smoothness of the polynomials. When \\(\\alpha\\) becomes very large the penalty term dominates and we get into the high bias zone. Thus \\(\\alpha\\) acts as a complexity parameter just like \\(d\\) did, with high complexity being \\(\\alpha \\to 0\\).\n\nA toy example\nEven though we are not doing any proofs, let us illustrate the concept of regularization using a line fit to a sine wave. We fit a straight line to 3 points, choosing 100 such sets of 3 points from the data set.\n\nxs=np.arange(-1.,1.,0.01)\nff = lambda x: np.sin(np.pi*x)\nffxs=ff(xs)\naxes=make_simple_plot()\nc0=sns.color_palette()[0]\nc1=sns.color_palette()[1]\naxes[0].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\naxes[1].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\nfrom sklearn.linear_model import Ridge\nD=np.empty((100,3), dtype=\"int\")\nprint(D.shape)\nfor i in range(100):\n    D[i,:] = np.random.choice(200, replace=False, size=3)\nfor i in range(100):\n    choices = D[i,:]\n    #regular fit\n    p1=np.polyfit(xs[choices], ffxs[choices],1)\n    #ridge fit\n    est = Ridge(alpha=1.0)\n    est.fit(xs[choices].reshape(-1,1), ffxs[choices])\n    axes[0].plot(xs, np.polyval(p1, xs), color=c1, alpha=0.2)\n    axes[1].plot(xs, est.predict(xs.reshape(-1,1)), color=c1, alpha=0.2)\naxes[0].set_title(\"Unregularized\");\naxes[1].set_title(\"Regularized with $\\\\alpha=1.0$\");\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n(100, 3)\n\n\n\n\n\n\n\n\n\nIn the left panel we plot unregularized straight line fits. The plot is hairy, since choosing 3 points from 200 between -1 and 1 dosent constrain the lines much at all. On the right panel, we plot the output of Ridge regression with \\(\\alpha=1\\). This corresponds to adding a term to the empirical risk of \\(\\alpha\\, (a_0^2 + a_1^2)\\) where \\(a_0\\) and \\(a_1\\) are the intercept and slope of the line respectively. Notice that the lines are much more constrained in this second plot. The penalty term has regularized the values of the intercept and slope, and forced the intercept to be closer to 0 and the lines to be flatter.\n\n\nContrast with complexity parameter validation\nNotice that in regularization, we are adding a term to the training error, once \\(\\alpha\\) is defined. It is this term that is estimated..\nWhen we were fitting for the degree of the polynomial, there was no explicit added term, we just fit the regular model. But there, as with regularization, the choice of the hyperparameter was made by comparing validation risks which were calculated by taking the parameters found with fixed hyperparameters on the training set."
  },
  {
    "objectID": "posts/regularization/index.html#regularization-of-the-romney-model-with-cross-validation",
    "href": "posts/regularization/index.html#regularization-of-the-romney-model-with-cross-validation",
    "title": "Regularization",
    "section": "Regularization of the Romney model with Cross-Validation",
    "text": "Regularization of the Romney model with Cross-Validation\n\ndef plot_functions(est, ax, df, alpha, xtest, Xtest, xtrain, ytrain):\n    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n    ax.plot(df.x, df.f, color='k', label='f')\n    ax.plot(xtrain, ytrain, 's', label=\"training\", alpha=0.4)\n    ax.plot(xtest, ytest, 's', label=\"testing\", alpha=0.6)\n    transx=np.arange(0,1.1,0.01)\n    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n    ax.plot(transx, est.predict(transX),  '.', label=\"alpha = %s\" % str(alpha))\n    ax.set_ylim((0, 1))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='lower right')\n    \ndef plot_coefficients(est, ax, alpha):\n    coef = est.coef_.ravel()\n    ax.semilogy(np.abs(coef), marker='o', label=\"alpha = %s\" % str(alpha))\n    ax.set_ylim((1e-1, 1e15))\n    ax.set_ylabel('abs(coefficient)')\n    ax.set_xlabel('coefficients')\n    ax.legend(loc='upper left')\n\nLets now go back to the Romney voting model and see what regularization does to the fits in that model. The addition of a penalty term to the risk or error causes us to choose a smaller subset of the entire set of complex \\(\\cal{H_{20}}\\) polynomials. This is shown in the diagram below where the balance between bias and variance occurs at some subset \\(S_{\\*}\\) of the set of 20th order polynomials indexed by \\(\\alpha_{\\*}\\) (there is an error on the diagram, the 13 there should actually be a 20).\n\n\n\nBias-variance tradeoff controlled by regularization strength alpha\n\n\nLets see what some of the \\(\\alpha\\)s do. The diagram below trains on the entire training set, for given values of \\(\\alpha\\), minimizing the penalty-term-added training error.\n\nfig, rows = plt.subplots(5, 2, figsize=(12, 16))\nd=20\nalphas = [0.0, 1e-5, 1e-3, 1, 10]\nXtrain = traintestlists[d]['train']\nXtest = traintestlists[d]['test']\nfor i, alpha in enumerate(alphas):\n    l,r=rows[i]\n    est = Ridge(alpha=alpha)\n    est.fit(Xtrain, ytrain)\n    plot_functions(est, l, df, alpha, xtest, Xtest, xtrain, ytrain )\n    plot_coefficients(est, r, alpha)\n\n\n\n\n\n\n\n\nAs you can see, as we increase \\(\\alpha\\) from 0 to 1, we start out overfitting, then doing well, and then, our fits, develop a mind of their own irrespective of data, as the penalty term dominates the risk.\nLets use cross-validation to figure what this critical \\(\\alpha_*\\) is. To do this we use the concept of a meta-estimator from scikit-learn. As the API paper puts it:\n\nIn scikit-learn, model selection is supported in two distinct meta-estimators, GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n\nThe concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\nclf = Ridge()\nparameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\ngridclassifier=GridSearchCV(clf, param_grid=parameters, cv=4, scoring=\"mean_squared_error\")\nThe GridSearchCV replaces the manual iteration over thefolds using KFolds and the averaging we did previously, doint it all for us. It takes a parameter grid in the shape of a dictionary as input, and sets \\(\\alpha\\) to the appropriate parameter values one by one. It then trains the model, cross-validation fashion, and gets the error. Finally it compares the errors for the different \\(\\alpha\\)’s, and picks the best choice model.\n\nfrom sklearn.metrics import make_scorer\n#, 1e-6, 1e-5, 1e-3, 1.0\nfrom sklearn.grid_search import GridSearchCV\ndef cv_optimize_ridge(X, y, n_folds=4):\n    clf = Ridge()\n    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n    #the scoring parameter below is the default one in ridge, but you can use a different one\n    #in the cross-validation phase if you want.\n    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=\"mean_squared_error\")\n    gs.fit(X, y)\n    return gs\n\n\nfitmodel = cv_optimize_ridge(Xtrain, ytrain, n_folds=4)\n\n\nfitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n\n(Ridge(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=None,\n    normalize=False, random_state=None, solver='auto', tol=0.001),\n {'alpha': 0.0005},\n -0.005863476137886476,\n [mean: -0.01156, std: 0.00816, params: {'alpha': 1e-08},\n  mean: -0.00643, std: 0.00350, params: {'alpha': 1e-06},\n  mean: -0.00618, std: 0.00355, params: {'alpha': 1e-05},\n  mean: -0.00596, std: 0.00358, params: {'alpha': 5e-05},\n  mean: -0.00589, std: 0.00369, params: {'alpha': 0.0001},\n  mean: -0.00586, std: 0.00424, params: {'alpha': 0.0005},\n  mean: -0.00592, std: 0.00446, params: {'alpha': 0.001},\n  mean: -0.00587, std: 0.00458, params: {'alpha': 0.01},\n  mean: -0.00606, std: 0.00406, params: {'alpha': 0.1},\n  mean: -0.01280, std: 0.00548, params: {'alpha': 1.0}])\n\n\nOur best model occurs for \\(\\alpha=0.01\\). We also output the mean cross-validation error at different \\(\\alpha\\) (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error).\nWe refit the estimator on old training set, and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0.\n\nalphawechoose = fitmodel.best_params_['alpha']\nclf = Ridge(alpha=alphawechoose).fit(Xtrain,ytrain)\n\n\ndef plot_functions_onall(est, ax, df, alpha, xtrain, ytrain, Xtrain, xtest, ytest):\n    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n    ax.plot(df.x, df.f, color='k', label='f')\n    ax.plot(xtrain, ytrain, 's', alpha=0.4, label=\"train\")\n    ax.plot(xtest, ytest, 's', alpha=0.6, label=\"test\")\n    transx=np.arange(0,1.1,0.01)\n    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n    ax.plot(transx, est.predict(transX), '.', alpha=0.6, label=\"alpha = %s\" % str(alpha))\n    #print est.predict(transX)\n    ax.set_ylim((0, 1))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='lower right')\n\n\nfig, rows = plt.subplots(1, 2, figsize=(12, 5))\nl,r=rows\nplot_functions_onall(clf, l, df, alphawechoose, xtrain, ytrain, Xtrain, xtest, ytest)\nplot_coefficients(clf, r, alphawechoose)\n\n\n\n\n\n\n\n\nAs we can see, the best fit model is now chosen from the entire set of 20th order polynomials, and a non-zero hyperparameter \\(\\alpha\\) that we fit for ensures that only smooth models amonst these polynomials are chosen, by setting most of the polynomial coefficients to something close to 0 (Lasso sets them exactly to 0)."
  },
  {
    "objectID": "posts/logisticbp/index.html",
    "href": "posts/logisticbp/index.html",
    "title": "Logistic Regression and Backpropagation",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]\n\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]"
  },
  {
    "objectID": "posts/logisticbp/index.html#logistic-regression-mle",
    "href": "posts/logisticbp/index.html#logistic-regression-mle",
    "title": "Logistic Regression and Backpropagation",
    "section": "Logistic Regression MLE",
    "text": "Logistic Regression MLE\nLogistic regression if one of the well known supervized learning algorithms used for classification.\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the ‘1’ samples from the ‘0’ samples, just like in the diagram above. In other words, we wish to find the “regression” line which divides the samples. Now, a line has the form \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) in 2-dimensions. On one side of this line we have\n\\[w_1 x_1 + w_2 x_2 + w_0 \\ge 0,\\]\nand on the other side we have\n\\[w_1 x_1 + w_2 x_2 + w_0 &lt; 0.\\]\nOur classification rule then becomes:\n\\[\n\\begin{eqnarray}\ny = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &if& \\v{w}\\cdot\\v{x} &lt; 0\n\\end{eqnarray}\n\\]\nwhere \\(\\v{x}\\) is the vector \\(\\{1,x_1, x_2,...,x_n\\}\\) where we have also generalized to more than 2 features.\nWhat hypotheses \\(h\\) can we use to achieve this? One way to do so is to use the sigmoid function:\n\\[h(z) = \\frac{1}{1 + e^{-z}}.\\]\nNotice that at \\(z=0\\) this function has the value 0.5. If \\(z &gt; 0\\), \\(h &gt; 0.5\\) and as \\(z \\to \\infty\\), \\(h \\to 1\\). If \\(z &lt; 0\\), \\(h &lt; 0.5\\) and as \\(z \\to -\\infty\\), \\(h \\to 0\\). As long as we identify any value of \\(y &gt; 0.5\\) as 1, and any \\(y &lt; 0.5\\) as 0, we can achieve what we wished above.\nThis function is plotted below:\n\nh = lambda z: 1./(1+np.exp(-z))\nzs=np.arange(-5,5,0.1)\nplt.plot(zs, h(zs), alpha=0.5);\n\n\n\n\n\n\n\n\nSo we then come up with our rule by identifying:\n\\[z = \\v{w}\\cdot\\v{x}.\\]\nThen \\(h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\ge 0\\) and \\(h(\\v{w}\\cdot\\v{x}) \\lt 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\lt 0\\), and:\n\\[\n\\begin{eqnarray}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray}\n\\]\nWe said above that if \\(h &gt; 0.5\\) we ought to identify the sample with \\(y=1\\)? One way of thinking about this is to identify \\(h(\\v{w}\\cdot\\v{x})\\) with the probability that the sample is a ‘1’ (\\(y=1\\)). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a ‘1’ is \\(\\ge 0.5\\).\nSo suppose we say then that the probability of \\(y=1\\) for a given \\(\\v{x}\\) is given by \\(h(\\v{w}\\cdot\\v{x})\\)?\nThen, the conditional probabilities of \\(y=1\\) or \\(y=0\\) given a particular sample’s features \\(\\v{x}\\) are:\n\\[\\begin{eqnarray}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray}\\]\nThese two can be written together as\n\\[P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} \\]\nThen multiplying over the samples we get the probability of the training \\(y\\) given \\(\\v{w}\\) and the \\(\\v{x}\\):\n\\[P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\]\nWhy use probabilities? Earlier, we talked about how the regression function \\(f(x)\\) never gives us the \\(y\\) exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently.\nWe said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(P(x,y)\\). Thus we need to model \\(y\\) at a given \\(x\\), written as \\(P(y \\mid x)\\), and since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) ,\\]\nand can obtain our joint probability (\\(P(x, y))\\).\nIndeed its important to realize that a particular sample can be thought of as a draw from some “true” probability distribution. If for example the probability of classifying a sample point as a ‘0’ was 0.1, and it turns out that the sample point was actually a ‘0’, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a ‘0’! But, of-course its more unlikely than its likely, and having good probabilities means that we’ll be likely right most of the time, which is what we want to achieve in classification.\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a ‘1’.\nNow if we maximize \\[P(y \\mid \\v{x},\\v{w})\\], we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This maximum likelihood estimation maximises the likelihood of the sample y,\n\\[\\like = P(y \\mid \\v{x},\\v{w}).\\]\nAgain, we can equivalently maximize\n\\[\\loglike = log(P(y \\mid \\v{x},\\v{w}))\\]\nsince the natural logarithm \\(log\\) is a monotonic function. This is known as maximizing the log-likelihood.\n\\[\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).\\]\nThus\n\\[\\begin{eqnarray}\n\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x_i})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x_i})) \\right )\n\\end{eqnarray}\\]\nThe negative of this log likelihood (henceforth abbreviated NLL), is also called the cross-entropy, for reasons that will become clearer soon.\n\\[ NLL = - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\\]\nWe can calculate the gradient of this cost function, and the hessian as well.\n\\[\\nabla_{\\v{w}} NLL = \\sum_i \\v{x_i}^T (p_i - y_i) = \\v{X}^T \\cdot ( \\v{p} - \\v{w} )\\]\n\\[H = \\v{X}^T diag(p_i (1 - p_i))\\v{X}\\] which is positive definite, making the cross-entropy loss convex with a global minimum.\nLogistic regression can be represented by the following diagram. This diagram uses the language of “units”. A linear unit is followed by a non-linear sigmoidal squashing unit, or more precisely a log-sigmoidal squashing unit which is then used to construct the cross-entropy loss.\n\n\n\nLogistic regression as a computational graph: input passes through a linear layer and sigmoid activation to produce the negative log-likelihood cost."
  },
  {
    "objectID": "posts/logisticbp/index.html#softmax-formulation",
    "href": "posts/logisticbp/index.html#softmax-formulation",
    "title": "Logistic Regression and Backpropagation",
    "section": "Softmax formulation",
    "text": "Softmax formulation\nThe softmax formulation of logistic regression comes from the desire togeneralize logistic regression to the multinomial case, that is more than 2 classes. Its instructive to see the two-class problem formulated in softmax as well.\nThe basic idea is to identify the probabilities \\(p_i\\) and \\(1-p_i\\) as two separate probabilities which are constrained to add to 1. That is\n\\[p_{1i} = p_i ; p_{2i} = 1 - p_i. \\]\nThen, the function \\(h\\) which is used to supply this probability can be reformulated as well:\n\\[p_{1i} = \\frac{e^{\\v{w_1} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}\\]\nand\n\\[p_{2i} = \\frac{e^{\\v{w_2} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}\\]\nThe constraint that these probabilities add to 1 is clearly satisfied, but notice that we now have double the number of parameters we had before. We can obtain the old interpretation of \\(p_{1i}\\) by multiplying both the numerator and denominator by \\(e^{-\\v{w_1} \\cdot \\v{x}}\\).\nThis then identifies \\(\\v{w} = \\v{w_1} - \\v{w_2}\\). In general, we can see that we can always translate the coefficients by a fixed amount \\(\\psi\\) without any change. This leads to a lack of “identification” of the parameters in the softmax formalism, which leads to problems for inference that we must fix. On the plus side, it also suggests a way to calculate softmax in a stable way (see the log-sum-exp trick). We’ll tackle both of these later.\nThe softmax formulation of logistic regression can be illustrated using the following diagram, where we now have 2 linear units. These units are now fed into a nonlinear log-softmax unit (which requires outputs of both the linear units) to produce two log-softmax outputs which is then fed to the NLL loss (cross-entropy).\n\n\n\nMulticlass logistic regression: two linear units feed into a softmax layer, producing class probabilities for the negative log-likelihood cost.\n\n\nIn this formalism, we can write the likelihood and thus NLL more succintly:\n\\[\\like = \\prod_i p_{1i}^{\\mathbb1_1(y_i)} p_{2i}^{\\mathbb1_2(y_i)}\\]\n\\[NLL = -\\sum_i \\left( \\mathbb1_1(y_i) log(p_{1i}) + \\mathbb1_2(y_i) log(p_{2i}) \\right)\\]\nWe are now left with 2 gradients (or a combined gradient if you combine both \\(\\v{w_1}\\) and \\(\\v{w_2}\\) into one vector:\n\\[\\frac{\\partial NLL}{\\partial \\v{w_1}} = -\\sum_i \\v{x_i} (y_i - p_{1i})\\]\nand\n\\[\\frac{\\partial NLL}{\\partial \\v{w_2}} = -\\sum_i \\v{x_i} (y_i - p_{2i})\\]"
  },
  {
    "objectID": "posts/logisticbp/index.html#layer-structure",
    "href": "posts/logisticbp/index.html#layer-structure",
    "title": "Logistic Regression and Backpropagation",
    "section": "Layer Structure",
    "text": "Layer Structure\nWriting the \\(NLL\\) slightly differently suggests a layer structure:\n\\[NLL = -\\sum_i \\left( \\mathbb1_1(y_i) log(SM_1(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x})) + \\mathbb1_2(y_i) log(SM_2(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x})) \\right)\\]\n\\[NLL = -\\sum_i \\left( \\mathbb1_1(y_i) LSM_1(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x}) + \\mathbb1_2(y_i) LSM_2(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x}) \\right)\\]\nwhere \\(SM_1 = \\frac{e^{\\v{w_1} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}\\) puts the first argument in the numerator. Ditto for \\(LSM_1\\) which is simply \\(log(SM_1)\\).\nThe layer structure suggested is captured in the diagram below. There are 4 layers, which we shall generally label using the notation \\(\\v{z}^l\\) where the vector on \\(z\\) indicates multiple values and the \\(l\\) indicates the number of the layer (it is NOT a power).\n\n\n\nThe computational graph with intermediate variables z labeled at each layer, preparing for the backpropagation derivation.\n\n\nFirst, the input:\n\\[\\v{z}^1 = \\v{x_i}\\]\nNotice here that we are writing this data-point by data-point. We will follow this structure for everything but the cost function. The dimension of this \\(\\v{z}^1\\) depends on the number of features \\(\\v{x_i}\\) has.\nThen, the second layer.\n\\[\\v{z}^2 = (z^2_1, z^2_2) = (\\v{w_1} \\cdot \\v{x_i}, \\v{w_2} \\cdot \\v{x_i}) = (\\v{w_1} \\cdot \\v{z^1_i}, \\v{w_2} \\cdot \\v{z^1_i})\\]\nThe dimension of \\(\\v{z}^2\\) is 2 corresponding to the two linear layers.\nThen the third layer is also two dimensional, corresponding to the 2 log-softmax functions we have.\n\\[\\v{z}^3 = (z^3_1, z^3_2) = \\left( LSM_1(z^2_1, z^2_2), LSM_2(z^2_1, z^2_2) \\right)\\]\nFinally, the fourth layer is just the cost layer, and is actually a scalar.\n\\[z^4 = NLL(\\v{z}^3) = NLL(z^3_1, z^3_2) = - \\sum_i \\left( \\mathbb1_1(y_i)z^3_1(i) + \\mathbb1_2(y_i)z^3_1(i) \\right)\\]\nNotice how these expressions are different from the fully expanded expressions we had earlier. Here each layer only depends on the previous layer. We shall utilize this structure soon to make our lives easier. Our tool to do this is backpropagation, which is just an example of Reverse Mode differentiation, which as can be surmised from the above structure, is a matter of taking derivatives by substitution, but in a particular order."
  },
  {
    "objectID": "posts/logisticbp/index.html#reverse-mode-differentiation",
    "href": "posts/logisticbp/index.html#reverse-mode-differentiation",
    "title": "Logistic Regression and Backpropagation",
    "section": "Reverse Mode Differentiation",
    "text": "Reverse Mode Differentiation\nWe wont go into many details, but the key observation is this. An operation like finding the loss in our logistic regression problem can be considered as an exercise in function composition, where the last function (\\(z^4\\), or the NLL cost) is a scalar. In other words, we are wanting to calculate:\n\\[Cost = f^{Loss}(\\v{f}^3(\\v{f}^2(\\v{f}^1(\\v{x}))))\\]\nwhere the vectorial function or the \\(\\v{x}\\) is a short form notation for both data and parameters. for this expression. Now, for gradient-descent, when i want to calculate the derivative or gradient with respect to data/parameters \\(\\v{x}\\), i can write:\n\\[\\nabla_{\\v{x}} Cost = \\frac{\\partial f^{Loss}}{\\partial \\v{f}^3}\\,\\frac{\\partial \\v{f}^3}{\\partial \\v{f}^2}\\,\\frac{\\partial \\v{f}^2}{\\partial \\v{f}^1}\\frac{\\partial \\v{f}^1}{\\partial \\v{x}}\\]\nNow, based on the observation that the first term in the above product is a vector while the second is a (Jacobian!) matrix, we can consider rewriting the product in this fashion:\n\\[\\nabla_{\\v{x}} Cost = (((\\frac{\\partial f^{Loss}}{\\partial \\v{f}^3}\\,\\frac{\\partial \\v{f}^3}{\\partial \\v{f}^2})\\,\\frac{\\partial \\v{f}^2}{\\partial \\v{f}^1})\\,\\frac{\\partial \\v{f}^1}{\\partial \\v{x}})\\]\nThis way of writing things always provides us with a vector times a matrix giving us a vector and saves a huge amount of memory, especially on large problems. This is the key idea of reverse mode auto diff.\nBackpropagation falls easily out of this. We add a “cost layer” to \\(z^4\\). The derivative of this layer with respect to \\(z^4\\) will always be 1. We then propagate this derivative back."
  },
  {
    "objectID": "posts/logisticbp/index.html#backpropagation",
    "href": "posts/logisticbp/index.html#backpropagation",
    "title": "Logistic Regression and Backpropagation",
    "section": "Backpropagation",
    "text": "Backpropagation\nEverything comes together now. Let us illustrate with a diagram:\n\n\n\nForward and backward passes through a three-layer network, with error signals δ propagating backward.\n\n\nWe can consider our calculations to now consist of two phases: a forward phase, and a backward phase.\nIn the forward phase, all we are doing is function composition, in the old fashioned way, starting from the bottom-most \\(\\vec{x_i}\\) later, and moving up the layer cake to the cost. (Notice how this is just .forward in pytorch.)\nRULE1: FORWARD\n\\[\\v{z}^{l+1} = \\v{f}^l (\\v{z}^l)\\]\nIn the reverse or backward phase, we now propagate the derivatives backward through the layer cake (pytoych: .backward). By derivatives we mean:\n\\[\\v{\\delta^l} = \\frac{\\partial C}{\\partial \\v{z}^l}\\]\nor (u for unit}\n\\[\\delta^l_u = \\frac{\\partial C}{\\partial z^l_u}\\].\nAnd the formula for this now is very simple, as we will do it iteratively as in reverse-mode differentiation.\nRULE2: BACKWARD\n\\[\\delta^l_u = \\frac{\\partial C}{\\partial z^l_u} = \\sum_v \\frac{\\partial C}{\\partial z^{l+1}_v} \\, \\frac{\\partial z^{l+1}_v}{\\partial z^l_u} = \\sum_v \\delta^{l+1}_v \\, \\frac{\\partial z^{l+1}_v}{\\partial z^l_u} \\]\nOnce again the first term in the product is obviously a vector, and the second term a matrix, so we can recursively get the derivatives all the way down. So this gives us the compositional derivative at any depth we want. In particular we will start with\n\\[\\delta^3_u = \\frac{\\partial z^{4}}{\\partial z^3_u} = \\frac{\\partial C}{\\partial z^3_u}\\]\nwhich is now simply a derivative of the NLL with respect to “dummy” variables representing the LSM functions.\nOne formula remains: the derivatives with respect to any parameter in any layer. In our case we have the \\(\\vec{w}\\) parameters at level-2, but these could be parameters at a higher level as well. For them we have\nRULE 3: Parameters\n\\[\\frac{\\partial C}{\\partial \\theta^l} = \\sum_u \\frac{\\partial C}{\\partial z^{l+1}_u} \\, \\frac{\\partial z^{l+1}_u}{\\partial \\theta^l} = \\sum_u \\delta^{l+1}_u \\frac{\\partial z^{l+1}_u}{\\partial \\theta^l}\\]\nAnother recursion! Uses the derivatives we calculate in the backward pass. Both those derivates and these are used to fill the variable.grad parts of the various parameters in pytorch. This is also the reason for the strange choice to hold the gradients along with the variables rather than with the cost: we back propagate those gradients into the variables, so to speak."
  },
  {
    "objectID": "posts/logisticbp/index.html#coding-a-layer",
    "href": "posts/logisticbp/index.html#coding-a-layer",
    "title": "Logistic Regression and Backpropagation",
    "section": "Coding a layer",
    "text": "Coding a layer\nThis extreme modularity suggests that we can define our own layers (or recursively, even our own combination of layers). even though we wont want to do this for logistic regression, this functionality is useful in defining Artificial Neural Network architectures.\nWhat we must specifically provide is a way to implement the 3 rules for a layer: how to get output \\(z\\) given input \\(z\\), how to get reverse output \\(\\delta\\)s given input ones, and how to differentiate the cost with respect to any local parameters.\n\n\n\nA generic layer l receives activations z from below and error signals δ from above, producing parameter gradients.\n\n\nSuch modularity allows for lots of experimentation."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html",
    "href": "posts/bayes_withsampling/index.html",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as  sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn.apionly as sns\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#frequentist-statistics",
    "href": "posts/bayes_withsampling/index.html#frequentist-statistics",
    "title": "Bayesian Statistics",
    "section": "Frequentist Statistics",
    "text": "Frequentist Statistics\nIn frequentist approach, a parameter estimate is computed by using some function on the data \\(D\\). In the world of frequentist there is a true value of the parameter, which is fixed, however the data are random.\nIn other words, assuming that a given distribution is a good description of the model, there is a true parameter \\(\\theta^{\\star}\\) that characterizes the population. Given the data set(sample) we have, we can estimate the parameter \\(\\hat{\\theta}\\). If we could replicate the experiment many times we would have the sampling distribution of \\(\\theta\\) which can be used to estimate the error on the parameter estimation. By sampling \\(M\\) Data sets \\(D_i\\), each of size \\(N\\), from some true model characterized by \\(\\theta^{\\star}\\) we are able to calculate \\(\\hat{\\theta}_i\\), one for each dataset. This is the sampling distribution.\n\nMaximum Likelihood\nA basic approach to parameter estimation is maximum likelihood (MLE). The goal here is to find the parameter estimates that maximize the likelihood.\nThe likelihood gives us a measure of how likely it is to observe values \\(D={d_1,...,d_n}\\) given the parameters \\(\\theta\\).\nAssumming iid, the likelihood is\n\\[L=\\Lik = \\prod_{i=1}^{n} p(d_i \\vert \\theta)\\]\nHow likely are the observations if the model is true?\nThis corresponds to maximizing the likelihood as a function of \\(\\theta\\) for a given set of observations.\n\\[ \\theta_{ML} = \\arg \\! \\max_{\\theta} \\Lik \\]\nNotice that this method wants to account for every point in the “training set”. So it overfits."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#the-bayesian-approach",
    "href": "posts/bayes_withsampling/index.html#the-bayesian-approach",
    "title": "Bayesian Statistics",
    "section": "The Bayesian Approach",
    "text": "The Bayesian Approach\nIn its essence, the Bayesian approach has two parts.\n\ntreat \\(\\theta\\) as a random variable instead, and to fix the data set. So we dont talk anymore about the data set as a sample from a population, but assume that its all we know about the world.\n\n\nAssociate with the parameter \\(\\theta\\) a prior distribution \\(p(\\theta)\\).\n\nThe prior distribution generally represents our belief on the parameter values when we have not observed any data yet. (I use the wiggle word generally as we might estimate this prior itself from data. This is a useful idea, although philosophically-bayesian purists will frown on it)\n\nPosterior Distribution\nIn a Bayesian context, the first goal is to estimate the posterior distribution over parameter values given our data. This is also known as posterior inference. In other words, we would like to know \\(p(\\theta \\vert D)\\) or \\(p(\\theta \\vert y)\\).\n\\[ p(\\theta \\vert y) = \\frac{p(y \\vert \\theta)\\,p(\\theta)}{p(y)} \\]\nwith the evidence \\(p(D)\\) or \\(p(y)\\) being given by the average of the likelihood (on existing data points) over the prior \\(E_{p(\\theta)}[\\cal{L}]\\):\n\\[p(y) = \\int d\\theta p(y \\vert \\theta) p(\\theta).\\]\nThe evidence is basically the normalization constant. But as we have seen, when we sample, we dont usually worry about the normalization…\nYou can remember this as:\n\\[ posterior = \\frac{likelihood \\times prior}{evidence} \\]\nThis diagram from McElreath’s book gives you an idea of what this might look like, and how the prior might affect the posterior in the absence of copius data…\n\n\n\nPrior times likelihood is proportional to posterior: three rows show how different priors (uniform, step, informative) combine with the same likelihood to produce different posteriors. From McElreath, Statistical Rethinking.\n\n\nWhat if \\(\\theta\\) is multidimensional, as it usually is? Then one can calculate the marginal posterior of one of the parameters by integrating over the other one:\n\\[p(\\theta_1 \\vert D) = \\int d\\theta_{-1} p(\\theta \\vert D).\\]\n\n\nPosterior Predictive\nRemember though at the end of the day, we care about how we are going to make predictions on future data, and not the values of the parameters. Thus what we wish to find is the distribution of a future data point \\(y^*\\), also known as the posterior predictive distribution:\n\\[p(y^* \\vert D=\\{y\\}) = \\int d\\theta p(y^* \\vert \\theta)p(\\theta \\vert \\{y\\})\\].\nIf you like, this is the average of the likelihood at a new point(s) \\(E_{p(\\theta \\vert D)}[p(y \\vert \\theta)]\\).\nIf you think about this, for example, from the perspective of a regression problem, this is the distribution for y at a new x (which in many cases is gaussian). This is not different from the frequentist case. But there the different y’s come from the different samples (typically realized in practice as bootstrap samples).\nWhere do priors come from? They are engineering assumptions we put in to help our models learn. Usually they have some regularizing effect. There is a branch of philosophy that takes the attitude that priors can be based on subjective belief. We dont usually do that in the sciences, but as long as you consistently define a probability system, subjective priors are fine to use.\n\n\nMaximum a posteriori\nThe posterior distribution is specified by a simple product of the likelihood (how likely is the data given the model that uses these parameter estimates) and the prior. In Bayesian data analysis, one way to apply a model to data is to find the maximum a posteriori (MAP) parameter values. The goal in this case is to find the parameter that maximize the posterior probability of the parameters given the data. In other words, we find the mode of the posterior distribution. This corresponds to:\n\\[\n\\begin{eqnarray}\n\\theta_{{\\rm MAP}} &=& \\arg \\max_{\\theta} \\, p(\\theta \\vert D)  \\nonumber \\\\\n                               & =& \\arg \\max_{\\theta}  \\frac{\\Lik \\, p(\\theta)}{p(D)}  \\nonumber \\\\\n                               & =& \\arg \\max_{\\theta}  \\, \\Lik \\, p(\\theta) \\nonumber \\\\\n\\end{eqnarray}\n\\]\nThis looks similar to the maximum likelihood estimation procedure. The difference is that the prior we set over the parameters does influence the parameter estimation.\nThe MAP is an example of a pont-estimate. In general point estimates come from decision risks. For example, the mean comes from a squared-errror risk. The MAP comes from 1-0 loss with equal weghts for all errors. We’ll come to this later.\n\n\nThe posterior predictive\nAt the end of the day we want to make predictions, here for the number of coin tosses (or globe throws) that come up heads (or water). This is given us by the postrior predictive, which is the average of the likelihood at the points where the data is wanted with the posterior.\nThe entire process is illustrated in this diagram, where the posterior is multiplied by the likelihood, one at each point \\(y\\) (number of samples), and then integrated over the parameters.\n\n\n\nThe posterior predictive distribution as a mixture: each parameter value implies a sampling distribution, weighted by the posterior probability, producing the marginal prediction. From McElreath, Statistical Rethinking."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#the-normal-model",
    "href": "posts/bayes_withsampling/index.html#the-normal-model",
    "title": "Bayesian Statistics",
    "section": "The Normal Model",
    "text": "The Normal Model\nA random variable \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Thus its density is given by :\n\\[ p(y \\vert \\mu, \\sigma^2) =  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{y-\\mu}{2 \\sigma})^2} \\]\nSuppose our model is \\(\\{y_1, \\ldots, y_n \\vert \\mu, \\sigma^2 \\} \\sim N(\\mu, \\sigma^2)\\) then the likelihood is\n\\[\np(y_1, \\ldots, y_n \\vert \\mu, \\sigma^2) =\n\\prod_{i=1}^{n} p(y_i \\vert \\mu, \\sigma^2)=\\prod_{i=1}^{n}  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{(y_i-\\mu)^2}{2\\sigma^2})} =\n\\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}}   \\exp \\left\\{  - \\frac{1}{2}  \\sum_i \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right\\}\n\\]\nWe can now write the posterior for this model thus:\n\\[ p( \\mu, \\sigma^2 \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{ - \\frac{1}{2\\sigma^2} \\sum (y_i - \\mu)^2 } \\, p(\\mu, \\sigma^2)\\]\nLets see the posterior of \\(\\mu\\) assuming we know \\(\\sigma^2\\).\n\nNormal Model for fixed \\(\\sigma\\)\nNow we wish to condition on a known \\(\\sigma^2\\). The prior probability distribution for it can then be written as:\n\\[p(\\sigma^2) = \\delta(\\sigma^2 -\\sigma_0^2)\\]\n(which does integrate to 1).\nNow, keep in mind that \\(p(\\mu, \\sigma^2) = p(\\mu \\vert \\sigma^2) p(\\sigma^2)\\) and we must carry out the integral over \\(\\sigma^2\\) to get the \\(\\mu\\) prior. Because of the delta distribution means that we can do everything by just substituting \\(\\sigma_0^2\\) in\nThus, we get the posterior:\n\\[ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2 = \\sigma_0^2)  \\propto p(\\mu \\vert \\sigma^2=\\sigma_0^2) \\,e^{ - \\frac{1}{2\\sigma_0^2} \\sum (y_i - \\mu)^2 }\\]\nwhere I have dropped the \\(\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\) factor as there is no stochasticity in it (its fixed)."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#example-of-the-normal-model-for-fixed-sigma",
    "href": "posts/bayes_withsampling/index.html#example-of-the-normal-model-for-fixed-sigma",
    "title": "Bayesian Statistics",
    "section": "Example of the normal model for fixed \\(\\sigma\\)",
    "text": "Example of the normal model for fixed \\(\\sigma\\)\nWe have data on the wing length in millimeters of a nine members of a particular species of moth. We wish to make inferences from those measurements on the population mean \\(\\mu\\). Other studies show the wing length to be around 19 mm. We also know that the length must be positive. We can choose a prior that is normal and most of the density is above zero (\\(\\mu=19.5,\\tau=10\\)). This is only a marginally informative prior.\nMany bayesians would prefer you choose relatively uninformative priors.\nThe measurements were: 16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8 giving \\(\\bar{y}=18.14\\).\n\nY = [16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8]\n#Data Quantities\nsig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\nmu_data = np.mean(Y)\nn = len(Y)\nprint(\"sigma\", sig, \"mu\", mu_data, \"n\", n)\n\nsigma 1.33092374864 mu 18.1444444444 n 9\n\n\n\n# Prior mean\nmu_prior = 19.5\n# prior std\nstd_prior = 10"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#sampling-by-code",
    "href": "posts/bayes_withsampling/index.html#sampling-by-code",
    "title": "Bayesian Statistics",
    "section": "Sampling by code",
    "text": "Sampling by code\nWe now set up code to do metropolis using logs of distributions:\n\ndef metropolis(logp, qdraw, stepsize, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted = 0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev, stepsize)\n        logp_star = logp(x_star)\n        logp_prev = logp(x_prev)\n        logpdfratio = logp_star -logp_prev\n        u = np.random.uniform()\n        if np.log(u) &lt;= logpdfratio:\n            samples[i] = x_star\n            x_prev = x_star\n            accepted += 1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\ndef prop(x, step):\n    return np.random.normal(x, step)\n\nRemember, that up to normalization, the posterior is the likelihood times the prior. Thus the log of the posterior is the sum of the logs of the likelihood and the prior.\n\nfrom scipy.stats import norm\nlogprior = lambda mu: norm.logpdf(mu, loc=mu_prior, scale=std_prior)\nloglike = lambda mu: np.sum(norm.logpdf(Y, loc=mu, scale=np.std(Y)))\nlogpost = lambda mu: loglike(mu) + logprior(mu)\n\nNow we sample:\n\nx0=np.random.uniform()\nnsamps=40000\nsamps, acc = metropolis(logpost, prop, 1, nsamps, x0)\n\nThe acceptance rate is reasonable. You should shoot for somewhere between 20 and 50%.\n\nacc/nsamps\n\n0.459925\n\n\n\ndef corrplot(trace, maxlags=50):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\n\nWhile thinning is not strictly needed, appropriately thinned, we lose any correlation faster and store less\n\ncorrplot(samps)\n\n\n\n\n\n\n\n\n\ncorrplot(samps[20000::]);\n\n\n\n\n\n\n\n\n\ncorrplot(samps[20000::4]);\n\n\n\n\n\n\n\n\n\nsns.distplot(samps[20000::4], bins=25);\nsns.distplot(samps[20000::], bins=25);\n\n\n\n\n\n\n\n\n\nlike_samples = norm.rvs(loc = mu_data, scale=sig, size=5000)\npost_samples = samps[20000::4]\nprior_samples = norm.rvs(loc = mu_prior, scale=tau, size=5000)"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#comparing-distributions",
    "href": "posts/bayes_withsampling/index.html#comparing-distributions",
    "title": "Bayesian Statistics",
    "section": "Comparing distributions",
    "text": "Comparing distributions\nWe plot samples from the prior against those from the sampling distribution (likelihood considered as a distribution in \\(\\theta\\) and those from the posterior.\n\nplt.hist(like_samples, bins=25, label=\"likelihood (sampling dist)\", alpha=0.3)\nplt.hist(prior_samples, bins=25, label=\"prior\", alpha=0.1)\nplt.hist(post_samples, bins=25, label=\"posterior\", alpha=0.4)\nplt.legend();\n\n\n\n\n\n\n\n\n\npost_pred_func = lambda post: norm.rvs(loc = post, scale = sig)\npost_pred_samples = post_pred_func(post_samples)\n\nWe then plot the posterior predictive against the sampling distribution. These are pretty close. Notice that both are wider than the distribution of the posterior, as they are distributions of \\(y\\) rather than \\(\\mu\\). It just so happens that these distributions are on the scale so it makes sense to plot them together here and compare them.\n\nplt.hist(like_samples, bins=25, label=\"likelihood (sampling dist)\", alpha=0.5)\nplt.hist(post_pred_samples, bins=25, label=\"posterior predictive\", alpha=0.3)\nplt.hist(post_samples, bins=25, label=\"posterior\", alpha=0.4)\nplt.legend();\n\n\n\n\n\n\n\n\n\nsns.distplot(like_samples);\nsns.distplot(post_pred_samples);"
  },
  {
    "objectID": "posts/expectations/index.html",
    "href": "posts/expectations/index.html",
    "title": "Expectations and the Law of Large Numbers",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/expectations/index.html#expectations",
    "href": "posts/expectations/index.html#expectations",
    "title": "Expectations and the Law of Large Numbers",
    "section": "Expectations",
    "text": "Expectations\n\\[ \\newcommand{\\E}[1]{E[#1]}\\]\nThe expectation value of a quantity with respect to the a density or probability mass function is the weighted sum of the quantity where the weights are probabilties from the distribution. For example, for the discrete random variable \\(X\\):\n\\[E_f[X] = \\sum_x x\\,f(x).\\]\nIn the continuous case the sum is replaced by an integral over the density:\n\\[E_f[X] = \\int x\\,f(x) dx = \\int x dF(x),\\]\nwhere the latter form makes it clear that you are weighing with probabilities from the distribution even in the continuous case.\nThe latter form is often used to establish notation. Thus, the expected value, or mean, or first moment, of X is defined to be \\[\nE_{f}{X} = \\int x dF(x) =\n\\begin{cases}\n\\sum_x x f(x) & \\text{if X is discrete}\\\\\n\\int x f(x) dx & \\text{if X is continuous}\n\\end{cases}\n\\] assuming that the sum (or integral) is well defined. The notation is a unifying notation which nevertheless has a grounding in measure theory; the discrete sum can be said to be an integral with respect to a counting measure.\nA note on notation: we’ll use \\(E_f\\) or sometimes even \\(E_F\\) when we need to make clear what the distribution is. If its clear (or we are being lazy) we might just drop the subscript. Nevertheless, wheneve you see an expectation, YOU MUST ASK, with what density/mass-function or distribution is it with respect to.\n\nThe mean of a distribution\n\\(E_f[X]\\) if often just called the mean of the mass function or density. This definition is analogous to the one for the arithmetic mean of a dataset: the only difference is that we want to give more weight to more probable values.\n\n\nLOTUS: Law of the unconscious statistician\nAlso known as The rule of the lazy statistician.\nTheorem:\nif \\(Y = r(X)\\), \\[\n\\E{Y} = \\int r(x) dF(x)\n\\]\nExample:\nSpecifically, let A be an event and let \\(r(x) = I_A (x)\\) where \\(I_A (x) = 1\\) if \\(x \\in A\\) and \\(I_A (x) = 0\\) if \\(x \\notin A\\). Then: \\[\n\\E{I_A (X)} = \\int I_A (x) dF(x) = \\int_A f_X (x) dx = p(X \\in A)\n\\]\n\n\nVariance of a distribution\nThe variance of a distribution is defined analogous to that of a dataset:\n\\[V_f[X] = E_f[(X-E_f[X])^2]\\].\nFor the Bernoulli distribution \\(f(x)=p=constant\\), and you are summing it over ones as opposed to 0’s, so the mean is just p. The variance is \\((1-p)^2\\times p +(-p)^2\\times (1-p) = p(1-p)(1-p+p) = p(1-p)\\).\nIn general, we can find this mean that by obtaining a large bunch of samples from the distribution and find their arithmetic mean. The justification for this is the Law of large numbers, which we’ll come to soon.\nHowever the intuition is obvious: for a large number of samples, the frequencies will tract probabilities well, so high probability samples with roughly the same value will re-occur, and a simple arithmetic sun will capture the curves of the distribution."
  },
  {
    "objectID": "posts/expectations/index.html#the-law-of-large-numbers",
    "href": "posts/expectations/index.html#the-law-of-large-numbers",
    "title": "Expectations and the Law of Large Numbers",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nImagine a sequence of length n of coin flips. Lets keep increasing the length of the sequence of coin flips n, and compute a running average \\(S_n\\) of the coin-flip random variables, \\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i .\\] We plot this running mean, and notice that it converges to the mean of the distribution from which the random variables are plucked, ie the Bernoulli distribution with p=0.5.\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\nrandom_flips = throw_a_coin(10000)\nrunning_means = np.zeros(10000)\nsequence_lengths = np.arange(1,10001,1)\nfor i in sequence_lengths:\n    running_means[i-1] = np.mean(random_flips[:i])\n\n\nplt.plot(sequence_lengths, running_means);\nplt.xscale('log')\n\n\n\n\n\n\n\n\nThis is an example of a very important theorem in statistics, the law of large numbers, which says this:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) values from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to the mean of \\(X\\) \\(\\mu\\) as \\(n \\to \\infty\\):\n\\[ S_n \\to \\mu \\, as \\, n \\to \\infty. \\]"
  },
  {
    "objectID": "posts/expectations/index.html#frequentist-interpretation-of-probability",
    "href": "posts/expectations/index.html#frequentist-interpretation-of-probability",
    "title": "Expectations and the Law of Large Numbers",
    "section": "Frequentist interpretation of probability",
    "text": "Frequentist interpretation of probability\nThe law of large numbers is what makes the frequentist interpretation of probability possible to use in practise.\nWe saw above from the LOTUS that if we consider any event \\(A\\) from a probability distribution \\(F\\) with random variable X, and consider the indicator function \\(I_A\\) such that:\n\\[\\begin{eqnarray}\nI_A(x) = 1 \\,&& if \\, x \\in A\\\\\nI_A(x) = 0 \\,&&  otherwise\n\\end{eqnarray}\\]\nwe have that:\n\\[E_{F}[I_A (X)] = p(X \\in A)\\]\nOne can think of variable \\(Z=I_A(X)\\) as Bernoulli random variable with parameter and thus p = P(A). The question then arises: how do we estimate this expectation value and thus the probability?\nNow if we take a long sequence from \\(X\\) and thus \\(Z\\), then the frequency of successes (where success means being in A) will converge by the law of large numbers to the true probability p."
  },
  {
    "objectID": "posts/testingtraining/index.html",
    "href": "posts/testingtraining/index.html",
    "title": "Learning Bounds and the Test Set",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/testingtraining/index.html#revisiting-the-model",
    "href": "posts/testingtraining/index.html#revisiting-the-model",
    "title": "Learning Bounds and the Test Set",
    "section": "Revisiting the model",
    "text": "Revisiting the model\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\nLet us assume that we have a “population” of 200 counties \\(x\\):\n\ndffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\nLets suppose now that the Lord came by and told us that the points in the plot below captures \\(f(x)\\) exactly.\n\nx=dffull.rfrac.values\nf=dffull.promney.values\nplt.plot(x,f,'.', alpha=0.3)\n\n\n\n\n\n\n\n\nNotice that our sampling of \\(x\\) is not quite uniform: there are more points around \\(x\\) of 0.7.\nNow, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample \\(\\cal{D}\\) of 30 data points. Such data is called in-sample data. Contrastingly, the entire population of data points is also called out-of-sample data.\n\ndf = pd.read_csv(\"data/noisysample.csv\")\ndf.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\naxes[0].plot(x,f, 'r.', alpha=0.2, label=\"population\");\naxes[1].plot(df.x,df.f, 'o', alpha=0.6, label=\"in-sample noiseless data $\\cal{D}$\");\naxes[1].plot(df.x,df.y, 's', alpha=0.6, label=\"in-sample noisy data $\\cal{D}$\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);"
  },
  {
    "objectID": "posts/testingtraining/index.html#testing-and-training-sets",
    "href": "posts/testingtraining/index.html#testing-and-training-sets",
    "title": "Learning Bounds and the Test Set",
    "section": "Testing and Training Sets",
    "text": "Testing and Training Sets\nThe process of learning has two parts:\n\nFit for a model by minimizing the in-sample risk\nHope that the in-sample risk approximates the out-of-sample risk well.\n\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out \\,of \\,sample} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nHoping does not befit us as scientists. How can we test that the in-sample risk approximates the out-of-sample risk well?\nThe “aha” moment comes when we realize that we can hold back some of our sample, and test the performance of our learner by trying it out on this held back part! Perhaps we can compute the error or risk on the held-out part, or “test” part of our sample, and have something to say about the out-of-sample error.\nLet us introduce some new terminology. We take the sample of data \\(\\cal{D}\\) that we have been given (our in-sample set) and split it into two parts:\n\nThe training set, which is the part of the data we use to fit a model\nThe testing set, a smaller part of the data set which we use to see how good our fit was.\n\nThis split is done by choosing points at random into these two sets. Typically we might take 80% of our data and put it in the training set, with the remaining amount going into the test set. This can be carried out in python using the train_test_split function from sklearn.cross_validation.\nThe split is shown in the diagram below:\n\n\n\nSplitting dataset D into training and test sets (image after Learning from Data)\n\n\nWe ARE taking a hit on the amount of data we have to train our model. The more data we have, the better we can do for our fits. But, you cannot figure out the generalization ability of a learner by looking at the same data it was trained on: there is nothing to generalize to, and as we know we can fit very complex models to training data which have no hope of generalizing (like an interpolator). Thus, to estimate the out-of-sample error or risk, we must leave data over to make this estimation.\nAt this point you are thinking: the test set is just another sample of the population, just like the training set. What guarantee do we have that it approximates the out-of-sample error well? And furthermore, if we pick 6 out of 30 points as a test set, why would you expect the estimate to be any good?\nWe will kind-of hand wavingly show later that the test set error is a good estimate of the out of sample error, especially for larger and larger test sets. You are right to worry that 6 points is perhaps too few, but thats what we have for now, and we shall work with them.\nWe are using the training set then, as our in-sample set, and the test set as a proxy for out-of-sample..\n\nfrom sklearn.cross_validation import train_test_split\ndatasize=df.shape[0]\n#split dataset using the index, as we have x,f, and y that we want to split.\nitrain,itest = train_test_split(range(30),train_size=24, test_size=6)\nxtrain= df.x[itrain].values\nftrain = df.f[itrain].values\nytrain = df.y[itrain].values\nxtest= df.x[itest].values\nftest = df.f[itest].values\nytest = df.y[itest].values\n\n\naxes=make_plot()\naxes[0].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\naxes[0].plot(df.x,df.y, 'o',alpha=0.6, label=\"$\\cal{D}$\");\naxes[1].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\naxes[1].plot(xtrain, ytrain, 's', label=\"training\")\naxes[1].plot(xtest, ytest, 's', label=\"testing\")\naxes[0].legend(loc=\"lower right\")\naxes[1].legend(loc=\"lower right\")"
  },
  {
    "objectID": "posts/testingtraining/index.html#a-digression-about-scikit-learn",
    "href": "posts/testingtraining/index.html#a-digression-about-scikit-learn",
    "title": "Learning Bounds and the Test Set",
    "section": "A digression about scikit-learn",
    "text": "A digression about scikit-learn\nScikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as train_test_split. It can be used in python by the incantation import sklearn.\nThe library has a very well defined interface. This makes the library a joy to use, and surely contributes to its popularity. As the scikit-learn API paper [Buitinck, Lars, et al. “API design for machine learning software: experiences from the scikit-learn project.” arXiv preprint arXiv:1309.0238 (2013).] says:\n\nAll objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: an estimator interface for building and ﬁtting models, a predictor interface for making predictions and a transformer interface for converting data. The estimator interface is at the core of the library. It deﬁnes instantiation mechanisms of objects and exposes a fit method for learning a model from training data. All supervised and unsupervised learning algorithms (e.g., for classiﬁcation, regression or clustering) are oﬀered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n\nEarlier we fit y using the python function polyfit. To get you familiarized with scikit-learn, we’ll use the “estimator” interface here, specifically the estimator PolynomialFeatures. The API paper again:\n\nSince it is common to modify or ﬁlter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which deﬁnes a transform method. It takes as input some new data X and yields as output a transformed version of X. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n\nTo start with we have one feature x, the fraction of religious people in a county, which we want to use to predict y, the fraction of people voting for Romney in that county. What we will do is the transformation:\n\\[ x \\rightarrow 1, x, x^2, x^3, ..., x^d \\]\nfor some power \\(d\\). Our job then is to fit for the coefficients of these features in the polynomial\n\\[ a_0 + a_1 x + a_2 x^2 + ... + a_d x^d. \\]\n\nTransformers in sklearn\nIn other words, we have transformed a function of one feature, into a (rather simple) linear function of many features. To do this we first construct the estimator as PolynomialFeatures(d), and then transform these features into a d-dimensional space using the method fit_transform.\n\n\n\nPolynomial feature transform: expanding x into a design matrix\n\n\nHere is an example. The reason for using [[1],[2],[3]] as opposed to [1,2,3] is that scikit-learn expects data to be stored in a two-dimensional array or matrix with size [n_samples, n_features].\n\nfrom sklearn.preprocessing import PolynomialFeatures\nPolynomialFeatures(3).fit_transform([[1],[2], [3]])\n\narray([[  1.,   1.,   1.,   1.],\n       [  1.,   2.,   4.,   8.],\n       [  1.,   3.,   9.,  27.]])\n\n\nTo transform [1,2,3] into [[1],[2],[3]] we need to do a reshape.\n\n\n\nNumPy reshape: converting a 1D array to a column vector\n\n\n\nnp.array([1,2,3]).reshape(-1,1)\n\narray([[1],\n       [2],\n       [3]])\n\n\nSo now we are in the recatangular, rows=samples, columns=features form expected by scikit-learn. Ok, so lets see the process to transform our 1-D dataset x into a d-dimensional one.\n\nxtrain\n\narray([ 0.33      ,  0.75868254,  0.52      ,  0.79      ,  0.63633949,\n        0.70533267,  0.71829603,  0.75841654,  0.63071361,  0.11      ,\n        0.82850909,  0.46      ,  0.64832591,  0.53596824,  0.91      ,\n        0.67      ,  0.76      ,  0.34      ,  0.56      ,  0.94      ,\n        0.6       ,  0.96      ,  0.43754875,  0.54      ])\n\n\n\nxtrain.reshape(-1,1)\n\narray([[ 0.33      ],\n       [ 0.75868254],\n       [ 0.52      ],\n       [ 0.79      ],\n       [ 0.63633949],\n       [ 0.70533267],\n       [ 0.71829603],\n       [ 0.75841654],\n       [ 0.63071361],\n       [ 0.11      ],\n       [ 0.82850909],\n       [ 0.46      ],\n       [ 0.64832591],\n       [ 0.53596824],\n       [ 0.91      ],\n       [ 0.67      ],\n       [ 0.76      ],\n       [ 0.34      ],\n       [ 0.56      ],\n       [ 0.94      ],\n       [ 0.6       ],\n       [ 0.96      ],\n       [ 0.43754875],\n       [ 0.54      ]])\n\n\n\nPolynomialFeatures(2).fit_transform(xtrain.reshape(-1,1))\n\narray([[ 1.        ,  0.33      ,  0.1089    ],\n       [ 1.        ,  0.75868254,  0.5755992 ],\n       [ 1.        ,  0.52      ,  0.2704    ],\n       [ 1.        ,  0.79      ,  0.6241    ],\n       [ 1.        ,  0.63633949,  0.40492794],\n       [ 1.        ,  0.70533267,  0.49749418],\n       [ 1.        ,  0.71829603,  0.51594919],\n       [ 1.        ,  0.75841654,  0.57519565],\n       [ 1.        ,  0.63071361,  0.39779966],\n       [ 1.        ,  0.11      ,  0.0121    ],\n       [ 1.        ,  0.82850909,  0.68642731],\n       [ 1.        ,  0.46      ,  0.2116    ],\n       [ 1.        ,  0.64832591,  0.42032648],\n       [ 1.        ,  0.53596824,  0.28726196],\n       [ 1.        ,  0.91      ,  0.8281    ],\n       [ 1.        ,  0.67      ,  0.4489    ],\n       [ 1.        ,  0.76      ,  0.5776    ],\n       [ 1.        ,  0.34      ,  0.1156    ],\n       [ 1.        ,  0.56      ,  0.3136    ],\n       [ 1.        ,  0.94      ,  0.8836    ],\n       [ 1.        ,  0.6       ,  0.36      ],\n       [ 1.        ,  0.96      ,  0.9216    ],\n       [ 1.        ,  0.43754875,  0.1914489 ],\n       [ 1.        ,  0.54      ,  0.2916    ]])\n\n\n\n\nFitting in sklearn\nOnce again, lets see the structure of scikit-learn needed to make these fits. .fit always takes two arguments:\nestimator.fit(Xtrain, ytrain).\nHere Xtrain must be in the form of an array of arrays, with the inner array each corresponding to one sample, and whose elements correspond to the feature values for that sample. (This means that the 4th element for each of these arrays, in our polynomial example, corresponds to the valueof \\(x^3\\) for each “sample” \\(x\\)). The ytrain is a simple array of responses..continuous for regression problems, and categorical values or 1-0’s for classification problems.\n\n\n\nScikit-learn train/test data layout: X_train, y_train, X_test, y_test\n\n\nThe test set Xtest has the same structure, and is used in the .predict interface. Once we have fit the estimator, we predict the results on the test set by:\nestimator.predict(Xtest).\nThe results of this are a simple array of predictions, of the same form and shape as ytest.\nA summary of the scikit-learn interface can be found here:\nhttp://nbviewer.jupyter.org/github/jakevdp/sklearn_pycon2015/blob/master/notebooks/02.2-Basic-Principles.ipynb#Recap:-Scikit-learn’s-estimator-interface\nLets put this alltogether. Below we write a function to create multiple datasets, one for each polynomial degree:\n\ndef make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist"
  },
  {
    "objectID": "posts/testingtraining/index.html#how-do-training-and-testing-error-change-with-complexity",
    "href": "posts/testingtraining/index.html#how-do-training-and-testing-error-change-with-complexity",
    "title": "Learning Bounds and the Test Set",
    "section": "How do training and testing error change with complexity?",
    "text": "How do training and testing error change with complexity?\nYou will recall that the big question we were left with earlier is: what order of polynomial should we use to fit the data? Which order is too biased? Which one has too much variance and is too complex? Let us try and answer this question.\nWe do this by fitting many different models (remember the fit is made by minimizing the empirical risk on the training set), each with increasing dimension d, and looking at the training-error and the test-error in each of these models. So we first try \\(\\cal{H}_0\\), then \\(\\cal{H}_1\\), then \\(\\cal{H}_2\\), and so on.\nSince we use PolynomialFeatures above, each increasing dimension gives us an additional feature. \\(\\cal{H}_5\\) has 6 features, a constant and the 5 powers of x. What we want to do is to find the coefficients of the 5-th order polynomial that best fits the data. Since the polynomial is linear in the coefficients (we multiply coefficients by powers-of-x features and sum it up), we use a learner called a LinearRegression model (remember that the “linear” in the regression refers to linearity in co-efficients). The scikit-learn interface to make such a fit is also very simple, the function fit. And once we have learned a model, we can predict using the function predict. The API paper again:\n\nThe predictor interface extends the notion of an estimator by adding a predict method that takes an array X_test and produces predictions for X_test, based on the learned parameters of the estimator.\n\nSo, for increasing polynomial degree, and thus feature dimension d, we fit a LinearRegression model on the traing set. We then use scikit-learn again to calculate the error or risk. We calculate the mean_squared_error between the model’s predictions and the data, BOTH on the training set and test set. We plot this error as a function of the defree of the polynomial d.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndegrees=range(21)\nerror_train=np.empty(len(degrees))\nerror_test=np.empty(len(degrees))\n\ntraintestlists=make_features(xtrain, xtest, degrees)\n\n\ntraintestlists[3]['train'], ytrain\n\n(array([[ 1.        ,  0.33      ,  0.1089    ,  0.035937  ],\n        [ 1.        ,  0.75868254,  0.5755992 ,  0.43669706],\n        [ 1.        ,  0.52      ,  0.2704    ,  0.140608  ],\n        [ 1.        ,  0.79      ,  0.6241    ,  0.493039  ],\n        [ 1.        ,  0.63633949,  0.40492794,  0.25767164],\n        [ 1.        ,  0.70533267,  0.49749418,  0.3508989 ],\n        [ 1.        ,  0.71829603,  0.51594919,  0.37060426],\n        [ 1.        ,  0.75841654,  0.57519565,  0.4362379 ],\n        [ 1.        ,  0.63071361,  0.39779966,  0.25089766],\n        [ 1.        ,  0.11      ,  0.0121    ,  0.001331  ],\n        [ 1.        ,  0.82850909,  0.68642731,  0.56871127],\n        [ 1.        ,  0.46      ,  0.2116    ,  0.097336  ],\n        [ 1.        ,  0.64832591,  0.42032648,  0.27250855],\n        [ 1.        ,  0.53596824,  0.28726196,  0.15396329],\n        [ 1.        ,  0.91      ,  0.8281    ,  0.753571  ],\n        [ 1.        ,  0.67      ,  0.4489    ,  0.300763  ],\n        [ 1.        ,  0.76      ,  0.5776    ,  0.438976  ],\n        [ 1.        ,  0.34      ,  0.1156    ,  0.039304  ],\n        [ 1.        ,  0.56      ,  0.3136    ,  0.175616  ],\n        [ 1.        ,  0.94      ,  0.8836    ,  0.830584  ],\n        [ 1.        ,  0.6       ,  0.36      ,  0.216     ],\n        [ 1.        ,  0.96      ,  0.9216    ,  0.884736  ],\n        [ 1.        ,  0.43754875,  0.1914489 ,  0.08376823],\n        [ 1.        ,  0.54      ,  0.2916    ,  0.157464  ]]),\n array([ 0.35817449,  0.64634662,  0.47094573,  0.80195369,  0.71040586,\n         0.64431987,  0.81167767,  0.81232659,  0.65597413,  0.18382092,\n         0.76638914,  0.52531463,  0.72006043,  0.53688748,  0.91261385,\n         0.89700996,  0.7612565 ,  0.23599998,  0.58004131,  0.93613422,\n         0.60188686,  0.87217807,  0.49208494,  0.61984169]))\n\n\n\ntraintestlists[3]['test'], ytest\n\n(array([[  1.00000000e+00,   6.60000000e-01,   4.35600000e-01,\n           2.87496000e-01],\n        [  1.00000000e+00,   2.30000000e-01,   5.29000000e-02,\n           1.21670000e-02],\n        [  1.00000000e+00,   8.09657516e-01,   6.55545293e-01,\n           5.30767174e-01],\n        [  1.00000000e+00,   7.00000000e-02,   4.90000000e-03,\n           3.43000000e-04],\n        [  1.00000000e+00,   9.00000000e-02,   8.10000000e-03,\n           7.29000000e-04],\n        [  1.00000000e+00,   7.49902667e-01,   5.62354010e-01,\n           4.21710772e-01]]),\n array([ 0.60311145,  0.05762073,  0.79714359,  0.13897264,  0.05051023,\n         0.74855785]))\n\n\n\nEstimating the out-of-sample error\nWe can then use mean_squared_error from sklearn to calculate the error between the predictions and actual ytest values. Below we calculate this error on both the training set (which we already fit on) and the test set (which we hadnt seen before), and plot how these errors change with the degree of the polynomial.\n\nest3 = LinearRegression()\nest3.fit(traintestlists[3]['train'], ytrain)\npred_on_train3=est3.predict(traintestlists[3]['train'])\npred_on_test3=est3.predict(traintestlists[3]['test'])\n\n\nprint(\"errtrain\",mean_squared_error(ytrain, pred_on_train3))\nprint(\"errtest\",mean_squared_error(ytest, pred_on_test3))\n\nerrtrain 0.00455053325387\nerrtest 0.00949690985891\n\n\nLet us now do this for a polynomial of degree 19\n\nest19 = LinearRegression()\nest19.fit(traintestlists[19]['train'], ytrain)\npred_on_train19=est19.predict(traintestlists[19]['train'])\npred_on_test19=est19.predict(traintestlists[19]['test'])\nprint(\"errtrain\",mean_squared_error(ytrain, pred_on_train19))\nprint(\"errtest\",mean_squared_error(ytest, pred_on_test19))\n\nerrtrain 0.00196640248639\nerrtest 14125204461.8\n\n\nYou can see that the test set error is larger, corresponding to an overfit model thats doing very well on some points and awful on other.\n\n\nFinding the appropriate complexity\nLets now carry out this minimization systematically for each polynomial degree d.\n\nfor d in degrees:#for increasing polynomial degrees 0,1,2...\n    Xtrain = traintestlists[d]['train']\n    Xtest = traintestlists[d]['test']\n    #set up model\n    #fit\n    #predict\n    #calculate mean squared error\n    #set up model\n    est = LinearRegression()\n    #fit\n    est.fit(Xtrain, ytrain)\n    #predict\n    prediction_on_training = est.predict(Xtrain)\n    prediction_on_test = est.predict(Xtest)\n    #calculate mean squared error\n    error_train[d] = mean_squared_error(ytrain, prediction_on_training)\n    error_test[d] = mean_squared_error(ytest, prediction_on_test)\n\n\nplt.plot(degrees, error_train, marker='o', label='train (in-sample)')\nplt.plot(degrees, error_test, marker='o', label='test')\nplt.axvline(np.argmin(error_test), 0,0.5, color='r', label=\"min test error at d=%d\"%np.argmin(error_test), alpha=0.3)\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper left')\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\nThe graph shows a very interesting structure. The training error decreases with increasing degree of the polynomial. This ought to make sense given what you know now: one can construct an arbitrarily complex polynomial to fit all the training data: indeed one could construct an order 24 polynomial which perfectly interpolates the 24 data points in the training set. You also know that this would do very badly on the test set as it would wiggle like mad to capture all the data points. And this is indeed what we see in the test set error.\nFor extremely low degree polynomials like \\(d=0\\) a flat line capturing the mean value of the data or \\(d=1\\) a straight line fitting the data, the polynomial is not curvy enough to capturve the conbtours of the data. We are in the bias/deterministic error regime, where we will always have some difference between the data and the fit since the hypothesis is too simple. But, for degrees higher than 5 or so, the polynomial starts to wiggle too much to capture the training data. The test set error increases as the predictive power of the polynomial goes down thanks to the contortions it must endure to fit the training data.\nThus the test set error first decreases as the model get more expressive, and then, once we exceed a certain level of complexity (here indexed by \\(d\\)), it increases. This idea can be used to identify just the right amount of complexity in the model by picking as the best hypothesis as the one that minimizes test set error or risk. In our case this happens around \\(d=4\\). (This exact number will depend on the random points chosen into the training and test sets) For complexity lower than this critical value, identified by the red vertical line in the diagram, the hypotheses underfit; for complexity higher, they overfit.\n\n\n\nBias-variance tradeoff: underfitting vs overfitting as complexity increases\n\n\nKeep in mind that as you see in the plot above this minimum can be shallow: in this case any of the low order polynomials would be “good enough”."
  },
  {
    "objectID": "posts/testingtraining/index.html#is-this-still-a-test-set",
    "href": "posts/testingtraining/index.html#is-this-still-a-test-set",
    "title": "Learning Bounds and the Test Set",
    "section": "Is this still a test set?",
    "text": "Is this still a test set?\nBut something should be troubling you about this discussion. We have made no discussion on the error bars on our error estimates, primarily because we have not carried out any resampling to make this possible.\nBut secondly we seem to be “visually fitting” a value of \\(d\\). It cant be kosher to use as a test set something you did some fitting on…\nWe have contaminated our test set. The moment we use it in the learning process, it is not a test set.\nThe answer to the second question is to use a validation set, and leave a separate test set aside. The answer to the first is to use cross-validation, which is a kind of resampling method that uses multiple validation sets!\nTO make some of these concepts more concrete, let us understand the mathematics behind finite sized samples and the learning process.\n\nLearning from finite sized samples\nIf we have very large samples, the law of large numbers tells us that we can estimate expectations nicely by making sample averages.\nHowever, we rarely have very large samples in learning situations (unlike when we are looking for posteriors). But, we can use Hoeffding’s inequality to understand how our sample quantities differ from the population ones.\nHoeffding’s inequality applies to the situation where we have a population of binary random variables with fraction \\(\\mu\\) of things of one type (heads vs tails, red vs green). We do not have access to this population, but rather, to a sample drawn with replacement from this population, where the fraction is \\(\\nu\\).\nThen (where the probability can be thought of as amongst many samples):\n\\[P(\\vert \\nu - \\mu \\vert &gt; \\epsilon) \\le 2e^{-2\\epsilon^2 N}\\]\nwhere N is the size of the sample. Clearly the sample fraction approaches the population fraction as N gets very large.\nTo put this in the context of the learning problem for a hypothesis \\(h\\), identify heads(1) with \\(h(x_i) \\ne f(x_i)\\) at sample \\(x_i\\), and tails(0) otherwise. Then \\(\\mu\\) is the error rate (also called the 1-0 loss) in the population, which we dont know, while \\(\\nu\\) is the same for the sample. It can be shown that similar results hold for the mean-squared error.\nThen one can say:\n\\[P(\\vert R_{in}(h) - R_{out}(h) \\vert &gt; \\epsilon) \\le 2e^{-2\\epsilon^2 N}\\]\nNow notice that we fit a \\(h=g\\) on the training sample. This means that we see as many hypothesis as there are in out hypothesis space. Typically this is infinite, but learning theory allows us to consider a finite effective hypothesis space size, as most hypothesis are not that different from each other. (This is formalized in VC theory, definitely out of scope for this class).\nThe problem here is that the Hoeffding inequality holds ONCE we have picked a hypothesis \\(h\\), as we need it to label the 1 and 0s. But over the training set we one by one pick all the models in the hypothesis space, before discarding all but one. Thus Hoeffding’s inequality does not hold.\nHowever what you can do is this: since the best fit \\(g\\) is one of the \\(h\\) in the hypothesis space \\(\\cal{H}\\), \\(g\\) must be either \\(h_1\\) OR \\(h_2\\) OR….and there are say effectively M such choices.\nThen:\n\\[P(\\vert R_{in}(g) - R_{out}(g) \\vert \\ge \\epsilon) &lt;= \\sum_{h_i \\in \\cal{H}}  P(\\vert R_{in}(h_i) - R_{out}(h_i) \\vert \\ge \\epsilon) &lt;=  2\\,M\\,e^{-2\\epsilon^2 N}\\]\nThus this tells us that for \\(N &gt;&gt; M\\) our in-sample risk and out-of-sample risk converge asymptotically and that minimizing our in-sample risk can be used as a proxy for minimizing the unknown out-of-sample risk.\nThus we do not have to hope any more and learning is feasible.\nThis also tells us something about complexity. M is a measure of this complexity, and it tells us that our bound is worse for more complex hypothesis spaces. This is our notion of overfitting.\nThe Hoeffding inequality can be repharased. Pick a tolerance \\(\\delta\\). Then, note that with probability \\(1 - 2\\,M\\,e^{-2\\epsilon^2 N}\\), \\(\\vert R_{out} - R_{in} \\vert &lt; \\epsilon\\). This means\n\\[R_{out} &lt;= R_{in} + \\epsilon\\]\nNow let \\(\\delta =  2\\,M\\,e^{-2\\epsilon^2 N}\\).\nThen, with probability \\(1-\\delta\\):\n\\[R_{out} &lt;= R_{in} + \\sqrt{\\frac{1}{2N}ln(\\frac{2M}{\\delta})}\\]\n\n\nWhat about the test set?\nThe bound above can now be used to understand why the test set idea is a good one. One objection to using a test set might be that it just seems to be another sample like the training sample. What so great about it? How do we know that low test error means we generalize well?\nThe key observation here is that the test set is looking at only one hypothesis because the fitting is already done on the training set. So \\(M=1\\) for this sample, and the “in-test-sample” error approaches the population error much faster! Also, the test set does not have an optimistic bias like the training set, which is why the training set bound had the larger effective M factor.\nThis is also why, once you start fitting for things like the complexity parameter on the test set, you cant call it a test set any more since we lose this tight guarantee.\nFinally, a test set has a cost. You have less data in the training set and must thus fit a less complex model."
  },
  {
    "objectID": "posts/bayesianregression/index.html",
    "href": "posts/bayesianregression/index.html",
    "title": "Bayesian Regression",
    "section": "",
    "text": "\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]\n%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nfrom scipy.stats import norm\nfrom scipy.stats import multivariate_normal\ndef cplot(f, ax=None):\n    if not ax:\n        plt.figure(figsize=(4,4))\n        ax=plt.gca()\n    xx,yy=np.mgrid[-1:1:.01,-1:1:.01]\n    pos = np.empty(xx.shape + (2,))\n    pos[:, :, 0] = xx\n    pos[:, :, 1] = yy\n    ax.contourf(xx, yy, f(pos))\n    #data = [x, y]\n    return ax\ndef plotSampleLines(mu, sigma, numberOfLines, dataPoints=None, ax=None):\n    #Plot the specified number of lines of the form y = w0 + w1*x in [-1,1]x[-1,1] by\n    # drawing w0, w1 from a bivariate normal distribution with specified values\n    # for mu = mean and sigma = covariance Matrix. Also plot the data points as\n    # blue circles. \n    #print \"datap\",dataPoints\n    if not ax:\n        plt.figure()\n        ax=plt.gca()\n    for i in range(numberOfLines):\n        w = np.random.multivariate_normal(mu,sigma)\n        func = lambda x: w[0] + w[1]*x\n        xx=np.array([-1,1])\n        ax.plot(xx,func(xx),'r', alpha=0.2)\n    if dataPoints:\n        ax.scatter(dataPoints[0],dataPoints[1])\n    ax.set_xlim([-1,1])\n    ax.set_ylim([-1,1])"
  },
  {
    "objectID": "posts/bayesianregression/index.html#the-bayesian-formulation-of-regression",
    "href": "posts/bayesianregression/index.html#the-bayesian-formulation-of-regression",
    "title": "Bayesian Regression",
    "section": "The Bayesian formulation of regression",
    "text": "The Bayesian formulation of regression\nLet us say we have data \\(D\\), of \\(n\\) observations\n$D={ ({}_1, y_1), ({}_2,y_2), , ({}_n, y_n) } $ where \\({\\bf x}\\) denotes an input vector of dimension \\(D\\) and \\(y\\) denotes a scalar output (dependent variable). All data points are combined into a \\(D \\times n\\) matrix \\(X\\). The model that determines the relationship between inputs and output is given by\n\\[ y   = \\bf x^{T} {\\bf w} + \\epsilon \\]\nwhere \\({\\bf w}\\) is a vector of parameters of the linear model. Usually there is a bias or offset is included, but for now we ignore it.\nWe assume that the additive noise \\(\\epsilon\\) is iid Gaussian with zero mean and variance \\(\\sigma_n^2\\)\n\\[ \\epsilon \\sim N(0, \\sigma^2_n) \\]\n\na0=-0.3\na1=0.5\nN=20\nnoiseSD=0.2\nu=np.random.rand(20)\nx=2.*u -1.\ndef randnms(mu, sigma, n):\n    return sigma*np.random.randn(n) + mu\ny=a0+a1*x+randnms(0.,noiseSD,N)\nplt.scatter(x,y)\n\n\n\n\n\n\n\n\n\nLikelihood\nThe likelihood is, because we assume independency, the product\n\\[\n\\begin{eqnarray} \\like &=& p(\\bf y|X,\\bf w) = \\prod_{i=1}^{n} p(y_i|\\bf X_i, \\bf w) =   \\prod_{i=1}^{n}  \\frac{1}{\\sqrt{2\\pi}\\sigma_n}\n   \\exp{ \\left( -\\frac{(y_i-\\bf X_i^T \\bf w)^2}{2\\sigma_n^2} \\right)}  \\nonumber \\\\\n   &\\propto &  \\exp{\\left( -\\frac{| \\bf y-X^T \\bf w|^2 }{2\\sigma_n^2} \\right)} \\propto N(X^T \\bf w,  \\sigma_n^2 I)\n\\end{eqnarray}\n\\]\nwhere \\(|x|\\) denotes the Euclidean length of vector \\(\\bf x\\).\n\nlikelihoodSD = noiseSD # Assume the likelihood precision is known.\nlikelihoodPrecision = 1./(likelihoodSD*likelihoodSD)\n\n\n\nPrior\nIn the Bayesian framework inference we need to specify a prior over the parameters that expresses our belief about the parameters before we take any measurements. A wise choice is a \\({\\bf w_0}\\) mean Gaussian with covariance matrix \\(\\Sigma\\)\n\\[\n\\bf w \\sim N(w_0, \\Sigma)\n\\]\nIf we assume that \\(\\Sigma\\) is a diagonal covariance matrix then\n\\[\\bf w \\sim N(w_0, \\tau^2 \\bf I)\\]\n\npriorMean = np.zeros(2)\npriorPrecision=2.0\nprior_covariance = lambda alpha: alpha*np.eye(2)#Covariance Matrix\npriorCovariance = prior_covariance(1/priorPrecision )\npriorPDF = lambda w: multivariate_normal.pdf(w,mean=priorMean,cov=priorCovariance)\npriorPDF([1,2])\n\n0.0021447551423913074\n\n\n\ncplot(priorPDF);\n\n\n\n\n\n\n\n\n\nplotSampleLines(priorMean,priorCovariance,15)\n\n\n\n\n\n\n\n\n\n\nPosterior\nWe can now continue with the standard Bayesian formalism\n\\[\n\\begin{eqnarray}\np(\\bf w| \\bf y,X) &\\propto& p(\\bf y | X, \\bf w) \\, p(\\bf w) \\nonumber \\\\\n                       &\\propto& \\exp{ \\left(- \\frac{1}{2 \\sigma_n^2}(\\bf y-X^T \\bf w)^T(\\bf y - X^T \\bf w) \\right)}\n                        \\exp{\\left( -\\frac{1}{2} \\bf w^T \\Sigma^{-1} \\bf w \\right)}  \\nonumber \\\\\n\\end{eqnarray}\n\\]\nIn the next step we `complete the square’ and obtain\n\\[\\begin{equation}\np(\\bf w| \\bf y,X)  \\propto  \\exp \\left( -\\frac{1}{2} (\\bf w - \\bar{\\bf w})^T  (\\frac{1}{\\sigma_n^2} X X^T + \\Sigma^{-1})(\\bf w - \\bar{\\bf w} )  \\right)\n\\end{equation}\\]\nThis is a Gaussian with inverse-covariance\n\\[A= \\sigma_n^{-2}XX^T +\\Sigma^{-1}\\]\nwhere the new mean is\n\\[\\bar{\\bf w} = A^{-1}\\Sigma^{-1}{\\bf w_0} + \\sigma_n^{-2}( A^{-1} X^T \\bf y )\\]\nTo make predictions for a test case we average over all possible parameter predictive distribution values, weighted by their posterior probability. This is in contrast to non Bayesian schemes, where a single parameter is typically chosen by some criterion.\n\n# Given the mean = priorMu and covarianceMatrix = priorSigma of a prior\n# Gaussian distribution over regression parameters; observed data, x\n# and y; and the likelihood precision, generate the posterior\n# distribution, postW via Bayesian updating and return the updated values\n# for mu and sigma. xtrain is a design matrix whose first column is the all\n# ones vector.\ndef update(x,y,likelihoodPrecision,priorMu,priorCovariance): \n    postCovInv  = np.linalg.inv(priorCovariance) + likelihoodPrecision*np.outer(x.T,x)\n    #The outer product looks wrong but when updating we need a 2x1 matrix while x is 1x2\n    postCovariance = np.linalg.inv(postCovInv)\n    postMu = np.dot(np.dot(postCovariance,np.linalg.inv(priorCovariance)),priorMu) + likelihoodPrecision*np.dot(postCovariance,np.outer(x.T,y)).flatten()\n    postW = lambda w: multivariate_normal.pdf(w,postMu,postCovariance)\n    return postW, postMu, postCovariance\n\n\n# For each iteration plot  the\n# posterior over the first i data points and sample lines whose\n# parameters are drawn from the corresponding posterior. \nfig, axes=plt.subplots(figsize=(12,30), nrows=5, ncols=2);\nmu = priorMean\ncov = priorCovariance\nmuhash={}\ncovhash={}\nk=0\nfor i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]:\n    postW,mu,cov = update(np.array([1,x[i]]),y[i],likelihoodPrecision,mu,cov)\n    muhash[i]=mu\n    covhash[i]=cov\n    if i in [1,4,7,10,19]:\n        cplot(postW, axes[k][0])\n        plotSampleLines(muhash[i],covhash[i],15, (x[0:i],y[0:i]), axes[k][1])\n        k=k+1"
  },
  {
    "objectID": "posts/bayesianregression/index.html#posterior-predictive-distribution",
    "href": "posts/bayesianregression/index.html#posterior-predictive-distribution",
    "title": "Bayesian Regression",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\nThus the predictive distribution at some \\(x^{*}\\) is given by averaging the output of all possible linear models w.r.t. the posterior\n\\[\n\\begin{eqnarray}\np(y^{*} | x^{*}, {\\bf x,y}) &=& \\int p({\\bf y}^{*}| {\\bf x}^{*}, {\\bf w} ) p(\\bf w| X, y)dw \\nonumber \\\\\n                                    &=& {\\cal N} \\left(y \\vert \\bar{\\bf w}^{T}x^{*}, \\sigma_n^2 + x^{*^T}A^{-1}x^{*} \\right),\n\\end{eqnarray}\n\\]\nwhich is again Gaussian, with a mean given by the posterior mean multiplied by the test input and the variance is a quadratic form of the test input with the posterior covariance matrix, showing that the predictive uncertainties grow with the magnitude of the test input, as one would expect for a linear model."
  },
  {
    "objectID": "posts/bayesianregression/index.html#regularization",
    "href": "posts/bayesianregression/index.html#regularization",
    "title": "Bayesian Regression",
    "section": "Regularization",
    "text": "Regularization\n\\(\\alpha = \\sigma_n^2/\\tau^2\\) (prior precision/likelihood precision) is the regularization parameter from ridge regression. An uninformative (tending to uniform) prior means no regularization which is the standard MLE result.\n\npriorPrecision/likelihoodPrecision\n\n0.08000000000000002\n\n\nBut now say you had a strong belief the both the slope and intercept ought to be 0. Or in other words you are trying to restrict your parameters to a certain range.\n\npriorPrecision=100.0\npriorCovariance = prior_covariance(1/priorPrecision )\npriorPDF = lambda w: multivariate_normal.pdf(w,mean=priorMean,cov=priorCovariance)\ncplot(priorPDF)\n\n\n\n\n\n\n\n\n\npriorPrecision/likelihoodPrecision\n\n4.000000000000001\n\n\n\nchoices=np.random.choice([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],4,replace=False)\n\n\nchoices\n\narray([ 1, 18, 13, 19])\n\n\n\n\n# For each iteration plot  the\n# posterior over the first i data points and sample lines whose\n# parameters are drawn from the corresponding posterior. \nfig, axes=plt.subplots(figsize=(12,30), nrows=4, ncols=2);\nmu = priorMean\ncov = priorCovariance\nmuhash={}\ncovhash={}\nk=0\nxnew=x[choices]\nynew=y[choices]\nfor j,i in enumerate(choices):\n    postW,mu,cov = update(np.array([1,xnew[j]]),ynew[j],likelihoodPrecision,mu,cov)\n    muhash[i]=mu\n    covhash[i]=cov\n    cplot(postW, axes[k][0])\n    plotSampleLines(muhash[i],covhash[i],15, (xnew[:j+1],ynew[:j+1]), axes[k][1])\n    k=k+1\n\n\n\n\n\n\n\n\n\nNotice how our prior tries to keep things as flat as possible!"
  },
  {
    "objectID": "posts/gradientdescent/index.html",
    "href": "posts/gradientdescent/index.html",
    "title": "Gradient Descent and SGD",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom scipy import stats \n\nfrom sklearn.datasets.samples_generator import make_regression\nA lot of the animations here were adapted from: http://tillbergmann.com/blog/python-gradient-descent.html\nA great discussion (and where momentum image was stolen from) is at http://sebastianruder.com/optimizing-gradient-descent/\nGradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. Gradient descent is a way to minimize an objective function \\(J_{\\theta}\\) parameterized by a model’s parameters \\(\\theta \\in \\mathbb{R}^d\\) by updating the parameters in the opposite direction of the gradient of the objective function \\(\\nabla_J J(\\theta)\\) w.r.t. to the parameters. The learning rate \\(\\eta\\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley.\nThere are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update."
  },
  {
    "objectID": "posts/gradientdescent/index.html#example-linear-regression",
    "href": "posts/gradientdescent/index.html#example-linear-regression",
    "title": "Gradient Descent and SGD",
    "section": "Example: Linear regression",
    "text": "Example: Linear regression\nLet’s see briefly how gradient descent can be useful to us in least squares regression. Let’s asssume we have an output variable \\(y\\) which we think depends linearly on the input vector \\(x\\). We approximate \\(y\\) by\n\\[f_\\theta (x) =\\theta^T x\\]\nThe cost function for our linear least squares regression will then be\n\\[J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (f_\\theta (x^{(i)}-y^{(i)})^2\\]\nWe create a regression problem using sklearn’s make_regression function:\n\n#code adapted from http://tillbergmann.com/blog/python-gradient-descent.html\nx, y = make_regression(n_samples = 100, \n                       n_features=1, \n                       n_informative=1, \n                       noise=20,\n                       random_state=2017)\n\n\nx = x.flatten()\n\n\nslope, intercept, _,_,_ = stats.linregress(x,y)\nbest_fit = np.vectorize(lambda x: x * slope + intercept)\n\n\nplt.plot(x,y, 'o', alpha=0.5)\ngrid = np.arange(-3,3,0.1)\nplt.plot(grid,best_fit(grid), '.')"
  },
  {
    "objectID": "posts/gradientdescent/index.html#batch-gradient-descent",
    "href": "posts/gradientdescent/index.html#batch-gradient-descent",
    "title": "Gradient Descent and SGD",
    "section": "Batch gradient descent",
    "text": "Batch gradient descent\nAssume that we have a vector of paramters \\(\\theta\\) and a cost function \\(J(\\theta)\\) which is simply the variable we want to minimize (our objective function). Typically, we will find that the objective function has the form:\n\\[J(\\theta) =\\sum_{i=1}^m J_i(\\theta)\\]\nwhere \\(J_i\\) is associated with the i-th observation in our data set. The batch gradient descent algorithm, starts with some initial feasible \\(\\theta\\) (which we can either fix or assign randomly) and then repeatedly performs the update:\n\\[\\theta := \\theta - \\eta \\nabla_{\\theta} J(\\theta) = \\theta -\\eta \\sum_{i=1}^m \\nabla J_i(\\theta)\\]\nwhere \\(\\eta\\) is a constant controlling step-size and is called the learning rate. Note that in order to make a single update, we need to calculate the gradient using the entire dataset. This can be very inefficient for large datasets.\nIn code, batch gradient descent looks like this:\nfor i in range(n_epochs):\n  params_grad = evaluate_gradient(loss_function, data, params)\n  params = params - learning_rate * params_grad`\nFor a given number of epochs \\(n_{epochs}\\), we first evaluate the gradient vector of the loss function using ALL examples in the data set, and then we update the parameters with a given learning rate. This is where Theano and automatic differentiation come in handy, and you will learn about them in lab.\nBatch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.\nIn the linear example it’s easy to see that our update step then takes the form:\n\\[\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^m (y^{(i)}-f_\\theta (x^{(i)})) x_j^{(i)}\\] for every \\(j\\) (note \\(\\theta_j\\) is simply the j-th component of the \\(\\theta\\) vector).\n\ndef gradient_descent(x, y, theta_init, step=0.001, maxsteps=0, precision=0.001, ):\n    costs = []\n    m = y.size # number of data points\n    theta = theta_init\n    history = [] # to store all thetas\n    preds = []\n    counter = 0\n    oldcost = 0\n    pred = np.dot(x, theta)\n    error = pred - y \n    currentcost = np.sum(error ** 2) / (2 * m)\n    preds.append(pred)\n    costs.append(currentcost)\n    history.append(theta)\n    counter+=1\n    while abs(currentcost - oldcost) &gt; precision:\n        oldcost=currentcost\n        gradient = x.T.dot(error)/m \n        theta = theta - step * gradient  # update\n        history.append(theta)\n        \n        pred = np.dot(x, theta)\n        error = pred - y \n        currentcost = np.sum(error ** 2) / (2 * m)\n        costs.append(currentcost)\n        \n        if counter % 25 == 0: preds.append(pred)\n        counter+=1\n        if maxsteps:\n            if counter == maxsteps:\n                break\n        \n    return history, costs, preds, counter\n\n\nnp.random.rand(2)\n\narray([ 0.75307882,  0.98388838])\n\n\n\nxaug = np.c_[np.ones(x.shape[0]), x]\ntheta_i = [-15, 40] + np.random.rand(2)\nhistory, cost, preds, iters = gradient_descent(xaug, y, theta_i)\ntheta = history[-1]\n\n\nprint(\"Gradient Descent: {:.2f}, {:.2f} {:d}\".format(theta[0], theta[1], iters))\nprint(\"Least Squares: {:.2f}, {:.2f}\".format(intercept, slope))\n\nGradient Descent: -3.93, 81.67 4454\nLeast Squares: -3.71, 82.90\n\n\n\ntheta\n\narray([ -3.92778924,  81.67155225])\n\n\nOne can plot the reduction of cost:\n\nplt.plot(range(len(cost)), cost);\n\n\n\n\n\n\n\n\nThe following animation shows how the regression line forms:\n\nfrom JSAnimation import IPython_display\n\n\ndef init():\n    line.set_data([], [])\n    return line,\n\ndef animate(i):\n    ys = preds[i]\n    line.set_data(xaug[:, 1], ys)\n    return line,\n\n\n\nfig = plt.figure(figsize=(10,6))\nax = plt.axes(xlim=(-3, 2.5), ylim=(-170, 170))\nax.plot(xaug[:,1],y, 'o')\nline, = ax.plot([], [], lw=2)\nplt.plot(xaug[:,1], best_fit(xaug[:,1]), 'k-', color = \"r\")\n\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                        frames=len(preds), interval=100)\nanim.save('images/gdline.mp4')\nanim\n\nIOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n\n\nRemember that the linear regression cost function is convex, and more precisely quadratic. We can see the path that gradient descent takes in arriving at the optimum:\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef error(X, Y, THETA):\n    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n\ndef make_3d_plot(xfinal, yfinal, zfinal, hist, cost, xaug, y):\n    ms = np.linspace(xfinal - 20 , xfinal + 20, 20)\n    bs = np.linspace(yfinal - 40 , yfinal + 40, 40)\n    M, B = np.meshgrid(ms, bs)\n    zs = np.array([error(xaug, y, theta) \n                   for theta in zip(np.ravel(M), np.ravel(B))])\n    Z = zs.reshape(M.shape)\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.1)\n    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=30)\n    ax.set_xlabel('Intercept')\n    ax.set_ylabel('Slope')\n    ax.set_zlabel('Cost')\n    ax.view_init(elev=30., azim=30)\n    ax.plot([xfinal], [yfinal], [zfinal] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7);\n    ax.plot([t[0] for t in hist], [t[1] for t in hist], cost , markerfacecolor='b', markeredgecolor='b', marker='.', markersize=5);\n    ax.plot([t[0] for t in hist], [t[1] for t in hist], 0 , alpha=0.5, markerfacecolor='r', markeredgecolor='r', marker='.', markersize=5)\n    \ndef gd_plot(xaug, y, theta, cost, hist):\n    make_3d_plot(theta[0], theta[1], cost[-1], hist, cost, xaug, y)\n\n\ngd_plot(xaug, y, theta, cost, history)"
  },
  {
    "objectID": "posts/gradientdescent/index.html#stochastic-gradient-descent",
    "href": "posts/gradientdescent/index.html#stochastic-gradient-descent",
    "title": "Gradient Descent and SGD",
    "section": "Stochastic gradient descent",
    "text": "Stochastic gradient descent\nAs noted, the gradient descent algorithm makes intuitive sense as it always proceeds in the direction of steepest descent (the gradient of \\(J\\)) and guarantees that we find a local minimum (global under certain assumptions on \\(J\\)). When we have very large data sets, however, the calculation of \\(\\nabla (J(\\theta))\\) can be costly as we must process every data point before making a single step (hence the name “batch”). An alternative approach, the stochastic gradient descent method, is to update \\(\\theta\\) sequentially with every observation. The updates then take the form:\n\\[\\theta := \\theta - \\alpha \\nabla_{\\theta} J_i(\\theta)\\]\nThis stochastic gradient approach allows us to start making progress on the minimization problem right away. It is computationally cheaper, but it results in a larger variance of the loss function in comparison with batch gradient descent.\nGenerally, the stochastic gradient descent method will get close to the optimal \\(\\theta\\) much faster than the batch method, but will never fully converge to the local (or global) minimum. Thus the stochastic gradient descent method is useful when we want a quick and dirty approximation for the solution to our optimization problem. A full recipe for stochastic gradient descent follows:\n\nInitialize the parameter vector \\(\\theta\\) and set the learning rate \\(\\alpha\\)\nRepeat until an acceptable approximation to the minimum is obtained:\n\nRandomly reshuffle the instances in the training data.\nFor \\(i=1,2,...m\\) do: \\(\\theta := \\theta - \\alpha \\nabla_\\theta J_i(\\theta)\\)\n\n\nThe reshuffling of the data is done to avoid a bias in the optimization algorithm by providing the data examples in a particular order. In code, the algorithm should look something like this:\nfor i in range(nb_epochs):\n  np.random.shuffle(data)\n  for example in data:\n    params_grad = evaluate_gradient(loss_function, example, params)\n    params = params - learning_rate * params_grad\nFor a given epoch, we first reshuffle the data, and then for a single example, we evaluate the gradient of the loss function and then update the params with the chosen learning rate.\nThe update for linear regression is:\n\\[\\theta_j := \\theta_j + \\alpha (y^{(i)}-f_\\theta (x^{(i)})) x_j^{(i)}\\]\n\ndef sgd(x, y, theta_init, step=0.001, maxsteps=0, precision=0.001, ):\n    costs = []\n    currentcosts = []\n    m = y.size # number of data points\n    oldtheta = 0\n    theta = theta_init\n    history = [] # to store all thetas\n    preds = []\n    grads = []\n    xs = []\n    ys = []\n    counter = 0\n    oldcost = 0\n    epoch = 0\n    i = 0 #index\n    xs.append(x[i,:])\n    ys.append([y[i]])\n    pred = np.dot(x[i,:], theta)\n    error = pred - y[i]\n    gradient = x[i,:].T*error\n    grads.append(gradient)\n    currentcost = np.sum(error ** 2) / 2\n    print(\"Init\", gradient, x[i,:],y[i])\n    print (\"Init2\", currentcost, theta)\n    currentcosts.append(currentcost)\n    counter+=1\n    preds.append(pred)\n    costsum = currentcost\n    costs.append(costsum/counter)\n    history.append(theta)\n    print(\"start\",counter, costs, oldcost)\n    while 1:\n        #while abs(costs[counter-1] - oldcost) &gt; precision:\n        #while np.linalg.norm(theta - oldtheta) &gt; precision:\n        gradient = x[i,:].T*error\n        grads.append(gradient)\n        oldtheta = theta\n        theta = theta - step * gradient  # update\n        history.append(theta)\n        i += 1\n        if i == m:#reached one past the end.\n            #break\n            epoch +=1\n            neworder = np.random.permutation(m)\n            x = x[neworder]\n            y = y[neworder]\n            i = 0\n        xs.append(x[i,:])\n        ys.append(y[i])\n        pred = np.dot(x[i,:], theta)\n        error = pred - y[i]\n        currentcost = np.sum(error ** 2) / 2\n        currentcosts.append(currentcost)\n        \n        #print(\"e/cc\",error, currentcost)\n        if counter % 25 == 0: preds.append(pred)\n        counter+=1\n        costsum += currentcost\n        oldcost = costs[counter-2]\n        costs.append(costsum/counter)\n        #print(counter, costs, oldcost)\n        if maxsteps:\n            #print(\"in maxsteps\")\n            if counter == maxsteps:\n                break\n        \n    return history, costs, preds, grads, counter, epoch, xs, ys, currentcosts\n\n\nhistory2, cost2, preds2, grads2, iters2, epoch2, x2, y2, cc2 = sgd(xaug, y, theta_i, maxsteps=5000, step=0.01)\n\nInit [-24.81938748  -0.8005103 ] [ 1.          0.03225343] 11.5348518902\nInit2 308.000997364 [-14.58474841  40.31239274]\nstart 1 [308.00099736389291] 0\n\n\n\nprint(iters2, history2[-1], epoch2, grads2[-1])\n\n5000 [ -3.18011506  82.93875465] 49 [ 10.47922297  -5.63332413]\n\n\n\nplt.plot(range(len(cost2[-10000:])), cost2[-10000:], alpha=0.4);\n\n\n\n\n\n\n\n\n\ngd_plot(xaug, y, theta, cost2, history2)\n\n\n\n\n\n\n\n\n\nplt.plot([t[0] for t in history2], [t[1] for t in history2],'o-', alpha=0.1)\n\n\n\n\n\n\n\n\n\nAnimating SGD\nHere is some code to make an animation of SGD. It shows how the risk surfaces being minimized change, and how the minimum desired is approached.\n\ndef error2(X, Y, THETA):\n    #print(\"XYT\", THETA, np.sum((X.dot(THETA) - Y)**2))\n    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n\n\ndef make_3d_plot2(num, it, xfinal, yfinal, zfinal, hist, cost, xaug, y):\n    ms = np.linspace(xfinal - 20 , xfinal + 20, 20)\n    bs = np.linspace(yfinal - 50 , yfinal + 50, 40)\n    M, B = np.meshgrid(ms, bs)\n    zs = np.array([error2(xaug, y, theta) \n                   for theta in zip(np.ravel(M), np.ravel(B))])\n    Z = zs.reshape(M.shape)\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.1)\n    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=30)\n    ax.set_xlabel('Intercept')\n    ax.set_ylabel('Slope')\n    ax.set_zlabel('Cost')\n    ax.view_init(elev=30., azim=30)\n    #print(\"hist\", xaug, y, hist, cost)\n    ax.plot([xfinal], [yfinal], [zfinal] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7);\n    #ax.plot([t[0] for t in hist], [t[1] for t in hist], cost , markerfacecolor='b', markeredgecolor='b', marker='.', markersize=5);\n    ax.plot([t[0] for t in hist], [t[1] for t in hist], 0 , alpha=0.5, markerfacecolor='r', markeredgecolor='r', marker='.', markersize=5)\n    ax.set_zlim([0, 3000])\n    plt.title(\"Iteration {}\".format(it))\n    plt.savefig(\"images/3danim{0:03d}.png\".format(num))\n    plt.close()\n\n\nprint(\"fthetas\",theta[0], theta[1], \"len\", len(history2))\nST = list(range(0, 750, 10)) + list(range(750, 5000, 250))\nlen(ST)\n\nfthetas -3.92778923799 81.6715522496 len 5000\n\n\n92\n\n\n\nfor i in range(len(ST)):\n    #print(history2[i*ST[i]], cc2[i*ST[i]])\n    make_3d_plot2(i, ST[i], theta[0], theta[1], cost2[-1], [history2[ST[i]]], [cc2[ST[i]]], np.array([x2[ST[i]]]), np.array([y2[ST[i]]]))\n\nUsing Imagemagick we can produce a gif animation: (convert -delay 20 -loop 1 3danim*.png animsgd.gif)\n(I set this animation to repeat just once. (loop 1). Reload this cell to see it again. On the web page right clicking the image might allow for an option to loop again)"
  },
  {
    "objectID": "posts/gradientdescent/index.html#mini-batch-gradient-descent",
    "href": "posts/gradientdescent/index.html#mini-batch-gradient-descent",
    "title": "Gradient Descent and SGD",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nWhat if instead of single example from the dataset, we use a batch of data examples witha given size every time we calculate the gradient:\n\\[\\theta = \\theta - \\eta \\nabla_{\\theta} J(\\theta; x^{(i:i+n)}; y^{(i:i+n)})\\]\nThis is what mini-batch gradient descent is about. Using mini-batches has the advantage that the variance in the loss function is reduced, while the computational burden is still reasonable, since we do not use the full dataset. The size of the mini-batches becomes another hyper-parameter of the problem. In standard implementations it ranges from 50 to 256. In code, mini-batch gradient descent looks like this:\nfor i in range(mb_epochs):\n  np.random.shuffle(data)\n  for batch in get_batches(data, batch_size=50):\n    params_grad = evaluate_gradient(loss_function, batch, params)\n    params = params - learning_rate * params_grad\nThe difference with SGD is that for each update we use a batch of 50 examples to estimate the gradient."
  },
  {
    "objectID": "posts/gradientdescent/index.html#variations-on-a-theme",
    "href": "posts/gradientdescent/index.html#variations-on-a-theme",
    "title": "Gradient Descent and SGD",
    "section": "Variations on a theme",
    "text": "Variations on a theme\n\nMomentum\nOften, the cost function has ravines near local optima, ie. areas where the shape of the function is significantly steeper in certain dimensions than in others. This migh result in a slow convergence to the optimum, since standard gradient descent will keep oscillating about these ravines. In the figures below, the left panel shows convergence without momentum, and the right panel shows the effect of adding momentum:\n\n\n\n&lt;img src=“http://sebastianruder.com/content/images/2015/12/without_momentum.gif”, width=300, height=300&gt;\n\n\n&lt;img src=“http://sebastianruder.com/content/images/2015/12/with_momentum.gif”, width=300, height=300&gt;\n\n\n\nOne way to overcome this problem is by using the concept of momentum, which is borrowed from physics. At each iteration, we remember the update \\(v = \\Delta \\theta\\) and use this velocity vector (which as the same dimension as \\(\\theta\\)) in the next update, which is constructed as a combination of the cost gradient and the previous update:\n\\[v_t = \\gamma v_{t-1} +  \\eta \\nabla_{\\theta} J(\\theta)\\] \\[\\theta = \\theta - v_t\\]\nThe effect of this is the following: the momentum terms increases for dimensions whose gradients point in the same direction, and reduces the importance of dimensions whose gradients change direction. This avoids oscillations and improves the chances of rapid convergence. The concept is analog to the a rock rolling down a hill: the gravitational field (cost function) accelerates the particule (weights vector), which accumulates momentum, becomes faster and faster and tends to keep travelling in the same direction. A commonly used value for the momentum parameter is \\(\\gamma = 0.5\\)."
  },
  {
    "objectID": "posts/understandingaic/index.html",
    "href": "posts/understandingaic/index.html",
    "title": "Understanding AIC",
    "section": "",
    "text": "This notebook is based on McElreath, Rethinking Statistics, Chapter 6.\nWhen we use the empirical distribution and sample quantities here we are working with our training sample (s).\nClearly we can calculate deviance on the validation and test samples as well to remedy this issue. And the results will be similar to what we found in lecture for MSE, with the training deviance decreasing with complexity and the testing deviance increasing at some point.\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/understandingaic/index.html#a-trick-to-generate-data",
    "href": "posts/understandingaic/index.html#a-trick-to-generate-data",
    "title": "Understanding AIC",
    "section": "A trick to generate data",
    "text": "A trick to generate data\nWe generate data from a gaussian with standard deviation 1 and means given by:\n\\[\\mu_i = 0.15 x_{1,i} - 0.4 x_{2,i}, y \\sim N(\\mu, 1).\\]\nThis is a 2 parameter model.\nWe use an interesting trick to generate this data, directly using the regression coefficients as correlations with the response variable.\n\ndef generate_data(N, k, rho=[0.15, -0.4]):\n    n_dim = 1 + len(rho)\n    if n_dim &lt; k:\n        n_dim = k\n    Rho = np.eye(n_dim)\n    for i,r in enumerate(rho):\n        Rho[0, i+1] = r\n    index_lower = np.tril_indices(n_dim, -1)\n    Rho[index_lower] = Rho.T[index_lower]\n    mean = n_dim * [0.]\n    Xtrain = np.random.multivariate_normal(mean, Rho, size=N)\n    Xtest = np.random.multivariate_normal(mean, Rho, size=N)\n    ytrain = Xtrain[:,0].copy()\n    Xtrain[:,0]=1.\n    ytest = Xtest[:,0].copy()\n    Xtest[:,0]=1.\n    return Xtrain[:,:k], ytrain, Xtest[:,:k], ytest\n\nWe want to generate data for 5 different cases, a one parameter (intercept) fit, a two parameter (intercept and \\(x_1\\)), three parameters (add a $x_2), and four and five parameters. Here is what the data looks like for 2 parameters:\n\ngenerate_data(20,2)\n\n(array([[ 1.        , -0.83978695],\n        [ 1.        , -0.60882982],\n        [ 1.        ,  1.02567296],\n        [ 1.        ,  0.24801809],\n        [ 1.        , -1.08181661],\n        [ 1.        , -1.85677575],\n        [ 1.        ,  1.82835523],\n        [ 1.        ,  0.35622585],\n        [ 1.        , -0.04159412],\n        [ 1.        ,  0.58678675],\n        [ 1.        , -0.24396323],\n        [ 1.        , -0.07081137],\n        [ 1.        ,  0.46510137],\n        [ 1.        , -1.02993129],\n        [ 1.        , -2.08756332],\n        [ 1.        ,  0.60666556],\n        [ 1.        ,  0.45913243],\n        [ 1.        ,  0.60083017],\n        [ 1.        , -1.05726496],\n        [ 1.        , -0.52258973]]),\n array([-1.11513393, -0.50856507,  0.50782261, -0.09031626,  0.41992084,\n        -0.82404287,  0.27567933,  0.3626567 ,  0.99109211,  1.14742966,\n         0.53597334, -1.2959274 ,  2.12659247,  0.09595858,  0.05845798,\n         0.47581813, -1.02115871,  0.83942264,  0.33097791, -1.07482199]),\n array([[ 1.        , -0.09664154],\n        [ 1.        ,  1.68464504],\n        [ 1.        , -1.63102144],\n        [ 1.        , -0.83585358],\n        [ 1.        , -1.0022563 ],\n        [ 1.        , -0.40901251],\n        [ 1.        ,  0.52024856],\n        [ 1.        ,  0.64056776],\n        [ 1.        , -0.26979402],\n        [ 1.        ,  0.57670424],\n        [ 1.        , -0.13580787],\n        [ 1.        , -0.74665431],\n        [ 1.        , -0.34801499],\n        [ 1.        , -0.53583385],\n        [ 1.        ,  1.49971783],\n        [ 1.        ,  0.47265248],\n        [ 1.        , -0.40158879],\n        [ 1.        ,  0.59618203],\n        [ 1.        ,  0.63314497],\n        [ 1.        , -0.85947691]]),\n array([-1.2830826 , -0.0773539 , -1.22779576, -1.43955577,  0.39940223,\n        -1.52159959, -0.41312511, -0.95244826,  0.20244849, -0.32376445,\n         0.09636247,  0.03435469,  0.29289397,  0.70749769, -2.20920373,\n         0.36671712, -0.73570139, -0.381103  , -0.3126861 , -0.61196652]))\n\n\nAnd for four parameters\n\ngenerate_data(20,4)\n\n(array([[  1.00000000e+00,  -5.64117484e-01,  -1.30408291e+00,\n          -4.06307198e-01],\n        [  1.00000000e+00,   2.45856192e-01,  -1.13160363e+00,\n           6.99099707e-01],\n        [  1.00000000e+00,  -5.92401483e-01,  -5.51929080e-01,\n           1.70288811e-01],\n        [  1.00000000e+00,   1.40350006e+00,  -7.42482462e-01,\n           6.90299071e-01],\n        [  1.00000000e+00,  -1.14026512e+00,   2.27882734e-01,\n          -2.80250494e-01],\n        [  1.00000000e+00,  -1.79114172e-01,   1.71257237e+00,\n           1.32182974e+00],\n        [  1.00000000e+00,   8.39677171e-01,  -2.07787502e-01,\n           1.20281542e+00],\n        [  1.00000000e+00,  -9.38668901e-01,  -5.87192846e-01,\n           9.91223102e-01],\n        [  1.00000000e+00,  -4.11883974e-01,  -1.31283133e+00,\n          -9.42131126e-01],\n        [  1.00000000e+00,   5.27622295e-01,   2.98370087e-01,\n          -3.13398528e-01],\n        [  1.00000000e+00,   1.75945182e+00,  -9.55446150e-01,\n          -5.65605486e-01],\n        [  1.00000000e+00,   3.66192473e-01,   1.39659262e+00,\n           5.25449367e-01],\n        [  1.00000000e+00,  -8.27065820e-01,   2.07687904e-01,\n          -4.07828527e-01],\n        [  1.00000000e+00,  -9.94963330e-01,  -3.45817822e-01,\n          -1.71339667e-01],\n        [  1.00000000e+00,   1.35510550e+00,   2.80027702e-01,\n           9.10748378e-03],\n        [  1.00000000e+00,  -5.79066026e-01,   1.67201032e+00,\n          -1.21304440e+00],\n        [  1.00000000e+00,   6.61699882e-01,   5.01830803e-01,\n           3.88336944e-01],\n        [  1.00000000e+00,   1.33639603e-01,   1.61570562e-01,\n          -1.96165163e-04],\n        [  1.00000000e+00,  -1.15492222e+00,  -1.19608702e+00,\n           7.44868766e-01],\n        [  1.00000000e+00,  -1.16104055e-03,   5.60105046e-01,\n           1.13087330e+00]]),\n array([ 0.59049849, -0.32849878,  0.17111991, -0.18214341, -1.06834661,\n        -2.01806297,  0.64395116,  1.36524519, -0.39568506,  0.50772571,\n         2.53131129, -1.54337658, -0.23082485,  0.23672394, -1.72834828,\n         0.03969115,  0.84937923, -0.04779334, -0.12796287,  0.25091162]),\n array([[ 1.        ,  0.87805634, -1.3252486 ,  0.9147951 ],\n        [ 1.        ,  0.04573015,  0.26129224, -0.39094182],\n        [ 1.        , -1.91453329,  0.26466733, -0.31760853],\n        [ 1.        ,  0.50704342, -0.91631862,  0.71741282],\n        [ 1.        , -1.12324824, -1.01977223, -0.52418926],\n        [ 1.        ,  0.33666476,  0.27513745,  0.63589532],\n        [ 1.        , -0.38934175,  1.00961086, -0.61853231],\n        [ 1.        , -0.79629895,  1.28994305, -1.54766776],\n        [ 1.        , -0.71721218,  0.15741145,  0.475622  ],\n        [ 1.        ,  1.27433083,  0.6941836 ,  0.90788968],\n        [ 1.        , -0.68170154, -0.04929347,  0.27673865],\n        [ 1.        , -0.08147767,  0.81537351, -0.55685094],\n        [ 1.        , -0.32449028, -0.25664319, -0.57435918],\n        [ 1.        ,  0.64398473,  0.08495211,  0.47839262],\n        [ 1.        , -0.91519445, -2.06795016, -0.42275664],\n        [ 1.        , -0.29770443,  1.08641811,  1.94490458],\n        [ 1.        , -0.19844572, -2.70288281,  0.69816899],\n        [ 1.        ,  0.2687054 ,  1.53633517, -0.84276465],\n        [ 1.        , -0.06377957, -0.05227578,  1.96973628],\n        [ 1.        , -2.04828043,  0.47438443, -2.28134046]]),\n array([ 0.02585828, -0.1284505 , -0.39679531, -0.22793629,  0.25824818,\n        -0.35346682, -1.30972978, -1.2250625 ,  0.97636939, -1.50891804,\n         0.84376139, -2.22258544,  0.47901623, -1.08042407,  2.35481165,\n         0.38210287,  1.27868801, -1.71473971,  0.6498696 , -0.27036068]))\n\n\n\nfrom scipy.stats import norm\nimport statsmodels.api as sm\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools"
  },
  {
    "objectID": "posts/understandingaic/index.html#analysis-n20",
    "href": "posts/understandingaic/index.html#analysis-n20",
    "title": "Understanding AIC",
    "section": "Analysis, n=20",
    "text": "Analysis, n=20\nHere is the main loop of our analysis. We take the 5 models we talked about. For each model we generate 10000 samples of the data, split into an equal sized (N=20 each) training and testing set. We fit the regression on the training set, and calculate the deviance on the training set. Notice how we have simply used the logpdf from scipy.stats. You can easily do this for other distributions.\nWe then use the fit to calculate the \\(\\mu\\) on the test set, and calculate the deviance there. We then find the average and the standard deviation across the 10000 simulations.\nWhy do we do 10000 simulations? These are our multiple samples from some hypothetical population.\n\nreps=10000\nresults_20 = {}\nfor k in range(1,6):\n    trdevs=np.zeros(reps)\n    tedevs=np.zeros(reps)\n    for r in range(reps):\n        Xtr, ytr, Xte, yte = generate_data(20, k)\n        ols = sm.OLS(ytr, Xtr).fit()\n        mutr = np.dot(Xtr, ols.params)\n        devtr = -2*np.sum(norm.logpdf(ytr, mutr, 1))\n        mute = np.dot(Xte, ols.params)\n        #print(mutr.shape, mute.shape)\n        devte = -2*np.sum(norm.logpdf(yte, mute, 1))\n        #print(k, r, devtr, devte)\n        trdevs[r] = devtr\n        tedevs[r] = devte\n    results_20[k] = (np.mean(trdevs), np.std(trdevs), np.mean(tedevs), np.std(tedevs))\n\n\nimport pandas as pd\ndf = pd.DataFrame(results_20).T\ndf = df.rename(columns = dict(zip(range(4), ['train', 'train_std', 'test', 'test_std'])))\ndf\n\n\n\n\n\n\n\n\ntrain\ntrain_std\ntest\ntest_std\n\n\n\n\n1\n55.669331\n6.185150\n57.688110\n6.825813\n\n\n2\n54.301259\n5.930990\n58.513912\n7.284745\n\n\n3\n50.669082\n4.778731\n56.111054\n6.668364\n\n\n4\n49.744479\n4.585390\n57.414846\n7.505377\n\n\n5\n49.026424\n4.431733\n58.718321\n8.279063\n\n\n\n\n\n\n\n\nimport seaborn.apionly as sns\ncolors = sns.color_palette()\ncolors\n\n[(0.12156862745098039, 0.4666666666666667, 0.7058823529411765),\n (1.0, 0.4980392156862745, 0.054901960784313725),\n (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),\n (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),\n (0.5803921568627451, 0.403921568627451, 0.7411764705882353),\n (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),\n (0.8901960784313725, 0.4666666666666667, 0.7607843137254902),\n (0.4980392156862745, 0.4980392156862745, 0.4980392156862745),\n (0.7372549019607844, 0.7411764705882353, 0.13333333333333333),\n (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)]\n\n\nWe plot the traing and testing deviances\n\nplt.plot(df.index, df.train, 'o', color = colors[0])\nplt.errorbar(df.index, df.train, yerr=df.train_std, fmt=None, color=colors[0])\nplt.plot(df.index+0.2, df.test, 'o', color = colors[1])\nplt.errorbar(df.index+0.2, df.test, yerr=df.test_std, fmt=None, color=colors[1])\nplt.xlabel(\"number of parameters\")\nplt.ylabel(\"deviance\")\nplt.title(\"N=20\");\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_axes.py:2818: MatplotlibDeprecationWarning: Use of None object as fmt keyword argument to suppress plotting of data values is deprecated since 1.4; use the string \"none\" instead.\n  warnings.warn(msg, mplDeprecation, stacklevel=1)\n\n\n\n\n\n\n\n\n\nNotice:\n\nthe best fit model may not be the original generating model. Remember that the choice of fit depends on the amount of data you have and the less data you have, the less parameters you should use\non average, out of sample deviance must be larger than in-sample deviance, through an individual pair may have that order reversed because of sample peculiarity."
  },
  {
    "objectID": "posts/understandingaic/index.html#aic-or-the-difference-in-deviances",
    "href": "posts/understandingaic/index.html#aic-or-the-difference-in-deviances",
    "title": "Understanding AIC",
    "section": "AIC, or the difference in deviances",
    "text": "AIC, or the difference in deviances\nLet us see the difference between the mean testing and training deviances. This is the difference in bias between the two sets.\n\ndf.test - df.train\n\n1    2.018779\n2    4.212654\n3    5.441971\n4    7.670368\n5    9.691897\ndtype: float64\n\n\nVoila, this seems to be roughly twice the number of parameters. In other words we might be able to get away without a test set if we “correct” the bias on the traing set by \\(2n_p\\). This is the observation that motivates the AIC.\n\nAnalysis N=100\n\nreps=10000\nresults_100 = {}\nfor k in range(1,6):\n    trdevs=np.zeros(reps)\n    tedevs=np.zeros(reps)\n    for r in range(reps):\n        Xtr, ytr, Xte, yte = generate_data(100, k)\n        ols = sm.OLS(ytr, Xtr).fit()\n        mutr = np.dot(Xtr, ols.params)\n        devtr = -2*np.sum(norm.logpdf(ytr, mutr, 1))\n        mute = np.dot(Xte, ols.params)\n        devte = -2*np.sum(norm.logpdf(yte, mute, 1))\n        #print(k, r, devtr, devte)\n        trdevs[r] = devtr\n        tedevs[r] = devte\n    results_100[k] = (np.mean(trdevs), np.std(trdevs), np.mean(tedevs), np.std(tedevs))\n\n\ndf100 = pd.DataFrame(results_100).T\ndf100 = df100.rename(columns = dict(zip(range(4), ['train', 'train_std', 'test', 'test_std'])))\ndf100\n\n\nplt.plot(df100.index, df100.train, 'o', color = colors[0])\nplt.errorbar(df100.index, df100.train, yerr=df100.train_std, fmt=None, color=colors[0])\nplt.plot(df100.index+0.2, df100.test, 'o', color = colors[1])\nplt.errorbar(df100.index+0.2, df100.test, yerr=df100.test_std, fmt=None, color=colors[1])\nplt.xlabel(\"number of parameters\")\nplt.ylabel(\"deviance\")\nplt.title(\"N=100\");\n\n\ndf100.test - df100.train\n\n1    1.900383\n2    3.908291\n3    5.193728\n4    7.003606\n5    8.649499\ndtype: float64\n\n\nWe get pretty much the same result at N=100."
  },
  {
    "objectID": "posts/understandingaic/index.html#assumptions-for-aic",
    "href": "posts/understandingaic/index.html#assumptions-for-aic",
    "title": "Understanding AIC",
    "section": "Assumptions for AIC",
    "text": "Assumptions for AIC\nThis observation leads to an estimate of the out-of-sample deviance by what is called an information criterion, the Akaike Information Criterion, or AIC:\n\\[AIC = D_{train} + 2n_p\\]\nwhich does carry as assumptions that\n\nthe likelihood is approximately multivariate gaussian\nthe sample size is much larger than the number of parameters\npriors are flat\nThe AIC does not assume that the true data generating process \\(p\\) is in the set of models being fitted. The overarching goal of the AIC approach to model selection is to select the “best” model for our given data set without assuming that the “true” model is in the family of models from which we’re selecting. The true model “cancels out” except in the expectation.\n\nWe wont derive the AIC here, but if you are interested, see http://www.stat.cmu.edu/~larry/=stat705/Lecture16.pdf\nWhy would we want to use such information criteria? Cross validation can be expensive, especially with multiple hyper-parameters."
  },
  {
    "objectID": "posts/understandingaic/index.html#aic-for-linear-regression",
    "href": "posts/understandingaic/index.html#aic-for-linear-regression",
    "title": "Understanding AIC",
    "section": "AIC for Linear Regression",
    "text": "AIC for Linear Regression\nThe AIC for a model is the training deviance plus twice the number of parameters:\n\\[AIC = D_{train} + 2n_p.\\]\nThat is, -2 times the log likelihood of the model.\nSo, one we find the MLE solution for the linear regression, we plugin the values we get, which are\n\\[\\sigma_{MLE}^2 =  \\frac{1}{N} RSS \\]\nwhere RSS is the sum of the squares of the errors.\n\\[AIC = -2(-\\frac{N}{2}(log(2\\pi) + log(\\sigma^2)) -2(-\\frac{1}{2\\sigma_{MLE}^2} \\times RSS) + 2p\\]\nThus:\n\\[D = Nlog(RSS/N) \\]\n\\[AIC = Nlog(RSS/N) + 2p + constant\\]\nSince the deviance for a OLS model is just proportional to the log(MSE) upto a proportionality, we’ll use the MSE to derive this split.\nThe fact that the (log-likelihood) and thus the deviance carries an expectation over the true distribution as estimated on the sample means that the Deviance is a stochastic quantity, varying from sample to sample."
  },
  {
    "objectID": "posts/understandingaic/index.html#a-complete-understanding-of-the-comparison-diagram",
    "href": "posts/understandingaic/index.html#a-complete-understanding-of-the-comparison-diagram",
    "title": "Understanding AIC",
    "section": "A complete understanding of the comparison diagram",
    "text": "A complete understanding of the comparison diagram\n(taken from McElreath, but see upstairs as well)\n\n\n\nIn-sample vs. out-of-sample deviance as model complexity increases, for N=20 and N=100. From McElreath, Statistical Rethinking.\n\n\nNow we are equipped to understand this diagram completely. Lets focus on the training (in) set first: blue points.\n\nThere is some irreducible noise which contributes to the deviance no matter the number of parameters.\nIf we could capture the true model exactly there would be no bias, and the deviance would go to that which comes from the irreducible noise.\nBut we cant, so the positions of the circles tells us how much bias plus irreducible noise we have\nThe error bars now tell us our variance, since they tell us how much our deviance, or MSE varies around our “mean” model. In real life our sample will lie somewhere along this error bar.\nThe training set deviances go down as the number of parameters increase. The test set deviances go down and then go up\nNotice that testing deviance is higher on a 2 parameter model than on a 1, even though our generating “true” model is a 2 parameter one. Deviance and the AIC do not pick the true model, but rather the one with the highest predictive accuracy."
  },
  {
    "objectID": "posts/noiseless_learning/index.html",
    "href": "posts/noiseless_learning/index.html",
    "title": "Learning Without Noise",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/noiseless_learning/index.html#the-process-of-learning",
    "href": "posts/noiseless_learning/index.html#the-process-of-learning",
    "title": "Learning Without Noise",
    "section": "The process of learning",
    "text": "The process of learning\nThere are challenges that occur in learning a model from data:\n\nsmall samples of data\nnoise in the data\nissues related to the complexity of the models we use\n\nLet us first ask the question: what is he process of learning from data in the absence of noise. This never really happens, but it is a way for us to understand the theory of approximation, and lets us build a base for understanding the learning from data with noise.\nLets say we are trying to predict is a human process such as an election. Here economic and sociological factors are important, such as poverty, race and religiousness. There are historical correlations between such factors and election outcomes which we might want to incorporate into our model. An example of such a model might be:\nThe odds of Romney winning a county against Obama in 2012 are a function of population religiosity, race, poverty, education, and other social and economic indicators. \nOur causal argument motivating this model here might be that religious people are more socially conservative and thus more likely to vote republican. This might not be the correct causation, but thats not entirely important for the prediction.\nAs long as a correlation exists, our model is more structured than 50-50 randomness, and we can try and make a prediction. Remember of-course, our model may even be wrong (see Box’s aphorism: https://en.wikipedia.org/wiki/All_models_are_wrong).\nWe’ll represent the variable being predicted, such as the probability of voting for Romney, by the letter \\(y\\), and the features or co-variates we use as an input in this probability by the letter \\(x\\). This \\(x\\) could be multi-dimensional, with \\(x_1\\) being poverty, \\(x_2\\) being race, and so on.\nWe then write\n\\[ y = f(x) \\]\nand our jobs is to take \\(x\\) such as data from the census about race, religiousness, and so on, and \\(y\\) as previous elections and the results of polls that pollsters come up with, and to make a predictive model for the elections. That is, we wish to estimate \\(f(x)\\).\n\nA real simple model\nTo gently step feet in the modelling world, lets see consider very simple model, where the probability of voting for Romney is a function only of how religious the population in a county is. This is a model I’ve cooked up, and the data is fake.\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\nLet us assume that we have a “population” of 200 counties \\(x\\):\n\ndf=pd.read_csv(\"data/religion.csv\")\ndf.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\nLets suppose now that the Lord came by and told us that the points in the plot below captures \\(f(x)\\) exactly. In other words, there is no specification error, and God knows the generating process exactly.\n\nx=df.rfrac.values\nf=df.promney.values\nplt.plot(x,f,'.', alpha=0.3)\n\n\n\n\n\n\n\n\nNotice that our sampling of \\(x\\) is not quite uniform: there are more points around \\(x\\) of 0.7.\nNow, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample \\(\\cal{D}\\) of 30 data points. Such data is called in-sample data. Contrastingly, the entire population of data points is also called out-of-sample data.\n\n#indexes=np.sort(np.random.choice(x.shape[0], size=30, replace=False))\ndfsample = pd.read_csv(\"data/noisysample.csv\")\ndfsample.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\nindexes = dfsample.i.values\n\n\nsamplex = x[indexes]\nsamplef = f[indexes]\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\naxes[1].plot(x,f, 'r.', alpha=0.2, label=\"population\");\naxes[1].plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n\n\n\n\n\n\n\nThe lightly shaded squares in the right panel plot are the in-sample \\(\\cal{D}\\) of 30 points given to us. Let us then pretend that we have forgotten the curve that the Lord gave us. Thus, all we know is the blue points on the plot on the right, and we have no clue about what the original curve was, nor do we remember the original “population”.\nThat is, imagine the Lord gave us \\(f\\) but then also gave us amnesia. Remember that such amnesia is the general case in learning, where we do not know the target function, but rather just have some data. Thus what we will be doing is trying to find functions that might have generated the 30 points of data that we can see in the hope that one of these functions might approximate \\(f\\) well, and provide us a predictive model for future data. This is known as fitting the data.\n\n\nThe Hypothesis or Model Space\nSuch a function, one that we use to fit the data, is called a hypothesis. We’ll use the notation \\(h\\) to denote a hypothesis. Lets consider as hypotheses for the data above, a particular class of functions called polynomials.\nA polynomial is a function that combines multiple powers of x linearly. You’ve probably seen these in school, when working with quadratic or cubic equations and functions:\n\\[\n\\begin{align*}\nh(x) &=& 9x - 7 && \\,(straight\\, line) \\\\\nh(x) &=& 4x^2 + 3x + 2 && \\,(quadratic) \\\\\nh(x) &=& 5x^3 - 31x^2 + 3x  && \\,(cubic).\n\\end{align*}\n\\]\nIn general, a polynomial can be written thus:\n\\[\n\\begin{eqnarray*}\nh(x) &=& a_0 + a_1 x^1 + a_2 x^2 + ... + a_n x^n \\\\\n      &=& \\sum_{i=0}^{n} a_i x^i\n\\end{eqnarray*}\n\\]\nThus, by linearly we mean a sum of coefficients \\(a_i\\) times powers of \\(x\\), \\(x^i\\). In other words, the polynomial is linear in its coefficients.\nLet us consider as the function we used to fit the data, a hypothesis \\(h\\) that is a straight line. We put the subscript \\(1\\) on the \\(h\\) to indicate that we are fitting the data with a polynomial of order 1, or a straight line. This looks like:\n\\[ h_1(x) = a_0 + a_1 x \\]\nWe’ll call the best fit straight line the function \\(g_1(x)\\). The “best fit” idea is this: amongst the set of all lines (i.e., all possible choices of \\(h_1(x)\\)), what is the best line \\(g_1(x)\\) that represents the in-sample data we have? (The subscript \\(1\\) on \\(g\\) is chosen to indicate the best fit polynomial of degree 1, ie the line amongst lines that fits the data best).\nThe best fit \\(g_1(x)\\) is calculated and shown in the figure below:\n\ng1 = np.poly1d(np.polyfit(x[indexes],f[indexes],1))\nplt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\nplt.plot(x,g1(x), 'b--', alpha=0.6, label=\"$g_1$\");\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nHow did we calculate the best fit? We’ll come to that in a bit, but in the meanwhile, lets formalize and generalize the notion of “best fit line amongst lines” a bit.\nThe set of all functions of a particular kind that we could have used to fit the data is called a Hypothesis Space. The words “particular kind” are deliberately vague: its our choice as to what we might want to put into a hypothesis space. A hypothesis space is denoted by the notation \\(\\cal{H}\\).\nLets consider the hypothesis space of all straight lines \\(h_1(x)\\). We’ll denote it as \\(\\cal{H}_1\\), with the subscript being used to mark the order of the polynomial. Another such space might be \\(\\cal{H}_2\\), the hypothesis space of all quadratic functions. A third such space might combine both of these together. We get to choose what we want to put into our hypothesis space.\nIn this set-up, what we have done in the code and plot above is this: we have found the best \\(g_1\\) to the data \\(\\cal{D}\\) from the functions in the hypothesis space \\(\\cal{H}_1\\). This is not the best fit from all possible functions, but rather, the best fit from the set of all the straight lines.\nThe hypothesis space is a concept we can use if we want to capture the complexity of a model you use to fit data. For example, since quadratics are more complex functions than straight lines (they curve more), \\(\\cal{H}_2\\) is more complex than \\(\\cal{H}_1\\).\n\n\nDeterministic Error or Bias\nNotice from the figure above that models in \\(\\cal{H}_1\\), i.e., straight lines, and the best-fit straight line \\(g_1\\) in particular, do not do a very good job of capturing the curve of the data (and thus the underlying function \\(f\\) that we are trying to approximate. Consider the more general case in the figure below, where a curvy \\(f\\) is approximated by a function \\(g\\) which just does not have the wiggling that \\(f\\) has.\n\n\n\nApproximation bias: the gap between true function f and best-fit hypothesis g\n\n\nThere is always going to be an error then, in approximating \\(f\\) by \\(g\\). This approximation error is shown in the figure by the blue shaded region, and its called bias, or deterministic error. The former name comes from the fact that \\(g\\) just does not wiggle the way \\(f\\) does (nothing will make a straight line curve). The latter name (which I first saw used in http://www.amlbook.com/ ) comes from the notion that if you did not know the target function \\(f\\), which is the case in most learning situations, you would have a hard time distinguishing this error from any other errors such as measurement and noise…\nGoing back to our model at hand, it is clear that the space of straight lines \\(\\cal{H_1}\\) does not capture the curving in the data. So let us consider the more complex hypothesis space \\(\\cal{H_{20}}\\), the set of all 20th order polynomials \\(h_{20}(x)\\):\n\\[h_{20}(x) = \\sum_{i=0}^{20} a_i x^i\\,.\\]\nTo see how a more complex hypothesis space does, lets find the best fit 20th order polynomial \\(g_{20}(x)\\).\n\ng20 = np.poly1d(np.polyfit(x[indexes],f[indexes],20))\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\nplt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\nplt.plot(x,g20(x), 'b--', alpha=0.6, label=\"$g_{10}$\");\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nVoila! You can see the 20th order polynomial does a much better job of tracking the points, because of the wiggle room it has in making a curve “go near or through” all the points as opposed to a straight line, which well, cant curve. Thus it would seem that \\(\\cal{H}_{20}\\) might be a better candidate hypothesis set from which to choose a best fit model.\nWe can quantify this by calculating some notion of the bias for both \\(g_1\\) and \\(g_{20}\\). To do this we calculate the square of the difference between f and the g’s on the population of 200 points i.e.:\n\\[B_1(x) = (g_1(x) - f(x))^2 \\,;\\,\\, B_{20}(x) = (g_{20}(x) - f(x))^2\\,.\\]\nSquaring makes sure that we are calculating a positive quantity.\n\nplt.plot(x, (g1(x)-f)**2, lw=3, label=\"$B_1(x)$\")\nplt.plot(x, (g20(x)-f)**2, lw=3,label=\"$B_{20}(x)$\");\nplt.xlabel(\"$x$\")\nplt.ylabel(\"population error\")\nplt.yscale(\"log\")\nplt.legend(loc=4);\nplt.title(\"Bias\");\n\n\n\n\n\n\n\n\nAs you can see the bias or approximation error is much smaller for \\(g_{20}\\).\nIs \\(g_{20}\\) the best model for this data from all possible models? Indeed, how do we find the best fit model from the best hypothesis space? This is what learning is all about.\nWe have used the python function np.polyfit to find \\(g_{1}\\) the best fit model in \\(\\cal{H_1}\\) and \\(g_{20}\\) the best fit model in \\(\\cal{H_{20}}\\), but how did we arrive at that conclusion? This is the subject of the next section."
  },
  {
    "objectID": "posts/noiseless_learning/index.html#how-to-learn-the-best-fit-model-in-a-hypothesis-space",
    "href": "posts/noiseless_learning/index.html#how-to-learn-the-best-fit-model-in-a-hypothesis-space",
    "title": "Learning Without Noise",
    "section": "How to learn the best fit model in a hypothesis space",
    "text": "How to learn the best fit model in a hypothesis space\nLet’s understand in an intuitive sense, what it means for a function to be a good fit to the data. Lets consider, for now, only the hypothesis space \\(\\cal{H}_{1}\\), the set of all straight lines. In the figure below, we draw against the data points (in red) one such line \\(h_1(x)\\) (in red).\nYou might think you want to do this statistically, using ML Estimation or similar, but note that at this point there is no statistical notion of a generating process. We’re just trying to approximate a function by another, with the latter being chosen amongst many in a hypothesis space.\n\n\n\nLinear regression fit with residuals shown\n\n\nThe natural way of thinking about a “best fit” would be to minimize the distance from the line to the points, for some notion of distance. In the diagram we depict one such notion of distance: the vertical distance from the points to the line. These distances are represented as thin black lines.\nThe next question that then arises is this: how exactly we define the measure of this vertical distance? We cant take the measure of distance to be the y-value of the point minus the y value of the line at the same x, ie \\(y_i - h_1(x_i)\\). Why? If we did this, then we could have points very far from the line, and as long as the total distance above was equal to the total distance below the line, we’d get a net distance of 0 even when the line is very far from the points.\nThus we must use a positive estimate of the distance as our measure. We could take either the absolute value of the distance, \\(\\vert y_i - h_1(x_i) \\vert\\), or the square of the distance as our measure, \\((y_i - h_1(x_i))^2\\). Both are reasonable choices, and we shall use the squared distance for now. (Now its probably clear to you why we defined bias in the last section as the pointwise square of the distance).\nWe sum this measure up over all our data points, to create whats known as the error functional or risk functional (also just called error, cost, or risk) of using line \\(h_1(x)\\) to fit our points \\(y_i \\in \\cal{D}\\) (this notation is to be read as “\\(y_i\\) in \\(\\cal{D}\\)”) :\n\\[ R_{\\cal{D}}(h_i(x)) = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} (y_i - h_1(x_i))^2 \\]\nwhere \\(N\\) is the number of points in \\(\\cal{D}\\).\nWhat this formula says is: the cost or risk is just the total squared distance to the line from the observation points. Here we use the word functional to denote that, just as in functional programming, the risk is a function of the function \\(h_1(x)\\).\nWe also make explicit the in-sample data \\(\\cal{D}\\), because the value of the risk depends upon the points at which we made our observation. If we had made these observations \\(y_i\\) at a different set of \\(x_i\\), the value of the risk would be somewhat different. The hope in learning is that the risk will not be too different, as we shall see in the next section\nNow, given these observations, and the hypothesis space \\(\\cal{H}_1\\), we minimize the risk over all possible functions in the hypothesis space to find the best fit function \\(g_1(x)\\):\n\\[ g_1(x) = \\arg\\min_{h_1(x) \\in \\cal{H}} R_{\\cal{D}}(h_1(x)).\\]\nHere the notation\n\\(\"\\arg\\min_{x} F(x)\"\\)\nmeans: give me the argument of the functional \\(x\\) at which \\(F(x)\\) is minmized. So, for us: give me the function \\(g_1(x) = h_1\\) at which the risk \\(R_{\\cal{D}}(h_1)\\) is minimized; i.e. the minimization is over functions \\(h_1\\).\nAnd this is exactly what the python function np.polyfit(x,h,n) does for us. It minimizes this squared-error with respect to the coefficients of the polynomial.\nThus we can in general write:\n\\[ g(x) = \\arg\\min_{h(x) \\in \\cal{H}} R_{\\cal{D}}(h(x)),\\]\nwhere \\(\\cal{H}\\) is a general hypothesis space of functions.\n\nThe Structure of Learning\nWe have a target function \\(f(x)\\) that we do not know. But we do have a sample of data points from it, \\((x_1,y_1), (x_2,y_2), ..., (x_n,y_n)\\). We call this the sample or training examples \\(\\cal{D}\\). We are interested in using this sample to estimate a function \\(g\\) to approximate the function \\(f\\), and which can be used for prediction at new data points, or on the entire population, also called out-of-sample prediction.\nNotice the way that statistics comes into this approximation problem is from the notion that we are trying to reconstruct the original function from a small-ish sample rather than a large-ish population.\nTo do this, we use an algorithm, called the learner, which chooses functions from a hypothesis set \\(\\cal{H}\\) and computes a cost measure or risk functional \\(R\\) (like the sum of the squared distance over all points in the data set) for each of these functions. It then chooses the function \\(g\\) which minimizes this cost measure amonst all the functions in \\(\\cal{H}\\), and thus gives us a final hypothesis \\(g\\) which we then use to approximate or estimate f everywhere, not just at the points in our data set.\nHere our learner is called Polynomial Regression, and it takes a hypothesis space \\(\\cal{H}_d\\) of degree \\(d\\) polynomials, minimizes the “squared-error” risk measure, and spits out a best-fit hypothesis \\(g_d\\).\n\n\n\nThe supervised learning framework: from target function to final hypothesis\n\n\n\n\nOut-of-Sample and in-sample\nWe write \\(g \\approx f\\), or \\(g\\) is the estimand of \\(f\\).In statistics books you will see \\(g\\) written as \\(\\hat{f}\\).\nWhy do we think that this might be a good idea? What are we really after?\nWhat we’d like to do is make good predictions. In the language of cost, what we are really after is to minimize the cost out-of-sample, on the population at large. But this presents us with a conundrum: how can we minimize the risk on points we havent yet seen?\nThis is why we (a) minimize the risk on the set of points that we have to find \\(g\\) and then (b) hope that once we have found our best model \\(g\\), our risk does not particularly change out-of-sample, or when using a different set of points\nWe are, as is usual in statistics, drawing conclusions about a population from a sample.\nIntuitively, to do this, we need to ask ourselves, how representative is our sample? Or more precisely, how representative is our sample of our training points of the population (or for that matter the new x that we want to predict for)?\nWe illustrate this below for our population of 200 data points and our sample of 30 data points (in red).\n\nplt.hist(x, normed=True, bins=30, alpha=0.7)\nsns.kdeplot(x)\nplt.plot(x[indexes], [1.0]*len(indexes),'o', alpha=0.8)\nplt.xlim([0,1]);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\nIn our example, if we only want to use \\(g\\), our estimand of \\(f\\) to predict for large \\(x\\), or more religious counties, we would need a good sampling of points \\(x\\) closer to 1. And, similarly, the new \\(x\\) we are using to make predictions would also need to be representative of those counties. We wont do well if we try and predict low-religiousness counties from a sample of high-religiousness ones. Or, if we do want to predict over the entire range of religiousness, our training sample better cover all \\(x\\) well.\nOur red points seem to follow our (god given) histogram well.\n\n\nThe relation to the Law of Large Numbers.\nThe process of minimization we do is called Empirical Risk Minimization (ERM) as we minimize the cost measure over the “empirically observed” training examples or points. But, on the assumption that we were given a training set representative of the population, ERM is just an attempt use of the law of large numbers.\nWhat we really want to calculate is:\n\\[R_{out}(h) =  E_{p(x)}[(h(x) - f(x))^2] = \\int dx p(x)  (h(x) - f(x))^2 .\\]\nAs usual we do not have access to the population but just some samples and thus we want to do something like:\n\\[R_{out}(h) = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{x_i \\sim p(x)} (h(x_i) - f(x_i))^2 = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{x_i \\sim p(x)} (h(x_i) - y_i)^2.\\]\nWe do not have an infinitely large training “sample”. On the assumption that its representative (i.e. drawn from \\(p(x)\\)) we calculate\n\\[R_{\\cal{D}}(h) =  \\sum_{x_i \\in \\cal{D}} (h(x_i) - y_i)^2.\\]\nWe could calculate the usual mean of sample means and all that, and shall see later that it is these quantities that are related to bias and variance."
  },
  {
    "objectID": "posts/noiseless_learning/index.html#statement-of-the-learning-problem.",
    "href": "posts/noiseless_learning/index.html#statement-of-the-learning-problem.",
    "title": "Learning Without Noise",
    "section": "Statement of the learning problem.",
    "text": "Statement of the learning problem.\nOnce we have done that, we can then intuitively say that, if we find a hypothesis \\(g\\) that minimizes the cost or risk over the training set; this hypothesis might do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nIn other words, we hope the empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small.\nIndeed, as we can see below, \\(g_{20}\\) does an excellent job on the population, not just on the sample.\n\n#plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\nplt.plot(x,g20(x), 'b--', alpha=0.9, lw=2, label=\"$g_{20}$\");\nplt.plot(x,f, 'o', alpha=0.2, label=\"population\");\nplt.legend(loc=4);"
  },
  {
    "objectID": "posts/doseplacebo/index.html",
    "href": "posts/doseplacebo/index.html",
    "title": "The Significance and Size of Effects",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline \n#this line above prepares IPython notebook for working with matplotlib\n\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n\nimport numpy as np # imports a fast numerical programming library\nimport scipy as sp #imports stats functions, amongst other things\nimport matplotlib as mpl # this actually imports matplotlib\nimport matplotlib.cm as cm #allows us easy access to colormaps\nimport matplotlib.pyplot as plt #sets up plotting under plt\nimport pandas as pd #lets us handle data as dataframes\n#sets up pandas table display\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nLet us get the data and put it into a dataframe.\nplacebo = [54, 51, 58, 44, 55, 52, 42, 47, 58, 46]\ndrug = [54, 73, 53, 70, 73, 68, 52, 65, 65]\ndosage = placebo + drug\nlabel = ['P']*len(placebo) + ['D']*len(drug)\ndf = pd.DataFrame(dict(dosage=dosage, label=label))\ndf\n\n\n\n\n\n\n\n\ndosage\nlabel\n\n\n\n\n0\n54\nP\n\n\n1\n51\nP\n\n\n2\n58\nP\n\n\n3\n44\nP\n\n\n4\n55\nP\n\n\n5\n52\nP\n\n\n6\n42\nP\n\n\n7\n47\nP\n\n\n8\n58\nP\n\n\n9\n46\nP\n\n\n10\n54\nD\n\n\n11\n73\nD\n\n\n12\n53\nD\n\n\n13\n70\nD\n\n\n14\n73\nD\n\n\n15\n68\nD\n\n\n16\n52\nD\n\n\n17\n65\nD\n\n\n18\n65\nD\nThe “mean” size of the effect in our sample is about 13.\nactuals = df.groupby('label').dosage.mean()\nactuals\n\nlabel\nD    63.666667\nP    50.700000\nName: dosage, dtype: float64\ndf.groupby('label').dosage.hist(bins=np.arange(30, 80, 1));\nactual_effect = actuals['D'] - actuals['P']\nactual_effect\n\n12.966666666666661"
  },
  {
    "objectID": "posts/doseplacebo/index.html#permutations-to-get-significance",
    "href": "posts/doseplacebo/index.html#permutations-to-get-significance",
    "title": "The Significance and Size of Effects",
    "section": "Permutations to get significance",
    "text": "Permutations to get significance\nCould it have happened by chance?\nWe permute, group-by labels again, and calculate the effect. This kind of randomization should “kill” the effect:\n\ntemp = np.random.permutation(df.label)\n\n\ntemp_series = df.groupby(temp).dosage.mean()\ntemp_series\n\nD    57.0\nP    56.7\nName: dosage, dtype: float64\n\n\n\ntemp_series['D'] - temp_series['P']\n\n0.29999999999999716\n\n\nIf we compare the distribution of effect sizes to the actual effect, this actual effect should be in a tail if it is significant…\n\nsig_means = np.zeros(10000)\nfor i in range(10000):\n    temp = np.random.permutation(df.label)\n    mean_series = df.groupby(temp).dosage.mean()\n    sig_means[i] = mean_series['D'] - mean_series['P']\n\n\nplt.hist(sig_means, bins=50, alpha=0.4);\nplt.axvline(actual_effect, 0, 1, color=\"red\");\n\n\n\n\n\n\n\n\nAs a comparison, consider the case in which placebos had a much wider spread, between 50, and 450. Simply add 13 to each placebo value to get a dosage value. The mean difference would still be 13. But now, 13 would be way inside the histogram, and the effect would not be a significant one, and could have happened by chance.\nStatistically significant does not mean important. Thats a question of, how large is the effect, or where are the confidence intervals for the effect. For instance, if a statistically significant increase in mortality was a mean of 5 days over 5 years by drug over placebo, you would not consider the effect important."
  },
  {
    "objectID": "posts/doseplacebo/index.html#bootstrap-to-estimate-size-of-effect",
    "href": "posts/doseplacebo/index.html#bootstrap-to-estimate-size-of-effect",
    "title": "The Significance and Size of Effects",
    "section": "Bootstrap to estimate size of effect",
    "text": "Bootstrap to estimate size of effect\nHere we randomize labels within the group, take means, and subtract. Here is an example\n\nplacebo_bs = np.random.choice(list(range(10)), size=(10000, 10))\ndrug_bs = np.random.choice(list(range(10, 19)), size=(10000, 9))\n\n\nplacebo_bs[0,:]\n\narray([7, 7, 1, 5, 1, 5, 4, 6, 0, 7])\n\n\n\ndf.iloc[placebo_bs[0,:]]\n\n\n\n\n\n\n\n\ndosage\nlabel\n\n\n\n\n7\n47\nP\n\n\n7\n47\nP\n\n\n1\n51\nP\n\n\n5\n52\nP\n\n\n1\n51\nP\n\n\n5\n52\nP\n\n\n4\n55\nP\n\n\n6\n42\nP\n\n\n0\n54\nP\n\n\n7\n47\nP\n\n\n\n\n\n\n\nHere is the effect:\n\ndf.iloc[drug_bs[0,:]].dosage.mean() - df.iloc[placebo_bs[0,:]].dosage.mean()\n\n14.977777777777774\n\n\nLet us do this 10000 times.\n\neffect_diffs = np.zeros(10000)\nfor i in range(10000):\n    effect_diffs[i] = df.iloc[drug_bs[i,:]].dosage.mean() - df.iloc[placebo_bs[i,:]].dosage.mean()\n\n\npercs = np.percentile(effect_diffs, [5, 50, 95])\npercs\n\narray([  7.53333333,  13.05      ,  18.12222222])\n\n\n\nplt.hist(effect_diffs, bins=100, alpha=0.2);\nplt.axvline(actual_effect, 0, 1, color=\"red\");\nfor p in percs:\n    plt.axvline(p, 0, 1, color=\"green\");\n\n\n\n\n\n\n\n\nThat is, 90% of the time, the drug is 7.53 to 18.12 more effective than placebo. The average value of placebo in our sample was 50. This makes the drug 13 to 33% more effective, roghly, which seems it might be an important effect.\nIf you have such a confidence interval, why do a significance test. Consider the extreme case of 2 data points, wel separated. The confidence interval is tight around the difference. But a permutation test would show that half the time, you will by random chance, get a difference just as big as the observed one. Intuitively this is too little data to show significance, and this “half the time” bears that out…"
  },
  {
    "objectID": "posts/globemodellab/index.html",
    "href": "posts/globemodellab/index.html",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "posts/globemodellab/index.html#formulation-of-the-problem",
    "href": "posts/globemodellab/index.html#formulation-of-the-problem",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Formulation of the problem",
    "text": "Formulation of the problem\nThis problem, taken from McElreath’s book, involves a seal (or a well trained human) tossing a globe, catching it on the nose, and noting down if the globe came down on water or land.\nThe seal tells us that the first 9 samples were:\nWLWWWLWLW.\nWe wish to understand the evolution of belief in the fraction of water on earth as the seal tosses the globe.\nSuppose \\(\\theta\\) is the true fraction of water covering the globe. Our data story if that \\(\\theta\\) then is the probability of the nose landing on water, with each throw or toss of the globe being independent.\nNow we build a probabilistic model for the problem, which we shall use to guide a process of Bayesian updating of the model as data comes in.\n\\[\\cal{L} = p(n,k|\\theta) = Binom(n,k, \\theta)=\\frac{n!}{k! (n-k)! } \\, \\theta^k \\, (1-\\theta)^{(n-k)} \\]\nSince our seal hasnt really seen any water or land, (strange, I know), it assigns equal probabilities, ie uniform probability to any value of \\(\\theta\\).\nThis is our prior information\nFor reasons of conjugacy we choose as prior the beta distribution, with \\(Beta(1,1)\\) being the uniform prior."
  },
  {
    "objectID": "posts/globemodellab/index.html#how-to-do-the-bayesian-process",
    "href": "posts/globemodellab/index.html#how-to-do-the-bayesian-process",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "How to do the Bayesian Process",
    "text": "How to do the Bayesian Process\nBayes theorem and the things we will go through\n\nGrid approximation\nQuadratic (Laplace) Approximation\nConjugate Priors\nMCMC (later)\nModel Checking"
  },
  {
    "objectID": "posts/globemodellab/index.html#grid-approximation",
    "href": "posts/globemodellab/index.html#grid-approximation",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nfrom scipy.stats import binom\n\n\nprior_pdf = lambda p: 1\nlike_pdf = lambda p: binom.pmf(k=6, n=9, p=p)\npost_pdf = lambda p: like_pdf(p)*prior_pdf(p)\n\n\np_grid = np.linspace(0., 1., 20)\np_grid\n\narray([ 0.        ,  0.05263158,  0.10526316,  0.15789474,  0.21052632,\n        0.26315789,  0.31578947,  0.36842105,  0.42105263,  0.47368421,\n        0.52631579,  0.57894737,  0.63157895,  0.68421053,  0.73684211,\n        0.78947368,  0.84210526,  0.89473684,  0.94736842,  1.        ])\n\n\n\nplt.plot(p_grid, post_pdf(p_grid),'o-');\n\n\n\n\n\n\n\n\n\np_grid = np.linspace(0., 1., 1000)\npost_vals = post_pdf(p_grid)\npost_vals_normed = post_vals/np.sum(post_vals)\ngrid_post_samples = np.random.choice(p_grid, size=10000, replace=True, p=post_vals_normed)\n\n\nplt.plot(p_grid, post_vals)\n\n\n\n\n\n\n\n\n\nsns.distplot(grid_post_samples)"
  },
  {
    "objectID": "posts/globemodellab/index.html#laplace-approximation",
    "href": "posts/globemodellab/index.html#laplace-approximation",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\n\np_start = 0.5\nfrom scipy.optimize import minimize\npost_pdf_inv = lambda p: -post_pdf(p)\nres = minimize(post_pdf_inv, p_start, method = 'Nelder-Mead', options={'disp': True})\n\nOptimization terminated successfully.\n         Current function value: -0.273129\n         Iterations: 13\n         Function evaluations: 26\n\n\n\nres\n\n final_simplex: (array([[ 0.66669922],\n       [ 0.66660156]]), array([-0.27312909, -0.27312907]))\n           fun: -0.27312909031345828\n       message: 'Optimization terminated successfully.'\n          nfev: 26\n           nit: 13\n        status: 0\n       success: True\n             x: array([ 0.66669922])\n\n\n\npost_MAP = res.x[0]\npost_MAP\n\n0.66669921875000038\n\n\n\ninsertbefore = np.searchsorted(p_grid, post_MAP)\ninsertbefore\n\n667\n\n\n\npostmapval = (post_vals[insertbefore-1] + post_vals[insertbefore])/2.\npostmapval\n\n0.27312632244812729\n\n\n\nplt.plot(p_grid, post_vals);\nplt.plot(p_grid, norm.pdf(p_grid, loc=post_MAP, scale=0.16))\n\n\n\n\n\n\n\n\n\nzq = lambda sigma: sigma*postmapval*np.sqrt(2*np.pi)\ndef fit_loss(sigma):\n    vec = (post_vals/zq(sigma)) - norm.pdf(p_grid, loc=post_MAP, scale=sigma)\n    return np.dot(vec, vec)\n\n\nres2 = minimize(fit_loss, 0.2, method = 'Nelder-Mead', options={'disp': True})\n\nOptimization terminated successfully.\n         Current function value: 23.987144\n         Iterations: 12\n         Function evaluations: 24\n\n\n\nres2\n\n final_simplex: (array([[ 0.14921875],\n       [ 0.14917969]]), array([ 23.9871437 ,  23.98715773]))\n           fun: 23.987143699357638\n       message: 'Optimization terminated successfully.'\n          nfev: 24\n           nit: 12\n        status: 0\n       success: True\n             x: array([ 0.14921875])\n\n\n\npost_SIG = res2.x[0]\npost_SIG\n\n0.14921875000000009\n\n\n\nfrozen_laplace = norm(post_MAP, post_SIG)\n\n\nplt.plot(p_grid, post_pdf(p_grid)/zq(post_SIG), label = \"normalized posterior\");\nplt.plot(p_grid, frozen_laplace.pdf(p_grid), label = \"laplace approx\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nzq(post_SIG)\n\n0.10215906016979832\n\n\nNow we can get samples from here:\n\nsns.distplot(frozen_laplace.rvs(10000))"
  },
  {
    "objectID": "posts/globemodellab/index.html#conjugate-priors",
    "href": "posts/globemodellab/index.html#conjugate-priors",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nThe mean of \\(Beta(\\alpha, \\beta)\\) is \\(\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\) while the variance is\n\\[V=\\mu (1- \\mu)/(\\alpha + \\beta + 1)\\]\n\nfrom scipy.stats import beta\nx=np.linspace(0., 1., 100)\nplt.plot(x, beta.pdf(x, 1, 1));\nplt.plot(x, beta.pdf(x, 1, 9));\nplt.plot(x, beta.pdf(x, 1.2, 9));\nplt.plot(x, beta.pdf(x, 2, 18));\n\n\n\n\n\n\n\n\nWe shall choose \\(\\alpha=1\\) and \\(\\beta=1\\) to be uniform.\n\\[ p(\\theta) = {\\rm Beta}(\\theta,\\alpha, \\beta) = \\frac{\\theta^{\\alpha-1} (1-x)^{\\beta-1} }{B(\\alpha, \\beta)} \\] where \\(B(\\alpha, \\beta)\\) is independent of \\(\\theta\\) and it is the normalization factor.\nFrom Bayes theorem, the posterior for \\(\\theta\\) is\n\\[ p(\\theta|D) \\propto  p(\\theta) \\, p(n,k|\\theta)  =  Binom(n,k, \\theta) \\,  {\\rm Beta}(\\theta,\\alpha, \\beta)  \\]\nwhich can be shown to be\n\\[{\\rm Beta}(\\theta, \\alpha+k, \\beta+n-k)\\]\n\nfrom scipy.stats import beta, binom\n\nplt.figure(figsize=( 15, 18))\n\nprior_params = np.array( [1.,1.] )  # FLAT \n\nx = np.linspace(0.00, 1, 125)\ndatastring = \"WLWWWLWLW\"\ndata=[]\nfor c in datastring:\n    data.append(1*(c=='W'))\ndata=np.array(data)\nprint(data)\nchoices=['Land','Water']\n\n\nfor i,v in enumerate(data):\n    plt.subplot(9,1,i+1)\n    prior_pdf = beta.pdf( x, *prior_params)\n    if v==1:\n        water = [1,0]\n    else:\n        water = [0,1]\n    posterior_params = prior_params + np.array( water )    # posteriors beta parameters\n    posterior_pdf = beta.pdf( x, *posterior_params)  # the posterior \n    prior_params = posterior_params\n    plt.plot( x,prior_pdf, label = r\"prior for this step\", lw =1, color =\"#348ABD\" )\n    plt.plot( x, posterior_pdf, label = \"posterior for this step\", lw= 3, color =\"#A60628\" )\n    plt.fill_between( x, 0, prior_pdf, color =\"#348ABD\", alpha = 0.15) \n    plt.fill_between( x, 0, posterior_pdf, color =\"#A60628\", alpha = 0.15) \n    \n    plt.legend(title = \"N=%d, %s\"%(i, choices[v]));\n    #plt.ylim( 0, 10)#\n\n[1 0 1 1 1 0 1 0 1]"
  },
  {
    "objectID": "posts/globemodellab/index.html#interrogating-the-posterior",
    "href": "posts/globemodellab/index.html#interrogating-the-posterior",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Interrogating the posterior",
    "text": "Interrogating the posterior\nSince we can sample from the posterior now after 9 observations, lets do so!\n\nsamples = beta.rvs(*posterior_params, size=10000)\nplt.hist(samples, bins=50, normed=True);\nsns.kdeplot(samples);\n\n\n\n\n\n\n\n\n\nSampling to summarize\nNow we can calculate all sorts of stuff.\nThe probability that the amount of water is less than 50%\n\nnp.mean(samples &lt; 0.5)\n\n0.17180000000000001\n\n\nThe probability by which we get 80% of the samples.\n\nnp.percentile(samples, 80)\n\n0.75998662608698764\n\n\nYou might try and find a credible interval. This, unlike the wierd definition of confidence intervals, is exactly what you think it is, the amount of probability mass between certain percentages, like the middle 95%\n\nnp.percentile(samples, [2.5, 97.5])\n\narray([ 0.35115415,  0.8774055 ])\n\n\nYou can make various point estimates: mean, median\n\nnp.mean(samples), np.median(samples), np.percentile(samples, 50) #last 2 are same\n\n(0.63736799839639757, 0.64714663472562717, 0.64714663472562717)\n\n\nA particularly important and useful point estimate that we just saw is the MAP, or “maximum a-posteriori” estimate, the value of the parameter at which the pdf (num-samples) reach a maximum. It can be obtained from the samples as well.\n\nsampleshisto = np.histogram(samples, bins=50)\n\n\nmaxcountindex = np.argmax(sampleshisto[0])\nmapvalue = sampleshisto[1][maxcountindex]\nprint(maxcountindex, mapvalue)\n\n33 0.694004782956\n\n\nThe mean of the posterior samples corresponds to minimizing the squared loss.\n\nmse = [np.mean((xi-samples)**2) for xi in x]\nplt.plot(x, mse);\nplt.axvline(np.mean(samples), 0, 1, color=\"r\")\nprint(\"Mean\",np.mean(samples));\n\nMean 0.635370253478"
  },
  {
    "objectID": "posts/globemodellab/index.html#sampling-to-simulate-prediction-the-posterior-predictive",
    "href": "posts/globemodellab/index.html#sampling-to-simulate-prediction-the-posterior-predictive",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Sampling to simulate prediction: the posterior predictive",
    "text": "Sampling to simulate prediction: the posterior predictive\nWhy would you want to simulate prediction?\n\nModel Checking\nSoftware Validation\nResearch Design\nForecasting\n\nIts easy to sample from any one probability to get the sampling distribution at a particular \\(\\theta\\)\n\npoint3samps = np.random.binomial( len(data), 0.3, size=10000);\npoint7samps = np.random.binomial( len(data), 0.7, size=10000);\nplt.hist(point3samps, lw=3, alpha=0.5, histtype=\"stepfilled\", bins=np.arange(11));\nplt.hist(point7samps, lw=3, alpha=0.3,histtype=\"stepfilled\", bins=np.arange(11));\n\n\n\n\n\n\n\n\nThe posterior predictive:\n\\[p(y^{*} \\vert D) = \\int d\\theta p(y^{*} \\vert \\theta) p(\\theta \\vert D)\\]\nseems to be a complex integral. But if you parse it, its not so complex. This diagram from McElreath helps:\n\n\n\nThe posterior predictive distribution as a mixture: each parameter value implies a sampling distribution, weighted by the posterior probability, producing the marginal prediction. From McElreath, Statistical Rethinking.\n\n\n\nPlug-in Approximation\nAlso, often, people will use the plug-in approximation by putting the posterior mean or MAP value\n\\[p(\\theta \\vert D) = \\delta(\\theta - \\theta_{MAP})\\]\nand then simply drawing the posterior predictive from :\n\\[p(y^{*} \\vert D) = p(y^{*} \\vert \\theta_{MAP})\\]\n(the same thing could be done for \\(\\theta_{mean}\\)).\n\npluginpreds = np.random.binomial( len(data), mapvalue, size = len(samples))\n\n\nplt.hist(pluginpreds, bins=np.arange(11));\n\n\n\n\n\n\n\n\nThis approximation is just sampling from the likelihood(sampling distribution), at a posterior-obtained value of \\(\\theta\\). It might be useful if the posterior is an expensive MCMC and the MAP is easier to find by optimization, and can be used in conjunction with quadratic (gaussian) approximations to the posterior, as we will see in variational inference. But for now we have all the samples, and it would be inane not to use them…\n\n\nThe posterior predictive from sampling\nBut really from the perspective of sampling, all we have to do is to first draw the thetas from the posterior, then draw y’s from the likelihood, and histogram the likelihood. This is the same logic as marginal posteriors, with the addition of the fact that we must draw y from the likelihood once we drew \\(\\theta\\). You might think that we have to draw multiple \\(y\\)s at a theta, but this is already taken care of for us because of the nature of sampling. We already have multiple \\(\\theta\\)s in a bin.\n\npostpred = np.random.binomial( len(data), samples);\n\n\npostpred\n\narray([7, 7, 5, ..., 6, 7, 5])\n\n\n\nsamples.shape, postpred.shape\n\n((10000,), (10000,))\n\n\n\nplt.hist(postpred, bins=np.arange(11), alpha=0.5, align=\"left\", label=\"predictive\")\nplt.hist(pluginpreds, bins=np.arange(11), alpha=0.2, align=\"left\", label=\"plug-in (MAP)\")\nplt.title('Posterior predictive')\nplt.xlabel('k')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nReplicative predictives\nThere is a different kind of predictive sampling that us useful (and what you might have thought was predictive sampling). This is replicative sampling. It can be used with both priors and posteriors; the former for model callibration and the latter for model checking. We shall see both of these soon.\nThe idea here is to generate an entire dataset from one of the parameter samples in the posterior. So you are not generating 10000 ys for 10000 thetas, but rather 10000 y’s per theta. (you can play the same game with the prior). This kind of inverts the diagram we saw earlier to produce the posterior predictive.\nOur usual sample vs replication 2D setup can come useful here. Consider generating 1000 y’s per replication for each theta.\n\npostpred.shape\n\n(10000,)\n\n\n\nreppostpred =np.empty((1000, 10000))\nfor i in range(1000):\n    reppostpred[i,:] = np.random.binomial( len(data), samples);\nreppostpred.shape\n\n(1000, 10000)\n\n\n\nper_theta_avgs = np.mean(reppostpred, axis=0)\nper_theta_avgs.shape\n\n(10000,)\n\n\n\nplt.scatter(samples, per_theta_avgs, alpha=0.1);\n\n\n\n\n\n\n\n\nIn particular, you will find that the number of switches is not consistent with what you see in our data. This might lead you to question our model…always a good thing..but note that we have very little data as yet to go on\n\ndata\n\narray([1, 0, 1, 1, 1, 0, 1, 0, 1])\n\n\n\ndata[:-1] != data[1:]\n\narray([ True,  True, False, False,  True,  True,  True,  True], dtype=bool)\n\n\n\nnp.sum(data[:-1] != data[1:])\n\n6"
  },
  {
    "objectID": "posts/globemodellab/index.html#exercise",
    "href": "posts/globemodellab/index.html#exercise",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Exercise",
    "text": "Exercise\nYou can interrogate the posterior-predictive, or simulated samples in other ways, asking about the longest run of water tosses, or the number of times the water/land switched. This is left as an exercise."
  },
  {
    "objectID": "posts/MLE/index.html",
    "href": "posts/MLE/index.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]\n\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]"
  },
  {
    "objectID": "posts/MLE/index.html#choosing-a-parametric-model",
    "href": "posts/MLE/index.html#choosing-a-parametric-model",
    "title": "Maximum Likelihood Estimation",
    "section": "Choosing a parametric model",
    "text": "Choosing a parametric model\nWhen we do data analysis in a parametric way, we start by characterizing our particular sample statistically then, using a probability distribution (or mass function). This distribution has some parameters. Lets refer to these as \\(\\theta\\).\nIf we assume that our data was generated by this distribution, then the notion of the true value of the parameter makes sense. Now, usually in life, there is no way of knowing if this was the true generating process, unless we have some physics or similar ideas behind the process. But lets stick with the myth that we can do this. Then let us call the true value of the parameters as \\(\\theta^*\\).\nTo know this true value, we’d typically need the entire large population, not the sample we have been given as data. So the best we can do us to make a parameter estimate \\(\\hat{\\theta}\\) from the data. In the context of frequentist statistics, the assumption is that the parameters are fixed, and that there is this true value (\\(\\theta^*\\)), and that we can make some estimate of this from our sample (\\(\\hat{\\theta}\\)).\nA distribution is induced on this estimate by considering many samples that could have been drawn from the population…remember that frequentist statistics fixes the parameters but considers data stochastic. This distribution is called the sampling distribution of the parameter \\(\\theta\\). (In general a sampling distribution can be considered for anything computed on the sample, such as a mean or variance or other moment).\nOur question is: how do we estimate \\(\\hat{\\theta}\\). And how do we compute this sampling distribution so that we can get a notion of the uncertainty that estimating from a sample rather than the population leaves us with?\nThe first question is tackled by the Maximum Likelihood estimate, or MLE. The second one is tackled by techniques like the bootstrap.\nLets learn about the MLE in the context of a particular distribution, the exponential.\n\nThe idea behind the MLE\nThe diagram below illustrates the idea behind the MLE.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nConsider two distributions in the same family, one with a parameter, lets call it \\(\\theta\\), of value 1.8 (blue) and another of value 5.8. (green). Let’s say we have 3 data points, at \\(x=1,2,3\\).\nMaximum likelihood starts by asking the question: conditional on the fixed value of \\(\\theta\\), which distribution is the data more likely to have come from?\nIn our case the blue is more likely since the product of the height of the 3 vertical blue bars is higher than that of the 3 green bars.\nIndeed the question that MLE asks is: how can we move and scale the distribution, that is, change \\(\\theta\\), until the product of the 3 bars is maximised!\nThat is, the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i \\mid \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nOften it is easier and numerically more stable to maximise the log likelyhood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nThe exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\nIt takes the form: \\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n0 & x &lt; 0.\n\\end{cases}\n\\]\nIn the case of the exponential distribution we have:\n\\[\n\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n\\]\nMaximizing this:\n\\[\n\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n\\]\nand thus:\n\\[\n\\frac{1}{\\est{\\lambda_{MLE}}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nwhich is the sample mean of our sample. Usually one is not so lucky and one must use numerical optimization techniques.\nA crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large."
  },
  {
    "objectID": "posts/MLE/index.html#inference",
    "href": "posts/MLE/index.html#inference",
    "title": "Maximum Likelihood Estimation",
    "section": "Inference",
    "text": "Inference\nJust having an estimate is no good. We will want to put confidence intervals on the estimation of the parameters. This presents a conundrum: we have access to only one sample, but want to compute a error estimate over multiple samples, using an estimator such as the standard deviation.\nAt this point we are wishing for the Lord to have given us other samples drawn from the population. But alas, no such luck…\nSo how then are we to find the sampling distribution of our parameters?\nIn the last two decades, resampling the ONE dataset we have has become computationally feasible. Resampling involves making new samples from the observations, each of which is analysed in the same way as out original dataset. One way to do this is the Bootstrap."
  },
  {
    "objectID": "posts/MLE/index.html#linear-regression-mle",
    "href": "posts/MLE/index.html#linear-regression-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Linear Regression MLE",
    "text": "Linear Regression MLE\nLinear regression is the workhorse algorithm thats used in many sciences, social and natural. The diagram below illustrates the probabilistic interpretation of linear regression, and the idea behind the MLE for linear regression. We illustrate a point \\((x_i, y_i)\\), and the corresponding prediction for \\(x_i\\) using the line, that is \\(yhat_i\\) or \\(\\hat{y}_i\\).\n\n\n\nProbabilistic interpretation of linear regression\n\n\nThe fundamental assumption for the probabilistic analysis of linear regression is that each \\(y_i\\) is gaussian distributed with mean \\(\\v{w}\\cdot\\v{x_i}\\) (the y predicted by the regression line so to speak) and variance \\(\\sigma^2\\):\n\\[ y_i \\sim N(\\v{w}\\cdot\\v{x_i}, \\sigma^2) .\\]\nWe can then write the likelihood:\n\\[\\cal{L} = p(\\v{y} | \\v{x}, \\v{w}, \\sigma) = \\prod_i p(\\v{y}_i | \\v{x}_i, \\v{w}, \\sigma)\\]\nGiven the canonical form of the gaussian:\n\\[N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},\\]\nwe can show that:\n\\[\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .\\]\nThe log likelihood \\(\\ell\\) then is given by:\n\\[\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .\\]\nIf you differentiate this with respect to \\(\\v{w}\\) and \\(\\sigma\\), you get the MLE values of the parameter estimates:\n\\[\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, \\]\nwhere \\(\\v{X}\\) is the design matrix created by stacking rows \\(\\v{x}_i\\), and\n\\[\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . \\]\nThese are the standard results of linear regression."
  },
  {
    "objectID": "posts/MLE/index.html#logistic-regression-mle",
    "href": "posts/MLE/index.html#logistic-regression-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Logistic Regression MLE",
    "text": "Logistic Regression MLE\nLogistic regression if one of the well known supervized learning algorithms used for classification.\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the ‘1’ samples from the ‘0’ samples, just like in the diagram above. In other words, we wish to find the “regression” line which divides the samples. Now, a line has the form \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) in 2-dimensions. On one side of this line we have\n\\[w_1 x_1 + w_2 x_2 + w_0 \\ge 0,\\]\nand on the other side we have\n\\[w_1 x_1 + w_2 x_2 + w_0 &lt; 0.\\]\nOur classification rule then becomes:\n\\[\n\\begin{eqnarray}\ny = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &if& \\v{w}\\cdot\\v{x} &lt; 0\n\\end{eqnarray}\n\\]\nwhere \\(\\v{x}\\) is the vector \\(\\{1,x_1, x_2,...,x_n\\}\\) where we have also generalized to more than 2 features.\nWhat hypotheses \\(h\\) can we use to achieve this? One way to do so is to use the sigmoid function:\n\\[h(z) = \\frac{1}{1 + e^{-z}}.\\]\nNotice that at \\(z=0\\) this function has the value 0.5. If \\(z &gt; 0\\), \\(h &gt; 0.5\\) and as \\(z \\to \\infty\\), \\(h \\to 1\\). If \\(z &lt; 0\\), \\(h &lt; 0.5\\) and as \\(z \\to -\\infty\\), \\(h \\to 0\\). As long as we identify any value of \\(y &gt; 0.5\\) as 1, and any \\(y &lt; 0.5\\) as 0, we can achieve what we wished above.\nThis function is plotted below:\n\nh = lambda z: 1./(1+np.exp(-z))\nzs=np.arange(-5,5,0.1)\nplt.plot(zs, h(zs), alpha=0.5);\n\n\n\n\n\n\n\n\nSo we then come up with our rule by identifying:\n\\[z = \\v{w}\\cdot\\v{x}.\\]\nThen \\(h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\ge 0\\) and \\(h(\\v{w}\\cdot\\v{x}) \\lt 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\lt 0\\), and:\n\\[\n\\begin{eqnarray}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray}\n\\]\nWe said above that if \\(h &gt; 0.5\\) we ought to identify the sample with \\(y=1\\)? One way of thinking about this is to identify \\(h(\\v{w}\\cdot\\v{x})\\) with the probability that the sample is a ‘1’ (\\(y=1\\)). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a ‘1’ is \\(\\ge 0.5\\).\nSo suppose we say then that the probability of \\(y=1\\) for a given \\(\\v{x}\\) is given by \\(h(\\v{w}\\cdot\\v{x})\\)?\nThen, the conditional probabilities of \\(y=1\\) or \\(y=0\\) given a particular sample’s features \\(\\v{x}\\) are:\n\\[\\begin{eqnarray}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray}\\]\nThese two can be written together as\n\\[P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} \\]\nThen multiplying over the samples we get the probability of the training \\(y\\) given \\(\\v{w}\\) and the \\(\\v{x}\\):\n\\[P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\]\nWhy use probabilities? Earlier, we talked about how the regression function \\(f(x)\\) never gives us the \\(y\\) exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently.\nWe said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(P(x,y)\\). Thus we need to model \\(y\\) at a given \\(x\\), written as \\(P(y \\mid x)\\), and since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) ,\\]\nand can obtain our joint probability (\\(P(x, y))\\).\nIndeed its important to realize that a particular sample can be thought of as a draw from some “true” probability distribution. If for example the probability of classifying a sample point as a ‘0’ was 0.1, and it turns out that the sample point was actually a ‘0’, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a ‘0’! But, of-course its more unlikely than its likely, and having good probabilities means that we’ll be likely right most of the time, which is what we want to achieve in classification.\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a ‘1’.\nNow if we maximize \\[P(y \\mid \\v{x},\\v{w})\\], we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This maximum likelihood estimation maximises the likelihood of the sample y,\n\\[\\like = P(y \\mid \\v{x},\\v{w}).\\]\nAgain, we can equivalently maximize\n\\[\\loglike = log(P(y \\mid \\v{x},\\v{w}))\\]\nsince the natural logarithm \\(log\\) is a monotonic function. This is known as maximizing the log-likelihood.\n\\[\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).\\]\nThus\n\\[\\begin{eqnarray}\n\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nopen\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Rahul Dave — physicist, engineer, and practitioner of machine learning, AI, and data science. I also teach.\nI did my Ph.D. in physics at the University of Pennsylvania, where my thesis on quintessence and the cosmic microwave background was among the early works introducing dark energy. After that I spent over a decade at Harvard — as a computational scientist at the Center for Astrophysics, then as a lecturer at SEAS and IACS, where I helped build courses like CS109 (Data Science), AM207 (Stochastic Methods and Bayesian Inference), and CS207 (Systems Development for Computational Science).\nI’m co-founder and Chief Scientist at Univ.AI, where we build AI solutions and teach machine learning. Through Univ.AI, I also consult on ML and data engineering problems — we’ve worked with clients across oil and energy, insurance, finance, and sports.\nI’m a Bayesian at heart. I like interesting problems at the intersection of statistics, machine learning, and computation — and I like explaining them clearly.\n\n\n\nInterests\n\nAI — how it works, why it works, and where it breaks\nBayesian inference and probabilistic modeling\nMachine learning and deep learning\nData visualization and communication\nCosmology and astrophysics\nDevelopment economics\n\n\n\nEducation\n\nPh.D. Physics, University of Pennsylvania, 2002\nB.S. Physics (Honors), St. Xavier’s College, University of Bombay, 1992\n\n\n\nElsewhere\n\nGitHub\nX / Twitter\nBluesky\nGoogle Scholar\nLinkedIn\n\n\n\nGet in Touch\nIf you’d like to work with me — consulting, AI/ML projects, or training — drop me a message below.\nFor questions about blog posts, course material, or anything else, find me on X / Twitter.\n\nName \nEmail \nMessage\n\n\nSend"
  },
  {
    "objectID": "til/open.html",
    "href": "til/open.html",
    "title": "open",
    "section": "",
    "text": "MacOS has a great command open. You can use it to open any file in any folder from the terminal in its default app. For example:\nopen bla.pdf\nwill open a file in Preview.\nSometimes you want another app. Then you can use the -a flag. Like so:\nopen -a /Applications/Typora.app bla.md"
  },
  {
    "objectID": "collections/mysoft/hooksett.html",
    "href": "collections/mysoft/hooksett.html",
    "title": "Hooksett",
    "section": "",
    "text": "Hooksett is a Python library that provides a flexible, extensible hook system for managing parameters, metrics, and artifacts in ML workflows."
  },
  {
    "objectID": "collections/mysoft/hooksett.html#example",
    "href": "collections/mysoft/hooksett.html#example",
    "title": "Hooksett",
    "section": "Example",
    "text": "Example\nAnnotate your ML class with tracked types, wire up a config loader and MLflow output, and every parameter and metric is automatically captured:\nfrom hooksett import tracked, HookManager\nfrom hooksett.hooks import YAMLConfigInput, TypeValidationHook, MLflowOutput\n\ntype Parameter[T] = T\ntype Metric[T] = T\n\n@tracked\nclass Trainer:\n    learning_rate: Parameter[float] = 0.01\n    batch_size: Parameter[int] = 32\n    epochs: Parameter[int] = 100\n    accuracy: Metric[float] = 0.0\n    loss: Metric[float] = 0.0\n\n    def train(self):\n        for epoch in range(self.epochs):\n            # training step ...\n            self.accuracy = evaluate(model)\n            self.loss = compute_loss(model)\n\n# load params from YAML, validate, and log everything to MLflow\nmanager = HookManager()\nmanager.add_input_hook(YAMLConfigInput(\"config.yaml\"))\nmanager.add_input_hook(TypeValidationHook())\nmanager.add_output_hook(MLflowOutput())"
  },
  {
    "objectID": "collections/mysoft/hooksett.html#features",
    "href": "collections/mysoft/hooksett.html#features",
    "title": "Hooksett",
    "section": "Features",
    "text": "Features\n\n@tracked class decorator — monitors attribute changes on class instances via Python descriptors\n@track_function decorator — tracks function parameters and local variables; values are saved to hooks once at function/method exit\nLocal variable tracking — annotate locals with Traced[T] inside methods or functions; only the final value at exit is captured\nAutomatic parameter loading — YAMLConfigInput hook loads configuration from YAML files into tracked attributes\nParameter validation — TypeValidationHook enforces type annotations; RangeValidationHook checks numeric bounds\nCustom type registry — define domain-specific tracked types (Parameter, Metric, Artifact, Prompt, Response, Feature) via register_tracked_type\nPluggable output hooks — TracedOutput for logging, MLflowOutput for experiment tracking, or write your own OutputHook\nSingleton HookManager — register input and output hooks once; all decorated classes and functions use them automatically\nSeparation of config and code — parameters live in YAML, validation in hooks, tracking in type annotations"
  },
  {
    "objectID": "collections/software/hamilton.html",
    "href": "collections/software/hamilton.html",
    "title": "Stitchfix Hamilton",
    "section": "",
    "text": "A scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/hamilton.html#why-choose-this-tool",
    "href": "collections/software/hamilton.html#why-choose-this-tool",
    "title": "Stitchfix Hamilton",
    "section": "",
    "text": "A scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/awk.html",
    "href": "collections/software/awk.html",
    "title": "Awk",
    "section": "",
    "text": "An old goody! For quick command line analysis of data.\n\nThe following examples were taken from the tldr page for awk:\nPrint the fifth column (a.k.a. field) in a space-separated file:\nawk '{print $5}' filename\nPrint the second column of the lines containing “foo” in a space-separated file:\nawk '/foo/ {print $2}' filename\nPrint the last column of each line in a file, using a comma (instead of space) as a field separator:\nawk -F ',' '{print $NF}' filename\nSum the values in the first column of a file and print the total:\nawk '{s+=$1} END {print s}' filename\nPrint every third line starting from the first line:\nawk 'NR%3==1' filename\nPrint different values based on conditions:\nawk '{if ($1 == \"foo\") print \"Exact match foo\"; else if ($1 ~ \"bar\") print \"Partial match bar\"; else print \"Baz\"}' filename\nPrint all lines where the 10th column value equals the specified value:\nawk '($10 == value)'\nPrint all the lines which the 10th column value is between a min and a max:\nawk '($10 &gt;= min_value && $10 &lt;= max_value)'"
  }
]