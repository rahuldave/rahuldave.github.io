[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "Claude Session Context",
    "section": "",
    "text": "This is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io.\n\n\n\n\nAM207 course wiki at ~/Attic/Projects/AM207/2018fall_wiki/wiki/ — lectures, notebooks, markdown notes\nLecture index at ~/Attic/Projects/AM207/2018fall_wiki/lectures/index.md (replace .html with .md for source files)\nEach lecture .md links to wiki notes; check for .ipynb (primary source) before falling back to .md\nUse /import-wiki-notes skill to import notes as blog posts\nHelper scripts in _scripts/:\n\nimport_notebook.py — creates posts/&lt;name&gt;/index.ipynb from wiki .ipynb (adds frontmatter, fixes paths, copies images). Run with --help.\nupdate_captions.py — updates image captions in notebook markdown cells. Used by /caption-images skill.\n\nAfter importing, run /caption-images to add captions with citations to embedded images\n\n\n\n\n\nLecture 1 (Intro & Probability): DONE\n\nboxloop — already existed, skipped\nprobability — notebook, imported to posts/probability/\ndistributions — markdown only, imported to posts/distributions.md\ndistrib-example — notebook, imported to posts/distrib-example/\n\nLecture 2 (Probability, Sampling, Laws, Monte Carlo): DONE\n\ndistributions — already imported (Lecture 1), skipped\nexpectations — notebook, imported to posts/expectations/\nsamplingclt — notebook, imported to posts/samplingclt/\nbasicmontecarlo — notebook, imported to posts/basicmontecarlo/\nmontecarlointegrals — notebook, imported to posts/montecarlointegrals/\n\nLecture 3 (From Monte Carlo to Frequentist Stats): DONE\n\nExpectations — already imported (Lecture 2), skipped\nSamplingCLT — already imported (Lecture 2), skipped\nbasicmontecarlo — already imported (Lecture 2), skipped\nmontecarlointegrals — already imported (Lecture 2), skipped\nfrequentist — markdown only, imported to posts/frequentist.md\nMLE — notebook, imported to posts/MLE/\n\nLecture 4 (MLE, Sampling, and Learning): DONE\n\nnoiseless_learning — notebook, imported to posts/noiseless_learning/\nnoisylearning — notebook, imported to posts/noisylearning/\nMLE — already imported (Lecture 3), skipped\ntestingtraining — notebook, imported to posts/testingtraining/\nvalidation — notebook, imported to posts/validation/\nregularization — notebook, imported to posts/regularization/\n\nLecture 5 (Regression, AIC, Info. Theory): DONE\n\ndoseplacebo — notebook, imported to posts/doseplacebo/\nnoiseless_learning — already imported (Lecture 4), skipped\nnoisylearning — already imported (Lecture 4), skipped\ntestingtraining — already imported (Lecture 4), skipped\nvalidation — already imported (Lecture 4), skipped\nregularization — already imported (Lecture 4), skipped\njensens — notebook, imported to posts/jensens/\nDivergence — notebook, imported to posts/divergence/\nunderstandingaic — notebook, imported to posts/understandingaic/\n\nLecture 6 (Risk and Information): DONE\n\njensens — already imported (Lecture 5), skipped\nDivergence — already imported (Lecture 5), skipped\nunderstandingaic — already imported (Lecture 5), skipped\nEntropy — notebook, imported to posts/entropy/\n\nLecture 7 (From Entropy to Bayes): DONE\n\nDivergence — already imported (Lecture 5), skipped\nEntropy — already imported (Lecture 6), skipped\nbayes_withsampling — notebook, imported to posts/bayes_withsampling/\nglobemodel — notebook, imported to posts/globemodel/\nsufstatexch — notebook, imported to posts/sufstatexch/\n\nLecture 8 (Bayes and Sampling): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nglobemodel — already imported (Lecture 7), skipped\nsufstatexch — already imported (Lecture 7), skipped\nglobemodellab — notebook, imported to posts/globemodellab/\nnormalmodel — notebook, imported to posts/normalmodel/\nlightspeed — source not found in wiki, skipped\ninversetransform — notebook, imported to posts/inversetransform/\nrejectionsampling — notebook, imported to posts/rejectionsampling/\nimportancesampling — notebook, imported to posts/importancesampling/\n\nLecture 9 (Bayes and Sampling, cont.): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nnormalmodel — already imported (Lecture 8), skipped\nsufstatexch — already imported (Lecture 7), skipped\nlightspeed — source not found in wiki, skipped\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nbayesianregression — notebook, imported to posts/bayesianregression/\nnormalreg — notebook, imported to posts/normalreg/ (includes Howell1.csv data)\n\nLecture 10 (Sampling and Gradient Descent): DONE\n\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nimportancesampling — already imported (Lecture 8), skipped\ngradientdescent — notebook, imported to posts/gradientdescent/ (SGD gif replaced with link)\nLogisticBP — notebook, imported to posts/logisticbp/\n\nLecture 11 (Gradient Descent and Neural Networks): DONE\n\ngradientdescent — already imported (Lecture 10), skipped\nLogisticBP — already imported (Lecture 10), skipped\nFuncTorch — notebook, imported to posts/functorch/\nnnreg — notebook, imported to posts/nnreg/\nMLP_Classification — notebook, imported to posts/mlp_classification/\n\nLecture 12 (Non Linear Approximation to Classification): DONE\n\nLogisticBP — already imported (Lecture 10), skipped\nFuncTorch — already imported (Lecture 11), skipped\nnnreg — already imported (Lecture 11), skipped\nMLP_Classification — already imported (Lecture 11), skipped\ntypesoflearning — notebook, imported to posts/typesoflearning/\ngenerativemodels — notebook, imported to posts/generativemodels/ (includes heights/weights CSV)\nutilityorrisk — notebook, imported to posts/utilityorrisk/\n\nLecture 13 (Classification, Mixtures, and EM): DONE\n\ntypesoflearning — already imported (Lecture 12), skipped\ngenerativemodels — already imported (Lecture 12), skipped\nEM — notebook, imported to posts/em/\n\nLecture 14 (EM and Hierarchical Models): DONE\n\nEM — already imported (Lecture 13), skipped\nhierarch — notebook, imported to posts/hierarch/\n\nLecture 15 (MCMC): DONE\n\nmarkov — notebook, imported to posts/markov/\nmetropolis — notebook, imported to posts/metropolis/\ndiscretemcmc — notebook, imported to posts/discretemcmc/\nbayes_withsampling — already imported (Lecture 7), skipped\nintrogibbs — notebook, imported to posts/introgibbs/\nhierarch — already imported (Lecture 14), skipped\n\nLecture 16 (MCMC and Gibbs): DONE\n\nmarkov — already imported (Lecture 15), skipped\nmetropolis — already imported (Lecture 15), skipped\ndiscretemcmc — already imported (Lecture 15), skipped\nbayes_withsampling — already imported (Lecture 7), skipped\nintrogibbs — already imported (Lecture 15), skipped\nhierarch — already imported (Lecture 14), skipped\nmetropolishastings — notebook, imported to posts/metropolishastings/\nMetropolisSupport — notebook, imported to posts/metropolissupport/\nswitchpoint — notebook, imported to posts/switchpoint/\ngibbsfromMH — markdown, imported to posts/gibbsfromMH/\ngibbsconj — notebook, imported to posts/gibbsconj/\n\nLecture 17 (Data Augmentation, Gibbs, and HMC): DONE\n\ndataaug — notebook, imported to posts/dataaug/\ntetchygibbs — notebook, imported to posts/tetchygibbs/\nhmcidea — markdown, imported to posts/hmcidea/\nhmcexplore — notebook, imported to posts/hmcexplore/\n\nLectures 18–26: NOT YET IMPORTED\n\n\n\n\n\nDates increase by 1 week per lecture, starting from 2025-01-08 (Lecture 1)\nAll notes within a lecture share the same date (the lecture’s date)\nIf a note was first imported in an earlier lecture, it keeps that earlier date\nLast date used: 2025-04-30 (Lecture 17)\nNext lecture (18) should use: 2025-05-07\n\n\n\n\n\nCanonical categories are in _categories.txt (root of project), one per line, sorted alphabetically\nCurrent categories: bayesian, classification, data, decision-theory, elections, hierarchical, information-theory, integration, interactive, macos, mcmc, models, montecarlo, neural-networks, optimization, orchestration, pipeline, probability, regression, sampling, statistics, visualization\nAll categories must be lowercase\nWhen importing, map source keywords to existing categories; propose new ones for user approval\nThe /import-wiki-notes skill enforces this workflow (step 7c)\n\n\n\n\n\nProfile photo: assets/profile.jpg (sourced from GitHub avatar)\nComprehensive bio context: ~/Context/rahul-dave-profile.md\nContact form via Formspree (ID: mykjwlyk) — forwards to rahuldave@univ.ai\nLinks: GitHub, X/Twitter, Bluesky (rahuldave.bsky.social), Google Scholar, LinkedIn\n\n\n\n\n\nincludes/discuss-links.html — “Discuss this post” links (Twitter, Bluesky, LinkedIn) on posts, til, and collections pages only\nUses IIFE (not DOMContentLoaded) because include-after-body runs after DOM is ready\nLinks are constructed at runtime from window.location.href — will use production domain automatically\nOpen Graph and Twitter Card metadata enabled in _quarto.yml\n\n\n\n\n\nAll listing pages sort by date desc: index.qmd, posts.qmd, til.qmd, collections.qmd\n\n\n\n\n\ndesigns/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — fork of depth, modernized with animations/glassmorphism\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nColorBrewer Blues palette (#eff3ff → #08306b)\nDark/light mode support\n\n\n\n--cb-blue-50: #eff3ff   --cb-blue-100: #c6dbef   --cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6  --cb-blue-400: #4292c6   --cb-blue-500: #2171b5\n--cb-blue-600: #08519c  --cb-blue-700: #084594   --cb-blue-800: #08306b\n\n\n\n\n\n\n\n\n_quarto.yml — project config: website type, navbar, SCSS theme (light/dark), TOC, margin references, Open Graph, Twitter Cards\nThemes: styles/modern-light.scss, styles/modern-dark.scss\nIncludes: includes/fonts.html, includes/brand-icon.html, includes/theme-toggle.html, includes/discuss-links.html\n\n\n\n\nNotebooks get their own folder with index.ipynb (or index.qmd for JS demos):\nposts/\n  probability/\n    index.ipynb        # URL becomes /posts/probability/\n    assets/            # images, data files referenced by THIS notebook only\n      venn.png\n      bishop-prob.png\n  votingforcongress/\n    index.ipynb\n    assets/\n      sep7.png\n  earth-demo/\n    index.qmd          # Three.js demo using .qmd format\n    assets/\n      earth-card.png   # Card thumbnail for listing (no content images in post)\nMarkdown posts stay as flat files, images go in shared posts/images/ or posts/data/:\nposts/\n  boxloop.md           # URL becomes /posts/boxloop\n  distributions.md\n  images/              # shared images for flat .md posts\n    2tosscdf.png\n  data/                # shared data for flat .md posts (if needed)\n\n\n\nNotebooks must have a raw cell (cell_type: “raw”) as the first cell with YAML:\n---\ntitle: \"Post Title\"\nsubtitle: \"Catchy one-liner for listing cards.\"\ndescription: \"Two-sentence summary for the post page.\"\ncategories:\n    - probability\n    - statistics\ndate: 2025-01-08\n---\n\nsubtitle appears on listing cards\ndescription is the longer summary\ncategories are used for filtering (must be from _categories.txt)\ndate controls sort order\nimage — optional, for posts without content images (e.g. image: assets/earth-card.png)\n\nFor markdown posts, the same YAML goes in the standard frontmatter block at the top.\n\n\n\nFor posts with no embedded images (e.g. interactive Three.js demos), use the browser agent to screenshot the rendered page, crop to the key visual, save to assets/, and add image: to frontmatter. See skill step 9 for details.\n\n\n\nUse .qmd format (not .ipynb) for JavaScript-heavy interactive content: - Load external JS via format: html: include-in-header: in frontmatter - Use Quarto’s fenced div syntax ::: {.classname} for layout containers - Inline &lt;script&gt; blocks at the end of the .qmd file - Read CSS custom properties (--cb-blue-*, --interactive-*) for theme integration - Example: posts/earth-demo/index.qmd\n\n\n\nmake preview                # Live dev server with hot reload\nmake render                 # Build full site to _site/\nmake build                  # Render + rsync _site/ to docs/ for GitHub Pages\nquarto render posts/probability/index.ipynb  # Render a single post\n\nrender produces _site/; build adds an rsync step to sync changed files into docs/\nCommit and push are intentionally separate from build\n\n\n\n\n\nNotebook posts: assets/filename.png (relative to the notebook’s folder)\nMarkdown posts: images/filename.png (relative to posts/)\nSite-wide assets already in /assets/ (e.g. lawoflargenumbers images) use /assets/... absolute paths\nProfile photo: assets/profile.jpg (JPEG, not PNG — watch for GitHub avatar format mismatch)\n\n\n\n\n\nAll categories must be lowercase — no Statistics, use statistics; no MonteCarlo, use montecarlo\nCanonical list in _categories.txt at project root\nThis prevents duplicate tags in Quarto listing filters\n\n\n\n\n\ntitle — post title\nsubtitle — catchy one-liner shown on listing cards\ndescription — 2-sentence content summary\ncategories — lowercase tags for filtering (from _categories.txt)\ndate — controls sort order (YYYY-MM-DD)\nimage — card thumbnail path (optional, for posts without content images)\n\n\n\n\n\nposts/ — main blog posts (shown on index page)\ntil/ — Today I Learned (shown only on TIL page, NOT on index)\ncollections/software/ — software tools (shown only on Collections page, NOT on index)\nindex.qmd listing contents should only include posts/ and posts/**/\n\n\n\n\n\nAfter adding/moving/renaming files, restart quarto preview — the live server caches resource IDs and will show “Bad resource ID” for changed files\nListing .qmd files must not have a stray --- after the YAML block (causes parse errors)\nListing contents paths should NOT have a leading / (use til/*.qmd not /til/*.qmd)\ninclude-after-body scripts run after DOMContentLoaded — use IIFE, not event listeners\nGitHub avatar downloads are JPEG even with .png URL — always check with file command and use correct extension"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "Claude Session Context",
    "section": "",
    "text": "This is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io."
  },
  {
    "objectID": "CLAUDE.html#content-source",
    "href": "CLAUDE.html#content-source",
    "title": "Claude Session Context",
    "section": "",
    "text": "AM207 course wiki at ~/Attic/Projects/AM207/2018fall_wiki/wiki/ — lectures, notebooks, markdown notes\nLecture index at ~/Attic/Projects/AM207/2018fall_wiki/lectures/index.md (replace .html with .md for source files)\nEach lecture .md links to wiki notes; check for .ipynb (primary source) before falling back to .md\nUse /import-wiki-notes skill to import notes as blog posts\nHelper scripts in _scripts/:\n\nimport_notebook.py — creates posts/&lt;name&gt;/index.ipynb from wiki .ipynb (adds frontmatter, fixes paths, copies images). Run with --help.\nupdate_captions.py — updates image captions in notebook markdown cells. Used by /caption-images skill.\n\nAfter importing, run /caption-images to add captions with citations to embedded images"
  },
  {
    "objectID": "CLAUDE.html#content-import-status-from-am207-wiki",
    "href": "CLAUDE.html#content-import-status-from-am207-wiki",
    "title": "Claude Session Context",
    "section": "",
    "text": "Lecture 1 (Intro & Probability): DONE\n\nboxloop — already existed, skipped\nprobability — notebook, imported to posts/probability/\ndistributions — markdown only, imported to posts/distributions.md\ndistrib-example — notebook, imported to posts/distrib-example/\n\nLecture 2 (Probability, Sampling, Laws, Monte Carlo): DONE\n\ndistributions — already imported (Lecture 1), skipped\nexpectations — notebook, imported to posts/expectations/\nsamplingclt — notebook, imported to posts/samplingclt/\nbasicmontecarlo — notebook, imported to posts/basicmontecarlo/\nmontecarlointegrals — notebook, imported to posts/montecarlointegrals/\n\nLecture 3 (From Monte Carlo to Frequentist Stats): DONE\n\nExpectations — already imported (Lecture 2), skipped\nSamplingCLT — already imported (Lecture 2), skipped\nbasicmontecarlo — already imported (Lecture 2), skipped\nmontecarlointegrals — already imported (Lecture 2), skipped\nfrequentist — markdown only, imported to posts/frequentist.md\nMLE — notebook, imported to posts/MLE/\n\nLecture 4 (MLE, Sampling, and Learning): DONE\n\nnoiseless_learning — notebook, imported to posts/noiseless_learning/\nnoisylearning — notebook, imported to posts/noisylearning/\nMLE — already imported (Lecture 3), skipped\ntestingtraining — notebook, imported to posts/testingtraining/\nvalidation — notebook, imported to posts/validation/\nregularization — notebook, imported to posts/regularization/\n\nLecture 5 (Regression, AIC, Info. Theory): DONE\n\ndoseplacebo — notebook, imported to posts/doseplacebo/\nnoiseless_learning — already imported (Lecture 4), skipped\nnoisylearning — already imported (Lecture 4), skipped\ntestingtraining — already imported (Lecture 4), skipped\nvalidation — already imported (Lecture 4), skipped\nregularization — already imported (Lecture 4), skipped\njensens — notebook, imported to posts/jensens/\nDivergence — notebook, imported to posts/divergence/\nunderstandingaic — notebook, imported to posts/understandingaic/\n\nLecture 6 (Risk and Information): DONE\n\njensens — already imported (Lecture 5), skipped\nDivergence — already imported (Lecture 5), skipped\nunderstandingaic — already imported (Lecture 5), skipped\nEntropy — notebook, imported to posts/entropy/\n\nLecture 7 (From Entropy to Bayes): DONE\n\nDivergence — already imported (Lecture 5), skipped\nEntropy — already imported (Lecture 6), skipped\nbayes_withsampling — notebook, imported to posts/bayes_withsampling/\nglobemodel — notebook, imported to posts/globemodel/\nsufstatexch — notebook, imported to posts/sufstatexch/\n\nLecture 8 (Bayes and Sampling): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nglobemodel — already imported (Lecture 7), skipped\nsufstatexch — already imported (Lecture 7), skipped\nglobemodellab — notebook, imported to posts/globemodellab/\nnormalmodel — notebook, imported to posts/normalmodel/\nlightspeed — source not found in wiki, skipped\ninversetransform — notebook, imported to posts/inversetransform/\nrejectionsampling — notebook, imported to posts/rejectionsampling/\nimportancesampling — notebook, imported to posts/importancesampling/\n\nLecture 9 (Bayes and Sampling, cont.): DONE\n\nbayes_withsampling — already imported (Lecture 7), skipped\nnormalmodel — already imported (Lecture 8), skipped\nsufstatexch — already imported (Lecture 7), skipped\nlightspeed — source not found in wiki, skipped\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nbayesianregression — notebook, imported to posts/bayesianregression/\nnormalreg — notebook, imported to posts/normalreg/ (includes Howell1.csv data)\n\nLecture 10 (Sampling and Gradient Descent): DONE\n\ninversetransform — already imported (Lecture 8), skipped\nrejectionsampling — already imported (Lecture 8), skipped\nimportancesampling — already imported (Lecture 8), skipped\ngradientdescent — notebook, imported to posts/gradientdescent/ (SGD gif replaced with link)\nLogisticBP — notebook, imported to posts/logisticbp/\n\nLecture 11 (Gradient Descent and Neural Networks): DONE\n\ngradientdescent — already imported (Lecture 10), skipped\nLogisticBP — already imported (Lecture 10), skipped\nFuncTorch — notebook, imported to posts/functorch/\nnnreg — notebook, imported to posts/nnreg/\nMLP_Classification — notebook, imported to posts/mlp_classification/\n\nLecture 12 (Non Linear Approximation to Classification): DONE\n\nLogisticBP — already imported (Lecture 10), skipped\nFuncTorch — already imported (Lecture 11), skipped\nnnreg — already imported (Lecture 11), skipped\nMLP_Classification — already imported (Lecture 11), skipped\ntypesoflearning — notebook, imported to posts/typesoflearning/\ngenerativemodels — notebook, imported to posts/generativemodels/ (includes heights/weights CSV)\nutilityorrisk — notebook, imported to posts/utilityorrisk/\n\nLecture 13 (Classification, Mixtures, and EM): DONE\n\ntypesoflearning — already imported (Lecture 12), skipped\ngenerativemodels — already imported (Lecture 12), skipped\nEM — notebook, imported to posts/em/\n\nLecture 14 (EM and Hierarchical Models): DONE\n\nEM — already imported (Lecture 13), skipped\nhierarch — notebook, imported to posts/hierarch/\n\nLecture 15 (MCMC): DONE\n\nmarkov — notebook, imported to posts/markov/\nmetropolis — notebook, imported to posts/metropolis/\ndiscretemcmc — notebook, imported to posts/discretemcmc/\nbayes_withsampling — already imported (Lecture 7), skipped\nintrogibbs — notebook, imported to posts/introgibbs/\nhierarch — already imported (Lecture 14), skipped\n\nLecture 16 (MCMC and Gibbs): DONE\n\nmarkov — already imported (Lecture 15), skipped\nmetropolis — already imported (Lecture 15), skipped\ndiscretemcmc — already imported (Lecture 15), skipped\nbayes_withsampling — already imported (Lecture 7), skipped\nintrogibbs — already imported (Lecture 15), skipped\nhierarch — already imported (Lecture 14), skipped\nmetropolishastings — notebook, imported to posts/metropolishastings/\nMetropolisSupport — notebook, imported to posts/metropolissupport/\nswitchpoint — notebook, imported to posts/switchpoint/\ngibbsfromMH — markdown, imported to posts/gibbsfromMH/\ngibbsconj — notebook, imported to posts/gibbsconj/\n\nLecture 17 (Data Augmentation, Gibbs, and HMC): DONE\n\ndataaug — notebook, imported to posts/dataaug/\ntetchygibbs — notebook, imported to posts/tetchygibbs/\nhmcidea — markdown, imported to posts/hmcidea/\nhmcexplore — notebook, imported to posts/hmcexplore/\n\nLectures 18–26: NOT YET IMPORTED"
  },
  {
    "objectID": "CLAUDE.html#post-date-scheme",
    "href": "CLAUDE.html#post-date-scheme",
    "title": "Claude Session Context",
    "section": "",
    "text": "Dates increase by 1 week per lecture, starting from 2025-01-08 (Lecture 1)\nAll notes within a lecture share the same date (the lecture’s date)\nIf a note was first imported in an earlier lecture, it keeps that earlier date\nLast date used: 2025-04-30 (Lecture 17)\nNext lecture (18) should use: 2025-05-07"
  },
  {
    "objectID": "CLAUDE.html#category-system",
    "href": "CLAUDE.html#category-system",
    "title": "Claude Session Context",
    "section": "",
    "text": "Canonical categories are in _categories.txt (root of project), one per line, sorted alphabetically\nCurrent categories: bayesian, classification, data, decision-theory, elections, hierarchical, information-theory, integration, interactive, macos, mcmc, models, montecarlo, neural-networks, optimization, orchestration, pipeline, probability, regression, sampling, statistics, visualization\nAll categories must be lowercase\nWhen importing, map source keywords to existing categories; propose new ones for user approval\nThe /import-wiki-notes skill enforces this workflow (step 7c)"
  },
  {
    "objectID": "CLAUDE.html#about-page",
    "href": "CLAUDE.html#about-page",
    "title": "Claude Session Context",
    "section": "",
    "text": "Profile photo: assets/profile.jpg (sourced from GitHub avatar)\nComprehensive bio context: ~/Context/rahul-dave-profile.md\nContact form via Formspree (ID: mykjwlyk) — forwards to rahuldave@univ.ai\nLinks: GitHub, X/Twitter, Bluesky (rahuldave.bsky.social), Google Scholar, LinkedIn"
  },
  {
    "objectID": "CLAUDE.html#social-discussion-links",
    "href": "CLAUDE.html#social-discussion-links",
    "title": "Claude Session Context",
    "section": "",
    "text": "includes/discuss-links.html — “Discuss this post” links (Twitter, Bluesky, LinkedIn) on posts, til, and collections pages only\nUses IIFE (not DOMContentLoaded) because include-after-body runs after DOM is ready\nLinks are constructed at runtime from window.location.href — will use production domain automatically\nOpen Graph and Twitter Card metadata enabled in _quarto.yml"
  },
  {
    "objectID": "CLAUDE.html#listing-sort-order",
    "href": "CLAUDE.html#listing-sort-order",
    "title": "Claude Session Context",
    "section": "",
    "text": "All listing pages sort by date desc: index.qmd, posts.qmd, til.qmd, collections.qmd"
  },
  {
    "objectID": "CLAUDE.html#design",
    "href": "CLAUDE.html#design",
    "title": "Claude Session Context",
    "section": "",
    "text": "designs/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — fork of depth, modernized with animations/glassmorphism\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nColorBrewer Blues palette (#eff3ff → #08306b)\nDark/light mode support\n\n\n\n--cb-blue-50: #eff3ff   --cb-blue-100: #c6dbef   --cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6  --cb-blue-400: #4292c6   --cb-blue-500: #2171b5\n--cb-blue-600: #08519c  --cb-blue-700: #084594   --cb-blue-800: #08306b"
  },
  {
    "objectID": "CLAUDE.html#quarto-site-structure",
    "href": "CLAUDE.html#quarto-site-structure",
    "title": "Claude Session Context",
    "section": "",
    "text": "_quarto.yml — project config: website type, navbar, SCSS theme (light/dark), TOC, margin references, Open Graph, Twitter Cards\nThemes: styles/modern-light.scss, styles/modern-dark.scss\nIncludes: includes/fonts.html, includes/brand-icon.html, includes/theme-toggle.html, includes/discuss-links.html\n\n\n\n\nNotebooks get their own folder with index.ipynb (or index.qmd for JS demos):\nposts/\n  probability/\n    index.ipynb        # URL becomes /posts/probability/\n    assets/            # images, data files referenced by THIS notebook only\n      venn.png\n      bishop-prob.png\n  votingforcongress/\n    index.ipynb\n    assets/\n      sep7.png\n  earth-demo/\n    index.qmd          # Three.js demo using .qmd format\n    assets/\n      earth-card.png   # Card thumbnail for listing (no content images in post)\nMarkdown posts stay as flat files, images go in shared posts/images/ or posts/data/:\nposts/\n  boxloop.md           # URL becomes /posts/boxloop\n  distributions.md\n  images/              # shared images for flat .md posts\n    2tosscdf.png\n  data/                # shared data for flat .md posts (if needed)\n\n\n\nNotebooks must have a raw cell (cell_type: “raw”) as the first cell with YAML:\n---\ntitle: \"Post Title\"\nsubtitle: \"Catchy one-liner for listing cards.\"\ndescription: \"Two-sentence summary for the post page.\"\ncategories:\n    - probability\n    - statistics\ndate: 2025-01-08\n---\n\nsubtitle appears on listing cards\ndescription is the longer summary\ncategories are used for filtering (must be from _categories.txt)\ndate controls sort order\nimage — optional, for posts without content images (e.g. image: assets/earth-card.png)\n\nFor markdown posts, the same YAML goes in the standard frontmatter block at the top.\n\n\n\nFor posts with no embedded images (e.g. interactive Three.js demos), use the browser agent to screenshot the rendered page, crop to the key visual, save to assets/, and add image: to frontmatter. See skill step 9 for details.\n\n\n\nUse .qmd format (not .ipynb) for JavaScript-heavy interactive content: - Load external JS via format: html: include-in-header: in frontmatter - Use Quarto’s fenced div syntax ::: {.classname} for layout containers - Inline &lt;script&gt; blocks at the end of the .qmd file - Read CSS custom properties (--cb-blue-*, --interactive-*) for theme integration - Example: posts/earth-demo/index.qmd\n\n\n\nmake preview                # Live dev server with hot reload\nmake render                 # Build full site to _site/\nmake build                  # Render + rsync _site/ to docs/ for GitHub Pages\nquarto render posts/probability/index.ipynb  # Render a single post\n\nrender produces _site/; build adds an rsync step to sync changed files into docs/\nCommit and push are intentionally separate from build\n\n\n\n\n\nNotebook posts: assets/filename.png (relative to the notebook’s folder)\nMarkdown posts: images/filename.png (relative to posts/)\nSite-wide assets already in /assets/ (e.g. lawoflargenumbers images) use /assets/... absolute paths\nProfile photo: assets/profile.jpg (JPEG, not PNG — watch for GitHub avatar format mismatch)\n\n\n\n\n\nAll categories must be lowercase — no Statistics, use statistics; no MonteCarlo, use montecarlo\nCanonical list in _categories.txt at project root\nThis prevents duplicate tags in Quarto listing filters\n\n\n\n\n\ntitle — post title\nsubtitle — catchy one-liner shown on listing cards\ndescription — 2-sentence content summary\ncategories — lowercase tags for filtering (from _categories.txt)\ndate — controls sort order (YYYY-MM-DD)\nimage — card thumbnail path (optional, for posts without content images)\n\n\n\n\n\nposts/ — main blog posts (shown on index page)\ntil/ — Today I Learned (shown only on TIL page, NOT on index)\ncollections/software/ — software tools (shown only on Collections page, NOT on index)\nindex.qmd listing contents should only include posts/ and posts/**/\n\n\n\n\n\nAfter adding/moving/renaming files, restart quarto preview — the live server caches resource IDs and will show “Bad resource ID” for changed files\nListing .qmd files must not have a stray --- after the YAML block (causes parse errors)\nListing contents paths should NOT have a leading / (use til/*.qmd not /til/*.qmd)\ninclude-after-body scripts run after DOMContentLoaded — use IIFE, not event listeners\nGitHub avatar downloads are JPEG even with .png URL — always check with file command and use correct extension"
  },
  {
    "objectID": "collections/software/prefect.html",
    "href": "collections/software/prefect.html",
    "title": "Prefect-2.0",
    "section": "",
    "text": "Prefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#why-choose-this-tool",
    "href": "collections/software/prefect.html#why-choose-this-tool",
    "title": "Prefect-2.0",
    "section": "",
    "text": "Prefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#more-about-the-tool",
    "href": "collections/software/prefect.html#more-about-the-tool",
    "title": "Prefect-2.0",
    "section": "More about the tool",
    "text": "More about the tool\nPrefect is organized around the notion of fllows. Flows can have subflows, and both of these can have tasks, but tasks cannot have sub-tasks. Flows have implementation as processes or as docker containers.\n\nflows can be run adhoc\nflows can be scheduled\nother DAG based software such as DVC pipelines, hamilton, and dbt can be run as prefect processes\nprefect does not seem to support event based activation of pipelines, although the ability to create deployments in python can enable us to create some such flow\nprefect is well integrated with dask, which we can then use for hyper-parameter optimizations on our cluster or other such distributed computations"
  },
  {
    "objectID": "collections/software/prefect.html#how-to-install",
    "href": "collections/software/prefect.html#how-to-install",
    "title": "Prefect-2.0",
    "section": "How to install",
    "text": "How to install\npip install -U prefect\nThe prefect orion UI will need proxying out of a cluster."
  },
  {
    "objectID": "collections/software/prefect.html#alternatives",
    "href": "collections/software/prefect.html#alternatives",
    "title": "Prefect-2.0",
    "section": "Alternatives",
    "text": "Alternatives\nSeveral alternatives exist. The old airflow and luigi are still around."
  },
  {
    "objectID": "collections/mysoft/deebase.html",
    "href": "collections/mysoft/deebase.html",
    "title": "DeeBase",
    "section": "",
    "text": "DeeBase is an async SQLAlchemy-based database library with a fastlite-inspired API that makes async database operations simple across multiple backends."
  },
  {
    "objectID": "collections/mysoft/deebase.html#example",
    "href": "collections/mysoft/deebase.html#example",
    "title": "DeeBase",
    "section": "Example",
    "text": "Example\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom deebase import Database, ForeignKey, Text\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    email: str\n    status: str = \"active\"\n\n@dataclass\nclass Post:\n    id: int\n    author_id: ForeignKey[int, \"user\"]\n    title: str\n    content: Text\n    created_at: datetime\n\ndb = Database(\"sqlite+aiosqlite:///blog.db\")\nusers = await db.create(User, pk='id')\nposts = await db.create(Post, pk='id')\n\nalice = await users.insert(User(id=None, name=\"Alice\", email=\"alice@example.com\"))\npost = await posts.insert(Post(\n    id=None, author_id=alice.id, title=\"Hello\",\n    content=\"First post!\", created_at=datetime.now()\n))\n\n# FK navigation\nauthor = await posts.fk.author_id(post)\nprint(author.name)  # \"Alice\"\n\n# Query\nall_posts = await posts()\npost = await posts[1]"
  },
  {
    "objectID": "collections/mysoft/deebase.html#features",
    "href": "collections/mysoft/deebase.html#features",
    "title": "DeeBase",
    "section": "Features",
    "text": "Features\n\nAsync/await first — built on SQLAlchemy 2.0+ with aiosqlite and asyncpg drivers\nErgonomic API — await users[1], await users(), await users.lookup(email=\"...\")\nType safety — optional @dataclass support with IDE autocomplete; or start with plain classes and call .dataclass() later\nRich type system — str, Text, int, float, bool, bytes, dict (JSON), datetime, date, time, Optional[T]\nForeign keys and defaults — ForeignKey[int, \"user\"] type annotation; SQL defaults from class field defaults\nFK navigation — await posts.fk.author_id(post) to fetch parent; await users.get_children(user, \"post\", \"author_id\") for children\nIndexes — simple, composite, unique, and named indexes via Index or create_index()\nFull-text search — BM25-ranked search via SQLite FTS5 and PostgreSQL pg_textsearch with automatic index sync\nViews — db.create_view() for JOINs and CTEs with full query API\nxtra() filtering — create scoped table views that auto-apply filters on all operations including inserts\nTransactions — atomic multi-operation commits with rollback\nDatabase reflection — await db.reflect() to work with existing databases; access tables via db.t.tablename\nCode generation — create_mod_from_tables() exports schemas as Python dataclasses\n6 exception types — NotFoundError, IntegrityError, ValidationError, SchemaError, ConnectionError, InvalidOperationError\nCLI — deebase init, deebase table create, deebase migrate, deebase sql, deebase data, and more\nMigrations — schema migrations with up/down support and version tracking\nFastAPI integration — create_crud_router() auto-generates CRUD endpoints with Swagger docs and FK validation\nAdmin interface — Django-like admin UI at /admin/ via deebase api serve --admin\nValidation layer — shared validation for CLI, admin, and API"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Why Do We Have Seasons?\n\n\nEarth’s 23.5° tilt is the secret — not distance from the Sun.\n\n\n\nvisualization\n\ninteractive\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\n\n\n\n\nExploring Hamiltonian Monte Carlo\n\n\nFrom typical sets to leapfrog integration: a detailed exploration of HMC and NUTS.\n\n\n\nmcmc\n\nsampling\n\noptimization\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Tetchy Gibbs Sampler\n\n\nGibbs sampling on a bimodal, highly-correlated posterior with lots of autocorrelation.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nData Augmentation\n\n\nIterative sampling via hidden variables using the Tanner-Wong algorithm and Gibbs.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Idea of Hamiltonian Monte Carlo\n\n\nUsing momentum and energy conservation to glide efficiently through the typical set.\n\n\n\nmcmc\n\nsampling\n\noptimization\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nImputation and Convergence\n\n\nA coal-mine disaster switchpoint model for convergence testing and posterior predictive checks.\n\n\n\nmcmc\n\nbayesian\n\nmodels\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMetropolis and Support Mismatch\n\n\nWhy combining rejection with Metropolis sampling wastes efficiency.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nGibbs Sampling with Conjugate Conditionals\n\n\nExploiting conjugate prior pairs for efficient Gibbs sampling.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nGibbs from Metropolis-Hastings\n\n\nProving that Gibbs sampling is a special case of MH with acceptance probability one.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\n\nSampling from complex distributions with asymmetric proposals and limited support.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom Annealing to Metropolis\n\n\nUnderstanding detailed balance and the Metropolis algorithm for sampling probability distributions.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chains and MCMC\n\n\nStationarity, reversibility, and the mathematical foundation for Metropolis sampling.\n\n\n\nmcmc\n\nsampling\n\nprobability\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nDiscrete MCMC\n\n\nMetropolis sampling from discrete distributions using proposal matrices.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Gibbs Sampling\n\n\nMCMC via alternating conditional sampling from a complex joint distribution.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Bayesian Models\n\n\nFrom partial pooling to shrinkage estimation with the rat tumors example.\n\n\n\nbayesian\n\nhierarchical\n\nsampling\n\n\n\n\nApr 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe EM Algorithm\n\n\nInferring parameters from incomplete data through iterative expectation and maximization.\n\n\n\nstatistics\n\nmodels\n\noptimization\n\n\n\n\nApr 2, 2025\n\n\n\n\n\n\n\n\n\n\n\nMixture Models and Types of Learning\n\n\nFrom generative classifiers to clustering: supervised, unsupervised, and semi-supervised approaches.\n\n\n\nclassification\n\nmodels\n\nstatistics\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nUtility, Risk, and Decision Theory\n\n\nFrom utility functions to Bayes actions: applying decision theory to regression and model comparison.\n\n\n\ndecision-theory\n\nbayesian\n\nmodels\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nGenerative vs Discriminative Models\n\n\nUnderstanding discriminative boundaries vs. generative class models through logistic regression and LDA.\n\n\n\nclassification\n\nmodels\n\nbayesian\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nHow Sigmoids Combine\n\n\nVisualizing neural network function approximation through hidden layer activations.\n\n\n\nneural-networks\n\noptimization\n\nvisualization\n\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nMulti-Layer Perceptron for Classification\n\n\nTraining neural networks to classify synthetic data with decision boundaries.\n\n\n\nneural-networks\n\noptimization\n\nmodels\n\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nRegression in PyTorch\n\n\nBuilding and training multi-layer perceptrons as universal function approximators.\n\n\n\nneural-networks\n\noptimization\n\nregression\n\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression and Backpropagation\n\n\nFrom MLE classification to computing gradients through layers.\n\n\n\noptimization\n\nmodels\n\nstatistics\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent and SGD\n\n\nOptimization by following the slope downhill.\n\n\n\noptimization\n\nregression\n\nmodels\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom the Normal Model to Regression\n\n\nBuilding a Bayesian regression from Kung San census data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\nsampling\n\n\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Regression\n\n\nPutting priors on regression coefficients and updating with data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\n\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Normal Model\n\n\nConjugate priors for the workhorse of statistics.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Inverse Transform\n\n\nTurning uniform random numbers into any distribution you want.\n\n\n\nsampling\n\nprobability\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\nSample where it matters most to compute integrals efficiently.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nLab: The Beta-Binomial Globe Model\n\n\nGrid approximation, quadratic approximation, and MCMC in practice.\n\n\n\nbayesian\n\nsampling\n\nmodels\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nRejection Sampling\n\n\nAccept or reject: a simple algorithm for hard distributions.\n\n\n\nsampling\n\nmontecarlo\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Beta-Binomial Globe Model\n\n\nConjugate priors, Bayesian updating, and decision theory on a globe toss.\n\n\n\nbayesian\n\nprobability\n\nmodels\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nSufficient Statistics and Exchangeability\n\n\nWhen less data tells you everything, and order doesn’t matter.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nTreating parameters as random variables changes everything.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\nsampling\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nEntropy and Maximum Entropy\n\n\nQuantifying uncertainty and the distributions it favors.\n\n\n\ninformation-theory\n\nprobability\n\nstatistics\n\n\n\n\nFeb 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nConvexity and Jensen’s Inequality\n\n\nWhy the average of a function isn’t the function of the average.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nDivergence and Deviance\n\n\nMeasuring how far your model is from the truth.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding AIC\n\n\nAn information-theoretic shortcut for model selection.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Significance and Size of Effects\n\n\nWhen a drug works, how much does it matter?\n\n\n\nstatistics\n\nregression\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning With Noise\n\n\nBias, variance, and the tradeoff that haunts every model.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\nTaming complexity by penalizing parameters.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Bounds and the Test Set\n\n\nHow to honestly evaluate what your model has learned.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Without Noise\n\n\nWhat happens when you fit a model to perfect data.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nValidation and Cross-Validation\n\n\nWhy one split is never enough.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Statistics\n\n\nFixed parameters, random data — the frequentist creed.\n\n\n\nstatistics\n\nprobability\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nFind the parameters that make your data most probable.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Integration\n\n\nWhen calculus is hard, sample instead.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\nprobability\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nBasic Monte Carlo\n\n\nLet randomness do the heavy lifting.\n\n\n\nmontecarlo\n\nprobability\n\nsampling\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nExpectations and the Law of Large Numbers\n\n\nWhat you expect is what you get — eventually.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nSampling and the Central Limit Theorem\n\n\nWhy everything looks normal in the limit.\n\n\n\nprobability\n\nstatistics\n\nmontecarlo\n\nsampling\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions Example: Elections\n\n\nSimulating a presidential election with coin flips.\n\n\n\nprobability\n\nstatistics\n\nelections\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\nThe shapes that randomness takes.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nFrom coin flips to Bayes’ theorem.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\nBuild, compute, critique, repeat.\n\n\n\nmodels\n\nprobability\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\nDoes the president’s party always lose seats in congress?\n\n\n\nelections\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\nFlip enough coins and the truth emerges.\n\n\n\nstatistics\n\nmontecarlo\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\nDon’t make your audience think.\n\n\n\nvisualization\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "collections.html",
    "href": "collections.html",
    "title": "Collections",
    "section": "",
    "text": "Software I’ve Written\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\nDeeBase\n\n\n \n\n\n\n\n\n\nJan 15, 2025\n\n\nHooksett\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\nSoftware I Like\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\nStitchfix Hamilton\n\n\n \n\n\n\n\n\n\nDec 3, 2023\n\n\nAwk\n\n\n \n\n\n\n\n\n\nDec 3, 2022\n\n\nPrefect-2.0\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nRahul’s Lair\n",
    "section": "",
    "text": "Data, Stats, ML and AI\n\n\n\nA collection of notebooks, tutorials, and deep dives into statistical inference, machine learning, AI, and the art of communicating with data."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "\nRahul’s Lair\n",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nWhy Do We Have Seasons?\n\n\nEarth’s 23.5° tilt is the secret — not distance from the Sun.\n\n\n\nvisualization\n\ninteractive\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\n\n\n\n\nExploring Hamiltonian Monte Carlo\n\n\nFrom typical sets to leapfrog integration: a detailed exploration of HMC and NUTS.\n\n\n\nmcmc\n\nsampling\n\noptimization\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Tetchy Gibbs Sampler\n\n\nGibbs sampling on a bimodal, highly-correlated posterior with lots of autocorrelation.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nData Augmentation\n\n\nIterative sampling via hidden variables using the Tanner-Wong algorithm and Gibbs.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Idea of Hamiltonian Monte Carlo\n\n\nUsing momentum and energy conservation to glide efficiently through the typical set.\n\n\n\nmcmc\n\nsampling\n\noptimization\n\n\n\n\nApr 30, 2025\n\n\n\n\n\n\n\n\n\n\n\nImputation and Convergence\n\n\nA coal-mine disaster switchpoint model for convergence testing and posterior predictive checks.\n\n\n\nmcmc\n\nbayesian\n\nmodels\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nMetropolis and Support Mismatch\n\n\nWhy combining rejection with Metropolis sampling wastes efficiency.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nGibbs Sampling with Conjugate Conditionals\n\n\nExploiting conjugate prior pairs for efficient Gibbs sampling.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nGibbs from Metropolis-Hastings\n\n\nProving that Gibbs sampling is a special case of MH with acceptance probability one.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Metropolis-Hastings Algorithm\n\n\nSampling from complex distributions with asymmetric proposals and limited support.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom Annealing to Metropolis\n\n\nUnderstanding detailed balance and the Metropolis algorithm for sampling probability distributions.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nMarkov Chains and MCMC\n\n\nStationarity, reversibility, and the mathematical foundation for Metropolis sampling.\n\n\n\nmcmc\n\nsampling\n\nprobability\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nDiscrete MCMC\n\n\nMetropolis sampling from discrete distributions using proposal matrices.\n\n\n\nmcmc\n\nsampling\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Gibbs Sampling\n\n\nMCMC via alternating conditional sampling from a complex joint distribution.\n\n\n\nmcmc\n\nsampling\n\nbayesian\n\n\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\nHierarchical Bayesian Models\n\n\nFrom partial pooling to shrinkage estimation with the rat tumors example.\n\n\n\nbayesian\n\nhierarchical\n\nsampling\n\n\n\n\nApr 9, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe EM Algorithm\n\n\nInferring parameters from incomplete data through iterative expectation and maximization.\n\n\n\nstatistics\n\nmodels\n\noptimization\n\n\n\n\nApr 2, 2025\n\n\n\n\n\n\n\n\n\n\n\nMixture Models and Types of Learning\n\n\nFrom generative classifiers to clustering: supervised, unsupervised, and semi-supervised approaches.\n\n\n\nclassification\n\nmodels\n\nstatistics\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nUtility, Risk, and Decision Theory\n\n\nFrom utility functions to Bayes actions: applying decision theory to regression and model comparison.\n\n\n\ndecision-theory\n\nbayesian\n\nmodels\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nGenerative vs Discriminative Models\n\n\nUnderstanding discriminative boundaries vs. generative class models through logistic regression and LDA.\n\n\n\nclassification\n\nmodels\n\nbayesian\n\n\n\n\nMar 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nHow Sigmoids Combine\n\n\nVisualizing neural network function approximation through hidden layer activations.\n\n\n\nneural-networks\n\noptimization\n\nvisualization\n\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nMulti-Layer Perceptron for Classification\n\n\nTraining neural networks to classify synthetic data with decision boundaries.\n\n\n\nneural-networks\n\noptimization\n\nmodels\n\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nRegression in PyTorch\n\n\nBuilding and training multi-layer perceptrons as universal function approximators.\n\n\n\nneural-networks\n\noptimization\n\nregression\n\n\n\n\nMar 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression and Backpropagation\n\n\nFrom MLE classification to computing gradients through layers.\n\n\n\noptimization\n\nmodels\n\nstatistics\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nGradient Descent and SGD\n\n\nOptimization by following the slope downhill.\n\n\n\noptimization\n\nregression\n\nmodels\n\n\n\n\nMar 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrom the Normal Model to Regression\n\n\nBuilding a Bayesian regression from Kung San census data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\nsampling\n\n\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Regression\n\n\nPutting priors on regression coefficients and updating with data.\n\n\n\nbayesian\n\nregression\n\nmodels\n\n\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Normal Model\n\n\nConjugate priors for the workhorse of statistics.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Inverse Transform\n\n\nTurning uniform random numbers into any distribution you want.\n\n\n\nsampling\n\nprobability\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nImportance Sampling\n\n\nSample where it matters most to compute integrals efficiently.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nLab: The Beta-Binomial Globe Model\n\n\nGrid approximation, quadratic approximation, and MCMC in practice.\n\n\n\nbayesian\n\nsampling\n\nmodels\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nRejection Sampling\n\n\nAccept or reject: a simple algorithm for hard distributions.\n\n\n\nsampling\n\nmontecarlo\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Beta-Binomial Globe Model\n\n\nConjugate priors, Bayesian updating, and decision theory on a globe toss.\n\n\n\nbayesian\n\nprobability\n\nmodels\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nSufficient Statistics and Exchangeability\n\n\nWhen less data tells you everything, and order doesn’t matter.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nTreating parameters as random variables changes everything.\n\n\n\nbayesian\n\nprobability\n\nstatistics\n\nsampling\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nEntropy and Maximum Entropy\n\n\nQuantifying uncertainty and the distributions it favors.\n\n\n\ninformation-theory\n\nprobability\n\nstatistics\n\n\n\n\nFeb 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nConvexity and Jensen’s Inequality\n\n\nWhy the average of a function isn’t the function of the average.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nDivergence and Deviance\n\n\nMeasuring how far your model is from the truth.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding AIC\n\n\nAn information-theoretic shortcut for model selection.\n\n\n\ninformation-theory\n\nstatistics\n\nmodels\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Significance and Size of Effects\n\n\nWhen a drug works, how much does it matter?\n\n\n\nstatistics\n\nregression\n\n\n\n\nFeb 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning With Noise\n\n\nBias, variance, and the tradeoff that haunts every model.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nRegularization\n\n\nTaming complexity by penalizing parameters.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Bounds and the Test Set\n\n\nHow to honestly evaluate what your model has learned.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nLearning Without Noise\n\n\nWhat happens when you fit a model to perfect data.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nValidation and Cross-Validation\n\n\nWhy one split is never enough.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Statistics\n\n\nFixed parameters, random data — the frequentist creed.\n\n\n\nstatistics\n\nprobability\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nFind the parameters that make your data most probable.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Integration\n\n\nWhen calculus is hard, sample instead.\n\n\n\nsampling\n\nmontecarlo\n\nintegration\n\nprobability\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nBasic Monte Carlo\n\n\nLet randomness do the heavy lifting.\n\n\n\nmontecarlo\n\nprobability\n\nsampling\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nExpectations and the Law of Large Numbers\n\n\nWhat you expect is what you get — eventually.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nSampling and the Central Limit Theorem\n\n\nWhy everything looks normal in the limit.\n\n\n\nprobability\n\nstatistics\n\nmontecarlo\n\nsampling\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions Example: Elections\n\n\nSimulating a presidential election with coin flips.\n\n\n\nprobability\n\nstatistics\n\nelections\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\nThe shapes that randomness takes.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nFrom coin flips to Bayes’ theorem.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\nBuild, compute, critique, repeat.\n\n\n\nmodels\n\nprobability\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\nDoes the president’s party always lose seats in congress?\n\n\n\nelections\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\nFlip enough coins and the truth emerges.\n\n\n\nstatistics\n\nmontecarlo\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\nDon’t make your audience think.\n\n\n\nvisualization\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/metropolishastings/index.html",
    "href": "posts/metropolishastings/index.html",
    "title": "The Metropolis-Hastings Algorithm",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_context('paper')\n\n\n\nIf you want to sample a distribution with limited or only positive support, you realize that a normal distribution as a proposal will still want to sample stuff in negative areas and outside the max support. So we wastefully make acceptance probability comparisons.\nOut intuition may be to reject samples outside the support. But as we shall show later, this makes our proposal asymmetric and we need to deal with this. This is because stepping to a negative number and coming back are not symmetric: one is rejected.\nIf we do it without taking into account this asymmetry, we will actually be sampling from a different distribution as we shall show later.\nHowever, we may also want to sample from a asymmetric proposal like a beta function because its guaranteed to be positive. However a beta distribution is not symmetric.\nThus Metropolis Hastings allows us to sample distributions that are defined on limited support.\nHere is the outline code for metropolis hastings.\n\ndef metropolis_hastings(p,q, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)\n        if np.random.uniform() &lt; min(1, pdfratio*proposalratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted +=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\nCompare it with the code for metropolis\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        #print(x_star, pdfratio)\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted +=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\n\nThe transition matrix (or kernel) can be written in this form for Metropolis-Hastings is also:\n\\[T(x_i \\vert x_{i-1}) = q(x_i \\vert x_{i-1})\\,A(x_i, x_{i-1}) +  \\delta(x_{i-1} - x_i)r(x_{i-1})\\]\nwhere now the acceptance probability has changed to:\n\\[A(x_i, x_{i-1}) = min(1,  \\frac{s(x_i) \\times q(x_{i-1} \\vert x_i)}{s(x_{i-1}) \\times q(x_i \\vert x_{i-1})}).\\]\nEverything else remains the same with the rejection term still:\n\\[r(x_i) = \\int dy q(y \\vert x_i)(1 - A(y, x_i)) = 1 - \\int dy q(y \\vert x_i) A(y, x_i).\\]\nNow, as in the Metropolis case, consider:\n\\[s(x_i)T( x_{i-1} \\vert x_i ) =  s(x_i) q(x_{i-1} \\vert x_{i})\\,A(x_{i-1}, x_{i}) +  s(x_i) \\delta(x_{i} - x_{i-1})r(x_{i})\\]\nand\n\\[s(x_{i-1})T( x_{i} \\vert x_{i-1} ) =  s(x_{i-1}) q(x_i \\vert x_{i-1})\\,A(x_i, x_{i-1}) +  s(x_{i-1})\\delta(x_{i-1} - x_i)r(x_{i-1})\\]\nThe second terms cancel.\nThe comparison of the first two terms is just a bit harder than in the Metropolis case, but the cases are the same. The first two terms are trivially equal when A=1 because the proposal-ration must be 1. If \\(s(x_i) \\times q(x_{i-1} \\vert x_i) &lt;  s(x_{i-1}) \\times q(x_i \\vert x_{i-1})\\) then \\(A(x_i, x_{i-1}) &lt; 1\\) and \\(A(x_{i-1}, x_{i}) = 1\\).\nThen we are comparing:\n\\[\\frac{s(x_i) \\times q(x_{i-1} \\vert x_i)}{s(x_{i-1}) \\times q(x_i \\vert x_{i-1})} \\times  s(x_{i-1}) q(x_i \\vert x_{i-1})\\]\nto \\(s(x_i) q(x_{i-1} \\vert x_{i})\\) which are again trivially equal.\nThe intuition behind this, is to correct the sampling of \\(q\\) to match \\(p\\). It corrects for any asymmetries in the proposal distribution. If the proposal prefers left over right, then we weigh the rightward moves more.\nIn the case of distributions with limited support (only positives, for example) one must be careful choosing a proposal. A good rule of thumb is that the proposal has the same or larger support then the target, with the same support being the best.\n\n\n\n\nLet us sample from \\(f(x)=0.554xe^{-(x/1.9)^2}\\) which is a weibull distribution used in response time analysis.\n\nf = lambda x: 0.554*x*np.exp(-(x/1.9)**2)\n\n\nxxx= np.linspace(0,10,100)\nplt.plot(xxx, f(xxx), 'r');\n\n\n\n\n\n\n\n\nA rule of thumb for choosing proposal distributions is to parametrize them in terms of their mean and variance or precision since that provides a notion of “centeredness” which we can use for our proposals \\(x_{i-1}\\). We then fix the variance to understand how widely we are sampling from.\nAs a proposal we shall use a Gamma Distribution with parametrization \\[Gamma(x\\tau, 1/\\tau)\\] in the shape-scale parametrization. The mean of this Gamma then is \\(shape*scale\\) so that x is the mean. \\(\\tau\\) is a precision parameter\n\nfrom scipy.stats import gamma\nt=10\ndef gammapdf(x_new, x_old):\n    return gamma.pdf(x_new,x_old*t,scale=1/t)\ndef gammadraw(x_old):\n    return gamma.rvs(x_old*t,scale=1/t)\n        \n\n\nx0=np.random.uniform()\nsamps,acc = metropolis_hastings(f, gammapdf, gammadraw, 100000, x0)\n\n\nacc\n\n83244\n\n\n\n# plot our sample histogram\nplt.hist(samps,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) \nsomesamps=samps[0::20000]\nfor i,s in enumerate(somesamps):\n    xs=np.linspace(s-3, s+3, 100)\n    plt.plot(xs, gamma.pdf(xs,s*t,scale=1/t),'k', lw=1)\nxx= np.linspace(0,10,100)\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend()\nplt.xlim([0,10])\nplt.show()\nprint(\"starting point was \", x0)\n\n\n\n\n\n\n\n\nstarting point was  0.9847232294704866\n\n\nRemember how we said that it takes some time for the Markov chain to reach a stationary state? We should remove the samples until that point. We can see the progress of the samples using a trace-plot\n\nplt.plot(samps, alpha=0.3);\n\n\n\n\n\n\n\n\n\nplt.plot(samps, alpha=0.3);\nplt.xlim([0, 5000])\n\n\n\n\n\n\n\n\nThe first many samples will contain signs of the initial condition, and will also reflect the fact that you have not found the stationary distribution as yet. We will see tests later for this, but as a rule of thumb, you always want to eliminate the first 5-10% of your samples. The appearance of white noise is a good sign.\n\nplt.plot(samps, alpha=0.3);\nplt.xlim([20000, 25000])\n\n\n\n\n\n\n\n\nNotice that strong autocorrelations persist at high sample number too. This is the nature of metropolis and MH samplers..your next move is clearly highly correlated with the previous one.\nThis is not a disaster in itself, but many people will thin their samples to have samples uncorrelated. Here I am leaving out a burnin of 20000, and am thinning to every 10.\n\nplt.plot(samps[20000::10], alpha=0.3);\n\n\n\n\n\n\n\n\nHere is the last 8000 samples for comparison where you can clearly see correlations. We shall talk more about these considerations later.\n\nplt.plot(samps[-8000:], alpha=0.3);\n\n\n\n\n\n\n\n\n\n\nWe could do the same thing with a normal, and using metropolis\n\ndef prop(x):\n    return np.random.normal(x, 0.6)\nsamps_m, acc_m = metropolis(f, prop, 100000, x0)\n\n\nacc_m\n\n79092\n\n\n\nplt.hist(samps_m,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) \nsomesamps=samps_m[0::20000]\nfor i,s in enumerate(somesamps):\n    xs=np.linspace(s-3, s+3, 100)\n    plt.plot(xs, norm.pdf(xs,s,0.6),'k', lw=1)\nxx= np.linspace(0,10,100)\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend()\nplt.xlim([-5,10])\nplt.show()\nprint(\"starting point was \", x0)\n\n\n\n\n\n\n\n\nstarting point was  0.9847232294704866\n\n\nI havent tuned either to get up the acceptance rates. It is a game you ought to play. A good proposal should have the acceptance rate neither too high nor too low. The former means the proposal is likely too narrow, while the latter means the proposal is likely too wide.\n\n\n\n\nTo show how we can use Metropolis Hastings to sample a discrete distribution, let us go back to our rainy sunny example from Markov Chains.\nThere, given a transition matrix, we found a corresponding stationary distribution for it. Here we weant to fix the stationary distribution, and somehow let metropolis-hastings find the appropriate transition matrix to get us there.\n\n# the transition matrix for our chain\ntransition_matrix = np.array([[0.3, 0.7],[0.5, 0.5]])\n\nprint(\"The transition matrix\")\nprint(transition_matrix)\n\nprint(\"Stationary distribution\")\nprint(np.linalg.matrix_power(transition_matrix,10))\n\nThe transition matrix\n[[ 0.3  0.7]\n [ 0.5  0.5]]\nStationary distribution\n[[ 0.41666673  0.58333327]\n [ 0.41666662  0.58333338]]\n\n\nTo sample from this, we need our final stationary pdf, which is a Bernoulli with p=0.416667. This is thus a kind of fake problem because knowing the pdf for a bernoulli, we can sample from it directly by using a uniform. But thats not our point here..\n\ndef rainsunpmf(state_int):\n    p = 0.416667\n    if state_int==0:\n        return p\n    else:#anything else is treated as a 1\n        return 1 - p\n\nNow, any move that transitions us from state 0 to state 1 and back would work. In particular, we could use a symmetric transition matrix and then use plain metropolis.\nYou might find it strange that we are calling the proposal a transition matrix. This is strictly an abuse of terminology. We just need anything that will get us a set of probablities that sum to 1 and is symettric!\n\nt_sym = np.array([[0.1, 0.9],[0.9, 0.1]])\ndef rainsunprop(sint_old):\n    return np.random.choice(2,p=t_sym[sint_old])\n\n\nsamps_dis, acc_dis = metropolis(rainsunpmf, rainsunprop, 1000, 1)\n\n\nacc_dis\n\n848\n\n\n\nplt.hist(samps_dis);\n\n\n\n\n\n\n\n\nIf I wish to use metropolis hastings I can choose an asymmetric proposal and I must provide a proposal pdf.\n\nt_asym = np.array([[0.1, 0.9],[0.3, 0.7]])\ndef rainsunprop2(sint_old):\n    return np.random.choice(2,p=t_asym[sint_old])\n\n\nt_asym\n\narray([[ 0.1,  0.9],\n       [ 0.3,  0.7]])\n\n\n\nt_asym[0][1]\n\n0.90000000000000002\n\n\n\ndef rainsunpropfunc(sint_new, sint_old):\n    return t_asym[sint_old][sint_new]\n\n\nsamps_dis2, acc_dis2 = metropolis_hastings(rainsunpmf, rainsunpropfunc, rainsunprop2, 1000, 1)\n\n\nacc_dis2\n\n797\n\n\n\nplt.hist(samps_dis2);"
  },
  {
    "objectID": "posts/metropolishastings/index.html#metropolis-hastings-1",
    "href": "posts/metropolishastings/index.html#metropolis-hastings-1",
    "title": "The Metropolis-Hastings Algorithm",
    "section": "",
    "text": "If you want to sample a distribution with limited or only positive support, you realize that a normal distribution as a proposal will still want to sample stuff in negative areas and outside the max support. So we wastefully make acceptance probability comparisons.\nOut intuition may be to reject samples outside the support. But as we shall show later, this makes our proposal asymmetric and we need to deal with this. This is because stepping to a negative number and coming back are not symmetric: one is rejected.\nIf we do it without taking into account this asymmetry, we will actually be sampling from a different distribution as we shall show later.\nHowever, we may also want to sample from a asymmetric proposal like a beta function because its guaranteed to be positive. However a beta distribution is not symmetric.\nThus Metropolis Hastings allows us to sample distributions that are defined on limited support.\nHere is the outline code for metropolis hastings.\n\ndef metropolis_hastings(p,q, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)\n        if np.random.uniform() &lt; min(1, pdfratio*proposalratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted +=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\nCompare it with the code for metropolis\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        #print(x_star, pdfratio)\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted +=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\n\nThe transition matrix (or kernel) can be written in this form for Metropolis-Hastings is also:\n\\[T(x_i \\vert x_{i-1}) = q(x_i \\vert x_{i-1})\\,A(x_i, x_{i-1}) +  \\delta(x_{i-1} - x_i)r(x_{i-1})\\]\nwhere now the acceptance probability has changed to:\n\\[A(x_i, x_{i-1}) = min(1,  \\frac{s(x_i) \\times q(x_{i-1} \\vert x_i)}{s(x_{i-1}) \\times q(x_i \\vert x_{i-1})}).\\]\nEverything else remains the same with the rejection term still:\n\\[r(x_i) = \\int dy q(y \\vert x_i)(1 - A(y, x_i)) = 1 - \\int dy q(y \\vert x_i) A(y, x_i).\\]\nNow, as in the Metropolis case, consider:\n\\[s(x_i)T( x_{i-1} \\vert x_i ) =  s(x_i) q(x_{i-1} \\vert x_{i})\\,A(x_{i-1}, x_{i}) +  s(x_i) \\delta(x_{i} - x_{i-1})r(x_{i})\\]\nand\n\\[s(x_{i-1})T( x_{i} \\vert x_{i-1} ) =  s(x_{i-1}) q(x_i \\vert x_{i-1})\\,A(x_i, x_{i-1}) +  s(x_{i-1})\\delta(x_{i-1} - x_i)r(x_{i-1})\\]\nThe second terms cancel.\nThe comparison of the first two terms is just a bit harder than in the Metropolis case, but the cases are the same. The first two terms are trivially equal when A=1 because the proposal-ration must be 1. If \\(s(x_i) \\times q(x_{i-1} \\vert x_i) &lt;  s(x_{i-1}) \\times q(x_i \\vert x_{i-1})\\) then \\(A(x_i, x_{i-1}) &lt; 1\\) and \\(A(x_{i-1}, x_{i}) = 1\\).\nThen we are comparing:\n\\[\\frac{s(x_i) \\times q(x_{i-1} \\vert x_i)}{s(x_{i-1}) \\times q(x_i \\vert x_{i-1})} \\times  s(x_{i-1}) q(x_i \\vert x_{i-1})\\]\nto \\(s(x_i) q(x_{i-1} \\vert x_{i})\\) which are again trivially equal.\nThe intuition behind this, is to correct the sampling of \\(q\\) to match \\(p\\). It corrects for any asymmetries in the proposal distribution. If the proposal prefers left over right, then we weigh the rightward moves more.\nIn the case of distributions with limited support (only positives, for example) one must be careful choosing a proposal. A good rule of thumb is that the proposal has the same or larger support then the target, with the same support being the best."
  },
  {
    "objectID": "posts/metropolishastings/index.html#mh-example",
    "href": "posts/metropolishastings/index.html#mh-example",
    "title": "The Metropolis-Hastings Algorithm",
    "section": "",
    "text": "Let us sample from \\(f(x)=0.554xe^{-(x/1.9)^2}\\) which is a weibull distribution used in response time analysis.\n\nf = lambda x: 0.554*x*np.exp(-(x/1.9)**2)\n\n\nxxx= np.linspace(0,10,100)\nplt.plot(xxx, f(xxx), 'r');\n\n\n\n\n\n\n\n\nA rule of thumb for choosing proposal distributions is to parametrize them in terms of their mean and variance or precision since that provides a notion of “centeredness” which we can use for our proposals \\(x_{i-1}\\). We then fix the variance to understand how widely we are sampling from.\nAs a proposal we shall use a Gamma Distribution with parametrization \\[Gamma(x\\tau, 1/\\tau)\\] in the shape-scale parametrization. The mean of this Gamma then is \\(shape*scale\\) so that x is the mean. \\(\\tau\\) is a precision parameter\n\nfrom scipy.stats import gamma\nt=10\ndef gammapdf(x_new, x_old):\n    return gamma.pdf(x_new,x_old*t,scale=1/t)\ndef gammadraw(x_old):\n    return gamma.rvs(x_old*t,scale=1/t)\n        \n\n\nx0=np.random.uniform()\nsamps,acc = metropolis_hastings(f, gammapdf, gammadraw, 100000, x0)\n\n\nacc\n\n83244\n\n\n\n# plot our sample histogram\nplt.hist(samps,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) \nsomesamps=samps[0::20000]\nfor i,s in enumerate(somesamps):\n    xs=np.linspace(s-3, s+3, 100)\n    plt.plot(xs, gamma.pdf(xs,s*t,scale=1/t),'k', lw=1)\nxx= np.linspace(0,10,100)\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend()\nplt.xlim([0,10])\nplt.show()\nprint(\"starting point was \", x0)\n\n\n\n\n\n\n\n\nstarting point was  0.9847232294704866\n\n\nRemember how we said that it takes some time for the Markov chain to reach a stationary state? We should remove the samples until that point. We can see the progress of the samples using a trace-plot\n\nplt.plot(samps, alpha=0.3);\n\n\n\n\n\n\n\n\n\nplt.plot(samps, alpha=0.3);\nplt.xlim([0, 5000])\n\n\n\n\n\n\n\n\nThe first many samples will contain signs of the initial condition, and will also reflect the fact that you have not found the stationary distribution as yet. We will see tests later for this, but as a rule of thumb, you always want to eliminate the first 5-10% of your samples. The appearance of white noise is a good sign.\n\nplt.plot(samps, alpha=0.3);\nplt.xlim([20000, 25000])\n\n\n\n\n\n\n\n\nNotice that strong autocorrelations persist at high sample number too. This is the nature of metropolis and MH samplers..your next move is clearly highly correlated with the previous one.\nThis is not a disaster in itself, but many people will thin their samples to have samples uncorrelated. Here I am leaving out a burnin of 20000, and am thinning to every 10.\n\nplt.plot(samps[20000::10], alpha=0.3);\n\n\n\n\n\n\n\n\nHere is the last 8000 samples for comparison where you can clearly see correlations. We shall talk more about these considerations later.\n\nplt.plot(samps[-8000:], alpha=0.3);\n\n\n\n\n\n\n\n\n\n\nWe could do the same thing with a normal, and using metropolis\n\ndef prop(x):\n    return np.random.normal(x, 0.6)\nsamps_m, acc_m = metropolis(f, prop, 100000, x0)\n\n\nacc_m\n\n79092\n\n\n\nplt.hist(samps_m,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) \nsomesamps=samps_m[0::20000]\nfor i,s in enumerate(somesamps):\n    xs=np.linspace(s-3, s+3, 100)\n    plt.plot(xs, norm.pdf(xs,s,0.6),'k', lw=1)\nxx= np.linspace(0,10,100)\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend()\nplt.xlim([-5,10])\nplt.show()\nprint(\"starting point was \", x0)\n\n\n\n\n\n\n\n\nstarting point was  0.9847232294704866\n\n\nI havent tuned either to get up the acceptance rates. It is a game you ought to play. A good proposal should have the acceptance rate neither too high nor too low. The former means the proposal is likely too narrow, while the latter means the proposal is likely too wide."
  },
  {
    "objectID": "posts/metropolishastings/index.html#rainy-sunny-reprise",
    "href": "posts/metropolishastings/index.html#rainy-sunny-reprise",
    "title": "The Metropolis-Hastings Algorithm",
    "section": "",
    "text": "To show how we can use Metropolis Hastings to sample a discrete distribution, let us go back to our rainy sunny example from Markov Chains.\nThere, given a transition matrix, we found a corresponding stationary distribution for it. Here we weant to fix the stationary distribution, and somehow let metropolis-hastings find the appropriate transition matrix to get us there.\n\n# the transition matrix for our chain\ntransition_matrix = np.array([[0.3, 0.7],[0.5, 0.5]])\n\nprint(\"The transition matrix\")\nprint(transition_matrix)\n\nprint(\"Stationary distribution\")\nprint(np.linalg.matrix_power(transition_matrix,10))\n\nThe transition matrix\n[[ 0.3  0.7]\n [ 0.5  0.5]]\nStationary distribution\n[[ 0.41666673  0.58333327]\n [ 0.41666662  0.58333338]]\n\n\nTo sample from this, we need our final stationary pdf, which is a Bernoulli with p=0.416667. This is thus a kind of fake problem because knowing the pdf for a bernoulli, we can sample from it directly by using a uniform. But thats not our point here..\n\ndef rainsunpmf(state_int):\n    p = 0.416667\n    if state_int==0:\n        return p\n    else:#anything else is treated as a 1\n        return 1 - p\n\nNow, any move that transitions us from state 0 to state 1 and back would work. In particular, we could use a symmetric transition matrix and then use plain metropolis.\nYou might find it strange that we are calling the proposal a transition matrix. This is strictly an abuse of terminology. We just need anything that will get us a set of probablities that sum to 1 and is symettric!\n\nt_sym = np.array([[0.1, 0.9],[0.9, 0.1]])\ndef rainsunprop(sint_old):\n    return np.random.choice(2,p=t_sym[sint_old])\n\n\nsamps_dis, acc_dis = metropolis(rainsunpmf, rainsunprop, 1000, 1)\n\n\nacc_dis\n\n848\n\n\n\nplt.hist(samps_dis);\n\n\n\n\n\n\n\n\nIf I wish to use metropolis hastings I can choose an asymmetric proposal and I must provide a proposal pdf.\n\nt_asym = np.array([[0.1, 0.9],[0.3, 0.7]])\ndef rainsunprop2(sint_old):\n    return np.random.choice(2,p=t_asym[sint_old])\n\n\nt_asym\n\narray([[ 0.1,  0.9],\n       [ 0.3,  0.7]])\n\n\n\nt_asym[0][1]\n\n0.90000000000000002\n\n\n\ndef rainsunpropfunc(sint_new, sint_old):\n    return t_asym[sint_old][sint_new]\n\n\nsamps_dis2, acc_dis2 = metropolis_hastings(rainsunpmf, rainsunpropfunc, rainsunprop2, 1000, 1)\n\n\nacc_dis2\n\n797\n\n\n\nplt.hist(samps_dis2);"
  },
  {
    "objectID": "posts/rejectionsampling/index.html",
    "href": "posts/rejectionsampling/index.html",
    "title": "Rejection Sampling",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(\"Setup Finished\")\n\nSetup Finished"
  },
  {
    "objectID": "posts/rejectionsampling/index.html#basic-rejection-sampling",
    "href": "posts/rejectionsampling/index.html#basic-rejection-sampling",
    "title": "Rejection Sampling",
    "section": "Basic Rejection Sampling",
    "text": "Basic Rejection Sampling\nThe basic idea, come up with by von Neumann is:\nIf you have a function you are trying to sample from, whose functional form is well known, basically accept the sample by generating a uniform random number at any \\(x\\) and accepting it if the value is below the value of the function at that \\(x\\).\nThis is illustrated in the diagram below:\n\n\n\nBasic rejection sampling: points drawn uniformly in the bounding box are accepted if they fall under the target density curve, rejected otherwise.\n\n\n\nThe process\n\nDraw \\(x\\) uniformly from \\([x_{min},\\, x_{max}]\\)\nDraw \\(y\\) uniformly from [0,\\(y_{max}\\)]\nif \\(y\\) &lt; f(\\(x\\)), accept the sample\notherwise reject it\nrepeat\n\nThis works as more samples will be accepted in the regions of \\(x\\)-space where the function \\(f\\) is higher: indeed they will be accepted in the ratio of the height of the function at any given \\(x\\) to \\(y_{max}\\).\nThe reason this all works is the frequentist interpretation of probability in each \\(x\\) sliver As we have more samples the accept-to-total ratio reflects the probablity mass in that sliver better.\n\n\nExample\nThe following code produces samples that follow the distribution \\(P(x)=e^{-x}\\) for \\(x=[0,10]\\) and generates a histogram of the sampled distribution.\n\n\nP = lambda x: np.exp(-x)\n\n# domain limits\nxmin = 0 # the lower limit of our domain\nxmax = 10 # the upper limit of our domain\n\n# range limit (supremum) for y\nymax = 1\n#you might have to do an optimization to find this.\n\nN = 10000 # the total of samples we wish to generate\naccepted = 0 # the number of accepted samples\nsamples = np.zeros(N)\ncount = 0 # the total count of proposals\n\n# generation loop\nwhile (accepted &lt; N):\n    \n    # pick a uniform number on [xmin, xmax) (e.g. 0...10)\n    x = np.random.uniform(xmin, xmax)\n    \n    # pick a uniform number on [0, ymax)\n    y = np.random.uniform(0,ymax)\n    \n    # Do the accept/reject comparison\n    if y &lt; P(x):\n        samples[accepted] = x\n        accepted += 1\n    \n    count +=1\n    \nprint(\"Count\",count, \"Accepted\", accepted)\n\n# get the histogram info\nhinfo = np.histogram(samples,30)\n\n# plot the histogram\nplt.hist(samples,bins=30, label=u'Samples');\n\n# plot our (normalized) function\nxvals=np.linspace(xmin, xmax, 1000)\nplt.plot(xvals, hinfo[0][0]*P(xvals), 'r', label=u'P(x)')\n\n# turn on the legend\nplt.legend();\n\nCount 99359 Accepted 10000\n\n\n\n\n\n\n\n\n\nNotice that \\(y_{max}\\) was just assumed here. In general we might have to do a maximization. This has a cost. We want to keep this cost low, or we might be spending some time there. If the optimization is complex, it might be cheaper to do something else…"
  },
  {
    "objectID": "posts/rejectionsampling/index.html#rejection-sampling-with-steroids",
    "href": "posts/rejectionsampling/index.html#rejection-sampling-with-steroids",
    "title": "Rejection Sampling",
    "section": "Rejection Sampling with Steroids",
    "text": "Rejection Sampling with Steroids\nThe simple rejection sampling method has fundamental problems.\nFor our simple example, it’s quite easy to determine the supremum. In practice, while you may know how to quickly (i.e. constant time) evaluate your function everywhere on the domain of interest, finding a bound very close to the supremum may not be a feasible calculation. In addition, even if you find a tight bound for the supremum, basic rejection sampling will still be very inefficient as you will reject many samples (especially in low density regions).\nFurthermore, if you support is infinitely large, you are not going to be able to reject from an infinitely long box in finite time!\nThis is a hard problem to solve and we will need other techniques to address this problem of low acceptance probability.\nHowever, it is possible to do a more efficient job while still taking advantage of the simplicity of rejection sampling. Our modified technique will introduce a proposal density \\(g(x)\\). This notion of a proposal density is one used in many sampling techniques, in different ways, but its importance will always lie in figuring ways to increase the acceptance rate.\nThe proposal density will have the following characteristics:\n\n\\(g(x)\\) is easy to sample from and (calculate the pdf)\nSome \\(M\\) between 1 and \\(\\infty\\) exists so that \\(M \\, g(x) &gt; f(x)\\) in your entire domain of interest\nideally \\(g(x)\\) will be somewhat close to \\(f\\) so that you’ll sample more in high density regions and much less in low density regions\n\nIts probably obvious that an optimal value for M is the supremum over your domain of interest of \\(f/g\\). At that \\(x\\) you will accept stuff with probability 1. In general you want \\(M\\) as close to 1 as possible, since the probability of acceptance is \\(1/M\\).\nYou can see that this is the case by finding the proportion of samples from \\(g(x)\\) that are accepted at each \\(x\\) and then averaging over \\(x\\):\n\\[\\int dx g(x) prop(x) =  \\int dx g(x) \\frac{f(x)}{Mg(x)} = \\frac{1}{M}\\int dx f(x) = \\frac{1}{M}\\]\nOnce you’ve picked a proposal distribution g, your modified rejection sampling technique is as follows:\n\nDraw \\(x\\) from your proposal distribution \\(g(x)\\)\nDraw \\(y\\) uniformly from [0,1]\nif \\(y\\) &lt; f(\\(x\\))/\\(M g(x)\\), accept the sample\notherwise reject it\nrepeat\n\nThe entire process is illustrated in the diagram below:\n\n\n\nRejection sampling with an envelope: using a scaled proposal distribution M·g(x) that bounds f(x) improves acceptance rates over a uniform bounding box.\n\n\nExample\nThe following code produces samples that follow the distribution \\(P(x)=e^{-x}\\) for \\(x=[0,10]\\) and generates a histogram of the sampled distribution.\n\n\np = lambda x: np.exp(-x)  # our distribution\ng = lambda x: 1/(x+1)  # our proposal pdf (we're thus choosing M to be 1)\ninvCDFg = lambda x: np.log(x +1) # generates our proposal using inverse sampling\n\n# domain limits\nxmin = 0 # the lower limit of our domain\nxmax = 10 # the upper limit of our domain\n\n# range limits for inverse sampling\numin = invCDFg(xmin)\numax = invCDFg(xmax)\n\nN = 10000 # the total of samples we wish to generate\naccepted = 0 # the number of accepted samples\nsamples = np.zeros(N)\ncount = 0 # the total count of proposals\n\n# generation loop\nwhile (accepted &lt; N):\n    \n    # Sample from g using inverse sampling\n    u = np.random.uniform(umin, umax)\n    xproposal = np.exp(u) - 1\n    \n    # pick a uniform number on [0, 1)\n    y = np.random.uniform(0,1)\n    \n    # Do the accept/reject comparison\n    if y &lt; p(xproposal)/g(xproposal):\n        samples[accepted] = xproposal\n        accepted += 1\n    \n    count +=1\n    \nprint(\"Count\", count, \"Accepted\", accepted)\n\n# get the histogram info\nhinfo = np.histogram(samples,50)\n\n# plot the histogram\nplt.hist(samples,bins=50, label=u'Samples');\n\n# plot our (normalized) function\nxvals=np.linspace(xmin, xmax, 1000)\nplt.plot(xvals, hinfo[0][0]*p(xvals), 'r', label=u'p(x)')\nplt.plot(xvals, hinfo[0][0]*g(xvals), 'k', label=u'g(x)')\n\n\n\n# turn on the legend\nplt.legend();\n\nCount 23692 Accepted 10000"
  },
  {
    "objectID": "posts/validation/index.html",
    "href": "posts/validation/index.html",
    "title": "Validation and Cross-Validation",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/validation/index.html#revisiting-the-model",
    "href": "posts/validation/index.html#revisiting-the-model",
    "title": "Validation and Cross-Validation",
    "section": "Revisiting the model",
    "text": "Revisiting the model\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\n\ndffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\n\nx=dffull.rfrac.values\nf=dffull.promney.values\n\n\ndf = pd.read_csv(\"data/noisysample.csv\")\ndf.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\nfrom sklearn.cross_validation import train_test_split\ndatasize=df.shape[0]\n#split dataset using the index, as we have x,f, and y that we want to split.\nitrain,itest = train_test_split(range(30),train_size=24, test_size=6)\nxtrain= df.x[itrain].values\nftrain = df.f[itrain].values\nytrain = df.y[itrain].values\nxtest= df.x[itest].values\nftest = df.f[itest].values\nytest = df.y[itest].values"
  },
  {
    "objectID": "posts/validation/index.html#validation",
    "href": "posts/validation/index.html#validation",
    "title": "Validation and Cross-Validation",
    "section": "Validation",
    "text": "Validation\nA separate validation set is needed because what we have done in picking a given polynomial degree \\(d\\) as the best hypothesis is that we have used the test set as a training set. How?\nOur process used the training set to fit for the parameters(values of the coefficients) of the polynomial of given degree \\(d\\) based on minimizing the traing set error (empirical risk minimization). We then calculated the error on the test set at that \\(d\\). If we go further and choose the best \\(d\\) based on minimizing the test set error, we have then “fit for” \\(d\\) on the test set. We will thus call \\(d\\) a hyperparameter of the model.\nIn this case, the test-set error will underestimate the true out-of-sample error. Furthermore, we have contaminated the test set by fitting for \\(d\\) on it; it is no longer a true test set.\nThus, we must introduce a new validation set on which the complexity parameter \\(d\\) is fit, and leave out a test set which we can use to estimate the true out-of-sample performance of our learner. The place of this set in the scheme of things is shown below:\n\n\n\nSplitting dataset D into training, validation, and test sets\n\n\nWe have split the old training set into a training set and a validation set, holding the old test aside for FINAL testing AFTER we have “fit” for complexity \\(d\\). Obviously we have decreased the size of the data available for training further, but this is a price we must pay for obtaining a good estimate of the out-of-sample risk \\(\\cal{E_{out}}\\) (also denoted as risk \\(R_{out}\\)) through the test risk \\(\\cal{E_{test}}\\) (\\(R_{test}\\)).\n\n\n\nValidation workflow: train, validate, choose hypothesis, retrain, test\n\n\nThe validation process is illustrated in these two figures. We first loop over all the hypothesis sets that we wish to consider: in our case this is a loop over the complexity parameter \\(d\\), the degree of the polynomials we will try and fit. Then for each degree \\(d\\), we obtain a best fit model \\(g^-_d\\) where the “minus” superscript indicates that we fit our model on the new training set which is obtained by removing (“minusing”) a validation chunk (often the same size as the test chunk) from the old training set. We then “test” this model on the validation chunk, obtaining the validation error for the best-fit polynomial coefficients and for degree \\(d\\). We move on to the next degree \\(d\\) and repeat the process, just like before. We compare all the validation set errors, just like we did with the test errors earlier, and pick the degree \\(d_*\\) which minimizes this validation set error.\n\n\n\nLooping over hypothesis sets with validation to select the best model\n\n\nHaving picked the hyperparameter \\(d_\\*\\), we retrain using the hypothesis set \\(\\cal{H_\\*}\\) on the entire old training-set to find the parameters of the polynomial of order \\(d_\\*\\) and the corresponding best fit hypothesis \\(g_\\*\\). Note that we left the minus off the \\(g\\) to indicate that it was trained on the entire old traing set. We now compute the test error on the test set as an estimate of the test risk \\(\\cal{E_{test}}\\).\nThus the validation set is the set on which the hyperparameter is fit. This method of splitting the data \\(\\cal{D}\\) is called the train-validate-test split.\n\nProperties of the validation set\nFirst assume that the validation set is acting like a test set. then, for the same reasons as in the case of a test set, the validation risk or error is an unbiased estimate of the out of sample risk. Secondly, the Hoeffding bound for a validation set is then for the same reason identical to that of the test set.\nMore often though the validation set is used in a model selection process. Here we wish to choose the complexity parameter \\(d\\), something we wrongly already attempted to do on our previous test set.\nNotice that the process of validation consists of fixing \\(d\\) and finding the best fit \\(g^\\*\\) on the training set. We then calculate as many risks as our parameter grid on the validation set with the different fit hypothesis, and choose the \\(d, g^\\*\\) combination with the lowest validation set risk. Now, \\(R_{val}(g^{-\\*}, d^\\*)\\) also has an optimistic bias, and its Hoeffding bound must now take into account the grid-size as the effecting size of the hypothesis space. This size from hyperparameters is typically a smaller size than that from parameters.\nWe finally now retrain on the entire train+validation set using the appropriate \\((g^{-\\*}, d^\\*)\\) combination. This works as training a given model with more data typically reduces the risk even further. (One can show this using learning curves but thats out of our scope).\n\n\nWorking it out\nWe carry out this process for one training/validation split below. Note the smaller size of the new training set. We hold the test set at the same size.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\ndef make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n\n\n#we split the training set down further\nintrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\nxntrain= df.x[intrain].values\nfntrain = df.f[intrain].values\nyntrain = df.y[intrain].values\nxnvalid= df.x[invalid].values\nfnvalid = df.f[invalid].values\nynvalid = df.y[invalid].values\n\ndegrees=range(21)\nerror_train=np.empty(len(degrees))\nerror_valid=np.empty(len(degrees))\ntrainvalidlists=make_features(xntrain, xnvalid, degrees)\n\n#we now train on the smaller training set\nfor d in degrees:#for increasing polynomial degrees 0,1,2...\n    #Create polynomials from x\n    Xntrain = trainvalidlists[d]['train']\n    Xnvalid = trainvalidlists[d]['test']\n    #fit a model linear in polynomial coefficients on the new smaller training set\n    est = LinearRegression()\n    est.fit(Xntrain, yntrain)\n    #predict on new training and validation sets and calculate mean squared error\n    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n\n#calculate the degree at which validation error is minimized\nmindeg = np.argmin(error_valid)\n#need to remake polynomial features on the whole training set\nttlist=make_features(xtrain, xtest, degrees)\nfeatures_at_mindeg = ttlist[mindeg]['train']\ntest_features_at_mindeg = ttlist[mindeg]['test']\n#fit on whole training set now. Put MSE in variable err.\n#your code here\nclf = LinearRegression()\nclf.fit(features_at_mindeg, ytrain) # fit\n#predict on the test set now and calculate error\npred = clf.predict(test_features_at_mindeg)\nerr = mean_squared_error(ytest, pred)\n\n\nplt.plot(degrees, error_train, marker='o', label='train (in-sample)')\nplt.plot(degrees, error_valid, marker='o', label='validation')\nplt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper left')\nplt.yscale(\"log\")\nprint(mindeg)\n\n2\n\n\n\n\n\n\n\n\n\nLets do this again, choosing a new random split between training and validation data:\n\nintrain,invalid = train_test_split(itrain,train_size=18, test_size=6)\nxntrain= df.x[intrain].values\nfntrain = df.f[intrain].values\nyntrain = df.y[intrain].values\nxnvalid= df.x[invalid].values\nfnvalid = df.f[invalid].values\nynvalid = df.y[invalid].values\n\ndegrees=range(21)\nerror_train=np.empty(len(degrees))\nerror_valid=np.empty(len(degrees))\ntrainvalidlists=make_features(xntrain, xnvalid, degrees)\n\nfor d in degrees:#for increasing polynomial degrees 0,1,2...\n    #Create polynomials from x\n    Xntrain = trainvalidlists[d]['train']\n    Xnvalid = trainvalidlists[d]['test']\n    #fit a model linear in polynomial coefficients on the training set\n    est = LinearRegression()\n    est.fit(Xntrain, yntrain)\n    #calculate mean squared error\n    error_train[d] = mean_squared_error(yntrain, est.predict(Xntrain))\n    error_valid[d] = mean_squared_error(ynvalid, est.predict(Xnvalid))\n\nmindeg = np.argmin(error_valid)\nttlist=make_features(xtrain, xtest, degrees)\nfeatures_at_mindeg = ttlist[mindeg]['train']\ntest_features_at_mindeg = ttlist[mindeg]['test']\nclf = LinearRegression()\nclf.fit(features_at_mindeg, ytrain) # fit\npred = clf.predict(test_features_at_mindeg)\nerr = mean_squared_error(ytest, pred)\n\n\nplt.plot(degrees, error_train, marker='o', label='train (in-sample)')\nplt.plot(degrees, error_valid, marker='o', label='validation')\nplt.plot([mindeg], [err], marker='s', markersize=10, label='test', alpha=0.5, color='r')\n\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='lower left')\nplt.yscale(\"log\")\nprint(mindeg)\n\n2\n\n\n\n\n\n\n\n\n\nThis time the validation error minimizing polynomial degree might change! What happened?"
  },
  {
    "objectID": "posts/validation/index.html#cross-validation",
    "href": "posts/validation/index.html#cross-validation",
    "title": "Validation and Cross-Validation",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nThe problem\n\nSince we are dealing with small data sizes here, you should worry that a given split exposes us to the peculiarity of the data set that got randomly chosen for us. This naturally leads us to want to choose multiple such random splits and somehow average over this process to find the “best” validation minimizing polynomial degree or complexity \\(d\\).\nThe multiple splits process also allows us to get an estimate of how consistent our prediction error is: in other words, just like in the hair example, it gives us a distribution. So far we have been channeling the hair through the bootstrap, but choosing multiple splits is another way to get different training samples..\nFurthermore the validation set that we left out has two competing demands on it. The larger the set is, the better is our estimate of the out-of-sample error. So we’d like to hold out as much as possible. But the smaller the validation set is, the more data we have to train ourmodel on. Thus we can fit a better, more expressive model. We want to balance these two desires, and additionally, not be exposed to any peculiarities that might randomly arise in any single train-validate split of the old training set.\n\n\n\nThe Idea\nTo deal with this we engage in a process called cross-validation, which is illustrated in the figure below, for a given hypothesis set \\(\\cal{H}_a\\) with complexity parameter \\(d=a\\) (the polynomial degree). We do the train/validate split, not once but multiple times.\nIn the figure below we create 4-folds from the training set part of our data set \\(\\cal{D}\\). By this we mean that we divide our set roughly into 4 equal parts. As illustrated below, this can be done in 4 different ways, or folds. In each fold we train a model on 3 of the parts. The model so trained is denoted as \\(g^-_{Fi}\\), for example \\(g^-_{F3}\\) . The minus sign in the superscript once again indicates that we are training on a reduced set. The \\(F3\\) indicates that this model was trained on the third fold. Note that the model trained on each fold will be different!\nFor each fold, after training the model, we calculate the risk or error on the remaining one validation part. We then add the validation errors together from the different folds, and divide by the number of folds to calculate an average error. Note again that this average error is an average over different models \\(g^-_{Fi}\\). We use this error as the validation error for \\(d=a\\) in the validation process described earlier.\n\n\n\nK-fold cross-validation: rotating the validation fold across the dataset\n\n\nNote that the number of folds is equal to the number of splits in the data. For example, if we have 5 splits, there will be 5 folds. To illustrate cross-validation consider below fits in \\(\\cal{H}_0\\) and \\(\\cal{H}_1\\) (means and straight lines) to a sine curve, with only 3 data points.\nWe have described cross-validation here from the perspective of sensibly fitting for the complexity hyperparameter \\(d\\). But we can use it just like a pure validation set as well, just making sure we arent getting strange results due to a wierdly sampled validation set. In that case, (it can also shown that) cross-validation error is an unbiased estimate of the out of sample-error.\nNotice that just like the bootstraps we do in frequentist inference, cross-validation is a re-sampling method. Indeed, a question might be, why not use bootstrap instead. See http://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio , and note that the so-called “out-of-bag” errors from “bagging” in random forests utilizes the bootstrap.\n\n\nThe entire description of K-fold Cross-validation\nWe put thogether this scheme to calculate the error for a given polynomial degree \\(d\\) with the method we used earlier to choose a model given the validation-set risk as a function of \\(d\\):\n\ncreate n_folds partitions of the training data.\nWe then train on n_folds -1 of these partitions, and test on the remaining partition. There are n_folds such combinations of partitions (or folds), and thus we obtain n_fold risks.\nWe average the error or risk of all such combinations to obtain, for each value of \\(d\\), \\(R_{dCV}\\).\nWe move on to the next value of \\(d\\), and repeat 3\nand then find the optimal value of d that minimizes risk \\(d=*\\).\nWe finally use that value to make the final fit in \\(\\cal{H}_*\\) on the entire old training set.\n\n\n\n\nCross-validation over multiple hypothesis sets, then retrain and test\n\n\nLet us now do 4-fold cross-validation on our Romney votes data set. We increase the complexity from degree 0 to degree 20. In each case we take the old training set, split in 4 ways into 4 folds, train on 3 folds, and calculate the validation error on the ramining one. We then average the erros over the four folds to get a cross-validation error for that \\(d\\). Then we did what we did before: find the hypothesis space \\(\\cal{H_*}\\) with the lowest cross-validation error, and refit it using the entire training set. We can then use the test set to estimate \\(E_{out}\\).\n\nfrom sklearn.cross_validation import KFold\nn_folds=4\ndegrees=range(21)\nresults=[]\nfor d in degrees:\n    hypothesisresults=[]\n    for train, test in KFold(24, n_folds): # split data into train/test groups, 4 times\n        tvlist=make_features(xtrain[train], xtrain[test], degrees)\n        clf = LinearRegression()\n        clf.fit(tvlist[d]['train'], ytrain[train]) # fit\n        hypothesisresults.append(mean_squared_error(ytrain[test], clf.predict(tvlist[d]['test']))) # evaluate score function on held-out data\n    results.append((np.mean(hypothesisresults), np.min(hypothesisresults), np.max(hypothesisresults), np.std(hypothesisresults))) # average\n\n\nmindeg = np.argmin([r[0] for r in results])\nttlist=make_features(xtrain, xtest, degrees)\n#fit on whole training set now.\nclf = LinearRegression()\nclf.fit(ttlist[mindeg]['train'], ytrain) # fit\npred = clf.predict(ttlist[mindeg]['test'])\nerr = mean_squared_error(pred, ytest)\nerrtr=mean_squared_error(ytrain, clf.predict(ttlist[mindeg]['train']))\nerrout=0.8*errtr+0.2*err\nc0=sns.color_palette()[0]\nc1=sns.color_palette()[1]\n#plt.errorbar(degrees, [r[0] for r in results], yerr=[r[1] for r in results], marker='o', label='CV error', alpha=0.5)\nplt.plot(degrees, [r[0] for r in results], marker='o', label='CV error', alpha=0.9)\nplt.fill_between(degrees, [r[1] for r in results], [r[2] for r in results], color=c0, alpha=0.2)\n\n\nplt.plot([mindeg], [err], 'o',  label='test set error')\nplt.plot([mindeg], [errout], 'o',  label='full sample error')\n\n\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper right')\nplt.yscale(\"log\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\nWe see that the cross-validation error minimizes at a low degree, and then increases. Because we have so few data points the spread in fold errors increases as well.\nSo now we have an average out of sample error, matched to the in-sample error, and error bars telling is that the entire order 1-8 polynomial region (roughly) is trustable…\n\n\nWhat does Cross Validation do?\nOne can think about the validation process as one that estimates \\(R_{out}\\) directly, on the validation set. It’s critical use is in the model selection process. Once you do that you can estimate \\(R_{out}\\) using the test set as usual, but now you have also got the benefit of a robust average and error bars.\nOne key subtlety to remember about cross-validation is that in the risk averaging process, you are actually averaging over different \\(g^-\\) models, with different parameters. You arrive at the least risk for the hyperparameter and then refit on the entire training set, which will likely give you slightly different parameters as well."
  },
  {
    "objectID": "posts/functorch/index.html",
    "href": "posts/functorch/index.html",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "A perceptron is simply a set-of-units with a construction reminiscent of logistic regression. It consists of an input, followed by a linear combination, and then a squeezing through a non-linearity such as a sigmoid, a tanh, or a RELU.\n\n\n\nA single perceptron: inputs pass through a linear combination followed by a non-linearity to produce the output.\n\n\nA multi-layer perceptron can be used to approximate any function. The Universal Approximation theorem states that any continuous function with finite support can be approximated by at-least a one hidden layer based perceptron.\nThis is not a free lunch. The number of units required in this layer may be very high, and it might be hard for SGD to actually find the “correct” combination.\n\n\n\nWe generate noisy data from a fairly complex function (in 1-D) to demonstrate:\n\nimport numpy as np\n#np.random.seed(99)\nf = lambda x: 0.2 + 0.4*x**2 + 0.3*x*np.sin(15*x) + 0.05*np.cos(20*x)\nxgrid = np.linspace(0.,1., 640)\nfgrid = f(xgrid)\nygrid = fgrid + 0.1*np.random.normal(size=640)\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(xgrid, fgrid, lw=2)\nplt.plot(xgrid, ygrid, '.')\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as fn\n\n\nfrom torch.autograd import Variable\n\nxdata = Variable(torch.Tensor(xgrid))\nydata = Variable(torch.Tensor(ygrid))\n\n\n\n\nHere is a general model class to fit an architecture of the style shown below:\n\n\n\nA multi-layer perceptron with input layer, hidden layer, and output layer. Each node applies a linear transformation followed by a non-linearity, with bias terms at each layer.\n\n\nThe basic structure is this: there is an input into a linear layer, which is then squeezed through a non-linearity. 0 or more hidden layers follow (we want atleast 1 hidden layer for universal approximation). At each of these layers, for each unit, we take all the output from the previous nonlinearity, linear-combine it with all the other non-linear outputs from the previous layer, and squeeze what we get through another non-linearity. Finally, we combine all these non-liner outputs using a linear unit into a y value. (we’d use a linear+sigmoid or linear+softmax for categorical outputs or classification).\nThe class below makes the structure explicit. Notice the use of nn.ModuleList. This is a pytorch peculiarity.\n\nclass MLRegP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, nonlinearity = fn.tanh, additional_hidden_wide=0):\n        super(MLRegP, self).__init__()\n        self.fc_initial = nn.Linear(input_dim, hidden_dim)\n        self.fc_mid = nn.ModuleList()\n        self.additional_hidden_wide = additional_hidden_wide\n        for i in range(self.additional_hidden_wide):\n            self.fc_mid.append(nn.Linear(hidden_dim, hidden_dim))\n        self.fc_final = nn.Linear(hidden_dim, 1)\n        self.nonlinearity = nonlinearity\n\n    def forward(self, x):\n        x = self.fc_initial(x)\n        x = self.nonlinearity(x)\n        for i in range(self.additional_hidden_wide):\n            x = self.fc_mid[i](x)\n            x = self.nonlinearity(x)\n        x = self.fc_final(x)\n        return x\n\n\n\n\nWe choose 1 hidden layer with 40 units. We print out the model to see what we get. The graph is built up by pytorch when forward is hit for the first time (thats how we can get away putting the nonlinearities there). Then when we backprop the gradients are transferred properly.\n\nmodel = MLRegP(1, 80, nonlinearity=fn.relu, additional_hidden_wide=0)\ncriterion = nn.MSELoss()\n\n\nprint(model)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=80)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=80, out_features=1)\n)\n\n\n\nimport torch.utils.data\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(xgrid.reshape(-1,1)), torch.from_numpy(ygrid))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n\n\nlr, epochs, batch_size = 1e-1 , 2000 , 64\noptimizer = torch.optim.SGD(model.parameters(), lr = lr )\naccum=[]\nfor k in range(epochs):\n    localaccum = []\n    for localx, localy in iter(loader):\n        localx = Variable(localx.float())\n        localy = Variable(localy.float())\n        output = model.forward(localx)\n        loss = criterion(output, localy)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        localaccum.append(loss.data[0])\n    accum.append(np.mean(localaccum))\nplt.plot(accum);                      \n\n\n\n\n\n\n\n\n\nfinaloutput = model.forward(xdata.view(-1,1))\n\n\nplt.plot(xgrid, fgrid, '.', alpha=0.1)\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nplt.plot(xgrid, finaloutput.data.numpy(), lw=2)\n\n\n\n\n\n\n\n\nWe see that RELU does a decent job. Because of the nature of RELU, the resulting function has sharp edges. Note that even though the universal approximation theorem says that we can approximate any function, stochastic noise means that the function the network thinks we are approximating need not be the function we want to approximate..\n\n\n\nWe get somewhat better results with the tanh nonlinearity, if we go with 2 layers. Play with the number of hidden layers and number of units per layer to see if you can do better!\n\nmodel2 = MLRegP(1, 40, nonlinearity=fn.tanh, additional_hidden_wide=1)\nprint(model2)\ncriterion = nn.MSELoss()\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(xgrid.reshape(-1,1)), torch.from_numpy(ygrid))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\nlr, epochs, batch_size = 0.1 , 4000 , 64\noptimizer = torch.optim.SGD(model2.parameters(), lr = lr )\naccum=[]\nfor k in range(epochs):\n    localaccum = []\n    for localx, localy in iter(loader):\n        localx = Variable(localx.float())\n        localy = Variable(localy.float())\n        output = model2.forward(localx)\n        loss = criterion(output, localy)\n        model2.zero_grad()\n        loss.backward()\n        optimizer.step()\n        localaccum.append(loss.data[0])\n    accum.append(np.mean(localaccum))\nplt.plot(accum);                      \n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=40)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=40, out_features=40)\n  )\n  (fc_final): Linear(in_features=40, out_features=1)\n)\n\n\n\n\n\n\n\n\n\n\nfinaloutput = model2.forward(xdata.view(-1,1))\nplt.plot(xgrid, fgrid, '.', alpha=0.1)\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nplt.plot(xgrid, finaloutput.data.numpy(), lw=2)"
  },
  {
    "objectID": "posts/functorch/index.html#what-is-a-perceptron",
    "href": "posts/functorch/index.html#what-is-a-perceptron",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "A perceptron is simply a set-of-units with a construction reminiscent of logistic regression. It consists of an input, followed by a linear combination, and then a squeezing through a non-linearity such as a sigmoid, a tanh, or a RELU.\n\n\n\nA single perceptron: inputs pass through a linear combination followed by a non-linearity to produce the output.\n\n\nA multi-layer perceptron can be used to approximate any function. The Universal Approximation theorem states that any continuous function with finite support can be approximated by at-least a one hidden layer based perceptron.\nThis is not a free lunch. The number of units required in this layer may be very high, and it might be hard for SGD to actually find the “correct” combination."
  },
  {
    "objectID": "posts/functorch/index.html#generate-data",
    "href": "posts/functorch/index.html#generate-data",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "We generate noisy data from a fairly complex function (in 1-D) to demonstrate:\n\nimport numpy as np\n#np.random.seed(99)\nf = lambda x: 0.2 + 0.4*x**2 + 0.3*x*np.sin(15*x) + 0.05*np.cos(20*x)\nxgrid = np.linspace(0.,1., 640)\nfgrid = f(xgrid)\nygrid = fgrid + 0.1*np.random.normal(size=640)\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(xgrid, fgrid, lw=2)\nplt.plot(xgrid, ygrid, '.')"
  },
  {
    "objectID": "posts/functorch/index.html#fitting-in-torch",
    "href": "posts/functorch/index.html#fitting-in-torch",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nfrom torch.nn import functional as fn\n\n\nfrom torch.autograd import Variable\n\nxdata = Variable(torch.Tensor(xgrid))\nydata = Variable(torch.Tensor(ygrid))"
  },
  {
    "objectID": "posts/functorch/index.html#the-model",
    "href": "posts/functorch/index.html#the-model",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "Here is a general model class to fit an architecture of the style shown below:\n\n\n\nA multi-layer perceptron with input layer, hidden layer, and output layer. Each node applies a linear transformation followed by a non-linearity, with bias terms at each layer.\n\n\nThe basic structure is this: there is an input into a linear layer, which is then squeezed through a non-linearity. 0 or more hidden layers follow (we want atleast 1 hidden layer for universal approximation). At each of these layers, for each unit, we take all the output from the previous nonlinearity, linear-combine it with all the other non-linear outputs from the previous layer, and squeeze what we get through another non-linearity. Finally, we combine all these non-liner outputs using a linear unit into a y value. (we’d use a linear+sigmoid or linear+softmax for categorical outputs or classification).\nThe class below makes the structure explicit. Notice the use of nn.ModuleList. This is a pytorch peculiarity.\n\nclass MLRegP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, nonlinearity = fn.tanh, additional_hidden_wide=0):\n        super(MLRegP, self).__init__()\n        self.fc_initial = nn.Linear(input_dim, hidden_dim)\n        self.fc_mid = nn.ModuleList()\n        self.additional_hidden_wide = additional_hidden_wide\n        for i in range(self.additional_hidden_wide):\n            self.fc_mid.append(nn.Linear(hidden_dim, hidden_dim))\n        self.fc_final = nn.Linear(hidden_dim, 1)\n        self.nonlinearity = nonlinearity\n\n    def forward(self, x):\n        x = self.fc_initial(x)\n        x = self.nonlinearity(x)\n        for i in range(self.additional_hidden_wide):\n            x = self.fc_mid[i](x)\n            x = self.nonlinearity(x)\n        x = self.fc_final(x)\n        return x"
  },
  {
    "objectID": "posts/functorch/index.html#relu-example",
    "href": "posts/functorch/index.html#relu-example",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "We choose 1 hidden layer with 40 units. We print out the model to see what we get. The graph is built up by pytorch when forward is hit for the first time (thats how we can get away putting the nonlinearities there). Then when we backprop the gradients are transferred properly.\n\nmodel = MLRegP(1, 80, nonlinearity=fn.relu, additional_hidden_wide=0)\ncriterion = nn.MSELoss()\n\n\nprint(model)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=80)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=80, out_features=1)\n)\n\n\n\nimport torch.utils.data\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(xgrid.reshape(-1,1)), torch.from_numpy(ygrid))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n\n\nlr, epochs, batch_size = 1e-1 , 2000 , 64\noptimizer = torch.optim.SGD(model.parameters(), lr = lr )\naccum=[]\nfor k in range(epochs):\n    localaccum = []\n    for localx, localy in iter(loader):\n        localx = Variable(localx.float())\n        localy = Variable(localy.float())\n        output = model.forward(localx)\n        loss = criterion(output, localy)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        localaccum.append(loss.data[0])\n    accum.append(np.mean(localaccum))\nplt.plot(accum);                      \n\n\n\n\n\n\n\n\n\nfinaloutput = model.forward(xdata.view(-1,1))\n\n\nplt.plot(xgrid, fgrid, '.', alpha=0.1)\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nplt.plot(xgrid, finaloutput.data.numpy(), lw=2)\n\n\n\n\n\n\n\n\nWe see that RELU does a decent job. Because of the nature of RELU, the resulting function has sharp edges. Note that even though the universal approximation theorem says that we can approximate any function, stochastic noise means that the function the network thinks we are approximating need not be the function we want to approximate.."
  },
  {
    "objectID": "posts/functorch/index.html#tanh-nonlinearity",
    "href": "posts/functorch/index.html#tanh-nonlinearity",
    "title": "Regression in PyTorch",
    "section": "",
    "text": "We get somewhat better results with the tanh nonlinearity, if we go with 2 layers. Play with the number of hidden layers and number of units per layer to see if you can do better!\n\nmodel2 = MLRegP(1, 40, nonlinearity=fn.tanh, additional_hidden_wide=1)\nprint(model2)\ncriterion = nn.MSELoss()\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(xgrid.reshape(-1,1)), torch.from_numpy(ygrid))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\nlr, epochs, batch_size = 0.1 , 4000 , 64\noptimizer = torch.optim.SGD(model2.parameters(), lr = lr )\naccum=[]\nfor k in range(epochs):\n    localaccum = []\n    for localx, localy in iter(loader):\n        localx = Variable(localx.float())\n        localy = Variable(localy.float())\n        output = model2.forward(localx)\n        loss = criterion(output, localy)\n        model2.zero_grad()\n        loss.backward()\n        optimizer.step()\n        localaccum.append(loss.data[0])\n    accum.append(np.mean(localaccum))\nplt.plot(accum);                      \n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=40)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=40, out_features=40)\n  )\n  (fc_final): Linear(in_features=40, out_features=1)\n)\n\n\n\n\n\n\n\n\n\n\nfinaloutput = model2.forward(xdata.view(-1,1))\nplt.plot(xgrid, fgrid, '.', alpha=0.1)\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nplt.plot(xgrid, finaloutput.data.numpy(), lw=2)"
  },
  {
    "objectID": "posts/gibbsfromMH/index.html",
    "href": "posts/gibbsfromMH/index.html",
    "title": "Gibbs from Metropolis-Hastings",
    "section": "",
    "text": "We need to show that Gibbs Sampling converges to our target distribution. We start with a componentwise MH update.\nWe shall use the following notation, assuming that \\(x\\) is multidimensional: \\(x_{k}^i\\) is the \\(k\\)th component of \\(x\\) at \\(i\\)th step, while \\(x_{-k}^i\\) is all other componets of \\(x\\) at the same step.\nNow consider the following proposal function:\n\\[q_k(x^* \\vert x^i) = \\begin{cases} p(x_k^* \\vert x_{-k}^i) & for \\,\\,x_{-k}^* = x_{-k}^i,\\\\ 0 & otherwise \\end{cases}\\]\n\n\n\nGraphical representation of the componentwise Gibbs update: proposing from the conditional distribution of one component while holding all others fixed.\n\n\nIn other words, you propose from the conditional distribution of the current component, given a fixed \\(x_{old}\\) value of the other components. You keep all the other component values the same. If you change any of the old component values, the proposal probability is 0. This should remind you of componentwise updating in MH (and block, as \\(k\\) could represent a block). Now you see where this conditional takes you in the \\(x_k\\) direction.\nUnder this proposal distribution, let us see what our MH acceptance probability term:\n\\[A = min(1, \\frac{p(x^*)}{p(x^i)}\\,\\frac{q_k(x^i \\vert x^*)}{q_k(x^* \\vert x^i)})\\]\ngives us. We use \\(p(x^\\*) = p(x_{-k}^\\*, x_k^\\*) = p(x_{k}^\\* \\vert  x_{-k}^\\*)p(x_{-k}^\\*)\\) to obtain\n\\[A = min(1, \\frac{p(x_{k}^* \\vert  x_{-k}^*)p(x_{-k}^*)}{p(x_{k}^i \\vert  x_{-k}^i)p(x_{-k}^i)}\\,\\frac{q_k(x^i \\vert x^*)}{q_k(x^* \\vert x^i)})\\]\nPutting in the definition of the proposal, we get\n\\[A = min(1, \\frac{p(x_{k}^* \\vert  x_{-k}^*)p(x_{-k}^*)}{p(x_{k}^i \\vert  x_{-k}^i)p(x_{-k}^i)}\\,\\frac{p(x_k^i \\vert x_{-k}^*)}{p(x_k^* \\vert x_{-k}^i)})\\]\nNow, because of the componentwise update, \\(x_{-k}^* = x_{-k}^i\\) and thus we get the middle 2 terms in the numerator and denominator cancelling, and the 1st and 3rd terms from the numerator cancelling the 3rd and 1st terms respectively. Thus our acceptance probability is 1.\nWe have constructed a MH sampler with no rejection!!"
  },
  {
    "objectID": "posts/gibbsconj/index.html",
    "href": "posts/gibbsconj/index.html",
    "title": "Gibbs Sampling with Conjugate Conditionals",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm, gamma\nfrom scipy.stats import distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nsns.set_style('whitegrid')\nsns.set_context('poster')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\nWe now going to take a look at a slightly more complicated case that was originally outlined in full generality by Casella and George (1992). Suppose we have a nasty looking joint distribution given as:\n\\[p(x,y) = \\binom{16}{y} x^{y+1} (1-x)^{19-y}\\]\n\n\nFor such a situation the two conditional distributions are not exactly obvious. Clearly we have a binomial term staring at us, so we should be looking to try and express part of the function as a binomial of the form,\n\\[p(\\theta \\vert \\pi ) = \\binom{n}{\\theta} \\pi^{\\theta} (1-\\pi)^{n-\\theta}\\]\nIt follows directly that for our example we have a binomial with \\(n=16\\) and \\(\\theta =y\\),\n\\[p( y \\vert x ) = g(x) \\binom{16}{y} x^{y+1} (1-x)^{16-y} .\\]\n\n\nSo, now we need the conditional for x|y, and we know from Bayes’ theorem that :\n\\[p(x \\vert y) = \\frac{p(y \\vert x)p(x)}{p(y)} \\]\nso what we should be looking for is a conjugate prior to a Binomial distribution, which is of course a Beta distibution:\n\\[Beta(\\alpha,\\beta) = x^{\\alpha-1}(1-x)^{\\beta-1}\\]\nWith this intuition in mind, the math is now trivial:\n\\[p(x \\vert y) = h(y) x^{\\alpha + y - 1}(1-x)^{\\beta + n - y -1}\\]\nwhich for our example question is simply:\n\\[p(x \\vert y) \\sim Beta(y+\\alpha,n-y+\\beta)\\]\nwith \\(\\alpha=2\\) and \\(\\beta=4\\).\n\n\n\n\nWith our conditionals formulated, we can move directly to our Gibbs sampler.\n\nfrom scipy.stats import binom, beta\nn=16\nalph=2.\nbet=4.\n\n\ndef gibbs(N=10000,thin=50):\n    x=1\n    y=1\n    samples=np.zeros((N,2))\n    for i in range(N):\n        for j in range(thin):\n            y=binom.rvs(n,x)\n            newalph=y+alph\n            newbet=n-y+bet\n            x=beta.rvs(newalph, newbet)\n          \n        samples[i,0]=x\n        samples[i,1]=y\n    return samples\n\n\nout=gibbs()\nplt.hist2d(out[:,0],out[:,1], normed=True, bins=50)\nplt.show()"
  },
  {
    "objectID": "posts/gibbsconj/index.html#looks-like-a-binomial",
    "href": "posts/gibbsconj/index.html#looks-like-a-binomial",
    "title": "Gibbs Sampling with Conjugate Conditionals",
    "section": "",
    "text": "For such a situation the two conditional distributions are not exactly obvious. Clearly we have a binomial term staring at us, so we should be looking to try and express part of the function as a binomial of the form,\n\\[p(\\theta \\vert \\pi ) = \\binom{n}{\\theta} \\pi^{\\theta} (1-\\pi)^{n-\\theta}\\]\nIt follows directly that for our example we have a binomial with \\(n=16\\) and \\(\\theta =y\\),\n\\[p( y \\vert x ) = g(x) \\binom{16}{y} x^{y+1} (1-x)^{16-y} .\\]\n\n\nSo, now we need the conditional for x|y, and we know from Bayes’ theorem that :\n\\[p(x \\vert y) = \\frac{p(y \\vert x)p(x)}{p(y)} \\]\nso what we should be looking for is a conjugate prior to a Binomial distribution, which is of course a Beta distibution:\n\\[Beta(\\alpha,\\beta) = x^{\\alpha-1}(1-x)^{\\beta-1}\\]\nWith this intuition in mind, the math is now trivial:\n\\[p(x \\vert y) = h(y) x^{\\alpha + y - 1}(1-x)^{\\beta + n - y -1}\\]\nwhich for our example question is simply:\n\\[p(x \\vert y) \\sim Beta(y+\\alpha,n-y+\\beta)\\]\nwith \\(\\alpha=2\\) and \\(\\beta=4\\)."
  },
  {
    "objectID": "posts/gibbsconj/index.html#the-sampler",
    "href": "posts/gibbsconj/index.html#the-sampler",
    "title": "Gibbs Sampling with Conjugate Conditionals",
    "section": "",
    "text": "With our conditionals formulated, we can move directly to our Gibbs sampler.\n\nfrom scipy.stats import binom, beta\nn=16\nalph=2.\nbet=4.\n\n\ndef gibbs(N=10000,thin=50):\n    x=1\n    y=1\n    samples=np.zeros((N,2))\n    for i in range(N):\n        for j in range(thin):\n            y=binom.rvs(n,x)\n            newalph=y+alph\n            newbet=n-y+bet\n            x=beta.rvs(newalph, newbet)\n          \n        samples[i,0]=x\n        samples[i,1]=y\n    return samples\n\n\nout=gibbs()\nplt.hist2d(out[:,0],out[:,1], normed=True, bins=50)\nplt.show()"
  },
  {
    "objectID": "posts/doseplacebo/index.html",
    "href": "posts/doseplacebo/index.html",
    "title": "The Significance and Size of Effects",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline \n#this line above prepares IPython notebook for working with matplotlib\n\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n\nimport numpy as np # imports a fast numerical programming library\nimport scipy as sp #imports stats functions, amongst other things\nimport matplotlib as mpl # this actually imports matplotlib\nimport matplotlib.cm as cm #allows us easy access to colormaps\nimport matplotlib.pyplot as plt #sets up plotting under plt\nimport pandas as pd #lets us handle data as dataframes\n#sets up pandas table display\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nLet us get the data and put it into a dataframe.\nplacebo = [54, 51, 58, 44, 55, 52, 42, 47, 58, 46]\ndrug = [54, 73, 53, 70, 73, 68, 52, 65, 65]\ndosage = placebo + drug\nlabel = ['P']*len(placebo) + ['D']*len(drug)\ndf = pd.DataFrame(dict(dosage=dosage, label=label))\ndf\n\n\n\n\n\n\n\n\ndosage\nlabel\n\n\n\n\n0\n54\nP\n\n\n1\n51\nP\n\n\n2\n58\nP\n\n\n3\n44\nP\n\n\n4\n55\nP\n\n\n5\n52\nP\n\n\n6\n42\nP\n\n\n7\n47\nP\n\n\n8\n58\nP\n\n\n9\n46\nP\n\n\n10\n54\nD\n\n\n11\n73\nD\n\n\n12\n53\nD\n\n\n13\n70\nD\n\n\n14\n73\nD\n\n\n15\n68\nD\n\n\n16\n52\nD\n\n\n17\n65\nD\n\n\n18\n65\nD\nThe “mean” size of the effect in our sample is about 13.\nactuals = df.groupby('label').dosage.mean()\nactuals\n\nlabel\nD    63.666667\nP    50.700000\nName: dosage, dtype: float64\ndf.groupby('label').dosage.hist(bins=np.arange(30, 80, 1));\nactual_effect = actuals['D'] - actuals['P']\nactual_effect\n\n12.966666666666661"
  },
  {
    "objectID": "posts/doseplacebo/index.html#permutations-to-get-significance",
    "href": "posts/doseplacebo/index.html#permutations-to-get-significance",
    "title": "The Significance and Size of Effects",
    "section": "Permutations to get significance",
    "text": "Permutations to get significance\nCould it have happened by chance?\nWe permute, group-by labels again, and calculate the effect. This kind of randomization should “kill” the effect:\n\ntemp = np.random.permutation(df.label)\n\n\ntemp_series = df.groupby(temp).dosage.mean()\ntemp_series\n\nD    57.0\nP    56.7\nName: dosage, dtype: float64\n\n\n\ntemp_series['D'] - temp_series['P']\n\n0.29999999999999716\n\n\nIf we compare the distribution of effect sizes to the actual effect, this actual effect should be in a tail if it is significant…\n\nsig_means = np.zeros(10000)\nfor i in range(10000):\n    temp = np.random.permutation(df.label)\n    mean_series = df.groupby(temp).dosage.mean()\n    sig_means[i] = mean_series['D'] - mean_series['P']\n\n\nplt.hist(sig_means, bins=50, alpha=0.4);\nplt.axvline(actual_effect, 0, 1, color=\"red\");\n\n\n\n\n\n\n\n\nAs a comparison, consider the case in which placebos had a much wider spread, between 50, and 450. Simply add 13 to each placebo value to get a dosage value. The mean difference would still be 13. But now, 13 would be way inside the histogram, and the effect would not be a significant one, and could have happened by chance.\nStatistically significant does not mean important. Thats a question of, how large is the effect, or where are the confidence intervals for the effect. For instance, if a statistically significant increase in mortality was a mean of 5 days over 5 years by drug over placebo, you would not consider the effect important."
  },
  {
    "objectID": "posts/doseplacebo/index.html#bootstrap-to-estimate-size-of-effect",
    "href": "posts/doseplacebo/index.html#bootstrap-to-estimate-size-of-effect",
    "title": "The Significance and Size of Effects",
    "section": "Bootstrap to estimate size of effect",
    "text": "Bootstrap to estimate size of effect\nHere we randomize labels within the group, take means, and subtract. Here is an example\n\nplacebo_bs = np.random.choice(list(range(10)), size=(10000, 10))\ndrug_bs = np.random.choice(list(range(10, 19)), size=(10000, 9))\n\n\nplacebo_bs[0,:]\n\narray([7, 7, 1, 5, 1, 5, 4, 6, 0, 7])\n\n\n\ndf.iloc[placebo_bs[0,:]]\n\n\n\n\n\n\n\n\ndosage\nlabel\n\n\n\n\n7\n47\nP\n\n\n7\n47\nP\n\n\n1\n51\nP\n\n\n5\n52\nP\n\n\n1\n51\nP\n\n\n5\n52\nP\n\n\n4\n55\nP\n\n\n6\n42\nP\n\n\n0\n54\nP\n\n\n7\n47\nP\n\n\n\n\n\n\n\nHere is the effect:\n\ndf.iloc[drug_bs[0,:]].dosage.mean() - df.iloc[placebo_bs[0,:]].dosage.mean()\n\n14.977777777777774\n\n\nLet us do this 10000 times.\n\neffect_diffs = np.zeros(10000)\nfor i in range(10000):\n    effect_diffs[i] = df.iloc[drug_bs[i,:]].dosage.mean() - df.iloc[placebo_bs[i,:]].dosage.mean()\n\n\npercs = np.percentile(effect_diffs, [5, 50, 95])\npercs\n\narray([  7.53333333,  13.05      ,  18.12222222])\n\n\n\nplt.hist(effect_diffs, bins=100, alpha=0.2);\nplt.axvline(actual_effect, 0, 1, color=\"red\");\nfor p in percs:\n    plt.axvline(p, 0, 1, color=\"green\");\n\n\n\n\n\n\n\n\nThat is, 90% of the time, the drug is 7.53 to 18.12 more effective than placebo. The average value of placebo in our sample was 50. This makes the drug 13 to 33% more effective, roghly, which seems it might be an important effect.\nIf you have such a confidence interval, why do a significance test. Consider the extreme case of 2 data points, wel separated. The confidence interval is tight around the difference. But a permutation test would show that half the time, you will by random chance, get a difference just as big as the observed one. Intuitively this is too little data to show significance, and this “half the time” bears that out…"
  },
  {
    "objectID": "posts/metropolissupport/index.html",
    "href": "posts/metropolissupport/index.html",
    "title": "Metropolis and Support Mismatch",
    "section": "",
    "text": "From https://darrenjw.wordpress.com/2012/06/04/metropolis-hastings-mcmc-when-the-proposal-and-target-have-differing-support/\n\n%matplotlib inline\nimport numpy as np\nimport scipy as  sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn.apionly as sns\nsns.set_style(\"whitegrid\")\n\nAs a simple example, lets target Gamma(2,1) or \\(xe^{-x}, x \\gt 0\\).\n\ntarget = lambda x: x*np.exp(-x)\nxx = np.linspace(0, 20, 1000)\nplt.plot(xx, target(xx));\n\n\n\n\n\n\n\n\n\n\nHere, copied from before, is the metropolis code.\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples\n\n\ndef prop(x):\n    return np.random.normal(x, 1.0)\nout = metropolis(target, prop, 100000, 1.0)\n\n\nsns.distplot(out)\nplt.plot(xx, target(xx));\n\n\n\n\n\n\n\n\nSince we use the functional form directly without checking for \\(x \\gt 0\\), we are not sampling on the correct support. This does not land up costing us, as the acceptance ratio being negative the first time we sample a negative \\(x\\) will ensure that we never sample a negative \\(x\\). We would be better using scipy.stats built in gamma support.\nWe have seen this before, in sampling from a weibull using a normal as well. Also from sampling from a function only defined on [0,1]. Some people consider the lax use of a larger-support proposal a bug. But it does not bite us anywhere but efficiency due to the mechanism of the acceptance ratio.\nLet us see what this lack of efficiency is:\n\ndef metropolis_instrument(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    acc1 = 0\n    rej_neg = 0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n            acc1 += 1\n        else:#we always get a sample\n            if x_star &lt; 0:\n                rej_neg += 1\n            samples[i]= x_prev\n            \n    return samples, acc1, rej_neg\n\n\nout2, a1, rn = out = metropolis_instrument(target, prop, 100000, 1.0)\n\n\na1/100000, rn/(100000 - a1)\n\n(0.7298, 0.3654700222057735)\n\n\nThus, out of a 73% acceptance, a full 36% is wasted on proposing negatives.\n\n\n\nYou might think that simply rejecting is ok, but you would be wrong. You are then sampling from some other distribution.\n\ndef metropolis_broken(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        while 1:\n            x_star = qdraw(x_prev)\n            if x_star &gt; 0:\n                break\n        \n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples\n\n\n\nout3 = metropolis_broken(target, prop, 100000, 1.0)\nsns.distplot(out3)\nplt.plot(xx, target(xx));\n\n\n\n\n\n\n\n\n\n\n\nTo fix this use Metropolis-Hastings instead and sample from a distribution eith the correct support, a truncated normal. Since the truncated normal is not symmetric:\n\\[ \\frac{e^{(x-x_0)^2}}{CDF(x)} != \\frac{e^{(x_0-x)^2}}{CDF(x_0)} \\]\nwe must use a MH Sampler\n\ndef metropolis_hastings(p,q, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        while 1:\n            x_star = qdraw(x_prev)\n            if x_star &gt; 0:\n                break\n        \n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)\n        if np.random.uniform() &lt; min(1, pdfratio*proposalratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted +=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\nfrom scipy.stats import norm\ndef prop2(x):\n    return x + np.random.normal()\ndef q(x_prev, x_star):\n    num = norm.cdf(x_prev)\n    return num\n\n\nout4, _ = metropolis_hastings(target, q, prop2, 100000, 1.0)\n\nNow we get the correct output!\n\nsns.distplot(out4)\nplt.plot(xx, target(xx));"
  },
  {
    "objectID": "posts/metropolissupport/index.html#using-metropolis-to-sample",
    "href": "posts/metropolissupport/index.html#using-metropolis-to-sample",
    "title": "Metropolis and Support Mismatch",
    "section": "",
    "text": "Here, copied from before, is the metropolis code.\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples\n\n\ndef prop(x):\n    return np.random.normal(x, 1.0)\nout = metropolis(target, prop, 100000, 1.0)\n\n\nsns.distplot(out)\nplt.plot(xx, target(xx));\n\n\n\n\n\n\n\n\nSince we use the functional form directly without checking for \\(x \\gt 0\\), we are not sampling on the correct support. This does not land up costing us, as the acceptance ratio being negative the first time we sample a negative \\(x\\) will ensure that we never sample a negative \\(x\\). We would be better using scipy.stats built in gamma support.\nWe have seen this before, in sampling from a weibull using a normal as well. Also from sampling from a function only defined on [0,1]. Some people consider the lax use of a larger-support proposal a bug. But it does not bite us anywhere but efficiency due to the mechanism of the acceptance ratio.\nLet us see what this lack of efficiency is:\n\ndef metropolis_instrument(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    acc1 = 0\n    rej_neg = 0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n            acc1 += 1\n        else:#we always get a sample\n            if x_star &lt; 0:\n                rej_neg += 1\n            samples[i]= x_prev\n            \n    return samples, acc1, rej_neg\n\n\nout2, a1, rn = out = metropolis_instrument(target, prop, 100000, 1.0)\n\n\na1/100000, rn/(100000 - a1)\n\n(0.7298, 0.3654700222057735)\n\n\nThus, out of a 73% acceptance, a full 36% is wasted on proposing negatives."
  },
  {
    "objectID": "posts/metropolissupport/index.html#a-wrong-built-in-regection-sampler",
    "href": "posts/metropolissupport/index.html#a-wrong-built-in-regection-sampler",
    "title": "Metropolis and Support Mismatch",
    "section": "",
    "text": "You might think that simply rejecting is ok, but you would be wrong. You are then sampling from some other distribution.\n\ndef metropolis_broken(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        while 1:\n            x_star = qdraw(x_prev)\n            if x_star &gt; 0:\n                break\n        \n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples\n\n\n\nout3 = metropolis_broken(target, prop, 100000, 1.0)\nsns.distplot(out3)\nplt.plot(xx, target(xx));"
  },
  {
    "objectID": "posts/metropolissupport/index.html#fix-using-mh",
    "href": "posts/metropolissupport/index.html#fix-using-mh",
    "title": "Metropolis and Support Mismatch",
    "section": "",
    "text": "To fix this use Metropolis-Hastings instead and sample from a distribution eith the correct support, a truncated normal. Since the truncated normal is not symmetric:\n\\[ \\frac{e^{(x-x_0)^2}}{CDF(x)} != \\frac{e^{(x_0-x)^2}}{CDF(x_0)} \\]\nwe must use a MH Sampler\n\ndef metropolis_hastings(p,q, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        while 1:\n            x_star = qdraw(x_prev)\n            if x_star &gt; 0:\n                break\n        \n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)\n        if np.random.uniform() &lt; min(1, pdfratio*proposalratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted +=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\nfrom scipy.stats import norm\ndef prop2(x):\n    return x + np.random.normal()\ndef q(x_prev, x_star):\n    num = norm.cdf(x_prev)\n    return num\n\n\nout4, _ = metropolis_hastings(target, q, prop2, 100000, 1.0)\n\nNow we get the correct output!\n\nsns.distplot(out4)\nplt.plot(xx, target(xx));"
  },
  {
    "objectID": "posts/noiseless_learning/index.html",
    "href": "posts/noiseless_learning/index.html",
    "title": "Learning Without Noise",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/noiseless_learning/index.html#the-process-of-learning",
    "href": "posts/noiseless_learning/index.html#the-process-of-learning",
    "title": "Learning Without Noise",
    "section": "The process of learning",
    "text": "The process of learning\nThere are challenges that occur in learning a model from data:\n\nsmall samples of data\nnoise in the data\nissues related to the complexity of the models we use\n\nLet us first ask the question: what is he process of learning from data in the absence of noise. This never really happens, but it is a way for us to understand the theory of approximation, and lets us build a base for understanding the learning from data with noise.\nLets say we are trying to predict is a human process such as an election. Here economic and sociological factors are important, such as poverty, race and religiousness. There are historical correlations between such factors and election outcomes which we might want to incorporate into our model. An example of such a model might be:\nThe odds of Romney winning a county against Obama in 2012 are a function of population religiosity, race, poverty, education, and other social and economic indicators. \nOur causal argument motivating this model here might be that religious people are more socially conservative and thus more likely to vote republican. This might not be the correct causation, but thats not entirely important for the prediction.\nAs long as a correlation exists, our model is more structured than 50-50 randomness, and we can try and make a prediction. Remember of-course, our model may even be wrong (see Box’s aphorism: https://en.wikipedia.org/wiki/All_models_are_wrong).\nWe’ll represent the variable being predicted, such as the probability of voting for Romney, by the letter \\(y\\), and the features or co-variates we use as an input in this probability by the letter \\(x\\). This \\(x\\) could be multi-dimensional, with \\(x_1\\) being poverty, \\(x_2\\) being race, and so on.\nWe then write\n\\[ y = f(x) \\]\nand our jobs is to take \\(x\\) such as data from the census about race, religiousness, and so on, and \\(y\\) as previous elections and the results of polls that pollsters come up with, and to make a predictive model for the elections. That is, we wish to estimate \\(f(x)\\).\n\nA real simple model\nTo gently step feet in the modelling world, lets see consider very simple model, where the probability of voting for Romney is a function only of how religious the population in a county is. This is a model I’ve cooked up, and the data is fake.\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\nLet us assume that we have a “population” of 200 counties \\(x\\):\n\ndf=pd.read_csv(\"data/religion.csv\")\ndf.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\nLets suppose now that the Lord came by and told us that the points in the plot below captures \\(f(x)\\) exactly. In other words, there is no specification error, and God knows the generating process exactly.\n\nx=df.rfrac.values\nf=df.promney.values\nplt.plot(x,f,'.', alpha=0.3)\n\n\n\n\n\n\n\n\nNotice that our sampling of \\(x\\) is not quite uniform: there are more points around \\(x\\) of 0.7.\nNow, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample \\(\\cal{D}\\) of 30 data points. Such data is called in-sample data. Contrastingly, the entire population of data points is also called out-of-sample data.\n\n#indexes=np.sort(np.random.choice(x.shape[0], size=30, replace=False))\ndfsample = pd.read_csv(\"data/noisysample.csv\")\ndfsample.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\nindexes = dfsample.i.values\n\n\nsamplex = x[indexes]\nsamplef = f[indexes]\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\naxes[1].plot(x,f, 'r.', alpha=0.2, label=\"population\");\naxes[1].plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n\n\n\n\n\n\n\nThe lightly shaded squares in the right panel plot are the in-sample \\(\\cal{D}\\) of 30 points given to us. Let us then pretend that we have forgotten the curve that the Lord gave us. Thus, all we know is the blue points on the plot on the right, and we have no clue about what the original curve was, nor do we remember the original “population”.\nThat is, imagine the Lord gave us \\(f\\) but then also gave us amnesia. Remember that such amnesia is the general case in learning, where we do not know the target function, but rather just have some data. Thus what we will be doing is trying to find functions that might have generated the 30 points of data that we can see in the hope that one of these functions might approximate \\(f\\) well, and provide us a predictive model for future data. This is known as fitting the data.\n\n\nThe Hypothesis or Model Space\nSuch a function, one that we use to fit the data, is called a hypothesis. We’ll use the notation \\(h\\) to denote a hypothesis. Lets consider as hypotheses for the data above, a particular class of functions called polynomials.\nA polynomial is a function that combines multiple powers of x linearly. You’ve probably seen these in school, when working with quadratic or cubic equations and functions:\n\\[\n\\begin{align*}\nh(x) &=& 9x - 7 && \\,(straight\\, line) \\\\\nh(x) &=& 4x^2 + 3x + 2 && \\,(quadratic) \\\\\nh(x) &=& 5x^3 - 31x^2 + 3x  && \\,(cubic).\n\\end{align*}\n\\]\nIn general, a polynomial can be written thus:\n\\[\n\\begin{eqnarray*}\nh(x) &=& a_0 + a_1 x^1 + a_2 x^2 + ... + a_n x^n \\\\\n      &=& \\sum_{i=0}^{n} a_i x^i\n\\end{eqnarray*}\n\\]\nThus, by linearly we mean a sum of coefficients \\(a_i\\) times powers of \\(x\\), \\(x^i\\). In other words, the polynomial is linear in its coefficients.\nLet us consider as the function we used to fit the data, a hypothesis \\(h\\) that is a straight line. We put the subscript \\(1\\) on the \\(h\\) to indicate that we are fitting the data with a polynomial of order 1, or a straight line. This looks like:\n\\[ h_1(x) = a_0 + a_1 x \\]\nWe’ll call the best fit straight line the function \\(g_1(x)\\). The “best fit” idea is this: amongst the set of all lines (i.e., all possible choices of \\(h_1(x)\\)), what is the best line \\(g_1(x)\\) that represents the in-sample data we have? (The subscript \\(1\\) on \\(g\\) is chosen to indicate the best fit polynomial of degree 1, ie the line amongst lines that fits the data best).\nThe best fit \\(g_1(x)\\) is calculated and shown in the figure below:\n\ng1 = np.poly1d(np.polyfit(x[indexes],f[indexes],1))\nplt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\nplt.plot(x,g1(x), 'b--', alpha=0.6, label=\"$g_1$\");\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nHow did we calculate the best fit? We’ll come to that in a bit, but in the meanwhile, lets formalize and generalize the notion of “best fit line amongst lines” a bit.\nThe set of all functions of a particular kind that we could have used to fit the data is called a Hypothesis Space. The words “particular kind” are deliberately vague: its our choice as to what we might want to put into a hypothesis space. A hypothesis space is denoted by the notation \\(\\cal{H}\\).\nLets consider the hypothesis space of all straight lines \\(h_1(x)\\). We’ll denote it as \\(\\cal{H}_1\\), with the subscript being used to mark the order of the polynomial. Another such space might be \\(\\cal{H}_2\\), the hypothesis space of all quadratic functions. A third such space might combine both of these together. We get to choose what we want to put into our hypothesis space.\nIn this set-up, what we have done in the code and plot above is this: we have found the best \\(g_1\\) to the data \\(\\cal{D}\\) from the functions in the hypothesis space \\(\\cal{H}_1\\). This is not the best fit from all possible functions, but rather, the best fit from the set of all the straight lines.\nThe hypothesis space is a concept we can use if we want to capture the complexity of a model you use to fit data. For example, since quadratics are more complex functions than straight lines (they curve more), \\(\\cal{H}_2\\) is more complex than \\(\\cal{H}_1\\).\n\n\nDeterministic Error or Bias\nNotice from the figure above that models in \\(\\cal{H}_1\\), i.e., straight lines, and the best-fit straight line \\(g_1\\) in particular, do not do a very good job of capturing the curve of the data (and thus the underlying function \\(f\\) that we are trying to approximate. Consider the more general case in the figure below, where a curvy \\(f\\) is approximated by a function \\(g\\) which just does not have the wiggling that \\(f\\) has.\n\n\n\nApproximation bias: the gap between true function f and best-fit hypothesis g\n\n\nThere is always going to be an error then, in approximating \\(f\\) by \\(g\\). This approximation error is shown in the figure by the blue shaded region, and its called bias, or deterministic error. The former name comes from the fact that \\(g\\) just does not wiggle the way \\(f\\) does (nothing will make a straight line curve). The latter name (which I first saw used in http://www.amlbook.com/ ) comes from the notion that if you did not know the target function \\(f\\), which is the case in most learning situations, you would have a hard time distinguishing this error from any other errors such as measurement and noise…\nGoing back to our model at hand, it is clear that the space of straight lines \\(\\cal{H_1}\\) does not capture the curving in the data. So let us consider the more complex hypothesis space \\(\\cal{H_{20}}\\), the set of all 20th order polynomials \\(h_{20}(x)\\):\n\\[h_{20}(x) = \\sum_{i=0}^{20} a_i x^i\\,.\\]\nTo see how a more complex hypothesis space does, lets find the best fit 20th order polynomial \\(g_{20}(x)\\).\n\ng20 = np.poly1d(np.polyfit(x[indexes],f[indexes],20))\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\nplt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\nplt.plot(x,g20(x), 'b--', alpha=0.6, label=\"$g_{10}$\");\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nVoila! You can see the 20th order polynomial does a much better job of tracking the points, because of the wiggle room it has in making a curve “go near or through” all the points as opposed to a straight line, which well, cant curve. Thus it would seem that \\(\\cal{H}_{20}\\) might be a better candidate hypothesis set from which to choose a best fit model.\nWe can quantify this by calculating some notion of the bias for both \\(g_1\\) and \\(g_{20}\\). To do this we calculate the square of the difference between f and the g’s on the population of 200 points i.e.:\n\\[B_1(x) = (g_1(x) - f(x))^2 \\,;\\,\\, B_{20}(x) = (g_{20}(x) - f(x))^2\\,.\\]\nSquaring makes sure that we are calculating a positive quantity.\n\nplt.plot(x, (g1(x)-f)**2, lw=3, label=\"$B_1(x)$\")\nplt.plot(x, (g20(x)-f)**2, lw=3,label=\"$B_{20}(x)$\");\nplt.xlabel(\"$x$\")\nplt.ylabel(\"population error\")\nplt.yscale(\"log\")\nplt.legend(loc=4);\nplt.title(\"Bias\");\n\n\n\n\n\n\n\n\nAs you can see the bias or approximation error is much smaller for \\(g_{20}\\).\nIs \\(g_{20}\\) the best model for this data from all possible models? Indeed, how do we find the best fit model from the best hypothesis space? This is what learning is all about.\nWe have used the python function np.polyfit to find \\(g_{1}\\) the best fit model in \\(\\cal{H_1}\\) and \\(g_{20}\\) the best fit model in \\(\\cal{H_{20}}\\), but how did we arrive at that conclusion? This is the subject of the next section."
  },
  {
    "objectID": "posts/noiseless_learning/index.html#how-to-learn-the-best-fit-model-in-a-hypothesis-space",
    "href": "posts/noiseless_learning/index.html#how-to-learn-the-best-fit-model-in-a-hypothesis-space",
    "title": "Learning Without Noise",
    "section": "How to learn the best fit model in a hypothesis space",
    "text": "How to learn the best fit model in a hypothesis space\nLet’s understand in an intuitive sense, what it means for a function to be a good fit to the data. Lets consider, for now, only the hypothesis space \\(\\cal{H}_{1}\\), the set of all straight lines. In the figure below, we draw against the data points (in red) one such line \\(h_1(x)\\) (in red).\nYou might think you want to do this statistically, using ML Estimation or similar, but note that at this point there is no statistical notion of a generating process. We’re just trying to approximate a function by another, with the latter being chosen amongst many in a hypothesis space.\n\n\n\nLinear regression fit with residuals shown\n\n\nThe natural way of thinking about a “best fit” would be to minimize the distance from the line to the points, for some notion of distance. In the diagram we depict one such notion of distance: the vertical distance from the points to the line. These distances are represented as thin black lines.\nThe next question that then arises is this: how exactly we define the measure of this vertical distance? We cant take the measure of distance to be the y-value of the point minus the y value of the line at the same x, ie \\(y_i - h_1(x_i)\\). Why? If we did this, then we could have points very far from the line, and as long as the total distance above was equal to the total distance below the line, we’d get a net distance of 0 even when the line is very far from the points.\nThus we must use a positive estimate of the distance as our measure. We could take either the absolute value of the distance, \\(\\vert y_i - h_1(x_i) \\vert\\), or the square of the distance as our measure, \\((y_i - h_1(x_i))^2\\). Both are reasonable choices, and we shall use the squared distance for now. (Now its probably clear to you why we defined bias in the last section as the pointwise square of the distance).\nWe sum this measure up over all our data points, to create whats known as the error functional or risk functional (also just called error, cost, or risk) of using line \\(h_1(x)\\) to fit our points \\(y_i \\in \\cal{D}\\) (this notation is to be read as “\\(y_i\\) in \\(\\cal{D}\\)”) :\n\\[ R_{\\cal{D}}(h_i(x)) = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} (y_i - h_1(x_i))^2 \\]\nwhere \\(N\\) is the number of points in \\(\\cal{D}\\).\nWhat this formula says is: the cost or risk is just the total squared distance to the line from the observation points. Here we use the word functional to denote that, just as in functional programming, the risk is a function of the function \\(h_1(x)\\).\nWe also make explicit the in-sample data \\(\\cal{D}\\), because the value of the risk depends upon the points at which we made our observation. If we had made these observations \\(y_i\\) at a different set of \\(x_i\\), the value of the risk would be somewhat different. The hope in learning is that the risk will not be too different, as we shall see in the next section\nNow, given these observations, and the hypothesis space \\(\\cal{H}_1\\), we minimize the risk over all possible functions in the hypothesis space to find the best fit function \\(g_1(x)\\):\n\\[ g_1(x) = \\arg\\min_{h_1(x) \\in \\cal{H}} R_{\\cal{D}}(h_1(x)).\\]\nHere the notation\n\\(\"\\arg\\min_{x} F(x)\"\\)\nmeans: give me the argument of the functional \\(x\\) at which \\(F(x)\\) is minmized. So, for us: give me the function \\(g_1(x) = h_1\\) at which the risk \\(R_{\\cal{D}}(h_1)\\) is minimized; i.e. the minimization is over functions \\(h_1\\).\nAnd this is exactly what the python function np.polyfit(x,h,n) does for us. It minimizes this squared-error with respect to the coefficients of the polynomial.\nThus we can in general write:\n\\[ g(x) = \\arg\\min_{h(x) \\in \\cal{H}} R_{\\cal{D}}(h(x)),\\]\nwhere \\(\\cal{H}\\) is a general hypothesis space of functions.\n\nThe Structure of Learning\nWe have a target function \\(f(x)\\) that we do not know. But we do have a sample of data points from it, \\((x_1,y_1), (x_2,y_2), ..., (x_n,y_n)\\). We call this the sample or training examples \\(\\cal{D}\\). We are interested in using this sample to estimate a function \\(g\\) to approximate the function \\(f\\), and which can be used for prediction at new data points, or on the entire population, also called out-of-sample prediction.\nNotice the way that statistics comes into this approximation problem is from the notion that we are trying to reconstruct the original function from a small-ish sample rather than a large-ish population.\nTo do this, we use an algorithm, called the learner, which chooses functions from a hypothesis set \\(\\cal{H}\\) and computes a cost measure or risk functional \\(R\\) (like the sum of the squared distance over all points in the data set) for each of these functions. It then chooses the function \\(g\\) which minimizes this cost measure amonst all the functions in \\(\\cal{H}\\), and thus gives us a final hypothesis \\(g\\) which we then use to approximate or estimate f everywhere, not just at the points in our data set.\nHere our learner is called Polynomial Regression, and it takes a hypothesis space \\(\\cal{H}_d\\) of degree \\(d\\) polynomials, minimizes the “squared-error” risk measure, and spits out a best-fit hypothesis \\(g_d\\).\n\n\n\nThe supervised learning framework: from target function to final hypothesis\n\n\n\n\nOut-of-Sample and in-sample\nWe write \\(g \\approx f\\), or \\(g\\) is the estimand of \\(f\\).In statistics books you will see \\(g\\) written as \\(\\hat{f}\\).\nWhy do we think that this might be a good idea? What are we really after?\nWhat we’d like to do is make good predictions. In the language of cost, what we are really after is to minimize the cost out-of-sample, on the population at large. But this presents us with a conundrum: how can we minimize the risk on points we havent yet seen?\nThis is why we (a) minimize the risk on the set of points that we have to find \\(g\\) and then (b) hope that once we have found our best model \\(g\\), our risk does not particularly change out-of-sample, or when using a different set of points\nWe are, as is usual in statistics, drawing conclusions about a population from a sample.\nIntuitively, to do this, we need to ask ourselves, how representative is our sample? Or more precisely, how representative is our sample of our training points of the population (or for that matter the new x that we want to predict for)?\nWe illustrate this below for our population of 200 data points and our sample of 30 data points (in red).\n\nplt.hist(x, normed=True, bins=30, alpha=0.7)\nsns.kdeplot(x)\nplt.plot(x[indexes], [1.0]*len(indexes),'o', alpha=0.8)\nplt.xlim([0,1]);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\nIn our example, if we only want to use \\(g\\), our estimand of \\(f\\) to predict for large \\(x\\), or more religious counties, we would need a good sampling of points \\(x\\) closer to 1. And, similarly, the new \\(x\\) we are using to make predictions would also need to be representative of those counties. We wont do well if we try and predict low-religiousness counties from a sample of high-religiousness ones. Or, if we do want to predict over the entire range of religiousness, our training sample better cover all \\(x\\) well.\nOur red points seem to follow our (god given) histogram well.\n\n\nThe relation to the Law of Large Numbers.\nThe process of minimization we do is called Empirical Risk Minimization (ERM) as we minimize the cost measure over the “empirically observed” training examples or points. But, on the assumption that we were given a training set representative of the population, ERM is just an attempt use of the law of large numbers.\nWhat we really want to calculate is:\n\\[R_{out}(h) =  E_{p(x)}[(h(x) - f(x))^2] = \\int dx p(x)  (h(x) - f(x))^2 .\\]\nAs usual we do not have access to the population but just some samples and thus we want to do something like:\n\\[R_{out}(h) = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{x_i \\sim p(x)} (h(x_i) - f(x_i))^2 = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{x_i \\sim p(x)} (h(x_i) - y_i)^2.\\]\nWe do not have an infinitely large training “sample”. On the assumption that its representative (i.e. drawn from \\(p(x)\\)) we calculate\n\\[R_{\\cal{D}}(h) =  \\sum_{x_i \\in \\cal{D}} (h(x_i) - y_i)^2.\\]\nWe could calculate the usual mean of sample means and all that, and shall see later that it is these quantities that are related to bias and variance."
  },
  {
    "objectID": "posts/noiseless_learning/index.html#statement-of-the-learning-problem.",
    "href": "posts/noiseless_learning/index.html#statement-of-the-learning-problem.",
    "title": "Learning Without Noise",
    "section": "Statement of the learning problem.",
    "text": "Statement of the learning problem.\nOnce we have done that, we can then intuitively say that, if we find a hypothesis \\(g\\) that minimizes the cost or risk over the training set; this hypothesis might do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nIn other words, we hope the empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small.\nIndeed, as we can see below, \\(g_{20}\\) does an excellent job on the population, not just on the sample.\n\n#plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\nplt.plot(x,g20(x), 'b--', alpha=0.9, lw=2, label=\"$g_{20}$\");\nplt.plot(x,f, 'o', alpha=0.2, label=\"population\");\nplt.legend(loc=4);"
  },
  {
    "objectID": "posts/understandingaic/index.html",
    "href": "posts/understandingaic/index.html",
    "title": "Understanding AIC",
    "section": "",
    "text": "This notebook is based on McElreath, Rethinking Statistics, Chapter 6.\nWhen we use the empirical distribution and sample quantities here we are working with our training sample (s).\nClearly we can calculate deviance on the validation and test samples as well to remedy this issue. And the results will be similar to what we found in lecture for MSE, with the training deviance decreasing with complexity and the testing deviance increasing at some point.\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/understandingaic/index.html#a-trick-to-generate-data",
    "href": "posts/understandingaic/index.html#a-trick-to-generate-data",
    "title": "Understanding AIC",
    "section": "A trick to generate data",
    "text": "A trick to generate data\nWe generate data from a gaussian with standard deviation 1 and means given by:\n\\[\\mu_i = 0.15 x_{1,i} - 0.4 x_{2,i}, y \\sim N(\\mu, 1).\\]\nThis is a 2 parameter model.\nWe use an interesting trick to generate this data, directly using the regression coefficients as correlations with the response variable.\n\ndef generate_data(N, k, rho=[0.15, -0.4]):\n    n_dim = 1 + len(rho)\n    if n_dim &lt; k:\n        n_dim = k\n    Rho = np.eye(n_dim)\n    for i,r in enumerate(rho):\n        Rho[0, i+1] = r\n    index_lower = np.tril_indices(n_dim, -1)\n    Rho[index_lower] = Rho.T[index_lower]\n    mean = n_dim * [0.]\n    Xtrain = np.random.multivariate_normal(mean, Rho, size=N)\n    Xtest = np.random.multivariate_normal(mean, Rho, size=N)\n    ytrain = Xtrain[:,0].copy()\n    Xtrain[:,0]=1.\n    ytest = Xtest[:,0].copy()\n    Xtest[:,0]=1.\n    return Xtrain[:,:k], ytrain, Xtest[:,:k], ytest\n\nWe want to generate data for 5 different cases, a one parameter (intercept) fit, a two parameter (intercept and \\(x_1\\)), three parameters (add a $x_2), and four and five parameters. Here is what the data looks like for 2 parameters:\n\ngenerate_data(20,2)\n\n(array([[ 1.        , -0.83978695],\n        [ 1.        , -0.60882982],\n        [ 1.        ,  1.02567296],\n        [ 1.        ,  0.24801809],\n        [ 1.        , -1.08181661],\n        [ 1.        , -1.85677575],\n        [ 1.        ,  1.82835523],\n        [ 1.        ,  0.35622585],\n        [ 1.        , -0.04159412],\n        [ 1.        ,  0.58678675],\n        [ 1.        , -0.24396323],\n        [ 1.        , -0.07081137],\n        [ 1.        ,  0.46510137],\n        [ 1.        , -1.02993129],\n        [ 1.        , -2.08756332],\n        [ 1.        ,  0.60666556],\n        [ 1.        ,  0.45913243],\n        [ 1.        ,  0.60083017],\n        [ 1.        , -1.05726496],\n        [ 1.        , -0.52258973]]),\n array([-1.11513393, -0.50856507,  0.50782261, -0.09031626,  0.41992084,\n        -0.82404287,  0.27567933,  0.3626567 ,  0.99109211,  1.14742966,\n         0.53597334, -1.2959274 ,  2.12659247,  0.09595858,  0.05845798,\n         0.47581813, -1.02115871,  0.83942264,  0.33097791, -1.07482199]),\n array([[ 1.        , -0.09664154],\n        [ 1.        ,  1.68464504],\n        [ 1.        , -1.63102144],\n        [ 1.        , -0.83585358],\n        [ 1.        , -1.0022563 ],\n        [ 1.        , -0.40901251],\n        [ 1.        ,  0.52024856],\n        [ 1.        ,  0.64056776],\n        [ 1.        , -0.26979402],\n        [ 1.        ,  0.57670424],\n        [ 1.        , -0.13580787],\n        [ 1.        , -0.74665431],\n        [ 1.        , -0.34801499],\n        [ 1.        , -0.53583385],\n        [ 1.        ,  1.49971783],\n        [ 1.        ,  0.47265248],\n        [ 1.        , -0.40158879],\n        [ 1.        ,  0.59618203],\n        [ 1.        ,  0.63314497],\n        [ 1.        , -0.85947691]]),\n array([-1.2830826 , -0.0773539 , -1.22779576, -1.43955577,  0.39940223,\n        -1.52159959, -0.41312511, -0.95244826,  0.20244849, -0.32376445,\n         0.09636247,  0.03435469,  0.29289397,  0.70749769, -2.20920373,\n         0.36671712, -0.73570139, -0.381103  , -0.3126861 , -0.61196652]))\n\n\nAnd for four parameters\n\ngenerate_data(20,4)\n\n(array([[  1.00000000e+00,  -5.64117484e-01,  -1.30408291e+00,\n          -4.06307198e-01],\n        [  1.00000000e+00,   2.45856192e-01,  -1.13160363e+00,\n           6.99099707e-01],\n        [  1.00000000e+00,  -5.92401483e-01,  -5.51929080e-01,\n           1.70288811e-01],\n        [  1.00000000e+00,   1.40350006e+00,  -7.42482462e-01,\n           6.90299071e-01],\n        [  1.00000000e+00,  -1.14026512e+00,   2.27882734e-01,\n          -2.80250494e-01],\n        [  1.00000000e+00,  -1.79114172e-01,   1.71257237e+00,\n           1.32182974e+00],\n        [  1.00000000e+00,   8.39677171e-01,  -2.07787502e-01,\n           1.20281542e+00],\n        [  1.00000000e+00,  -9.38668901e-01,  -5.87192846e-01,\n           9.91223102e-01],\n        [  1.00000000e+00,  -4.11883974e-01,  -1.31283133e+00,\n          -9.42131126e-01],\n        [  1.00000000e+00,   5.27622295e-01,   2.98370087e-01,\n          -3.13398528e-01],\n        [  1.00000000e+00,   1.75945182e+00,  -9.55446150e-01,\n          -5.65605486e-01],\n        [  1.00000000e+00,   3.66192473e-01,   1.39659262e+00,\n           5.25449367e-01],\n        [  1.00000000e+00,  -8.27065820e-01,   2.07687904e-01,\n          -4.07828527e-01],\n        [  1.00000000e+00,  -9.94963330e-01,  -3.45817822e-01,\n          -1.71339667e-01],\n        [  1.00000000e+00,   1.35510550e+00,   2.80027702e-01,\n           9.10748378e-03],\n        [  1.00000000e+00,  -5.79066026e-01,   1.67201032e+00,\n          -1.21304440e+00],\n        [  1.00000000e+00,   6.61699882e-01,   5.01830803e-01,\n           3.88336944e-01],\n        [  1.00000000e+00,   1.33639603e-01,   1.61570562e-01,\n          -1.96165163e-04],\n        [  1.00000000e+00,  -1.15492222e+00,  -1.19608702e+00,\n           7.44868766e-01],\n        [  1.00000000e+00,  -1.16104055e-03,   5.60105046e-01,\n           1.13087330e+00]]),\n array([ 0.59049849, -0.32849878,  0.17111991, -0.18214341, -1.06834661,\n        -2.01806297,  0.64395116,  1.36524519, -0.39568506,  0.50772571,\n         2.53131129, -1.54337658, -0.23082485,  0.23672394, -1.72834828,\n         0.03969115,  0.84937923, -0.04779334, -0.12796287,  0.25091162]),\n array([[ 1.        ,  0.87805634, -1.3252486 ,  0.9147951 ],\n        [ 1.        ,  0.04573015,  0.26129224, -0.39094182],\n        [ 1.        , -1.91453329,  0.26466733, -0.31760853],\n        [ 1.        ,  0.50704342, -0.91631862,  0.71741282],\n        [ 1.        , -1.12324824, -1.01977223, -0.52418926],\n        [ 1.        ,  0.33666476,  0.27513745,  0.63589532],\n        [ 1.        , -0.38934175,  1.00961086, -0.61853231],\n        [ 1.        , -0.79629895,  1.28994305, -1.54766776],\n        [ 1.        , -0.71721218,  0.15741145,  0.475622  ],\n        [ 1.        ,  1.27433083,  0.6941836 ,  0.90788968],\n        [ 1.        , -0.68170154, -0.04929347,  0.27673865],\n        [ 1.        , -0.08147767,  0.81537351, -0.55685094],\n        [ 1.        , -0.32449028, -0.25664319, -0.57435918],\n        [ 1.        ,  0.64398473,  0.08495211,  0.47839262],\n        [ 1.        , -0.91519445, -2.06795016, -0.42275664],\n        [ 1.        , -0.29770443,  1.08641811,  1.94490458],\n        [ 1.        , -0.19844572, -2.70288281,  0.69816899],\n        [ 1.        ,  0.2687054 ,  1.53633517, -0.84276465],\n        [ 1.        , -0.06377957, -0.05227578,  1.96973628],\n        [ 1.        , -2.04828043,  0.47438443, -2.28134046]]),\n array([ 0.02585828, -0.1284505 , -0.39679531, -0.22793629,  0.25824818,\n        -0.35346682, -1.30972978, -1.2250625 ,  0.97636939, -1.50891804,\n         0.84376139, -2.22258544,  0.47901623, -1.08042407,  2.35481165,\n         0.38210287,  1.27868801, -1.71473971,  0.6498696 , -0.27036068]))\n\n\n\nfrom scipy.stats import norm\nimport statsmodels.api as sm\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools"
  },
  {
    "objectID": "posts/understandingaic/index.html#analysis-n20",
    "href": "posts/understandingaic/index.html#analysis-n20",
    "title": "Understanding AIC",
    "section": "Analysis, n=20",
    "text": "Analysis, n=20\nHere is the main loop of our analysis. We take the 5 models we talked about. For each model we generate 10000 samples of the data, split into an equal sized (N=20 each) training and testing set. We fit the regression on the training set, and calculate the deviance on the training set. Notice how we have simply used the logpdf from scipy.stats. You can easily do this for other distributions.\nWe then use the fit to calculate the \\(\\mu\\) on the test set, and calculate the deviance there. We then find the average and the standard deviation across the 10000 simulations.\nWhy do we do 10000 simulations? These are our multiple samples from some hypothetical population.\n\nreps=10000\nresults_20 = {}\nfor k in range(1,6):\n    trdevs=np.zeros(reps)\n    tedevs=np.zeros(reps)\n    for r in range(reps):\n        Xtr, ytr, Xte, yte = generate_data(20, k)\n        ols = sm.OLS(ytr, Xtr).fit()\n        mutr = np.dot(Xtr, ols.params)\n        devtr = -2*np.sum(norm.logpdf(ytr, mutr, 1))\n        mute = np.dot(Xte, ols.params)\n        #print(mutr.shape, mute.shape)\n        devte = -2*np.sum(norm.logpdf(yte, mute, 1))\n        #print(k, r, devtr, devte)\n        trdevs[r] = devtr\n        tedevs[r] = devte\n    results_20[k] = (np.mean(trdevs), np.std(trdevs), np.mean(tedevs), np.std(tedevs))\n\n\nimport pandas as pd\ndf = pd.DataFrame(results_20).T\ndf = df.rename(columns = dict(zip(range(4), ['train', 'train_std', 'test', 'test_std'])))\ndf\n\n\n\n\n\n\n\n\ntrain\ntrain_std\ntest\ntest_std\n\n\n\n\n1\n55.669331\n6.185150\n57.688110\n6.825813\n\n\n2\n54.301259\n5.930990\n58.513912\n7.284745\n\n\n3\n50.669082\n4.778731\n56.111054\n6.668364\n\n\n4\n49.744479\n4.585390\n57.414846\n7.505377\n\n\n5\n49.026424\n4.431733\n58.718321\n8.279063\n\n\n\n\n\n\n\n\nimport seaborn.apionly as sns\ncolors = sns.color_palette()\ncolors\n\n[(0.12156862745098039, 0.4666666666666667, 0.7058823529411765),\n (1.0, 0.4980392156862745, 0.054901960784313725),\n (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),\n (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),\n (0.5803921568627451, 0.403921568627451, 0.7411764705882353),\n (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),\n (0.8901960784313725, 0.4666666666666667, 0.7607843137254902),\n (0.4980392156862745, 0.4980392156862745, 0.4980392156862745),\n (0.7372549019607844, 0.7411764705882353, 0.13333333333333333),\n (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)]\n\n\nWe plot the traing and testing deviances\n\nplt.plot(df.index, df.train, 'o', color = colors[0])\nplt.errorbar(df.index, df.train, yerr=df.train_std, fmt=None, color=colors[0])\nplt.plot(df.index+0.2, df.test, 'o', color = colors[1])\nplt.errorbar(df.index+0.2, df.test, yerr=df.test_std, fmt=None, color=colors[1])\nplt.xlabel(\"number of parameters\")\nplt.ylabel(\"deviance\")\nplt.title(\"N=20\");\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_axes.py:2818: MatplotlibDeprecationWarning: Use of None object as fmt keyword argument to suppress plotting of data values is deprecated since 1.4; use the string \"none\" instead.\n  warnings.warn(msg, mplDeprecation, stacklevel=1)\n\n\n\n\n\n\n\n\n\nNotice:\n\nthe best fit model may not be the original generating model. Remember that the choice of fit depends on the amount of data you have and the less data you have, the less parameters you should use\non average, out of sample deviance must be larger than in-sample deviance, through an individual pair may have that order reversed because of sample peculiarity."
  },
  {
    "objectID": "posts/understandingaic/index.html#aic-or-the-difference-in-deviances",
    "href": "posts/understandingaic/index.html#aic-or-the-difference-in-deviances",
    "title": "Understanding AIC",
    "section": "AIC, or the difference in deviances",
    "text": "AIC, or the difference in deviances\nLet us see the difference between the mean testing and training deviances. This is the difference in bias between the two sets.\n\ndf.test - df.train\n\n1    2.018779\n2    4.212654\n3    5.441971\n4    7.670368\n5    9.691897\ndtype: float64\n\n\nVoila, this seems to be roughly twice the number of parameters. In other words we might be able to get away without a test set if we “correct” the bias on the traing set by \\(2n_p\\). This is the observation that motivates the AIC.\n\nAnalysis N=100\n\nreps=10000\nresults_100 = {}\nfor k in range(1,6):\n    trdevs=np.zeros(reps)\n    tedevs=np.zeros(reps)\n    for r in range(reps):\n        Xtr, ytr, Xte, yte = generate_data(100, k)\n        ols = sm.OLS(ytr, Xtr).fit()\n        mutr = np.dot(Xtr, ols.params)\n        devtr = -2*np.sum(norm.logpdf(ytr, mutr, 1))\n        mute = np.dot(Xte, ols.params)\n        devte = -2*np.sum(norm.logpdf(yte, mute, 1))\n        #print(k, r, devtr, devte)\n        trdevs[r] = devtr\n        tedevs[r] = devte\n    results_100[k] = (np.mean(trdevs), np.std(trdevs), np.mean(tedevs), np.std(tedevs))\n\n\ndf100 = pd.DataFrame(results_100).T\ndf100 = df100.rename(columns = dict(zip(range(4), ['train', 'train_std', 'test', 'test_std'])))\ndf100\n\n\nplt.plot(df100.index, df100.train, 'o', color = colors[0])\nplt.errorbar(df100.index, df100.train, yerr=df100.train_std, fmt=None, color=colors[0])\nplt.plot(df100.index+0.2, df100.test, 'o', color = colors[1])\nplt.errorbar(df100.index+0.2, df100.test, yerr=df100.test_std, fmt=None, color=colors[1])\nplt.xlabel(\"number of parameters\")\nplt.ylabel(\"deviance\")\nplt.title(\"N=100\");\n\n\ndf100.test - df100.train\n\n1    1.900383\n2    3.908291\n3    5.193728\n4    7.003606\n5    8.649499\ndtype: float64\n\n\nWe get pretty much the same result at N=100."
  },
  {
    "objectID": "posts/understandingaic/index.html#assumptions-for-aic",
    "href": "posts/understandingaic/index.html#assumptions-for-aic",
    "title": "Understanding AIC",
    "section": "Assumptions for AIC",
    "text": "Assumptions for AIC\nThis observation leads to an estimate of the out-of-sample deviance by what is called an information criterion, the Akaike Information Criterion, or AIC:\n\\[AIC = D_{train} + 2n_p\\]\nwhich does carry as assumptions that\n\nthe likelihood is approximately multivariate gaussian\nthe sample size is much larger than the number of parameters\npriors are flat\nThe AIC does not assume that the true data generating process \\(p\\) is in the set of models being fitted. The overarching goal of the AIC approach to model selection is to select the “best” model for our given data set without assuming that the “true” model is in the family of models from which we’re selecting. The true model “cancels out” except in the expectation.\n\nWe wont derive the AIC here, but if you are interested, see http://www.stat.cmu.edu/~larry/=stat705/Lecture16.pdf\nWhy would we want to use such information criteria? Cross validation can be expensive, especially with multiple hyper-parameters."
  },
  {
    "objectID": "posts/understandingaic/index.html#aic-for-linear-regression",
    "href": "posts/understandingaic/index.html#aic-for-linear-regression",
    "title": "Understanding AIC",
    "section": "AIC for Linear Regression",
    "text": "AIC for Linear Regression\nThe AIC for a model is the training deviance plus twice the number of parameters:\n\\[AIC = D_{train} + 2n_p.\\]\nThat is, -2 times the log likelihood of the model.\nSo, one we find the MLE solution for the linear regression, we plugin the values we get, which are\n\\[\\sigma_{MLE}^2 =  \\frac{1}{N} RSS \\]\nwhere RSS is the sum of the squares of the errors.\n\\[AIC = -2(-\\frac{N}{2}(log(2\\pi) + log(\\sigma^2)) -2(-\\frac{1}{2\\sigma_{MLE}^2} \\times RSS) + 2p\\]\nThus:\n\\[D = Nlog(RSS/N) \\]\n\\[AIC = Nlog(RSS/N) + 2p + constant\\]\nSince the deviance for a OLS model is just proportional to the log(MSE) upto a proportionality, we’ll use the MSE to derive this split.\nThe fact that the (log-likelihood) and thus the deviance carries an expectation over the true distribution as estimated on the sample means that the Deviance is a stochastic quantity, varying from sample to sample."
  },
  {
    "objectID": "posts/understandingaic/index.html#a-complete-understanding-of-the-comparison-diagram",
    "href": "posts/understandingaic/index.html#a-complete-understanding-of-the-comparison-diagram",
    "title": "Understanding AIC",
    "section": "A complete understanding of the comparison diagram",
    "text": "A complete understanding of the comparison diagram\n(taken from McElreath, but see upstairs as well)\n\n\n\nIn-sample vs. out-of-sample deviance as model complexity increases, for N=20 and N=100. From McElreath, Statistical Rethinking.\n\n\nNow we are equipped to understand this diagram completely. Lets focus on the training (in) set first: blue points.\n\nThere is some irreducible noise which contributes to the deviance no matter the number of parameters.\nIf we could capture the true model exactly there would be no bias, and the deviance would go to that which comes from the irreducible noise.\nBut we cant, so the positions of the circles tells us how much bias plus irreducible noise we have\nThe error bars now tell us our variance, since they tell us how much our deviance, or MSE varies around our “mean” model. In real life our sample will lie somewhere along this error bar.\nThe training set deviances go down as the number of parameters increase. The test set deviances go down and then go up\nNotice that testing deviance is higher on a 2 parameter model than on a 1, even though our generating “true” model is a 2 parameter one. Deviance and the AIC do not pick the true model, but rather the one with the highest predictive accuracy."
  },
  {
    "objectID": "posts/switchpoint/index.html",
    "href": "posts/switchpoint/index.html",
    "title": "Imputation and Convergence",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n\n\nThis is a model of coal-mine diasaters in England. Somewhere around 1900, regulation was introduced, and in response, miing became safer. But if we were forensically looking at such data, we would be able to detect such change using a switchpoint model. We’d then have to search for the causality.\n\n\n\ndisasters_data = np.array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n                         3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n                         2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0,\n                         1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n                         0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n                         3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n                         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n\nn_years = len(disasters_data)\n\nplt.figure(figsize=(12.5, 3.5))\nplt.bar(np.arange(1851, 1962), disasters_data, color=\"#348ABD\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Disasters\")\nplt.title(\"UK coal mining disasters, 1851-1962\")\nplt.xlim(1851, 1962);\n\n\n\n\n\n\n\n\nOne can see the swtich roughly in the picture above.\n\n\n\nWe’ll assume a Poisson model for the mine disasters; appropriate because the counts are low.\n\\[\ny \\vert \\tau, \\lambda_1, \\lambda_2 \\sim Poisson(r_t)\\\\\nr_t = \\lambda_1 \\,{\\rm if}\\, t &lt; \\tau \\,{\\rm else}\\, \\lambda_2 \\,{\\rm for}\\, t \\in [t_l, t_h]\\\\\n\\tau \\sim DiscreteUniform(t_l, t_h)\\\\\n\\lambda_1 \\sim Exp(a)\\\\\n\\lambda_2 \\sim Exp(b)\\\\\n\\]\nThe rate parameter varies before and after the switchpoint, which itseld has a discrete-uniform prior on it. Rate parameters get exponential priors.\n\nimport pymc3 as pm\nfrom pymc3.math import switch\nwith pm.Model() as coaldis1:\n    early_mean = pm.Exponential('early_mean', 1)\n    late_mean = pm.Exponential('late_mean', 1)\n    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=n_years)\n    rate = switch(switchpoint &gt;= np.arange(n_years), early_mean, late_mean)\n    disasters = pm.Poisson('disasters', mu=rate, observed=disasters_data)\n\n\npm.model_to_graphviz(coaldis1)\n\n\n\n\n\n\n\n\nLet us interrogate our model about the various parts of it. Notice that our stochastics are logs of the rate params and the switchpoint, while our deterministics are the rate parameters themselves.\n\ncoaldis1.vars #stochastics\n\n[early_mean_log__, late_mean_log__, switchpoint]\n\n\n\ntype(coaldis1['early_mean_log__'])\n\npymc3.model.FreeRV\n\n\n\ncoaldis1.deterministics #deterministics\n\n[early_mean, late_mean]\n\n\nLabelled variables show up in traces, or for predictives. We also list the “likelihood” stochastics.\n\ncoaldis1.named_vars\n\n{'disasters': disasters,\n 'early_mean': early_mean,\n 'early_mean_log__': early_mean_log__,\n 'late_mean': late_mean,\n 'late_mean_log__': late_mean_log__,\n 'switchpoint': switchpoint}\n\n\n\ncoaldis1.observed_RVs, type(coaldis1['disasters'])\n\n([disasters], pymc3.model.ObservedRV)\n\n\nThe DAG based structure and notation used in pymc3 and similar software makes no distinction between random variables and data. Everything is a node, and some nodes are conditioned upon. This is reminiscent of the likelihood being considered a function of its parameters. But you can consider it as a function of data with fixed parameters and sample from it.\nYou can sample from the distributions in pymc3.\n\nplt.hist(switchpoint.random(size=1000));\n\n\n\n\n\n\n\n\n\nearly_mean.transformed, switchpoint.distribution\n\n(early_mean_log__,\n &lt;pymc3.distributions.discrete.DiscreteUniform at 0x129f73b00&gt;)\n\n\n\nswitchpoint.distribution.defaults\n\n('mode',)\n\n\n\ned=pm.Exponential.dist(1)\nprint(type(ed))\ned.random(size=10)\n\n&lt;class 'pymc3.distributions.continuous.Exponential'&gt;\n\n\narray([ 0.82466332,  0.10209366,  3.35122292,  0.22771453,  1.35351198,\n        0.697511  ,  0.04523932,  0.36786232,  0.12309128,  0.90947997])\n\n\n\ntype(switchpoint), type(early_mean)\n\n(pymc3.model.FreeRV, pymc3.model.TransformedRV)\n\n\nMost importantly, anything distribution-like must have a logp method. This is what enables calculating the acceptance ratio for sampling:\n\nswitchpoint.logp({'switchpoint':55, 'early_mean_log__':1, 'late_mean_log__':1})\n\narray(-4.718498871295094)\n\n\nOk, enough talk, lets sample:\n\nwith coaldis1:\n    #stepper=pm.Metropolis()\n    #trace = pm.sample(40000, step=stepper)\n    trace = pm.sample(40000)\n\nMultiprocess sampling (2 chains in 2 jobs)\nCompoundStep\n&gt;NUTS: [late_mean, early_mean]\n&gt;Metropolis: [switchpoint]\nSampling 2 chains: 100%|██████████| 81000/81000 [00:53&lt;00:00, 1522.59draws/s]\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\npm.summary(trace[4000::5])\n\n\n\n\n\n\n\n\nmean\nsd\nmc_error\nhpd_2.5\nhpd_97.5\nn_eff\nRhat\n\n\n\n\nswitchpoint\n38.983542\n2.421821\n0.027554\n33.000000\n43.000000\n7206.241420\n0.999931\n\n\nearly_mean\n3.070557\n0.283927\n0.002575\n2.537039\n3.641404\n13267.970663\n0.999966\n\n\nlate_mean\n0.936715\n0.118837\n0.001056\n0.709629\n1.174810\n13197.164982\n1.000034\n\n\n\n\n\n\n\n\nt2=trace[4000::5]\npm.traceplot(t2);\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_base.py:3449: MatplotlibDeprecationWarning: \nThe `ymin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `bottom` instead.\n  alternative='`bottom`', obj_type='argument')\n\n\n\n\n\n\n\n\n\nA forestplot gives us 95% credible intervals…\n\npm.forestplot(t2);\n\n\n\n\n\n\n\n\n\npm.autocorrplot(t2);\n\n\n\n\n\n\n\n\n\nplt.hist(trace['switchpoint']);\n\n\n\n\n\n\n\n\n\npm.trace_to_dataframe(t2).corr()\n\n\n\n\n\n\n\n\nswitchpoint\nearly_mean\nlate_mean\n\n\n\n\nswitchpoint\n1.000000\n-0.257867\n-0.235158\n\n\nearly_mean\n-0.257867\n1.000000\n0.058679\n\n\nlate_mean\n-0.235158\n0.058679\n1.000000\n\n\n\n\n\n\n\n\n\n\n\nImputation of missing data vaues has a very nice process in Bayesian stats: just sample them from the posterior predictive. There is a very nice process to do this built into pync3..you could abuse this to calculate predictives at arbitrary points. (There is a better way for that, though, using Theano shared variables, so you might want to restrict this process to the situation where you need to impute a few values only).\nBelow we use -999 to handle mising data:\n\ndisasters_missing = np.array([ 4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,\n1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n\n\ndisasters_masked = np.ma.masked_values(disasters_missing, value=-999)\ndisasters_masked\n\nmasked_array(data = [4 5 4 0 1 4 3 4 0 6 3 3 4 0 2 6 3 3 5 4 5 3 1 4 4 1 5 5 3 4 2 5 2 2 3 4 2\n 1 3 -- 2 1 1 1 1 3 0 0 1 0 1 1 0 0 3 1 0 3 2 2 0 1 1 1 0 1 0 1 0 0 0 2 1 0\n 0 0 1 1 0 2 3 3 1 -- 2 1 1 1 1 2 4 2 0 0 1 4 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1],\n             mask = [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False  True False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False  True\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False],\n       fill_value = -999)\n\n\n\nwith pm.Model() as missing_data_model:\n    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=len(disasters_masked))\n    early_mean = pm.Exponential('early_mean', lam=1.)\n    late_mean = pm.Exponential('late_mean', lam=1.)\n    idx = np.arange(len(disasters_masked))\n    rate = pm.Deterministic('rate', switch(switchpoint &gt;= idx, early_mean, late_mean))\n    disasters = pm.Poisson('disasters', rate, observed=disasters_masked)\n\n\npm.model_to_graphviz(missing_data_model)\n\n\n\n\n\n\n\n\nBy supplying a masked array to the likelihood part of our model, we ensure that the masked data points show up in our traces:\n\nwith missing_data_model:\n    stepper=pm.Metropolis()\n    trace_missing = pm.sample(40000, step=stepper)\n\nMultiprocess sampling (2 chains in 2 jobs)\nCompoundStep\n&gt;Metropolis: [disasters_missing]\n&gt;Metropolis: [late_mean]\n&gt;Metropolis: [early_mean]\n&gt;Metropolis: [switchpoint]\nSampling 2 chains: 100%|██████████| 81000/81000 [00:40&lt;00:00, 2008.25draws/s]\nThe number of effective samples is smaller than 10% for some parameters.\n\n\n\ntm2=trace_missing[4000::5]\n\n\npm.summary(tm2)\n\n\n\n\n\n\n\n\nmean\nsd\nmc_error\nhpd_2.5\nhpd_97.5\nn_eff\nRhat\n\n\n\n\nswitchpoint\n38.721806\n2.460939\n0.043931\n34.000000\n43.000000\n3701.337711\n1.000138\n\n\ndisasters_missing__0\n2.100694\n1.790212\n0.039944\n0.000000\n5.000000\n2415.221405\n0.999964\n\n\ndisasters_missing__1\n0.907778\n0.945733\n0.010209\n0.000000\n3.000000\n6751.376254\n1.000121\n\n\nearly_mean\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nlate_mean\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__0\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__1\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__2\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__3\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__4\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__5\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__6\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__7\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__8\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__9\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__10\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__11\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__12\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__13\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__14\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__15\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__16\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__17\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__18\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__19\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__20\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__21\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__22\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__23\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__24\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrate__81\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__82\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__83\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__84\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__85\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__86\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__87\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__88\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__89\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__90\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__91\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__92\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__93\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__94\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__95\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__96\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__97\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__98\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__99\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__100\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__101\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__102\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__103\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__104\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__105\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__106\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__107\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__108\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__109\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__110\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\n\n\n116 rows × 7 columns\n\n\n\n\nmissing_data_model.vars\n\n[switchpoint, early_mean_log__, late_mean_log__, disasters_missing]\n\n\n\npm.traceplot(tm2);\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_base.py:3449: MatplotlibDeprecationWarning: \nThe `ymin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `bottom` instead.\n  alternative='`bottom`', obj_type='argument')\n\n\n\n\n\n\n\n\n\n\n\n\nGoing back to the original model…\n\n\nAs a visual check, we plot histograms or kdeplots every 500 samples and check that they look identical.\n\nimport matplotlib.pyplot as plt\n\nemtrace = t2['early_mean']\n\nfig, axes = plt.subplots(2, 5, figsize=(14,6))\naxes = axes.ravel()\nfor i in range(10):\n    axes[i].hist(emtrace[500*i:500*(i+1)], normed=True, alpha=0.2)\n    sns.kdeplot(emtrace[500*i:500*(i+1)], ax=axes[i])\nplt.tight_layout()\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \nThe 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n  alternative=\"'density'\", removal=\"3.1\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe gewecke test tests that the difference of means of chain-parts written as a Z-score oscilates between 1 and -1\n\\[\\vert \\mu_{\\theta_1}  - \\mu_{\\theta_2}  \\vert &lt; 2 \\sigma_{\\theta_1 - \\theta_2} \\]\n\nfrom pymc3 import geweke\n    \nz = geweke(t2, intervals=15)[0]\n\n\nz\n\n{'early_mean': array([[  0.00000000e+00,   5.61498762e-02],\n        [  2.57000000e+02,   3.30181736e-02],\n        [  5.14000000e+02,  -1.13386479e-02],\n        [  7.71000000e+02,  -1.08683708e-02],\n        [  1.02800000e+03,  -3.18609424e-02],\n        [  1.28500000e+03,  -2.44007294e-02],\n        [  1.54200000e+03,  -9.42940333e-03],\n        [  1.79900000e+03,   1.38612211e-02],\n        [  2.05600000e+03,   2.44459326e-02],\n        [  2.31300000e+03,   1.76236156e-02],\n        [  2.57000000e+03,  -1.83317207e-02],\n        [  2.82700000e+03,  -2.08740078e-02],\n        [  3.08400000e+03,  -3.78760002e-02],\n        [  3.34100000e+03,  -2.33616055e-02],\n        [  3.59800000e+03,  -8.93753177e-02]]),\n 'early_mean_log__': array([[  0.00000000e+00,   5.40712421e-02],\n        [  2.57000000e+02,   3.20225634e-02],\n        [  5.14000000e+02,  -1.21207399e-02],\n        [  7.71000000e+02,  -1.53444650e-02],\n        [  1.02800000e+03,  -3.41697516e-02],\n        [  1.28500000e+03,  -2.60208461e-02],\n        [  1.54200000e+03,  -9.58032469e-03],\n        [  1.79900000e+03,   1.20771272e-02],\n        [  2.05600000e+03,   2.27472686e-02],\n        [  2.31300000e+03,   1.62502583e-02],\n        [  2.57000000e+03,  -1.83595794e-02],\n        [  2.82700000e+03,  -2.06877678e-02],\n        [  3.08400000e+03,  -3.68628168e-02],\n        [  3.34100000e+03,  -1.96692332e-02],\n        [  3.59800000e+03,  -9.00174724e-02]]),\n 'late_mean': array([[  0.00000000e+00,   4.08287159e-02],\n        [  2.57000000e+02,  -2.92363937e-02],\n        [  5.14000000e+02,  -5.38270045e-02],\n        [  7.71000000e+02,   5.63678903e-03],\n        [  1.02800000e+03,   1.34028136e-02],\n        [  1.28500000e+03,   6.38604399e-02],\n        [  1.54200000e+03,   4.89896588e-02],\n        [  1.79900000e+03,   6.99272457e-02],\n        [  2.05600000e+03,   3.55314031e-02],\n        [  2.31300000e+03,   9.88599384e-03],\n        [  2.57000000e+03,   1.72258915e-02],\n        [  2.82700000e+03,   4.12609613e-02],\n        [  3.08400000e+03,   2.16946625e-02],\n        [  3.34100000e+03,   3.15327875e-02],\n        [  3.59800000e+03,   1.50735157e-02]]),\n 'late_mean_log__': array([[  0.00000000e+00,   4.19113445e-02],\n        [  2.57000000e+02,  -2.82149481e-02],\n        [  5.14000000e+02,  -5.57252602e-02],\n        [  7.71000000e+02,   4.42927658e-04],\n        [  1.02800000e+03,   1.01563960e-02],\n        [  1.28500000e+03,   6.69668358e-02],\n        [  1.54200000e+03,   5.47312239e-02],\n        [  1.79900000e+03,   7.11500876e-02],\n        [  2.05600000e+03,   3.36857501e-02],\n        [  2.31300000e+03,   7.20640643e-03],\n        [  2.57000000e+03,   1.52483860e-02],\n        [  2.82700000e+03,   3.62744473e-02],\n        [  3.08400000e+03,   2.07380857e-02],\n        [  3.34100000e+03,   2.89119928e-02],\n        [  3.59800000e+03,   1.12619091e-02]]),\n 'switchpoint': array([[  0.00000000e+00,  -4.39653271e-02],\n        [  2.57000000e+02,  -2.69927320e-02],\n        [  5.14000000e+02,   2.04644938e-02],\n        [  7.71000000e+02,  -1.34952058e-02],\n        [  1.02800000e+03,   9.72819474e-05],\n        [  1.28500000e+03,  -2.60017648e-02],\n        [  1.54200000e+03,  -9.28102162e-02],\n        [  1.79900000e+03,  -9.90854028e-02],\n        [  2.05600000e+03,  -1.61978444e-02],\n        [  2.31300000e+03,  -1.96622006e-03],\n        [  2.57000000e+03,  -7.48155186e-02],\n        [  2.82700000e+03,  -4.25450296e-02],\n        [  3.08400000e+03,  -2.14223408e-04],\n        [  3.34100000e+03,  -2.63551341e-02],\n        [  3.59800000e+03,   5.57747320e-03]])}\n\n\nHere is a plot for early_mean. You sould really be plotting all of these…\n\nz['early_mean'].T\n\narray([[  0.00000000e+00,   2.57000000e+02,   5.14000000e+02,\n          7.71000000e+02,   1.02800000e+03,   1.28500000e+03,\n          1.54200000e+03,   1.79900000e+03,   2.05600000e+03,\n          2.31300000e+03,   2.57000000e+03,   2.82700000e+03,\n          3.08400000e+03,   3.34100000e+03,   3.59800000e+03],\n       [  5.61498762e-02,   3.30181736e-02,  -1.13386479e-02,\n         -1.08683708e-02,  -3.18609424e-02,  -2.44007294e-02,\n         -9.42940333e-03,   1.38612211e-02,   2.44459326e-02,\n          1.76236156e-02,  -1.83317207e-02,  -2.08740078e-02,\n         -3.78760002e-02,  -2.33616055e-02,  -8.93753177e-02]])\n\n\n\nplt.scatter(*z['early_mean'].T)\nplt.axhline(-1, 0, 1, linestyle='dotted')\nplt.axhline(1, 0, 1, linestyle='dotted')\n\n\n\n\n\n\n\n\n\n\n\nFor this test, which calculates\n\\[\\hat{R} = \\sqrt{\\frac{\\hat{Var}(\\theta)}{w}}\\]\nwe need more than 1-chain. This is done through njobs=4 (the defaukt is 2 and reported in pm.summary). See the trace below:\n\nwith coaldis1:\n    stepper=pm.Metropolis()\n    tr2 = pm.sample(40000, step=stepper, njobs=4)\n\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;Metropolis: [switchpoint]\n&gt;Metropolis: [late_mean]\n&gt;Metropolis: [early_mean]\nSampling 4 chains: 100%|██████████| 162000/162000 [00:57&lt;00:00, 2837.84draws/s]\nThe number of effective samples is smaller than 10% for some parameters.\n\n\n\ntr2\n\n&lt;MultiTrace: 4 chains, 40000 iterations, 5 variables&gt;\n\n\n\ntr2_cut = tr2[4000::5]\n\n\nfrom pymc3 import gelman_rubin\n\ngelman_rubin(tr2_cut)\n\n{'early_mean': 1.0001442735491479,\n 'late_mean': 1.0000078726931823,\n 'switchpoint': 1.0002808010976048}\n\n\nFor the best results, each chain should be initialized to highly dispersed starting values for each stochastic node.\nA foresplot will show you the credible-interval consistency of our chains..\n\nfrom pymc3 import forestplot\n\nforestplot(tr2_cut)\n\nGridSpec(1, 2, width_ratios=[3, 1])\n\n\n\n\n\n\n\n\n\n\n\n\nThis can be probed by plotting the correlation plot and effective sample size\n\nfrom pymc3 import effective_n\n\neffective_n(tr2_cut)\n\n{'early_mean': 13037.380191598249,\n 'late_mean': 15375.337202610448,\n 'switchpoint': 11955.470326961806}\n\n\n\npm.autocorrplot(tr2_cut);\n\n\n\n\n\n\n\n\n\npm.autocorrplot(tr2);\n\n\n\n\n\n\n\n\n\n\n\n\nFinally let us peek into posterior predictive checks: something we’ll talk more about soon.\n\nwith coaldis1:\n    sim = pm.sample_ppc(t2, samples=200)\n\n100%|██████████| 200/200 [00:02&lt;00:00, 99.38it/s]\n\n\n\nsim['disasters'].shape\n\n(200, 111)\n\n\nThis gives us 200 samples at each of the 111 diasters we have data on.\nWe plot the first 4 posteriors against actual data for consistency…\n\nfig, axes = plt.subplots(1, 4, figsize=(12, 6))\nprint(axes.shape)\nfor obs, s, ax in zip(disasters_data, sim['disasters'].T, axes):\n    print(obs)\n    ax.hist(s, bins=10)\n    ax.plot(obs+0.5, 1, 'ro')\n\n(4,)\n4\n5\n4\n0"
  },
  {
    "objectID": "posts/switchpoint/index.html#a-switchpoint-model",
    "href": "posts/switchpoint/index.html#a-switchpoint-model",
    "title": "Imputation and Convergence",
    "section": "",
    "text": "This is a model of coal-mine diasaters in England. Somewhere around 1900, regulation was introduced, and in response, miing became safer. But if we were forensically looking at such data, we would be able to detect such change using a switchpoint model. We’d then have to search for the causality.\n\n\n\ndisasters_data = np.array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n                         3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n                         2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0,\n                         1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n                         0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n                         3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n                         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n\nn_years = len(disasters_data)\n\nplt.figure(figsize=(12.5, 3.5))\nplt.bar(np.arange(1851, 1962), disasters_data, color=\"#348ABD\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Disasters\")\nplt.title(\"UK coal mining disasters, 1851-1962\")\nplt.xlim(1851, 1962);\n\n\n\n\n\n\n\n\nOne can see the swtich roughly in the picture above.\n\n\n\nWe’ll assume a Poisson model for the mine disasters; appropriate because the counts are low.\n\\[\ny \\vert \\tau, \\lambda_1, \\lambda_2 \\sim Poisson(r_t)\\\\\nr_t = \\lambda_1 \\,{\\rm if}\\, t &lt; \\tau \\,{\\rm else}\\, \\lambda_2 \\,{\\rm for}\\, t \\in [t_l, t_h]\\\\\n\\tau \\sim DiscreteUniform(t_l, t_h)\\\\\n\\lambda_1 \\sim Exp(a)\\\\\n\\lambda_2 \\sim Exp(b)\\\\\n\\]\nThe rate parameter varies before and after the switchpoint, which itseld has a discrete-uniform prior on it. Rate parameters get exponential priors.\n\nimport pymc3 as pm\nfrom pymc3.math import switch\nwith pm.Model() as coaldis1:\n    early_mean = pm.Exponential('early_mean', 1)\n    late_mean = pm.Exponential('late_mean', 1)\n    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=n_years)\n    rate = switch(switchpoint &gt;= np.arange(n_years), early_mean, late_mean)\n    disasters = pm.Poisson('disasters', mu=rate, observed=disasters_data)\n\n\npm.model_to_graphviz(coaldis1)\n\n\n\n\n\n\n\n\nLet us interrogate our model about the various parts of it. Notice that our stochastics are logs of the rate params and the switchpoint, while our deterministics are the rate parameters themselves.\n\ncoaldis1.vars #stochastics\n\n[early_mean_log__, late_mean_log__, switchpoint]\n\n\n\ntype(coaldis1['early_mean_log__'])\n\npymc3.model.FreeRV\n\n\n\ncoaldis1.deterministics #deterministics\n\n[early_mean, late_mean]\n\n\nLabelled variables show up in traces, or for predictives. We also list the “likelihood” stochastics.\n\ncoaldis1.named_vars\n\n{'disasters': disasters,\n 'early_mean': early_mean,\n 'early_mean_log__': early_mean_log__,\n 'late_mean': late_mean,\n 'late_mean_log__': late_mean_log__,\n 'switchpoint': switchpoint}\n\n\n\ncoaldis1.observed_RVs, type(coaldis1['disasters'])\n\n([disasters], pymc3.model.ObservedRV)\n\n\nThe DAG based structure and notation used in pymc3 and similar software makes no distinction between random variables and data. Everything is a node, and some nodes are conditioned upon. This is reminiscent of the likelihood being considered a function of its parameters. But you can consider it as a function of data with fixed parameters and sample from it.\nYou can sample from the distributions in pymc3.\n\nplt.hist(switchpoint.random(size=1000));\n\n\n\n\n\n\n\n\n\nearly_mean.transformed, switchpoint.distribution\n\n(early_mean_log__,\n &lt;pymc3.distributions.discrete.DiscreteUniform at 0x129f73b00&gt;)\n\n\n\nswitchpoint.distribution.defaults\n\n('mode',)\n\n\n\ned=pm.Exponential.dist(1)\nprint(type(ed))\ned.random(size=10)\n\n&lt;class 'pymc3.distributions.continuous.Exponential'&gt;\n\n\narray([ 0.82466332,  0.10209366,  3.35122292,  0.22771453,  1.35351198,\n        0.697511  ,  0.04523932,  0.36786232,  0.12309128,  0.90947997])\n\n\n\ntype(switchpoint), type(early_mean)\n\n(pymc3.model.FreeRV, pymc3.model.TransformedRV)\n\n\nMost importantly, anything distribution-like must have a logp method. This is what enables calculating the acceptance ratio for sampling:\n\nswitchpoint.logp({'switchpoint':55, 'early_mean_log__':1, 'late_mean_log__':1})\n\narray(-4.718498871295094)\n\n\nOk, enough talk, lets sample:\n\nwith coaldis1:\n    #stepper=pm.Metropolis()\n    #trace = pm.sample(40000, step=stepper)\n    trace = pm.sample(40000)\n\nMultiprocess sampling (2 chains in 2 jobs)\nCompoundStep\n&gt;NUTS: [late_mean, early_mean]\n&gt;Metropolis: [switchpoint]\nSampling 2 chains: 100%|██████████| 81000/81000 [00:53&lt;00:00, 1522.59draws/s]\nThe number of effective samples is smaller than 25% for some parameters.\n\n\n\npm.summary(trace[4000::5])\n\n\n\n\n\n\n\n\nmean\nsd\nmc_error\nhpd_2.5\nhpd_97.5\nn_eff\nRhat\n\n\n\n\nswitchpoint\n38.983542\n2.421821\n0.027554\n33.000000\n43.000000\n7206.241420\n0.999931\n\n\nearly_mean\n3.070557\n0.283927\n0.002575\n2.537039\n3.641404\n13267.970663\n0.999966\n\n\nlate_mean\n0.936715\n0.118837\n0.001056\n0.709629\n1.174810\n13197.164982\n1.000034\n\n\n\n\n\n\n\n\nt2=trace[4000::5]\npm.traceplot(t2);\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_base.py:3449: MatplotlibDeprecationWarning: \nThe `ymin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `bottom` instead.\n  alternative='`bottom`', obj_type='argument')\n\n\n\n\n\n\n\n\n\nA forestplot gives us 95% credible intervals…\n\npm.forestplot(t2);\n\n\n\n\n\n\n\n\n\npm.autocorrplot(t2);\n\n\n\n\n\n\n\n\n\nplt.hist(trace['switchpoint']);\n\n\n\n\n\n\n\n\n\npm.trace_to_dataframe(t2).corr()\n\n\n\n\n\n\n\n\nswitchpoint\nearly_mean\nlate_mean\n\n\n\n\nswitchpoint\n1.000000\n-0.257867\n-0.235158\n\n\nearly_mean\n-0.257867\n1.000000\n0.058679\n\n\nlate_mean\n-0.235158\n0.058679\n1.000000"
  },
  {
    "objectID": "posts/switchpoint/index.html#imputation",
    "href": "posts/switchpoint/index.html#imputation",
    "title": "Imputation and Convergence",
    "section": "",
    "text": "Imputation of missing data vaues has a very nice process in Bayesian stats: just sample them from the posterior predictive. There is a very nice process to do this built into pync3..you could abuse this to calculate predictives at arbitrary points. (There is a better way for that, though, using Theano shared variables, so you might want to restrict this process to the situation where you need to impute a few values only).\nBelow we use -999 to handle mising data:\n\ndisasters_missing = np.array([ 4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,\n1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])\n\n\ndisasters_masked = np.ma.masked_values(disasters_missing, value=-999)\ndisasters_masked\n\nmasked_array(data = [4 5 4 0 1 4 3 4 0 6 3 3 4 0 2 6 3 3 5 4 5 3 1 4 4 1 5 5 3 4 2 5 2 2 3 4 2\n 1 3 -- 2 1 1 1 1 3 0 0 1 0 1 1 0 0 3 1 0 3 2 2 0 1 1 1 0 1 0 1 0 0 0 2 1 0\n 0 0 1 1 0 2 3 3 1 -- 2 1 1 1 1 2 4 2 0 0 1 4 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1],\n             mask = [False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False  True False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False False False False False False False False False  True\n False False False False False False False False False False False False\n False False False False False False False False False False False False\n False False False],\n       fill_value = -999)\n\n\n\nwith pm.Model() as missing_data_model:\n    switchpoint = pm.DiscreteUniform('switchpoint', lower=0, upper=len(disasters_masked))\n    early_mean = pm.Exponential('early_mean', lam=1.)\n    late_mean = pm.Exponential('late_mean', lam=1.)\n    idx = np.arange(len(disasters_masked))\n    rate = pm.Deterministic('rate', switch(switchpoint &gt;= idx, early_mean, late_mean))\n    disasters = pm.Poisson('disasters', rate, observed=disasters_masked)\n\n\npm.model_to_graphviz(missing_data_model)\n\n\n\n\n\n\n\n\nBy supplying a masked array to the likelihood part of our model, we ensure that the masked data points show up in our traces:\n\nwith missing_data_model:\n    stepper=pm.Metropolis()\n    trace_missing = pm.sample(40000, step=stepper)\n\nMultiprocess sampling (2 chains in 2 jobs)\nCompoundStep\n&gt;Metropolis: [disasters_missing]\n&gt;Metropolis: [late_mean]\n&gt;Metropolis: [early_mean]\n&gt;Metropolis: [switchpoint]\nSampling 2 chains: 100%|██████████| 81000/81000 [00:40&lt;00:00, 2008.25draws/s]\nThe number of effective samples is smaller than 10% for some parameters.\n\n\n\ntm2=trace_missing[4000::5]\n\n\npm.summary(tm2)\n\n\n\n\n\n\n\n\nmean\nsd\nmc_error\nhpd_2.5\nhpd_97.5\nn_eff\nRhat\n\n\n\n\nswitchpoint\n38.721806\n2.460939\n0.043931\n34.000000\n43.000000\n3701.337711\n1.000138\n\n\ndisasters_missing__0\n2.100694\n1.790212\n0.039944\n0.000000\n5.000000\n2415.221405\n0.999964\n\n\ndisasters_missing__1\n0.907778\n0.945733\n0.010209\n0.000000\n3.000000\n6751.376254\n1.000121\n\n\nearly_mean\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nlate_mean\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__0\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__1\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__2\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__3\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__4\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__5\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__6\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__7\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__8\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__9\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__10\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__11\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__12\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__13\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__14\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__15\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__16\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__17\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__18\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__19\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__20\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__21\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__22\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__23\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\nrate__24\n3.086498\n0.288385\n0.003494\n2.547688\n3.681163\n6722.815884\n0.999999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrate__81\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__82\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__83\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__84\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__85\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__86\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__87\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__88\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__89\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__90\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__91\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__92\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__93\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__94\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__95\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__96\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__97\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__98\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__99\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__100\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__101\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__102\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__103\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__104\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__105\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__106\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__107\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__108\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__109\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\nrate__110\n0.931478\n0.118384\n0.001440\n0.713553\n1.177016\n7011.001490\n1.000466\n\n\n\n\n116 rows × 7 columns\n\n\n\n\nmissing_data_model.vars\n\n[switchpoint, early_mean_log__, late_mean_log__, disasters_missing]\n\n\n\npm.traceplot(tm2);\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_base.py:3449: MatplotlibDeprecationWarning: \nThe `ymin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `bottom` instead.\n  alternative='`bottom`', obj_type='argument')"
  },
  {
    "objectID": "posts/switchpoint/index.html#convergence-of-our-model",
    "href": "posts/switchpoint/index.html#convergence-of-our-model",
    "title": "Imputation and Convergence",
    "section": "",
    "text": "Going back to the original model…\n\n\nAs a visual check, we plot histograms or kdeplots every 500 samples and check that they look identical.\n\nimport matplotlib.pyplot as plt\n\nemtrace = t2['early_mean']\n\nfig, axes = plt.subplots(2, 5, figsize=(14,6))\naxes = axes.ravel()\nfor i in range(10):\n    axes[i].hist(emtrace[500*i:500*(i+1)], normed=True, alpha=0.2)\n    sns.kdeplot(emtrace[500*i:500*(i+1)], ax=axes[i])\nplt.tight_layout()\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6499: MatplotlibDeprecationWarning: \nThe 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n  alternative=\"'density'\", removal=\"3.1\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe gewecke test tests that the difference of means of chain-parts written as a Z-score oscilates between 1 and -1\n\\[\\vert \\mu_{\\theta_1}  - \\mu_{\\theta_2}  \\vert &lt; 2 \\sigma_{\\theta_1 - \\theta_2} \\]\n\nfrom pymc3 import geweke\n    \nz = geweke(t2, intervals=15)[0]\n\n\nz\n\n{'early_mean': array([[  0.00000000e+00,   5.61498762e-02],\n        [  2.57000000e+02,   3.30181736e-02],\n        [  5.14000000e+02,  -1.13386479e-02],\n        [  7.71000000e+02,  -1.08683708e-02],\n        [  1.02800000e+03,  -3.18609424e-02],\n        [  1.28500000e+03,  -2.44007294e-02],\n        [  1.54200000e+03,  -9.42940333e-03],\n        [  1.79900000e+03,   1.38612211e-02],\n        [  2.05600000e+03,   2.44459326e-02],\n        [  2.31300000e+03,   1.76236156e-02],\n        [  2.57000000e+03,  -1.83317207e-02],\n        [  2.82700000e+03,  -2.08740078e-02],\n        [  3.08400000e+03,  -3.78760002e-02],\n        [  3.34100000e+03,  -2.33616055e-02],\n        [  3.59800000e+03,  -8.93753177e-02]]),\n 'early_mean_log__': array([[  0.00000000e+00,   5.40712421e-02],\n        [  2.57000000e+02,   3.20225634e-02],\n        [  5.14000000e+02,  -1.21207399e-02],\n        [  7.71000000e+02,  -1.53444650e-02],\n        [  1.02800000e+03,  -3.41697516e-02],\n        [  1.28500000e+03,  -2.60208461e-02],\n        [  1.54200000e+03,  -9.58032469e-03],\n        [  1.79900000e+03,   1.20771272e-02],\n        [  2.05600000e+03,   2.27472686e-02],\n        [  2.31300000e+03,   1.62502583e-02],\n        [  2.57000000e+03,  -1.83595794e-02],\n        [  2.82700000e+03,  -2.06877678e-02],\n        [  3.08400000e+03,  -3.68628168e-02],\n        [  3.34100000e+03,  -1.96692332e-02],\n        [  3.59800000e+03,  -9.00174724e-02]]),\n 'late_mean': array([[  0.00000000e+00,   4.08287159e-02],\n        [  2.57000000e+02,  -2.92363937e-02],\n        [  5.14000000e+02,  -5.38270045e-02],\n        [  7.71000000e+02,   5.63678903e-03],\n        [  1.02800000e+03,   1.34028136e-02],\n        [  1.28500000e+03,   6.38604399e-02],\n        [  1.54200000e+03,   4.89896588e-02],\n        [  1.79900000e+03,   6.99272457e-02],\n        [  2.05600000e+03,   3.55314031e-02],\n        [  2.31300000e+03,   9.88599384e-03],\n        [  2.57000000e+03,   1.72258915e-02],\n        [  2.82700000e+03,   4.12609613e-02],\n        [  3.08400000e+03,   2.16946625e-02],\n        [  3.34100000e+03,   3.15327875e-02],\n        [  3.59800000e+03,   1.50735157e-02]]),\n 'late_mean_log__': array([[  0.00000000e+00,   4.19113445e-02],\n        [  2.57000000e+02,  -2.82149481e-02],\n        [  5.14000000e+02,  -5.57252602e-02],\n        [  7.71000000e+02,   4.42927658e-04],\n        [  1.02800000e+03,   1.01563960e-02],\n        [  1.28500000e+03,   6.69668358e-02],\n        [  1.54200000e+03,   5.47312239e-02],\n        [  1.79900000e+03,   7.11500876e-02],\n        [  2.05600000e+03,   3.36857501e-02],\n        [  2.31300000e+03,   7.20640643e-03],\n        [  2.57000000e+03,   1.52483860e-02],\n        [  2.82700000e+03,   3.62744473e-02],\n        [  3.08400000e+03,   2.07380857e-02],\n        [  3.34100000e+03,   2.89119928e-02],\n        [  3.59800000e+03,   1.12619091e-02]]),\n 'switchpoint': array([[  0.00000000e+00,  -4.39653271e-02],\n        [  2.57000000e+02,  -2.69927320e-02],\n        [  5.14000000e+02,   2.04644938e-02],\n        [  7.71000000e+02,  -1.34952058e-02],\n        [  1.02800000e+03,   9.72819474e-05],\n        [  1.28500000e+03,  -2.60017648e-02],\n        [  1.54200000e+03,  -9.28102162e-02],\n        [  1.79900000e+03,  -9.90854028e-02],\n        [  2.05600000e+03,  -1.61978444e-02],\n        [  2.31300000e+03,  -1.96622006e-03],\n        [  2.57000000e+03,  -7.48155186e-02],\n        [  2.82700000e+03,  -4.25450296e-02],\n        [  3.08400000e+03,  -2.14223408e-04],\n        [  3.34100000e+03,  -2.63551341e-02],\n        [  3.59800000e+03,   5.57747320e-03]])}\n\n\nHere is a plot for early_mean. You sould really be plotting all of these…\n\nz['early_mean'].T\n\narray([[  0.00000000e+00,   2.57000000e+02,   5.14000000e+02,\n          7.71000000e+02,   1.02800000e+03,   1.28500000e+03,\n          1.54200000e+03,   1.79900000e+03,   2.05600000e+03,\n          2.31300000e+03,   2.57000000e+03,   2.82700000e+03,\n          3.08400000e+03,   3.34100000e+03,   3.59800000e+03],\n       [  5.61498762e-02,   3.30181736e-02,  -1.13386479e-02,\n         -1.08683708e-02,  -3.18609424e-02,  -2.44007294e-02,\n         -9.42940333e-03,   1.38612211e-02,   2.44459326e-02,\n          1.76236156e-02,  -1.83317207e-02,  -2.08740078e-02,\n         -3.78760002e-02,  -2.33616055e-02,  -8.93753177e-02]])\n\n\n\nplt.scatter(*z['early_mean'].T)\nplt.axhline(-1, 0, 1, linestyle='dotted')\nplt.axhline(1, 0, 1, linestyle='dotted')\n\n\n\n\n\n\n\n\n\n\n\nFor this test, which calculates\n\\[\\hat{R} = \\sqrt{\\frac{\\hat{Var}(\\theta)}{w}}\\]\nwe need more than 1-chain. This is done through njobs=4 (the defaukt is 2 and reported in pm.summary). See the trace below:\n\nwith coaldis1:\n    stepper=pm.Metropolis()\n    tr2 = pm.sample(40000, step=stepper, njobs=4)\n\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;Metropolis: [switchpoint]\n&gt;Metropolis: [late_mean]\n&gt;Metropolis: [early_mean]\nSampling 4 chains: 100%|██████████| 162000/162000 [00:57&lt;00:00, 2837.84draws/s]\nThe number of effective samples is smaller than 10% for some parameters.\n\n\n\ntr2\n\n&lt;MultiTrace: 4 chains, 40000 iterations, 5 variables&gt;\n\n\n\ntr2_cut = tr2[4000::5]\n\n\nfrom pymc3 import gelman_rubin\n\ngelman_rubin(tr2_cut)\n\n{'early_mean': 1.0001442735491479,\n 'late_mean': 1.0000078726931823,\n 'switchpoint': 1.0002808010976048}\n\n\nFor the best results, each chain should be initialized to highly dispersed starting values for each stochastic node.\nA foresplot will show you the credible-interval consistency of our chains..\n\nfrom pymc3 import forestplot\n\nforestplot(tr2_cut)\n\nGridSpec(1, 2, width_ratios=[3, 1])\n\n\n\n\n\n\n\n\n\n\n\n\nThis can be probed by plotting the correlation plot and effective sample size\n\nfrom pymc3 import effective_n\n\neffective_n(tr2_cut)\n\n{'early_mean': 13037.380191598249,\n 'late_mean': 15375.337202610448,\n 'switchpoint': 11955.470326961806}\n\n\n\npm.autocorrplot(tr2_cut);\n\n\n\n\n\n\n\n\n\npm.autocorrplot(tr2);"
  },
  {
    "objectID": "posts/switchpoint/index.html#posterior-predictive-checks",
    "href": "posts/switchpoint/index.html#posterior-predictive-checks",
    "title": "Imputation and Convergence",
    "section": "",
    "text": "Finally let us peek into posterior predictive checks: something we’ll talk more about soon.\n\nwith coaldis1:\n    sim = pm.sample_ppc(t2, samples=200)\n\n100%|██████████| 200/200 [00:02&lt;00:00, 99.38it/s]\n\n\n\nsim['disasters'].shape\n\n(200, 111)\n\n\nThis gives us 200 samples at each of the 111 diasters we have data on.\nWe plot the first 4 posteriors against actual data for consistency…\n\nfig, axes = plt.subplots(1, 4, figsize=(12, 6))\nprint(axes.shape)\nfor obs, s, ax in zip(disasters_data, sim['disasters'].T, axes):\n    print(obs)\n    ax.hist(s, bins=10)\n    ax.plot(obs+0.5, 1, 'ro')\n\n(4,)\n4\n5\n4\n0"
  },
  {
    "objectID": "posts/dataaug/index.html",
    "href": "posts/dataaug/index.html",
    "title": "Data Augmentation",
    "section": "",
    "text": "The idea is to construct iterative algorithms for sampling based on the introduction of unobserved data or hidden variables. Does the iterative part sound familiar? We did that in Gibbs sampling.\nWe’ll soon see s deterministic version of this idea when we talk about the Expectation Maximization Algorithm (Dempster, Laird, and Rubin (1977)). Here we’ll see a stochastic version from Tanner and Wong’s (1987) Data Augmentation algorithm for posterior sampling. This was also explored in the physics by Swendsen and Wang’s (1987) algorithm for sampling from Ising and Potts models. (Look it up, it relates to your homework!)\nIndeed the general idea of introducing a hidden variable will also be exploited to introduce slice sampling and Hamiltonian Monte Carlo. Thus we shall see that the method is useful not only in “theory” to understand the decomposition of outcomes through hidden factors, but also in a practical way to construct sampling algorithms\nThe difference from Gibbs Sampling here is that we are thinking of a 1 (or lower) dimensional distribution or posterior we want to sample from, say \\(x\\), and the other variable, say \\(y\\), is to be treated as latent.\nThe game is, like in Gibbs, to construct a joint \\(p(x,y)\\) such that we can sample from \\(p(x \\vert y)\\) and \\(p(y \\vert x)\\), and then find the marginal\n\\[p(x) = \\int dy\\,p(x,y).\\]\nThe simplest form of a Data aumentation algorithm looks like this:\n\nDraw \\(Y\\sim p_{Y \\vert X}(. \\vert x)\\) and call the observed value y\nDraw \\(X_{n+1} \\sim p_{X \\vert Y}(. \\vert y)\\)\n\nHere is an example\n\n%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\nimport seaborn as sns\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nSuppose that \\(p_X\\) is the standard normal density, i.e.,\n\\[p(x) = e^{-x^2/2}/\\sqrt{2\\pi}\\].\nWe’ll pretend we dont know how to sample from it.\nTake\n\\[p(x, y) = 1/(\\sqrt{2\\pi}) \\exp{\\left\\{-(x^2 - \\sqrt{2} xy + y^2) \\right\\} } \\]\nwhich is a bivariate normal density with means equal to zero, variances equal to one, and correlation equal to \\(1/\\sqrt{2}\\). The two conditionals are normal, as we can see by completing the square and neglecting the part of the function that only depends on the variable not being conditioned upon (\\(e^{-y^2 }\\) and \\(e^{-x^2 }\\) respectively for the conditionals below).\n\\[\nY \\vert X = x \\, \\, \\sim N(x/\\sqrt{2}, 1/2) \\,\\,\\,  \\rm{and} \\,\\,\\, X \\vert Y= y \\,\\,\\sim N(y/\\sqrt{2}, 1/2) \\]\nThe x-marginal is\n\\[\\propto e^{-x^{2}/2} \\int e^{-(y-x/\\sqrt{2})^2 }dy \\]\nand clearly thus gets back the old normal we want to draw from.\n\nN=100000\nx=np.zeros(N)\nx[0] = np.random.rand() # INITIALIZE\nfor i in np.arange(1,N):\n    Y=sp.stats.norm.rvs(x[i-1]/np.sqrt(2), 0.5)\n    x[i]=sp.stats.norm.rvs(Y/np.sqrt(2), 0.5)\n\n\nplt.hist(x, bins=30, alpha=0.3, normed=True);\nsns.kdeplot(x)\nplt.xlabel('x')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\n\n\n\nLets start from the “transition kernel” that we identified when we learnt about gibbs sampling\n\\[ h(x', x) = h(x' \\vert x) = \\int_Y p(x' \\vert y) \\, p(y \\vert x)\\, dy \\]\nwhere we have:\n\\[ p(x') = \\int h(x' \\vert x ) p(x)\\,dx,\\]\nthe stationarity condition.\nSince we are dealing with probability densities, \\(h\\) is always positive.\nAlso note\n\\[\n\\begin{eqnarray}\n  \\int h(x' \\vert x) dx' = \\int_{X} \\int_Y p(x' \\vert y) \\, p(y \\vert x) \\, dy \\, dx' = \\int_Y p(y \\vert x) \\left[ \\int_X p(x' \\vert y) dx' \\right] dy  = \\int_Y p(y \\vert x) dy  = 1\n\\end{eqnarray}\n\\]\nTherefore for each fixed \\(x\\), \\(h(x' \\vert x)\\) is non-negative and integrates to 1. The function \\(h\\) therefore could be a Markov Chain transition density and if the current state is \\(x_n\\) then the density of the next state would be \\(h(. \\vert x_n)\\).\nAlso note that the \\(h(x' \\vert x)\\, p(x)\\) is symmetric in \\((x,x')\\).\n\\[ h(x' \\vert x)\\, p(x) = p(x) \\int_Y p(x' \\vert y) \\, p(y \\vert x) \\, dy = \\int_Y \\frac{p(x',y)\\, p(x,y) }{p(y)} \\, dy. \\]\nThe rhs is symmetric in \\((x,x')\\) and so is $ h(x’ x) p(x)$.\nThe Markov chain generated with transition probability \\(h(x' \\vert x)\\) is REVERSIBLE and thus supports detailed balance.\n\n\n\nNow consider the practical issue of simulating the Markov chain \\(X\\). Given that the current state of the chain is \\(X_n = x\\), how do we draw \\(X_{n+1}\\) from the \\(h(. \\vert x)\\)? The answer is based on a sequential simulation technique that we now describe.\nSuppose we would like to simulate a random vector from some pdf, \\(p_U(u)\\), but we cannot do this directly. Suppose further that \\(p_U\\) is the u-marginal of the joint pdf \\(p_{U,V} (u, v)\\) and that we have the ability to make draws from \\(p_V(v)\\) and from \\(p_{U,V} (u,v)\\) for fixed \\(v\\). If we draw \\(V\\sim p_V(.)\\), and then, conditional on \\(V = v\\), we draw \\(U \\sim p_{U,V}(. \\vert v)\\), then the observed pair, \\((u, v)\\), is a draw from \\(p_{U,V}\\), which means that \\(u\\) is a draw from \\(p_U\\). We now can explain how it is used to simulate from \\(h(. \\vert x)\\).\nDefine\n\\[ H(x',y \\vert  x) = p(x' \\vert y) p(y \\vert x) \\]\nWe apply the procedure above with \\(h(. \\vert x)\\) and \\(H(.,. \\vert x)\\) playing the roles of \\(p_U(.)\\) and \\(p_{U,V}(.,.)\\) respectively. We of course need the marginal of \\(H(x', y \\vert x)\\) which is \\(p(y \\vert x)\\) and the conditional density of \\(X'\\) given \\(Y=y\\) which is\n\\[ \\frac{ H(x',y \\vert x) }{p(y \\vert x)} = p(x' \\vert y) \\]\nwhich gives us the procedure above:\n\nDraw \\(Y\\sim p_{Y \\vert X}(. \\vert x)\\) and call the observed value y\nDraw \\(X_{n+1} \\sim p_{X \\vert Y}(. \\vert y)\\)"
  },
  {
    "objectID": "posts/dataaug/index.html#example",
    "href": "posts/dataaug/index.html#example",
    "title": "Data Augmentation",
    "section": "",
    "text": "Suppose that \\(p_X\\) is the standard normal density, i.e.,\n\\[p(x) = e^{-x^2/2}/\\sqrt{2\\pi}\\].\nWe’ll pretend we dont know how to sample from it.\nTake\n\\[p(x, y) = 1/(\\sqrt{2\\pi}) \\exp{\\left\\{-(x^2 - \\sqrt{2} xy + y^2) \\right\\} } \\]\nwhich is a bivariate normal density with means equal to zero, variances equal to one, and correlation equal to \\(1/\\sqrt{2}\\). The two conditionals are normal, as we can see by completing the square and neglecting the part of the function that only depends on the variable not being conditioned upon (\\(e^{-y^2 }\\) and \\(e^{-x^2 }\\) respectively for the conditionals below).\n\\[\nY \\vert X = x \\, \\, \\sim N(x/\\sqrt{2}, 1/2) \\,\\,\\,  \\rm{and} \\,\\,\\, X \\vert Y= y \\,\\,\\sim N(y/\\sqrt{2}, 1/2) \\]\nThe x-marginal is\n\\[\\propto e^{-x^{2}/2} \\int e^{-(y-x/\\sqrt{2})^2 }dy \\]\nand clearly thus gets back the old normal we want to draw from.\n\nN=100000\nx=np.zeros(N)\nx[0] = np.random.rand() # INITIALIZE\nfor i in np.arange(1,N):\n    Y=sp.stats.norm.rvs(x[i-1]/np.sqrt(2), 0.5)\n    x[i]=sp.stats.norm.rvs(Y/np.sqrt(2), 0.5)\n\n\nplt.hist(x, bins=30, alpha=0.3, normed=True);\nsns.kdeplot(x)\nplt.xlabel('x')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j"
  },
  {
    "objectID": "posts/dataaug/index.html#data-augmentation-is-a-markov-chain-monte-carlo",
    "href": "posts/dataaug/index.html#data-augmentation-is-a-markov-chain-monte-carlo",
    "title": "Data Augmentation",
    "section": "",
    "text": "Lets start from the “transition kernel” that we identified when we learnt about gibbs sampling\n\\[ h(x', x) = h(x' \\vert x) = \\int_Y p(x' \\vert y) \\, p(y \\vert x)\\, dy \\]\nwhere we have:\n\\[ p(x') = \\int h(x' \\vert x ) p(x)\\,dx,\\]\nthe stationarity condition.\nSince we are dealing with probability densities, \\(h\\) is always positive.\nAlso note\n\\[\n\\begin{eqnarray}\n  \\int h(x' \\vert x) dx' = \\int_{X} \\int_Y p(x' \\vert y) \\, p(y \\vert x) \\, dy \\, dx' = \\int_Y p(y \\vert x) \\left[ \\int_X p(x' \\vert y) dx' \\right] dy  = \\int_Y p(y \\vert x) dy  = 1\n\\end{eqnarray}\n\\]\nTherefore for each fixed \\(x\\), \\(h(x' \\vert x)\\) is non-negative and integrates to 1. The function \\(h\\) therefore could be a Markov Chain transition density and if the current state is \\(x_n\\) then the density of the next state would be \\(h(. \\vert x_n)\\).\nAlso note that the \\(h(x' \\vert x)\\, p(x)\\) is symmetric in \\((x,x')\\).\n\\[ h(x' \\vert x)\\, p(x) = p(x) \\int_Y p(x' \\vert y) \\, p(y \\vert x) \\, dy = \\int_Y \\frac{p(x',y)\\, p(x,y) }{p(y)} \\, dy. \\]\nThe rhs is symmetric in \\((x,x')\\) and so is $ h(x’ x) p(x)$.\nThe Markov chain generated with transition probability \\(h(x' \\vert x)\\) is REVERSIBLE and thus supports detailed balance."
  },
  {
    "objectID": "posts/dataaug/index.html#sequential-simulation",
    "href": "posts/dataaug/index.html#sequential-simulation",
    "title": "Data Augmentation",
    "section": "",
    "text": "Now consider the practical issue of simulating the Markov chain \\(X\\). Given that the current state of the chain is \\(X_n = x\\), how do we draw \\(X_{n+1}\\) from the \\(h(. \\vert x)\\)? The answer is based on a sequential simulation technique that we now describe.\nSuppose we would like to simulate a random vector from some pdf, \\(p_U(u)\\), but we cannot do this directly. Suppose further that \\(p_U\\) is the u-marginal of the joint pdf \\(p_{U,V} (u, v)\\) and that we have the ability to make draws from \\(p_V(v)\\) and from \\(p_{U,V} (u,v)\\) for fixed \\(v\\). If we draw \\(V\\sim p_V(.)\\), and then, conditional on \\(V = v\\), we draw \\(U \\sim p_{U,V}(. \\vert v)\\), then the observed pair, \\((u, v)\\), is a draw from \\(p_{U,V}\\), which means that \\(u\\) is a draw from \\(p_U\\). We now can explain how it is used to simulate from \\(h(. \\vert x)\\).\nDefine\n\\[ H(x',y \\vert  x) = p(x' \\vert y) p(y \\vert x) \\]\nWe apply the procedure above with \\(h(. \\vert x)\\) and \\(H(.,. \\vert x)\\) playing the roles of \\(p_U(.)\\) and \\(p_{U,V}(.,.)\\) respectively. We of course need the marginal of \\(H(x', y \\vert x)\\) which is \\(p(y \\vert x)\\) and the conditional density of \\(X'\\) given \\(Y=y\\) which is\n\\[ \\frac{ H(x',y \\vert x) }{p(y \\vert x)} = p(x' \\vert y) \\]\nwhich gives us the procedure above:\n\nDraw \\(Y\\sim p_{Y \\vert X}(. \\vert x)\\) and call the observed value y\nDraw \\(X_{n+1} \\sim p_{X \\vert Y}(. \\vert y)\\)"
  },
  {
    "objectID": "posts/discretemcmc/index.html",
    "href": "posts/discretemcmc/index.html",
    "title": "Discrete MCMC",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\nIn simulated annealing, we carried out combinatorical oprimization by sampling from a state space where each state was a vector of baseball simulation features.\nSince Metropolis MCMC is the same algorithm, it should be clear that its possible to simulate discrete possibilities in MCMC as long as you choose proposals which satisfy detailed balance.\nAs an example, consider simulating a poisson distribution. Since its discrete, the proposal wont be a continuous \\(q(x,y)\\) (the proposal probability to go from y to x), but rather a matrix indexed by a variable that corresponds to (indexes) the various states that can be obtained.\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted=0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n            accepted+=1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\n\nThe poisson pmf is:\n\\[p(k) = e^{-\\mu}\\frac{\\mu^k}{k!}.\\]\n\nfrom scipy.stats import poisson\nxxx= np.arange(1,20,1)\nplt.plot(xxx, poisson.pmf(xxx, mu=5), 'o'); \n\n\n\n\n\n\n\n\nTo sample from this distribution, we must create a proposal matrix which allows us to go from any integer output to any other in a finite number of steps. This matrix must be symmetric, since we wish to use Metropolis.\nA simple such matrix, which is although a bit slow, would be one which has immediate off-diagonal elements (from Stats 580 at Iowa state..)\n\n\n\nSymmetric random-walk proposal matrix Q for discrete MCMC: each state proposes to stay or move to an adjacent state with equal probability.\n\n\n\ndef prop_pdf(ito, ifrom):\n    if ito == ifrom - 1:\n        return 0.5\n    elif ito == ifrom + 1:\n        return 0.5\n    elif ito == ifrom and ito == 0:#needed to make first row sum to 1\n        return 0.5\n    else:\n        return 0\n\n\ndef prop_draw(ifrom):\n    u = np.random.uniform()\n    if ifrom !=0:\n        if u &lt; 1/2:\n            ito = ifrom -1\n        else:\n            ito = ifrom + 1\n    else:\n        if u &lt; 1/2:\n            ito=0\n        else:\n            ito=1\n    return ito\n\n\nrv = poisson(5)\nsamps, acc = metropolis(rv.pmf, prop_draw, 50000, 1)\n\n\nacc\n\n41463\n\n\n\nxxx = np.arange(0,samps.max())\nplt.hist(samps, bins=xxx, normed=True, align='left');\nplt.plot(xxx, rv.pmf(xxx),'o');"
  },
  {
    "objectID": "posts/discretemcmc/index.html#example-sampling-a-poisson",
    "href": "posts/discretemcmc/index.html#example-sampling-a-poisson",
    "title": "Discrete MCMC",
    "section": "",
    "text": "The poisson pmf is:\n\\[p(k) = e^{-\\mu}\\frac{\\mu^k}{k!}.\\]\n\nfrom scipy.stats import poisson\nxxx= np.arange(1,20,1)\nplt.plot(xxx, poisson.pmf(xxx, mu=5), 'o'); \n\n\n\n\n\n\n\n\nTo sample from this distribution, we must create a proposal matrix which allows us to go from any integer output to any other in a finite number of steps. This matrix must be symmetric, since we wish to use Metropolis.\nA simple such matrix, which is although a bit slow, would be one which has immediate off-diagonal elements (from Stats 580 at Iowa state..)\n\n\n\nSymmetric random-walk proposal matrix Q for discrete MCMC: each state proposes to stay or move to an adjacent state with equal probability.\n\n\n\ndef prop_pdf(ito, ifrom):\n    if ito == ifrom - 1:\n        return 0.5\n    elif ito == ifrom + 1:\n        return 0.5\n    elif ito == ifrom and ito == 0:#needed to make first row sum to 1\n        return 0.5\n    else:\n        return 0\n\n\ndef prop_draw(ifrom):\n    u = np.random.uniform()\n    if ifrom !=0:\n        if u &lt; 1/2:\n            ito = ifrom -1\n        else:\n            ito = ifrom + 1\n    else:\n        if u &lt; 1/2:\n            ito=0\n        else:\n            ito=1\n    return ito\n\n\nrv = poisson(5)\nsamps, acc = metropolis(rv.pmf, prop_draw, 50000, 1)\n\n\nacc\n\n41463\n\n\n\nxxx = np.arange(0,samps.max())\nplt.hist(samps, bins=xxx, normed=True, align='left');\nplt.plot(xxx, rv.pmf(xxx),'o');"
  },
  {
    "objectID": "posts/bayesianregression/index.html",
    "href": "posts/bayesianregression/index.html",
    "title": "Bayesian Regression",
    "section": "",
    "text": "\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]\n%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nfrom scipy.stats import norm\nfrom scipy.stats import multivariate_normal\ndef cplot(f, ax=None):\n    if not ax:\n        plt.figure(figsize=(4,4))\n        ax=plt.gca()\n    xx,yy=np.mgrid[-1:1:.01,-1:1:.01]\n    pos = np.empty(xx.shape + (2,))\n    pos[:, :, 0] = xx\n    pos[:, :, 1] = yy\n    ax.contourf(xx, yy, f(pos))\n    #data = [x, y]\n    return ax\ndef plotSampleLines(mu, sigma, numberOfLines, dataPoints=None, ax=None):\n    #Plot the specified number of lines of the form y = w0 + w1*x in [-1,1]x[-1,1] by\n    # drawing w0, w1 from a bivariate normal distribution with specified values\n    # for mu = mean and sigma = covariance Matrix. Also plot the data points as\n    # blue circles. \n    #print \"datap\",dataPoints\n    if not ax:\n        plt.figure()\n        ax=plt.gca()\n    for i in range(numberOfLines):\n        w = np.random.multivariate_normal(mu,sigma)\n        func = lambda x: w[0] + w[1]*x\n        xx=np.array([-1,1])\n        ax.plot(xx,func(xx),'r', alpha=0.2)\n    if dataPoints:\n        ax.scatter(dataPoints[0],dataPoints[1])\n    ax.set_xlim([-1,1])\n    ax.set_ylim([-1,1])"
  },
  {
    "objectID": "posts/bayesianregression/index.html#the-bayesian-formulation-of-regression",
    "href": "posts/bayesianregression/index.html#the-bayesian-formulation-of-regression",
    "title": "Bayesian Regression",
    "section": "The Bayesian formulation of regression",
    "text": "The Bayesian formulation of regression\nLet us say we have data \\(D\\), of \\(n\\) observations\n$D={ ({}_1, y_1), ({}_2,y_2), , ({}_n, y_n) } $ where \\({\\bf x}\\) denotes an input vector of dimension \\(D\\) and \\(y\\) denotes a scalar output (dependent variable). All data points are combined into a \\(D \\times n\\) matrix \\(X\\). The model that determines the relationship between inputs and output is given by\n\\[ y   = \\bf x^{T} {\\bf w} + \\epsilon \\]\nwhere \\({\\bf w}\\) is a vector of parameters of the linear model. Usually there is a bias or offset is included, but for now we ignore it.\nWe assume that the additive noise \\(\\epsilon\\) is iid Gaussian with zero mean and variance \\(\\sigma_n^2\\)\n\\[ \\epsilon \\sim N(0, \\sigma^2_n) \\]\n\na0=-0.3\na1=0.5\nN=20\nnoiseSD=0.2\nu=np.random.rand(20)\nx=2.*u -1.\ndef randnms(mu, sigma, n):\n    return sigma*np.random.randn(n) + mu\ny=a0+a1*x+randnms(0.,noiseSD,N)\nplt.scatter(x,y)\n\n\n\n\n\n\n\n\n\nLikelihood\nThe likelihood is, because we assume independency, the product\n\\[\n\\begin{eqnarray} \\like &=& p(\\bf y|X,\\bf w) = \\prod_{i=1}^{n} p(y_i|\\bf X_i, \\bf w) =   \\prod_{i=1}^{n}  \\frac{1}{\\sqrt{2\\pi}\\sigma_n}\n   \\exp{ \\left( -\\frac{(y_i-\\bf X_i^T \\bf w)^2}{2\\sigma_n^2} \\right)}  \\nonumber \\\\\n   &\\propto &  \\exp{\\left( -\\frac{| \\bf y-X^T \\bf w|^2 }{2\\sigma_n^2} \\right)} \\propto N(X^T \\bf w,  \\sigma_n^2 I)\n\\end{eqnarray}\n\\]\nwhere \\(|x|\\) denotes the Euclidean length of vector \\(\\bf x\\).\n\nlikelihoodSD = noiseSD # Assume the likelihood precision is known.\nlikelihoodPrecision = 1./(likelihoodSD*likelihoodSD)\n\n\n\nPrior\nIn the Bayesian framework inference we need to specify a prior over the parameters that expresses our belief about the parameters before we take any measurements. A wise choice is a \\({\\bf w_0}\\) mean Gaussian with covariance matrix \\(\\Sigma\\)\n\\[\n\\bf w \\sim N(w_0, \\Sigma)\n\\]\nIf we assume that \\(\\Sigma\\) is a diagonal covariance matrix then\n\\[\\bf w \\sim N(w_0, \\tau^2 \\bf I)\\]\n\npriorMean = np.zeros(2)\npriorPrecision=2.0\nprior_covariance = lambda alpha: alpha*np.eye(2)#Covariance Matrix\npriorCovariance = prior_covariance(1/priorPrecision )\npriorPDF = lambda w: multivariate_normal.pdf(w,mean=priorMean,cov=priorCovariance)\npriorPDF([1,2])\n\n0.0021447551423913074\n\n\n\ncplot(priorPDF);\n\n\n\n\n\n\n\n\n\nplotSampleLines(priorMean,priorCovariance,15)\n\n\n\n\n\n\n\n\n\n\nPosterior\nWe can now continue with the standard Bayesian formalism\n\\[\n\\begin{eqnarray}\np(\\bf w| \\bf y,X) &\\propto& p(\\bf y | X, \\bf w) \\, p(\\bf w) \\nonumber \\\\\n                       &\\propto& \\exp{ \\left(- \\frac{1}{2 \\sigma_n^2}(\\bf y-X^T \\bf w)^T(\\bf y - X^T \\bf w) \\right)}\n                        \\exp{\\left( -\\frac{1}{2} \\bf w^T \\Sigma^{-1} \\bf w \\right)}  \\nonumber \\\\\n\\end{eqnarray}\n\\]\nIn the next step we `complete the square’ and obtain\n\\[\\begin{equation}\np(\\bf w| \\bf y,X)  \\propto  \\exp \\left( -\\frac{1}{2} (\\bf w - \\bar{\\bf w})^T  (\\frac{1}{\\sigma_n^2} X X^T + \\Sigma^{-1})(\\bf w - \\bar{\\bf w} )  \\right)\n\\end{equation}\\]\nThis is a Gaussian with inverse-covariance\n\\[A= \\sigma_n^{-2}XX^T +\\Sigma^{-1}\\]\nwhere the new mean is\n\\[\\bar{\\bf w} = A^{-1}\\Sigma^{-1}{\\bf w_0} + \\sigma_n^{-2}( A^{-1} X^T \\bf y )\\]\nTo make predictions for a test case we average over all possible parameter predictive distribution values, weighted by their posterior probability. This is in contrast to non Bayesian schemes, where a single parameter is typically chosen by some criterion.\n\n# Given the mean = priorMu and covarianceMatrix = priorSigma of a prior\n# Gaussian distribution over regression parameters; observed data, x\n# and y; and the likelihood precision, generate the posterior\n# distribution, postW via Bayesian updating and return the updated values\n# for mu and sigma. xtrain is a design matrix whose first column is the all\n# ones vector.\ndef update(x,y,likelihoodPrecision,priorMu,priorCovariance): \n    postCovInv  = np.linalg.inv(priorCovariance) + likelihoodPrecision*np.outer(x.T,x)\n    #The outer product looks wrong but when updating we need a 2x1 matrix while x is 1x2\n    postCovariance = np.linalg.inv(postCovInv)\n    postMu = np.dot(np.dot(postCovariance,np.linalg.inv(priorCovariance)),priorMu) + likelihoodPrecision*np.dot(postCovariance,np.outer(x.T,y)).flatten()\n    postW = lambda w: multivariate_normal.pdf(w,postMu,postCovariance)\n    return postW, postMu, postCovariance\n\n\n# For each iteration plot  the\n# posterior over the first i data points and sample lines whose\n# parameters are drawn from the corresponding posterior. \nfig, axes=plt.subplots(figsize=(12,30), nrows=5, ncols=2);\nmu = priorMean\ncov = priorCovariance\nmuhash={}\ncovhash={}\nk=0\nfor i in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]:\n    postW,mu,cov = update(np.array([1,x[i]]),y[i],likelihoodPrecision,mu,cov)\n    muhash[i]=mu\n    covhash[i]=cov\n    if i in [1,4,7,10,19]:\n        cplot(postW, axes[k][0])\n        plotSampleLines(muhash[i],covhash[i],15, (x[0:i],y[0:i]), axes[k][1])\n        k=k+1"
  },
  {
    "objectID": "posts/bayesianregression/index.html#posterior-predictive-distribution",
    "href": "posts/bayesianregression/index.html#posterior-predictive-distribution",
    "title": "Bayesian Regression",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\nThus the predictive distribution at some \\(x^{*}\\) is given by averaging the output of all possible linear models w.r.t. the posterior\n\\[\n\\begin{eqnarray}\np(y^{*} | x^{*}, {\\bf x,y}) &=& \\int p({\\bf y}^{*}| {\\bf x}^{*}, {\\bf w} ) p(\\bf w| X, y)dw \\nonumber \\\\\n                                    &=& {\\cal N} \\left(y \\vert \\bar{\\bf w}^{T}x^{*}, \\sigma_n^2 + x^{*^T}A^{-1}x^{*} \\right),\n\\end{eqnarray}\n\\]\nwhich is again Gaussian, with a mean given by the posterior mean multiplied by the test input and the variance is a quadratic form of the test input with the posterior covariance matrix, showing that the predictive uncertainties grow with the magnitude of the test input, as one would expect for a linear model."
  },
  {
    "objectID": "posts/bayesianregression/index.html#regularization",
    "href": "posts/bayesianregression/index.html#regularization",
    "title": "Bayesian Regression",
    "section": "Regularization",
    "text": "Regularization\n\\(\\alpha = \\sigma_n^2/\\tau^2\\) (prior precision/likelihood precision) is the regularization parameter from ridge regression. An uninformative (tending to uniform) prior means no regularization which is the standard MLE result.\n\npriorPrecision/likelihoodPrecision\n\n0.08000000000000002\n\n\nBut now say you had a strong belief the both the slope and intercept ought to be 0. Or in other words you are trying to restrict your parameters to a certain range.\n\npriorPrecision=100.0\npriorCovariance = prior_covariance(1/priorPrecision )\npriorPDF = lambda w: multivariate_normal.pdf(w,mean=priorMean,cov=priorCovariance)\ncplot(priorPDF)\n\n\n\n\n\n\n\n\n\npriorPrecision/likelihoodPrecision\n\n4.000000000000001\n\n\n\nchoices=np.random.choice([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],4,replace=False)\n\n\nchoices\n\narray([ 1, 18, 13, 19])\n\n\n\n\n# For each iteration plot  the\n# posterior over the first i data points and sample lines whose\n# parameters are drawn from the corresponding posterior. \nfig, axes=plt.subplots(figsize=(12,30), nrows=4, ncols=2);\nmu = priorMean\ncov = priorCovariance\nmuhash={}\ncovhash={}\nk=0\nxnew=x[choices]\nynew=y[choices]\nfor j,i in enumerate(choices):\n    postW,mu,cov = update(np.array([1,xnew[j]]),ynew[j],likelihoodPrecision,mu,cov)\n    muhash[i]=mu\n    covhash[i]=cov\n    cplot(postW, axes[k][0])\n    plotSampleLines(muhash[i],covhash[i],15, (xnew[:j+1],ynew[:j+1]), axes[k][1])\n    k=k+1\n\n\n\n\n\n\n\n\n\nNotice how our prior tries to keep things as flat as possible!"
  },
  {
    "objectID": "posts/testingtraining/index.html",
    "href": "posts/testingtraining/index.html",
    "title": "Learning Bounds and the Test Set",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/testingtraining/index.html#revisiting-the-model",
    "href": "posts/testingtraining/index.html#revisiting-the-model",
    "title": "Learning Bounds and the Test Set",
    "section": "Revisiting the model",
    "text": "Revisiting the model\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\nLet us assume that we have a “population” of 200 counties \\(x\\):\n\ndffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\nLets suppose now that the Lord came by and told us that the points in the plot below captures \\(f(x)\\) exactly.\n\nx=dffull.rfrac.values\nf=dffull.promney.values\nplt.plot(x,f,'.', alpha=0.3)\n\n\n\n\n\n\n\n\nNotice that our sampling of \\(x\\) is not quite uniform: there are more points around \\(x\\) of 0.7.\nNow, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample \\(\\cal{D}\\) of 30 data points. Such data is called in-sample data. Contrastingly, the entire population of data points is also called out-of-sample data.\n\ndf = pd.read_csv(\"data/noisysample.csv\")\ndf.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\naxes[0].plot(x,f, 'r.', alpha=0.2, label=\"population\");\naxes[1].plot(df.x,df.f, 'o', alpha=0.6, label=\"in-sample noiseless data $\\cal{D}$\");\naxes[1].plot(df.x,df.y, 's', alpha=0.6, label=\"in-sample noisy data $\\cal{D}$\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);"
  },
  {
    "objectID": "posts/testingtraining/index.html#testing-and-training-sets",
    "href": "posts/testingtraining/index.html#testing-and-training-sets",
    "title": "Learning Bounds and the Test Set",
    "section": "Testing and Training Sets",
    "text": "Testing and Training Sets\nThe process of learning has two parts:\n\nFit for a model by minimizing the in-sample risk\nHope that the in-sample risk approximates the out-of-sample risk well.\n\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out \\,of \\,sample} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nHoping does not befit us as scientists. How can we test that the in-sample risk approximates the out-of-sample risk well?\nThe “aha” moment comes when we realize that we can hold back some of our sample, and test the performance of our learner by trying it out on this held back part! Perhaps we can compute the error or risk on the held-out part, or “test” part of our sample, and have something to say about the out-of-sample error.\nLet us introduce some new terminology. We take the sample of data \\(\\cal{D}\\) that we have been given (our in-sample set) and split it into two parts:\n\nThe training set, which is the part of the data we use to fit a model\nThe testing set, a smaller part of the data set which we use to see how good our fit was.\n\nThis split is done by choosing points at random into these two sets. Typically we might take 80% of our data and put it in the training set, with the remaining amount going into the test set. This can be carried out in python using the train_test_split function from sklearn.cross_validation.\nThe split is shown in the diagram below:\n\n\n\nSplitting dataset D into training and test sets (image after Learning from Data)\n\n\nWe ARE taking a hit on the amount of data we have to train our model. The more data we have, the better we can do for our fits. But, you cannot figure out the generalization ability of a learner by looking at the same data it was trained on: there is nothing to generalize to, and as we know we can fit very complex models to training data which have no hope of generalizing (like an interpolator). Thus, to estimate the out-of-sample error or risk, we must leave data over to make this estimation.\nAt this point you are thinking: the test set is just another sample of the population, just like the training set. What guarantee do we have that it approximates the out-of-sample error well? And furthermore, if we pick 6 out of 30 points as a test set, why would you expect the estimate to be any good?\nWe will kind-of hand wavingly show later that the test set error is a good estimate of the out of sample error, especially for larger and larger test sets. You are right to worry that 6 points is perhaps too few, but thats what we have for now, and we shall work with them.\nWe are using the training set then, as our in-sample set, and the test set as a proxy for out-of-sample..\n\nfrom sklearn.cross_validation import train_test_split\ndatasize=df.shape[0]\n#split dataset using the index, as we have x,f, and y that we want to split.\nitrain,itest = train_test_split(range(30),train_size=24, test_size=6)\nxtrain= df.x[itrain].values\nftrain = df.f[itrain].values\nytrain = df.y[itrain].values\nxtest= df.x[itest].values\nftest = df.f[itest].values\nytest = df.y[itest].values\n\n\naxes=make_plot()\naxes[0].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\naxes[0].plot(df.x,df.y, 'o',alpha=0.6, label=\"$\\cal{D}$\");\naxes[1].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\naxes[1].plot(xtrain, ytrain, 's', label=\"training\")\naxes[1].plot(xtest, ytest, 's', label=\"testing\")\naxes[0].legend(loc=\"lower right\")\naxes[1].legend(loc=\"lower right\")"
  },
  {
    "objectID": "posts/testingtraining/index.html#a-digression-about-scikit-learn",
    "href": "posts/testingtraining/index.html#a-digression-about-scikit-learn",
    "title": "Learning Bounds and the Test Set",
    "section": "A digression about scikit-learn",
    "text": "A digression about scikit-learn\nScikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as train_test_split. It can be used in python by the incantation import sklearn.\nThe library has a very well defined interface. This makes the library a joy to use, and surely contributes to its popularity. As the scikit-learn API paper [Buitinck, Lars, et al. “API design for machine learning software: experiences from the scikit-learn project.” arXiv preprint arXiv:1309.0238 (2013).] says:\n\nAll objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: an estimator interface for building and ﬁtting models, a predictor interface for making predictions and a transformer interface for converting data. The estimator interface is at the core of the library. It deﬁnes instantiation mechanisms of objects and exposes a fit method for learning a model from training data. All supervised and unsupervised learning algorithms (e.g., for classiﬁcation, regression or clustering) are oﬀered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n\nEarlier we fit y using the python function polyfit. To get you familiarized with scikit-learn, we’ll use the “estimator” interface here, specifically the estimator PolynomialFeatures. The API paper again:\n\nSince it is common to modify or ﬁlter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which deﬁnes a transform method. It takes as input some new data X and yields as output a transformed version of X. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n\nTo start with we have one feature x, the fraction of religious people in a county, which we want to use to predict y, the fraction of people voting for Romney in that county. What we will do is the transformation:\n\\[ x \\rightarrow 1, x, x^2, x^3, ..., x^d \\]\nfor some power \\(d\\). Our job then is to fit for the coefficients of these features in the polynomial\n\\[ a_0 + a_1 x + a_2 x^2 + ... + a_d x^d. \\]\n\nTransformers in sklearn\nIn other words, we have transformed a function of one feature, into a (rather simple) linear function of many features. To do this we first construct the estimator as PolynomialFeatures(d), and then transform these features into a d-dimensional space using the method fit_transform.\n\n\n\nPolynomial feature transform: expanding x into a design matrix\n\n\nHere is an example. The reason for using [[1],[2],[3]] as opposed to [1,2,3] is that scikit-learn expects data to be stored in a two-dimensional array or matrix with size [n_samples, n_features].\n\nfrom sklearn.preprocessing import PolynomialFeatures\nPolynomialFeatures(3).fit_transform([[1],[2], [3]])\n\narray([[  1.,   1.,   1.,   1.],\n       [  1.,   2.,   4.,   8.],\n       [  1.,   3.,   9.,  27.]])\n\n\nTo transform [1,2,3] into [[1],[2],[3]] we need to do a reshape.\n\n\n\nNumPy reshape: converting a 1D array to a column vector\n\n\n\nnp.array([1,2,3]).reshape(-1,1)\n\narray([[1],\n       [2],\n       [3]])\n\n\nSo now we are in the recatangular, rows=samples, columns=features form expected by scikit-learn. Ok, so lets see the process to transform our 1-D dataset x into a d-dimensional one.\n\nxtrain\n\narray([ 0.33      ,  0.75868254,  0.52      ,  0.79      ,  0.63633949,\n        0.70533267,  0.71829603,  0.75841654,  0.63071361,  0.11      ,\n        0.82850909,  0.46      ,  0.64832591,  0.53596824,  0.91      ,\n        0.67      ,  0.76      ,  0.34      ,  0.56      ,  0.94      ,\n        0.6       ,  0.96      ,  0.43754875,  0.54      ])\n\n\n\nxtrain.reshape(-1,1)\n\narray([[ 0.33      ],\n       [ 0.75868254],\n       [ 0.52      ],\n       [ 0.79      ],\n       [ 0.63633949],\n       [ 0.70533267],\n       [ 0.71829603],\n       [ 0.75841654],\n       [ 0.63071361],\n       [ 0.11      ],\n       [ 0.82850909],\n       [ 0.46      ],\n       [ 0.64832591],\n       [ 0.53596824],\n       [ 0.91      ],\n       [ 0.67      ],\n       [ 0.76      ],\n       [ 0.34      ],\n       [ 0.56      ],\n       [ 0.94      ],\n       [ 0.6       ],\n       [ 0.96      ],\n       [ 0.43754875],\n       [ 0.54      ]])\n\n\n\nPolynomialFeatures(2).fit_transform(xtrain.reshape(-1,1))\n\narray([[ 1.        ,  0.33      ,  0.1089    ],\n       [ 1.        ,  0.75868254,  0.5755992 ],\n       [ 1.        ,  0.52      ,  0.2704    ],\n       [ 1.        ,  0.79      ,  0.6241    ],\n       [ 1.        ,  0.63633949,  0.40492794],\n       [ 1.        ,  0.70533267,  0.49749418],\n       [ 1.        ,  0.71829603,  0.51594919],\n       [ 1.        ,  0.75841654,  0.57519565],\n       [ 1.        ,  0.63071361,  0.39779966],\n       [ 1.        ,  0.11      ,  0.0121    ],\n       [ 1.        ,  0.82850909,  0.68642731],\n       [ 1.        ,  0.46      ,  0.2116    ],\n       [ 1.        ,  0.64832591,  0.42032648],\n       [ 1.        ,  0.53596824,  0.28726196],\n       [ 1.        ,  0.91      ,  0.8281    ],\n       [ 1.        ,  0.67      ,  0.4489    ],\n       [ 1.        ,  0.76      ,  0.5776    ],\n       [ 1.        ,  0.34      ,  0.1156    ],\n       [ 1.        ,  0.56      ,  0.3136    ],\n       [ 1.        ,  0.94      ,  0.8836    ],\n       [ 1.        ,  0.6       ,  0.36      ],\n       [ 1.        ,  0.96      ,  0.9216    ],\n       [ 1.        ,  0.43754875,  0.1914489 ],\n       [ 1.        ,  0.54      ,  0.2916    ]])\n\n\n\n\nFitting in sklearn\nOnce again, lets see the structure of scikit-learn needed to make these fits. .fit always takes two arguments:\nestimator.fit(Xtrain, ytrain).\nHere Xtrain must be in the form of an array of arrays, with the inner array each corresponding to one sample, and whose elements correspond to the feature values for that sample. (This means that the 4th element for each of these arrays, in our polynomial example, corresponds to the valueof \\(x^3\\) for each “sample” \\(x\\)). The ytrain is a simple array of responses..continuous for regression problems, and categorical values or 1-0’s for classification problems.\n\n\n\nScikit-learn train/test data layout: X_train, y_train, X_test, y_test\n\n\nThe test set Xtest has the same structure, and is used in the .predict interface. Once we have fit the estimator, we predict the results on the test set by:\nestimator.predict(Xtest).\nThe results of this are a simple array of predictions, of the same form and shape as ytest.\nA summary of the scikit-learn interface can be found here:\nhttp://nbviewer.jupyter.org/github/jakevdp/sklearn_pycon2015/blob/master/notebooks/02.2-Basic-Principles.ipynb#Recap:-Scikit-learn’s-estimator-interface\nLets put this alltogether. Below we write a function to create multiple datasets, one for each polynomial degree:\n\ndef make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist"
  },
  {
    "objectID": "posts/testingtraining/index.html#how-do-training-and-testing-error-change-with-complexity",
    "href": "posts/testingtraining/index.html#how-do-training-and-testing-error-change-with-complexity",
    "title": "Learning Bounds and the Test Set",
    "section": "How do training and testing error change with complexity?",
    "text": "How do training and testing error change with complexity?\nYou will recall that the big question we were left with earlier is: what order of polynomial should we use to fit the data? Which order is too biased? Which one has too much variance and is too complex? Let us try and answer this question.\nWe do this by fitting many different models (remember the fit is made by minimizing the empirical risk on the training set), each with increasing dimension d, and looking at the training-error and the test-error in each of these models. So we first try \\(\\cal{H}_0\\), then \\(\\cal{H}_1\\), then \\(\\cal{H}_2\\), and so on.\nSince we use PolynomialFeatures above, each increasing dimension gives us an additional feature. \\(\\cal{H}_5\\) has 6 features, a constant and the 5 powers of x. What we want to do is to find the coefficients of the 5-th order polynomial that best fits the data. Since the polynomial is linear in the coefficients (we multiply coefficients by powers-of-x features and sum it up), we use a learner called a LinearRegression model (remember that the “linear” in the regression refers to linearity in co-efficients). The scikit-learn interface to make such a fit is also very simple, the function fit. And once we have learned a model, we can predict using the function predict. The API paper again:\n\nThe predictor interface extends the notion of an estimator by adding a predict method that takes an array X_test and produces predictions for X_test, based on the learned parameters of the estimator.\n\nSo, for increasing polynomial degree, and thus feature dimension d, we fit a LinearRegression model on the traing set. We then use scikit-learn again to calculate the error or risk. We calculate the mean_squared_error between the model’s predictions and the data, BOTH on the training set and test set. We plot this error as a function of the defree of the polynomial d.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\ndegrees=range(21)\nerror_train=np.empty(len(degrees))\nerror_test=np.empty(len(degrees))\n\ntraintestlists=make_features(xtrain, xtest, degrees)\n\n\ntraintestlists[3]['train'], ytrain\n\n(array([[ 1.        ,  0.33      ,  0.1089    ,  0.035937  ],\n        [ 1.        ,  0.75868254,  0.5755992 ,  0.43669706],\n        [ 1.        ,  0.52      ,  0.2704    ,  0.140608  ],\n        [ 1.        ,  0.79      ,  0.6241    ,  0.493039  ],\n        [ 1.        ,  0.63633949,  0.40492794,  0.25767164],\n        [ 1.        ,  0.70533267,  0.49749418,  0.3508989 ],\n        [ 1.        ,  0.71829603,  0.51594919,  0.37060426],\n        [ 1.        ,  0.75841654,  0.57519565,  0.4362379 ],\n        [ 1.        ,  0.63071361,  0.39779966,  0.25089766],\n        [ 1.        ,  0.11      ,  0.0121    ,  0.001331  ],\n        [ 1.        ,  0.82850909,  0.68642731,  0.56871127],\n        [ 1.        ,  0.46      ,  0.2116    ,  0.097336  ],\n        [ 1.        ,  0.64832591,  0.42032648,  0.27250855],\n        [ 1.        ,  0.53596824,  0.28726196,  0.15396329],\n        [ 1.        ,  0.91      ,  0.8281    ,  0.753571  ],\n        [ 1.        ,  0.67      ,  0.4489    ,  0.300763  ],\n        [ 1.        ,  0.76      ,  0.5776    ,  0.438976  ],\n        [ 1.        ,  0.34      ,  0.1156    ,  0.039304  ],\n        [ 1.        ,  0.56      ,  0.3136    ,  0.175616  ],\n        [ 1.        ,  0.94      ,  0.8836    ,  0.830584  ],\n        [ 1.        ,  0.6       ,  0.36      ,  0.216     ],\n        [ 1.        ,  0.96      ,  0.9216    ,  0.884736  ],\n        [ 1.        ,  0.43754875,  0.1914489 ,  0.08376823],\n        [ 1.        ,  0.54      ,  0.2916    ,  0.157464  ]]),\n array([ 0.35817449,  0.64634662,  0.47094573,  0.80195369,  0.71040586,\n         0.64431987,  0.81167767,  0.81232659,  0.65597413,  0.18382092,\n         0.76638914,  0.52531463,  0.72006043,  0.53688748,  0.91261385,\n         0.89700996,  0.7612565 ,  0.23599998,  0.58004131,  0.93613422,\n         0.60188686,  0.87217807,  0.49208494,  0.61984169]))\n\n\n\ntraintestlists[3]['test'], ytest\n\n(array([[  1.00000000e+00,   6.60000000e-01,   4.35600000e-01,\n           2.87496000e-01],\n        [  1.00000000e+00,   2.30000000e-01,   5.29000000e-02,\n           1.21670000e-02],\n        [  1.00000000e+00,   8.09657516e-01,   6.55545293e-01,\n           5.30767174e-01],\n        [  1.00000000e+00,   7.00000000e-02,   4.90000000e-03,\n           3.43000000e-04],\n        [  1.00000000e+00,   9.00000000e-02,   8.10000000e-03,\n           7.29000000e-04],\n        [  1.00000000e+00,   7.49902667e-01,   5.62354010e-01,\n           4.21710772e-01]]),\n array([ 0.60311145,  0.05762073,  0.79714359,  0.13897264,  0.05051023,\n         0.74855785]))\n\n\n\nEstimating the out-of-sample error\nWe can then use mean_squared_error from sklearn to calculate the error between the predictions and actual ytest values. Below we calculate this error on both the training set (which we already fit on) and the test set (which we hadnt seen before), and plot how these errors change with the degree of the polynomial.\n\nest3 = LinearRegression()\nest3.fit(traintestlists[3]['train'], ytrain)\npred_on_train3=est3.predict(traintestlists[3]['train'])\npred_on_test3=est3.predict(traintestlists[3]['test'])\n\n\nprint(\"errtrain\",mean_squared_error(ytrain, pred_on_train3))\nprint(\"errtest\",mean_squared_error(ytest, pred_on_test3))\n\nerrtrain 0.00455053325387\nerrtest 0.00949690985891\n\n\nLet us now do this for a polynomial of degree 19\n\nest19 = LinearRegression()\nest19.fit(traintestlists[19]['train'], ytrain)\npred_on_train19=est19.predict(traintestlists[19]['train'])\npred_on_test19=est19.predict(traintestlists[19]['test'])\nprint(\"errtrain\",mean_squared_error(ytrain, pred_on_train19))\nprint(\"errtest\",mean_squared_error(ytest, pred_on_test19))\n\nerrtrain 0.00196640248639\nerrtest 14125204461.8\n\n\nYou can see that the test set error is larger, corresponding to an overfit model thats doing very well on some points and awful on other.\n\n\nFinding the appropriate complexity\nLets now carry out this minimization systematically for each polynomial degree d.\n\nfor d in degrees:#for increasing polynomial degrees 0,1,2...\n    Xtrain = traintestlists[d]['train']\n    Xtest = traintestlists[d]['test']\n    #set up model\n    #fit\n    #predict\n    #calculate mean squared error\n    #set up model\n    est = LinearRegression()\n    #fit\n    est.fit(Xtrain, ytrain)\n    #predict\n    prediction_on_training = est.predict(Xtrain)\n    prediction_on_test = est.predict(Xtest)\n    #calculate mean squared error\n    error_train[d] = mean_squared_error(ytrain, prediction_on_training)\n    error_test[d] = mean_squared_error(ytest, prediction_on_test)\n\n\nplt.plot(degrees, error_train, marker='o', label='train (in-sample)')\nplt.plot(degrees, error_test, marker='o', label='test')\nplt.axvline(np.argmin(error_test), 0,0.5, color='r', label=\"min test error at d=%d\"%np.argmin(error_test), alpha=0.3)\nplt.ylabel('mean squared error')\nplt.xlabel('degree')\nplt.legend(loc='upper left')\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\nThe graph shows a very interesting structure. The training error decreases with increasing degree of the polynomial. This ought to make sense given what you know now: one can construct an arbitrarily complex polynomial to fit all the training data: indeed one could construct an order 24 polynomial which perfectly interpolates the 24 data points in the training set. You also know that this would do very badly on the test set as it would wiggle like mad to capture all the data points. And this is indeed what we see in the test set error.\nFor extremely low degree polynomials like \\(d=0\\) a flat line capturing the mean value of the data or \\(d=1\\) a straight line fitting the data, the polynomial is not curvy enough to capturve the conbtours of the data. We are in the bias/deterministic error regime, where we will always have some difference between the data and the fit since the hypothesis is too simple. But, for degrees higher than 5 or so, the polynomial starts to wiggle too much to capture the training data. The test set error increases as the predictive power of the polynomial goes down thanks to the contortions it must endure to fit the training data.\nThus the test set error first decreases as the model get more expressive, and then, once we exceed a certain level of complexity (here indexed by \\(d\\)), it increases. This idea can be used to identify just the right amount of complexity in the model by picking as the best hypothesis as the one that minimizes test set error or risk. In our case this happens around \\(d=4\\). (This exact number will depend on the random points chosen into the training and test sets) For complexity lower than this critical value, identified by the red vertical line in the diagram, the hypotheses underfit; for complexity higher, they overfit.\n\n\n\nBias-variance tradeoff: underfitting vs overfitting as complexity increases\n\n\nKeep in mind that as you see in the plot above this minimum can be shallow: in this case any of the low order polynomials would be “good enough”."
  },
  {
    "objectID": "posts/testingtraining/index.html#is-this-still-a-test-set",
    "href": "posts/testingtraining/index.html#is-this-still-a-test-set",
    "title": "Learning Bounds and the Test Set",
    "section": "Is this still a test set?",
    "text": "Is this still a test set?\nBut something should be troubling you about this discussion. We have made no discussion on the error bars on our error estimates, primarily because we have not carried out any resampling to make this possible.\nBut secondly we seem to be “visually fitting” a value of \\(d\\). It cant be kosher to use as a test set something you did some fitting on…\nWe have contaminated our test set. The moment we use it in the learning process, it is not a test set.\nThe answer to the second question is to use a validation set, and leave a separate test set aside. The answer to the first is to use cross-validation, which is a kind of resampling method that uses multiple validation sets!\nTO make some of these concepts more concrete, let us understand the mathematics behind finite sized samples and the learning process.\n\nLearning from finite sized samples\nIf we have very large samples, the law of large numbers tells us that we can estimate expectations nicely by making sample averages.\nHowever, we rarely have very large samples in learning situations (unlike when we are looking for posteriors). But, we can use Hoeffding’s inequality to understand how our sample quantities differ from the population ones.\nHoeffding’s inequality applies to the situation where we have a population of binary random variables with fraction \\(\\mu\\) of things of one type (heads vs tails, red vs green). We do not have access to this population, but rather, to a sample drawn with replacement from this population, where the fraction is \\(\\nu\\).\nThen (where the probability can be thought of as amongst many samples):\n\\[P(\\vert \\nu - \\mu \\vert &gt; \\epsilon) \\le 2e^{-2\\epsilon^2 N}\\]\nwhere N is the size of the sample. Clearly the sample fraction approaches the population fraction as N gets very large.\nTo put this in the context of the learning problem for a hypothesis \\(h\\), identify heads(1) with \\(h(x_i) \\ne f(x_i)\\) at sample \\(x_i\\), and tails(0) otherwise. Then \\(\\mu\\) is the error rate (also called the 1-0 loss) in the population, which we dont know, while \\(\\nu\\) is the same for the sample. It can be shown that similar results hold for the mean-squared error.\nThen one can say:\n\\[P(\\vert R_{in}(h) - R_{out}(h) \\vert &gt; \\epsilon) \\le 2e^{-2\\epsilon^2 N}\\]\nNow notice that we fit a \\(h=g\\) on the training sample. This means that we see as many hypothesis as there are in out hypothesis space. Typically this is infinite, but learning theory allows us to consider a finite effective hypothesis space size, as most hypothesis are not that different from each other. (This is formalized in VC theory, definitely out of scope for this class).\nThe problem here is that the Hoeffding inequality holds ONCE we have picked a hypothesis \\(h\\), as we need it to label the 1 and 0s. But over the training set we one by one pick all the models in the hypothesis space, before discarding all but one. Thus Hoeffding’s inequality does not hold.\nHowever what you can do is this: since the best fit \\(g\\) is one of the \\(h\\) in the hypothesis space \\(\\cal{H}\\), \\(g\\) must be either \\(h_1\\) OR \\(h_2\\) OR….and there are say effectively M such choices.\nThen:\n\\[P(\\vert R_{in}(g) - R_{out}(g) \\vert \\ge \\epsilon) &lt;= \\sum_{h_i \\in \\cal{H}}  P(\\vert R_{in}(h_i) - R_{out}(h_i) \\vert \\ge \\epsilon) &lt;=  2\\,M\\,e^{-2\\epsilon^2 N}\\]\nThus this tells us that for \\(N &gt;&gt; M\\) our in-sample risk and out-of-sample risk converge asymptotically and that minimizing our in-sample risk can be used as a proxy for minimizing the unknown out-of-sample risk.\nThus we do not have to hope any more and learning is feasible.\nThis also tells us something about complexity. M is a measure of this complexity, and it tells us that our bound is worse for more complex hypothesis spaces. This is our notion of overfitting.\nThe Hoeffding inequality can be repharased. Pick a tolerance \\(\\delta\\). Then, note that with probability \\(1 - 2\\,M\\,e^{-2\\epsilon^2 N}\\), \\(\\vert R_{out} - R_{in} \\vert &lt; \\epsilon\\). This means\n\\[R_{out} &lt;= R_{in} + \\epsilon\\]\nNow let \\(\\delta =  2\\,M\\,e^{-2\\epsilon^2 N}\\).\nThen, with probability \\(1-\\delta\\):\n\\[R_{out} &lt;= R_{in} + \\sqrt{\\frac{1}{2N}ln(\\frac{2M}{\\delta})}\\]\n\n\nWhat about the test set?\nThe bound above can now be used to understand why the test set idea is a good one. One objection to using a test set might be that it just seems to be another sample like the training sample. What so great about it? How do we know that low test error means we generalize well?\nThe key observation here is that the test set is looking at only one hypothesis because the fitting is already done on the training set. So \\(M=1\\) for this sample, and the “in-test-sample” error approaches the population error much faster! Also, the test set does not have an optimistic bias like the training set, which is why the training set bound had the larger effective M factor.\nThis is also why, once you start fitting for things like the complexity parameter on the test set, you cant call it a test set any more since we lose this tight guarantee.\nFinally, a test set has a cost. You have less data in the training set and must thus fit a less complex model."
  },
  {
    "objectID": "posts/mlp_classification/index.html",
    "href": "posts/mlp_classification/index.html",
    "title": "Multi-Layer Perceptron for Classification",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn.apionly as sns\nsns.set_context(\"poster\")\n\nTwo additional imports here, seaborn and tqdm. Install via pip or conda\n\nc0=sns.color_palette()[0]\nc1=sns.color_palette()[1]\nc2=sns.color_palette()[2]\n\n\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\ncm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\ndef points_plot(ax, Xtr, Xte, ytr, yte, clf_predict, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.3, psize=20):\n    h = .02\n    X=np.concatenate((Xtr, Xte))\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n\n    Z = clf_predict(np.c_[xx.ravel(), yy.ravel()])\n    ZZ = Z.reshape(xx.shape)\n    plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n    showtr = ytr\n    showte = yte\n    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n    # and testing points\n    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    return ax,xx,yy\n\n\n\nIn order to illustrate classification by a MLP, we first create some noisy moon shaped data. The noise level here and the amount of data is the first thing you might want to experiment with to understand the interplay of amount of data, noise level, number of parameters in the model we use to fit, and overfitting as illustrated by jagged boundaries.\nWe standardize the data so that it is distributed about 0 as well\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\ndataX, datay = make_moons(noise=0.35, n_samples=400)\ndataX = StandardScaler().fit_transform(dataX)\nX_train, X_test, y_train, y_test = train_test_split(dataX, datay, test_size=.4)\n\n\nh=.02\nx_min, x_max = dataX[:, 0].min() - .5, dataX[:, 0].max() + .5\ny_min, y_max = dataX[:, 1].min() - .5, dataX[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# just plot the dataset first\ncm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\nax = plt.gca()\n# Plot the training points\nax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.5, s=30)\n# and testing points\nax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.5, s=30)\nax.set_xlim(xx.min(), xx.max())\nax.set_ylim(yy.min(), yy.max())\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as fn\nfrom torch.autograd import Variable\nimport torch.utils.data\n\n\n\n\nWe wrap the construction of our network\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, nonlinearity = fn.tanh, additional_hidden_wide=0):\n        super(MLP, self).__init__()\n        self.fc_initial = nn.Linear(input_dim, hidden_dim)\n        self.fc_mid = nn.ModuleList()\n        self.additional_hidden_wide = additional_hidden_wide\n        for i in range(self.additional_hidden_wide):\n            self.fc_mid.append(nn.Linear(hidden_dim, hidden_dim))\n        if self.additional_hidden_wide != -1:\n            self.fc_final = nn.Linear(hidden_dim, output_dim)\n        self.nonlinearity = nonlinearity\n\n    def forward(self, x):\n        x = self.fc_initial(x)\n        x = self.nonlinearity(x)\n        if self.additional_hidden_wide != -1:\n            for i in range(self.additional_hidden_wide):\n                x = self.fc_mid[i](x)\n                x = self.nonlinearity(x)\n            x = self.fc_final(x)\n        return x\n\nWe use it to train. Notice the double-&gt;float casting. Numpy defautlts to double but torch defaulta to float to enable memory efficient GPU usage.\n\nnp.dtype(np.float).itemsize, np.dtype(np.double).itemsize\n\n(8, 8)\n\n\nBut torch floats are 4 byte as can be seen from here: http://pytorch.org/docs/master/tensors.html\n\n\nPoints to note:\n\nprinting a model prints its layers, handy. Note that we implemented layers as functions. The autodiff graph is constructed on the fly on the first forward pass and used in backward.\nwe had to cast to float\nmodel.parameters gives us params, model.named_parameters() gives us assigned names. You can set your own names when you create a layer\nwe create an iterator over the data, more precisely over batches by doing iter(loader). This dispatches to the __iter__ method of the dataloader. (see https://github.com/pytorch/pytorch/blob/4157562c37c76902c79e7eca275951f3a4b1ef78/torch/utils/data/dataloader.py#L416) Always explore source code to understand what is going on\n\n\nmodel2 = MLP(input_dim=2, hidden_dim=3, output_dim=2, nonlinearity=fn.tanh, additional_hidden_wide=1)\nprint(model2)\ncriterion = nn.CrossEntropyLoss(size_average=True)\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\nlr, epochs, batch_size = 1e-1 , 1000 , 64\noptimizer = torch.optim.SGD(model2.parameters(), lr = lr )\naccum=[]\nfor k in range(epochs):\n    localaccum = []\n    for localx, localy in iter(loader):\n        localx = Variable(localx.float())\n        localy = Variable(localy.long())\n        output = model2.forward(localx)\n        loss = criterion(output, localy)\n        model2.zero_grad()\n        loss.backward()\n        optimizer.step()\n        localaccum.append(loss.data[0])\n    accum.append(np.mean(localaccum))\nplt.plot(accum);                      \n\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=3)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=3, out_features=3)\n  )\n  (fc_final): Linear(in_features=3, out_features=2)\n)\n\n\n\n\n\n\n\n\n\nThe out put from the foward pass is run on the entire test set. Since pytorch tracks layers upto but before the loss, this handily gives us the softmax output, which we can then use np.argmax on.\n\ntestoutput = model2.forward(Variable(torch.from_numpy(X_test).float()))\ntestoutput\n\nVariable containing:\n-1.3997  1.1056\n 0.4321 -0.1878\n 0.2325 -0.0586\n-1.4748  1.1468\n 3.1649 -2.2684\n 2.9101 -2.0822\n 1.1887 -0.7474\n 1.4796 -1.0149\n-3.5469  2.5426\n-3.7733  2.6948\n-0.2148  0.2858\n 3.9723 -2.8604\n-1.9199  1.4439\n 0.3520 -0.1263\n 3.3732 -2.4237\n 2.5095 -1.7848\n 2.8704 -2.0525\n-1.8319  1.3913\n-0.0806  0.1818\n-0.3542  0.3635\n-0.7936  0.6824\n-0.0041  0.1306\n 2.1060 -1.4357\n 0.2766 -0.1130\n 0.8709 -0.5103\n-1.3461  1.0524\n 3.3596 -2.4143\n-0.7062  0.6287\n 1.5634 -1.0407\n-0.0492  0.1600\n 0.0524  0.0948\n 1.8808 -1.3182\n-3.4808  2.4987\n-1.9241  1.4596\n 2.3021 -1.5925\n-0.7218  0.6272\n-3.6070  2.5861\n-3.5477  2.5420\n-2.4397  1.7995\n-1.6359  1.2494\n 0.9166 -0.5441\n-2.9952  2.1791\n-0.9801  0.8223\n-1.1452  0.9344\n 0.1102  0.0475\n-3.6323  2.6063\n-2.7851  2.0277\n-3.1108  2.2546\n-0.5392  0.5021\n-0.8784  0.7372\n-3.9028  2.7874\n 2.6638 -1.8992\n-3.0897  2.2326\n 2.7642 -1.9722\n-2.2850  1.6921\n-1.7232  1.3218\n 4.2042 -3.0399\n 1.5363 -1.0084\n 1.0733 -0.6613\n 4.3506 -3.1474\n-0.8530  0.7152\n 4.2824 -3.0974\n-0.0404  0.1513\n 0.1457 -0.0290\n-0.9239  0.7579\n-1.3236  1.0476\n-2.5510  1.8800\n 1.2802 -0.8159\n-3.4907  2.5036\n-0.2133  0.2611\n 1.7668 -1.1816\n 3.1958 -2.2577\n-0.7845  0.6720\n-2.2681  1.6901\n-3.7748  2.6956\n 2.9403 -2.0981\n 1.5907 -1.0496\n 1.3040 -0.8349\n 0.1481  0.0123\n 0.8719 -0.5111\n-0.3822  0.4076\n-1.1132  0.8936\n 1.1114 -0.6897\n 0.0516  0.0699\n-1.0205  0.8473\n-0.9822  0.8226\n-1.4213  1.1121\n 1.3364 -0.8580\n-1.7266  1.3185\n 1.8002 -1.2186\n-1.1045  0.8875\n 0.3044 -0.0899\n-1.6213  1.2412\n-0.3257  0.3136\n-0.6594  0.5660\n-0.0061  0.1307\n-0.0723  0.1303\n 2.2622 -1.5531\n 0.6425 -0.3400\n 3.5527 -2.5243\n 1.4703 -0.9585\n 0.4742 -0.2152\n-3.9635  2.8235\n-2.0605  1.5491\n-3.7193  2.6581\n 1.3573 -0.9252\n 2.8911 -2.0675\n-2.6437  1.9390\n-0.7654  0.6714\n 1.1626 -0.7502\n 0.3224 -0.1071\n-2.2417  1.6723\n 0.5332 -0.3175\n 0.6907 -0.3770\n 0.0929  0.0572\n 2.2506 -1.5903\n-2.1372  1.5943\n 2.4597 -1.7284\n-0.9419  0.7867\n 1.3708 -0.8835\n 0.4274 -0.1803\n-1.8787  1.4144\n-2.8720  2.0867\n-3.2267  2.3252\n 3.5142 -2.5284\n 0.5442 -0.2674\n-1.0442  0.8467\n 0.2991 -0.0894\n-0.5332  0.4947\n-1.9302  1.4491\n-0.3737  0.4014\n-0.2904  0.3255\n-3.1304  2.2694\n 0.2306 -0.0791\n-3.0481  2.2154\n 0.7065 -0.4357\n 0.0990  0.0610\n-1.1890  0.9566\n 0.2239 -0.0386\n 3.4539 -2.4829\n-0.2503  0.3136\n-3.7716  2.6952\n-3.8336  2.7356\n-0.5169  0.4874\n 3.9846 -2.8736\n-2.4657  1.8112\n-1.8777  1.4178\n-1.7955  1.3672\n-0.2697  0.3276\n 2.2395 -1.5380\n-2.7763  2.0324\n 0.3578 -0.1327\n 0.0418  0.0952\n 1.6378 -1.0839\n 1.5454 -1.0322\n-3.6984  2.6439\n 0.4830 -0.2253\n-0.1812  0.2496\n 0.9076 -0.5379\n-3.3711  2.4227\n[torch.FloatTensor of size 160x2]\n\n\n\ny_pred = testoutput.data.numpy().argmax(axis=1)\ny_pred\n\narray([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n       0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1])\n\n\nYou can write your own but we import some metrics from sklearn\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nconfusion_matrix(y_test, y_pred)\n\narray([[66, 16],\n       [ 5, 73]])\n\n\n\naccuracy_score(y_test, y_pred)\n\n0.86875000000000002\n\n\nWe can wrap this machinery in a function, and pass this function to points_plot to predict on a grid and thus give us a boundary viz\n\ndef make_pred(X_set):\n    output = model2.forward(Variable(torch.from_numpy(X_set).float()))\n    return output.data.numpy().argmax(axis=1)\n\n\nwith sns.plotting_context('poster'):\n    ax = plt.gca()\n    points_plot(ax, X_train, X_test, y_train, y_test, make_pred);\n\n\n\n\n\n\n\n\n\n\n\n\nSince we want to run many experiments, we’ll go ahead and wrap our fitting process in a sklearn style interface. Another example of such an interface is here\n\nfrom tqdm import tnrange, tqdm_notebook\nclass MLPClassifier:\n    \n    def __init__(self, input_dim, hidden_dim, \n                 output_dim, nonlinearity = fn.tanh, \n                 additional_hidden_wide=0):\n        self._pytorch_model = MLP(input_dim, hidden_dim, output_dim, nonlinearity, additional_hidden_wide)\n        self._criterion = nn.CrossEntropyLoss(size_average=True)\n        self._fit_params = dict(lr=0.1, epochs=200, batch_size=64)\n        self._optim = torch.optim.SGD(self._pytorch_model.parameters(), lr = self._fit_params['lr'] )\n        \n    def __repr__(self):\n        num=0\n        for k, p in self._pytorch_model.named_parameters():\n            numlist = list(p.data.numpy().shape)\n            if len(numlist)==2:\n                num += numlist[0]*numlist[1]\n            else:\n                num+= numlist[0]\n        return repr(self._pytorch_model)+\"\\n\"+repr(self._fit_params)+\"\\nNum Params: {}\".format(num)\n    \n    def set_fit_params(self, *, lr=0.1, epochs=200, batch_size=64):\n        self._fit_params['batch_size'] = batch_size\n        self._fit_params['epochs'] = epochs\n        self._fit_params['lr'] = lr\n        self._optim = torch.optim.SGD(self._pytorch_model.parameters(), lr = self._fit_params['lr'] )\n        \n    def fit(self, X_train, y_train):\n        dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n        loader = torch.utils.data.DataLoader(dataset, batch_size=self._fit_params['batch_size'], shuffle=True)\n        self._accum=[]\n        for k in tnrange(self._fit_params['epochs']):\n            localaccum = []\n            for localx, localy in iter(loader):\n                localx = Variable(localx.float())\n                localy = Variable(localy.long())\n                output = self._pytorch_model.forward(localx)\n                loss = self._criterion(output, localy)\n                self._pytorch_model.zero_grad()\n                loss.backward()\n                self._optim.step()\n                localaccum.append(loss.data[0])\n            self._accum.append(np.mean(localaccum))\n        \n    def plot_loss(self):\n        plt.plot(self._accum, label=\"{}\".format(self))\n        plt.legend()\n        plt.show()\n        \n    def plot_boundary(self, X_train, X_test, y_train, y_test):\n        points_plot(plt.gca(), X_train, X_test, y_train, y_test, self.predict);\n        plt.text(1, 1, \"{}\".format(self), fontsize=12)\n        plt.show()\n        \n    def predict(self, X_test):\n        output = self._pytorch_model.forward(Variable(torch.from_numpy(X_test).float()))\n        return output.data.numpy().argmax(axis=1)\n        \n\nSome points about this:\n\nwe provide the ability to change the fitting parameters\nby implementing a __repr__ we let an instance of this class print something useful. Specifically we created a count of the number of parameters so that we can get a comparison of data size to parameter size.\n\n\n\n\n\nlogistic = MLPClassifier(input_dim=2, hidden_dim=2, output_dim=2, nonlinearity=lambda x: x, additional_hidden_wide=-1)\nlogistic.set_fit_params(epochs=1000)\nprint(logistic)\nlogistic.fit(X_train,y_train)\n\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n  )\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 6\n\n\n\n\n\n\n\n\n\nwith sns.plotting_context('poster'):\n    logistic.plot_loss()\n\n\n\n\n\n\n\n\n\nypred = logistic.predict(X_test)\n#training and test accuracy\naccuracy_score(y_train, logistic.predict(X_train)), accuracy_score(y_test, ypred)\n\n(0.84583333333333333, 0.80625000000000002)\n\n\n\nwith sns.plotting_context('poster'):\n    logistic.plot_boundary(X_train, X_test, y_train, y_test)\n\n\n\n\n\n\n\n\n\nclf = MLPClassifier(input_dim=2, hidden_dim=20, output_dim=2, nonlinearity=fn.tanh, additional_hidden_wide=1)\nclf.set_fit_params(epochs=1000)\nprint(clf)\nclf.fit(X_train,y_train)\n\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=20)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=20, out_features=20)\n  )\n  (fc_final): Linear(in_features=20, out_features=2)\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 522\n\n\n\n\n\n\n\n\n\nwith sns.plotting_context('poster'):\n    clf.plot_loss()\n\n\n\n\n\n\n\n\n\nypred = clf.predict(X_test)\n#training and test accuracy\naccuracy_score(y_train, clf.predict(X_train)), accuracy_score(y_test, ypred)\n\n(0.875, 0.875)\n\n\n\nwith sns.plotting_context('poster'):\n    clf.plot_boundary(X_train, X_test, y_train, y_test)\n\n\n\n\n\n\n\n\n\n\n\nHere is space for you to play. You might want to collect accuracies on the traing and test set and plot on a grid of these parameters or some other visualization. Notice how you might want to adjust number of epochs for convergence.\n\nfor additional in [0, 2, 4]:\n    for hdim in [2, 10, 100, 1000]:\n        print('====================')\n        print('Additional', additional, \"hidden\", hdim)\n        clf = MLPClassifier(input_dim=2, hidden_dim=hdim, output_dim=2, nonlinearity=fn.tanh, additional_hidden_wide=additional)\n        if additional &gt; 2 and hdim &gt; 50:\n            clf.set_fit_params(epochs=1000)\n        else:\n            clf.set_fit_params(epochs=500)\n        print(clf)\n        clf.fit(X_train,y_train)\n        with sns.plotting_context('poster'):\n            clf.plot_loss()\n            clf.plot_boundary(X_train, X_test, y_train, y_test)\n        print(\"Train acc\", accuracy_score(y_train, clf.predict(X_train)))\n        print(\"Test acc\", accuracy_score(y_test, clf.predict(X_test)))\n\n====================\nAdditional 0 hidden 2\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=2, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.866666666667\nTest acc 0.8375\n====================\nAdditional 0 hidden 10\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=10)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=10, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.870833333333\nTest acc 0.88125\n====================\nAdditional 0 hidden 100\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=100)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=100, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.870833333333\nTest acc 0.8875\n====================\nAdditional 0 hidden 1000\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=1000)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=1000, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 5002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.833333333333\nTest acc 0.8\n====================\nAdditional 2 hidden 2\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=2, out_features=2)\n    (1): Linear(in_features=2, out_features=2)\n  )\n  (fc_final): Linear(in_features=2, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.875\nTest acc 0.825\n====================\nAdditional 2 hidden 10\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=10)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=10, out_features=10)\n    (1): Linear(in_features=10, out_features=10)\n  )\n  (fc_final): Linear(in_features=10, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 272\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.8625\nTest acc 0.90625\n====================\nAdditional 2 hidden 100\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=100)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=100, out_features=100)\n    (1): Linear(in_features=100, out_features=100)\n  )\n  (fc_final): Linear(in_features=100, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 20702\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.891666666667\nTest acc 0.85\n====================\nAdditional 2 hidden 1000\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=1000)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=1000, out_features=1000)\n    (1): Linear(in_features=1000, out_features=1000)\n  )\n  (fc_final): Linear(in_features=1000, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 2007002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.8375\nTest acc 0.8125\n====================\nAdditional 4 hidden 2\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=2, out_features=2)\n    (1): Linear(in_features=2, out_features=2)\n    (2): Linear(in_features=2, out_features=2)\n    (3): Linear(in_features=2, out_features=2)\n  )\n  (fc_final): Linear(in_features=2, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 36\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.508333333333\nTest acc 0.4875\n====================\nAdditional 4 hidden 10\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=10)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=10, out_features=10)\n    (1): Linear(in_features=10, out_features=10)\n    (2): Linear(in_features=10, out_features=10)\n    (3): Linear(in_features=10, out_features=10)\n  )\n  (fc_final): Linear(in_features=10, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 492\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.8625\nTest acc 0.85\n====================\nAdditional 4 hidden 100\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=100)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=100, out_features=100)\n    (1): Linear(in_features=100, out_features=100)\n    (2): Linear(in_features=100, out_features=100)\n    (3): Linear(in_features=100, out_features=100)\n  )\n  (fc_final): Linear(in_features=100, out_features=2)\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 40902\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.916666666667\nTest acc 0.84375\n====================\nAdditional 4 hidden 1000\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=1000)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=1000, out_features=1000)\n    (1): Linear(in_features=1000, out_features=1000)\n    (2): Linear(in_features=1000, out_features=1000)\n    (3): Linear(in_features=1000, out_features=1000)\n  )\n  (fc_final): Linear(in_features=1000, out_features=2)\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 4009002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.866666666667\nTest acc 0.80625"
  },
  {
    "objectID": "posts/mlp_classification/index.html#create-some-noisy-moon-shaped-data",
    "href": "posts/mlp_classification/index.html#create-some-noisy-moon-shaped-data",
    "title": "Multi-Layer Perceptron for Classification",
    "section": "",
    "text": "In order to illustrate classification by a MLP, we first create some noisy moon shaped data. The noise level here and the amount of data is the first thing you might want to experiment with to understand the interplay of amount of data, noise level, number of parameters in the model we use to fit, and overfitting as illustrated by jagged boundaries.\nWe standardize the data so that it is distributed about 0 as well\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\ndataX, datay = make_moons(noise=0.35, n_samples=400)\ndataX = StandardScaler().fit_transform(dataX)\nX_train, X_test, y_train, y_test = train_test_split(dataX, datay, test_size=.4)\n\n\nh=.02\nx_min, x_max = dataX[:, 0].min() - .5, dataX[:, 0].max() + .5\ny_min, y_max = dataX[:, 1].min() - .5, dataX[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# just plot the dataset first\ncm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\nax = plt.gca()\n# Plot the training points\nax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, alpha=0.5, s=30)\n# and testing points\nax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.5, s=30)\nax.set_xlim(xx.min(), xx.max())\nax.set_ylim(yy.min(), yy.max())\n\n\n\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as fn\nfrom torch.autograd import Variable\nimport torch.utils.data"
  },
  {
    "objectID": "posts/mlp_classification/index.html#writing-a-multi-layer-perceptron-class",
    "href": "posts/mlp_classification/index.html#writing-a-multi-layer-perceptron-class",
    "title": "Multi-Layer Perceptron for Classification",
    "section": "",
    "text": "We wrap the construction of our network\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, nonlinearity = fn.tanh, additional_hidden_wide=0):\n        super(MLP, self).__init__()\n        self.fc_initial = nn.Linear(input_dim, hidden_dim)\n        self.fc_mid = nn.ModuleList()\n        self.additional_hidden_wide = additional_hidden_wide\n        for i in range(self.additional_hidden_wide):\n            self.fc_mid.append(nn.Linear(hidden_dim, hidden_dim))\n        if self.additional_hidden_wide != -1:\n            self.fc_final = nn.Linear(hidden_dim, output_dim)\n        self.nonlinearity = nonlinearity\n\n    def forward(self, x):\n        x = self.fc_initial(x)\n        x = self.nonlinearity(x)\n        if self.additional_hidden_wide != -1:\n            for i in range(self.additional_hidden_wide):\n                x = self.fc_mid[i](x)\n                x = self.nonlinearity(x)\n            x = self.fc_final(x)\n        return x\n\nWe use it to train. Notice the double-&gt;float casting. Numpy defautlts to double but torch defaulta to float to enable memory efficient GPU usage.\n\nnp.dtype(np.float).itemsize, np.dtype(np.double).itemsize\n\n(8, 8)\n\n\nBut torch floats are 4 byte as can be seen from here: http://pytorch.org/docs/master/tensors.html\n\n\nPoints to note:\n\nprinting a model prints its layers, handy. Note that we implemented layers as functions. The autodiff graph is constructed on the fly on the first forward pass and used in backward.\nwe had to cast to float\nmodel.parameters gives us params, model.named_parameters() gives us assigned names. You can set your own names when you create a layer\nwe create an iterator over the data, more precisely over batches by doing iter(loader). This dispatches to the __iter__ method of the dataloader. (see https://github.com/pytorch/pytorch/blob/4157562c37c76902c79e7eca275951f3a4b1ef78/torch/utils/data/dataloader.py#L416) Always explore source code to understand what is going on\n\n\nmodel2 = MLP(input_dim=2, hidden_dim=3, output_dim=2, nonlinearity=fn.tanh, additional_hidden_wide=1)\nprint(model2)\ncriterion = nn.CrossEntropyLoss(size_average=True)\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\nlr, epochs, batch_size = 1e-1 , 1000 , 64\noptimizer = torch.optim.SGD(model2.parameters(), lr = lr )\naccum=[]\nfor k in range(epochs):\n    localaccum = []\n    for localx, localy in iter(loader):\n        localx = Variable(localx.float())\n        localy = Variable(localy.long())\n        output = model2.forward(localx)\n        loss = criterion(output, localy)\n        model2.zero_grad()\n        loss.backward()\n        optimizer.step()\n        localaccum.append(loss.data[0])\n    accum.append(np.mean(localaccum))\nplt.plot(accum);                      \n\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=3)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=3, out_features=3)\n  )\n  (fc_final): Linear(in_features=3, out_features=2)\n)\n\n\n\n\n\n\n\n\n\nThe out put from the foward pass is run on the entire test set. Since pytorch tracks layers upto but before the loss, this handily gives us the softmax output, which we can then use np.argmax on.\n\ntestoutput = model2.forward(Variable(torch.from_numpy(X_test).float()))\ntestoutput\n\nVariable containing:\n-1.3997  1.1056\n 0.4321 -0.1878\n 0.2325 -0.0586\n-1.4748  1.1468\n 3.1649 -2.2684\n 2.9101 -2.0822\n 1.1887 -0.7474\n 1.4796 -1.0149\n-3.5469  2.5426\n-3.7733  2.6948\n-0.2148  0.2858\n 3.9723 -2.8604\n-1.9199  1.4439\n 0.3520 -0.1263\n 3.3732 -2.4237\n 2.5095 -1.7848\n 2.8704 -2.0525\n-1.8319  1.3913\n-0.0806  0.1818\n-0.3542  0.3635\n-0.7936  0.6824\n-0.0041  0.1306\n 2.1060 -1.4357\n 0.2766 -0.1130\n 0.8709 -0.5103\n-1.3461  1.0524\n 3.3596 -2.4143\n-0.7062  0.6287\n 1.5634 -1.0407\n-0.0492  0.1600\n 0.0524  0.0948\n 1.8808 -1.3182\n-3.4808  2.4987\n-1.9241  1.4596\n 2.3021 -1.5925\n-0.7218  0.6272\n-3.6070  2.5861\n-3.5477  2.5420\n-2.4397  1.7995\n-1.6359  1.2494\n 0.9166 -0.5441\n-2.9952  2.1791\n-0.9801  0.8223\n-1.1452  0.9344\n 0.1102  0.0475\n-3.6323  2.6063\n-2.7851  2.0277\n-3.1108  2.2546\n-0.5392  0.5021\n-0.8784  0.7372\n-3.9028  2.7874\n 2.6638 -1.8992\n-3.0897  2.2326\n 2.7642 -1.9722\n-2.2850  1.6921\n-1.7232  1.3218\n 4.2042 -3.0399\n 1.5363 -1.0084\n 1.0733 -0.6613\n 4.3506 -3.1474\n-0.8530  0.7152\n 4.2824 -3.0974\n-0.0404  0.1513\n 0.1457 -0.0290\n-0.9239  0.7579\n-1.3236  1.0476\n-2.5510  1.8800\n 1.2802 -0.8159\n-3.4907  2.5036\n-0.2133  0.2611\n 1.7668 -1.1816\n 3.1958 -2.2577\n-0.7845  0.6720\n-2.2681  1.6901\n-3.7748  2.6956\n 2.9403 -2.0981\n 1.5907 -1.0496\n 1.3040 -0.8349\n 0.1481  0.0123\n 0.8719 -0.5111\n-0.3822  0.4076\n-1.1132  0.8936\n 1.1114 -0.6897\n 0.0516  0.0699\n-1.0205  0.8473\n-0.9822  0.8226\n-1.4213  1.1121\n 1.3364 -0.8580\n-1.7266  1.3185\n 1.8002 -1.2186\n-1.1045  0.8875\n 0.3044 -0.0899\n-1.6213  1.2412\n-0.3257  0.3136\n-0.6594  0.5660\n-0.0061  0.1307\n-0.0723  0.1303\n 2.2622 -1.5531\n 0.6425 -0.3400\n 3.5527 -2.5243\n 1.4703 -0.9585\n 0.4742 -0.2152\n-3.9635  2.8235\n-2.0605  1.5491\n-3.7193  2.6581\n 1.3573 -0.9252\n 2.8911 -2.0675\n-2.6437  1.9390\n-0.7654  0.6714\n 1.1626 -0.7502\n 0.3224 -0.1071\n-2.2417  1.6723\n 0.5332 -0.3175\n 0.6907 -0.3770\n 0.0929  0.0572\n 2.2506 -1.5903\n-2.1372  1.5943\n 2.4597 -1.7284\n-0.9419  0.7867\n 1.3708 -0.8835\n 0.4274 -0.1803\n-1.8787  1.4144\n-2.8720  2.0867\n-3.2267  2.3252\n 3.5142 -2.5284\n 0.5442 -0.2674\n-1.0442  0.8467\n 0.2991 -0.0894\n-0.5332  0.4947\n-1.9302  1.4491\n-0.3737  0.4014\n-0.2904  0.3255\n-3.1304  2.2694\n 0.2306 -0.0791\n-3.0481  2.2154\n 0.7065 -0.4357\n 0.0990  0.0610\n-1.1890  0.9566\n 0.2239 -0.0386\n 3.4539 -2.4829\n-0.2503  0.3136\n-3.7716  2.6952\n-3.8336  2.7356\n-0.5169  0.4874\n 3.9846 -2.8736\n-2.4657  1.8112\n-1.8777  1.4178\n-1.7955  1.3672\n-0.2697  0.3276\n 2.2395 -1.5380\n-2.7763  2.0324\n 0.3578 -0.1327\n 0.0418  0.0952\n 1.6378 -1.0839\n 1.5454 -1.0322\n-3.6984  2.6439\n 0.4830 -0.2253\n-0.1812  0.2496\n 0.9076 -0.5379\n-3.3711  2.4227\n[torch.FloatTensor of size 160x2]\n\n\n\ny_pred = testoutput.data.numpy().argmax(axis=1)\ny_pred\n\narray([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n       0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n       0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1])\n\n\nYou can write your own but we import some metrics from sklearn\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nconfusion_matrix(y_test, y_pred)\n\narray([[66, 16],\n       [ 5, 73]])\n\n\n\naccuracy_score(y_test, y_pred)\n\n0.86875000000000002\n\n\nWe can wrap this machinery in a function, and pass this function to points_plot to predict on a grid and thus give us a boundary viz\n\ndef make_pred(X_set):\n    output = model2.forward(Variable(torch.from_numpy(X_set).float()))\n    return output.data.numpy().argmax(axis=1)\n\n\nwith sns.plotting_context('poster'):\n    ax = plt.gca()\n    points_plot(ax, X_train, X_test, y_train, y_test, make_pred);"
  },
  {
    "objectID": "posts/mlp_classification/index.html#making-a-scikit-learn-like-interface",
    "href": "posts/mlp_classification/index.html#making-a-scikit-learn-like-interface",
    "title": "Multi-Layer Perceptron for Classification",
    "section": "",
    "text": "Since we want to run many experiments, we’ll go ahead and wrap our fitting process in a sklearn style interface. Another example of such an interface is here\n\nfrom tqdm import tnrange, tqdm_notebook\nclass MLPClassifier:\n    \n    def __init__(self, input_dim, hidden_dim, \n                 output_dim, nonlinearity = fn.tanh, \n                 additional_hidden_wide=0):\n        self._pytorch_model = MLP(input_dim, hidden_dim, output_dim, nonlinearity, additional_hidden_wide)\n        self._criterion = nn.CrossEntropyLoss(size_average=True)\n        self._fit_params = dict(lr=0.1, epochs=200, batch_size=64)\n        self._optim = torch.optim.SGD(self._pytorch_model.parameters(), lr = self._fit_params['lr'] )\n        \n    def __repr__(self):\n        num=0\n        for k, p in self._pytorch_model.named_parameters():\n            numlist = list(p.data.numpy().shape)\n            if len(numlist)==2:\n                num += numlist[0]*numlist[1]\n            else:\n                num+= numlist[0]\n        return repr(self._pytorch_model)+\"\\n\"+repr(self._fit_params)+\"\\nNum Params: {}\".format(num)\n    \n    def set_fit_params(self, *, lr=0.1, epochs=200, batch_size=64):\n        self._fit_params['batch_size'] = batch_size\n        self._fit_params['epochs'] = epochs\n        self._fit_params['lr'] = lr\n        self._optim = torch.optim.SGD(self._pytorch_model.parameters(), lr = self._fit_params['lr'] )\n        \n    def fit(self, X_train, y_train):\n        dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n        loader = torch.utils.data.DataLoader(dataset, batch_size=self._fit_params['batch_size'], shuffle=True)\n        self._accum=[]\n        for k in tnrange(self._fit_params['epochs']):\n            localaccum = []\n            for localx, localy in iter(loader):\n                localx = Variable(localx.float())\n                localy = Variable(localy.long())\n                output = self._pytorch_model.forward(localx)\n                loss = self._criterion(output, localy)\n                self._pytorch_model.zero_grad()\n                loss.backward()\n                self._optim.step()\n                localaccum.append(loss.data[0])\n            self._accum.append(np.mean(localaccum))\n        \n    def plot_loss(self):\n        plt.plot(self._accum, label=\"{}\".format(self))\n        plt.legend()\n        plt.show()\n        \n    def plot_boundary(self, X_train, X_test, y_train, y_test):\n        points_plot(plt.gca(), X_train, X_test, y_train, y_test, self.predict);\n        plt.text(1, 1, \"{}\".format(self), fontsize=12)\n        plt.show()\n        \n    def predict(self, X_test):\n        output = self._pytorch_model.forward(Variable(torch.from_numpy(X_test).float()))\n        return output.data.numpy().argmax(axis=1)\n        \n\nSome points about this:\n\nwe provide the ability to change the fitting parameters\nby implementing a __repr__ we let an instance of this class print something useful. Specifically we created a count of the number of parameters so that we can get a comparison of data size to parameter size."
  },
  {
    "objectID": "posts/mlp_classification/index.html#the-simplest-model-and-a-more-complex-model",
    "href": "posts/mlp_classification/index.html#the-simplest-model-and-a-more-complex-model",
    "title": "Multi-Layer Perceptron for Classification",
    "section": "",
    "text": "logistic = MLPClassifier(input_dim=2, hidden_dim=2, output_dim=2, nonlinearity=lambda x: x, additional_hidden_wide=-1)\nlogistic.set_fit_params(epochs=1000)\nprint(logistic)\nlogistic.fit(X_train,y_train)\n\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n  )\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 6\n\n\n\n\n\n\n\n\n\nwith sns.plotting_context('poster'):\n    logistic.plot_loss()\n\n\n\n\n\n\n\n\n\nypred = logistic.predict(X_test)\n#training and test accuracy\naccuracy_score(y_train, logistic.predict(X_train)), accuracy_score(y_test, ypred)\n\n(0.84583333333333333, 0.80625000000000002)\n\n\n\nwith sns.plotting_context('poster'):\n    logistic.plot_boundary(X_train, X_test, y_train, y_test)\n\n\n\n\n\n\n\n\n\nclf = MLPClassifier(input_dim=2, hidden_dim=20, output_dim=2, nonlinearity=fn.tanh, additional_hidden_wide=1)\nclf.set_fit_params(epochs=1000)\nprint(clf)\nclf.fit(X_train,y_train)\n\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=20)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=20, out_features=20)\n  )\n  (fc_final): Linear(in_features=20, out_features=2)\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 522\n\n\n\n\n\n\n\n\n\nwith sns.plotting_context('poster'):\n    clf.plot_loss()\n\n\n\n\n\n\n\n\n\nypred = clf.predict(X_test)\n#training and test accuracy\naccuracy_score(y_train, clf.predict(X_train)), accuracy_score(y_test, ypred)\n\n(0.875, 0.875)\n\n\n\nwith sns.plotting_context('poster'):\n    clf.plot_boundary(X_train, X_test, y_train, y_test)"
  },
  {
    "objectID": "posts/mlp_classification/index.html#experimentation-space",
    "href": "posts/mlp_classification/index.html#experimentation-space",
    "title": "Multi-Layer Perceptron for Classification",
    "section": "",
    "text": "Here is space for you to play. You might want to collect accuracies on the traing and test set and plot on a grid of these parameters or some other visualization. Notice how you might want to adjust number of epochs for convergence.\n\nfor additional in [0, 2, 4]:\n    for hdim in [2, 10, 100, 1000]:\n        print('====================')\n        print('Additional', additional, \"hidden\", hdim)\n        clf = MLPClassifier(input_dim=2, hidden_dim=hdim, output_dim=2, nonlinearity=fn.tanh, additional_hidden_wide=additional)\n        if additional &gt; 2 and hdim &gt; 50:\n            clf.set_fit_params(epochs=1000)\n        else:\n            clf.set_fit_params(epochs=500)\n        print(clf)\n        clf.fit(X_train,y_train)\n        with sns.plotting_context('poster'):\n            clf.plot_loss()\n            clf.plot_boundary(X_train, X_test, y_train, y_test)\n        print(\"Train acc\", accuracy_score(y_train, clf.predict(X_train)))\n        print(\"Test acc\", accuracy_score(y_test, clf.predict(X_test)))\n\n====================\nAdditional 0 hidden 2\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=2, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.866666666667\nTest acc 0.8375\n====================\nAdditional 0 hidden 10\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=10)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=10, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 52\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.870833333333\nTest acc 0.88125\n====================\nAdditional 0 hidden 100\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=100)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=100, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.870833333333\nTest acc 0.8875\n====================\nAdditional 0 hidden 1000\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=1000)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=1000, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 5002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.833333333333\nTest acc 0.8\n====================\nAdditional 2 hidden 2\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=2, out_features=2)\n    (1): Linear(in_features=2, out_features=2)\n  )\n  (fc_final): Linear(in_features=2, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 24\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.875\nTest acc 0.825\n====================\nAdditional 2 hidden 10\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=10)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=10, out_features=10)\n    (1): Linear(in_features=10, out_features=10)\n  )\n  (fc_final): Linear(in_features=10, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 272\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.8625\nTest acc 0.90625\n====================\nAdditional 2 hidden 100\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=100)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=100, out_features=100)\n    (1): Linear(in_features=100, out_features=100)\n  )\n  (fc_final): Linear(in_features=100, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 20702\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.891666666667\nTest acc 0.85\n====================\nAdditional 2 hidden 1000\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=1000)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=1000, out_features=1000)\n    (1): Linear(in_features=1000, out_features=1000)\n  )\n  (fc_final): Linear(in_features=1000, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 2007002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.8375\nTest acc 0.8125\n====================\nAdditional 4 hidden 2\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=2)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=2, out_features=2)\n    (1): Linear(in_features=2, out_features=2)\n    (2): Linear(in_features=2, out_features=2)\n    (3): Linear(in_features=2, out_features=2)\n  )\n  (fc_final): Linear(in_features=2, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 36\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.508333333333\nTest acc 0.4875\n====================\nAdditional 4 hidden 10\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=10)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=10, out_features=10)\n    (1): Linear(in_features=10, out_features=10)\n    (2): Linear(in_features=10, out_features=10)\n    (3): Linear(in_features=10, out_features=10)\n  )\n  (fc_final): Linear(in_features=10, out_features=2)\n)\n{'lr': 0.1, 'epochs': 500, 'batch_size': 64}\nNum Params: 492\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.8625\nTest acc 0.85\n====================\nAdditional 4 hidden 100\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=100)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=100, out_features=100)\n    (1): Linear(in_features=100, out_features=100)\n    (2): Linear(in_features=100, out_features=100)\n    (3): Linear(in_features=100, out_features=100)\n  )\n  (fc_final): Linear(in_features=100, out_features=2)\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 40902\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.916666666667\nTest acc 0.84375\n====================\nAdditional 4 hidden 1000\nMLP(\n  (fc_initial): Linear(in_features=2, out_features=1000)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=1000, out_features=1000)\n    (1): Linear(in_features=1000, out_features=1000)\n    (2): Linear(in_features=1000, out_features=1000)\n    (3): Linear(in_features=1000, out_features=1000)\n  )\n  (fc_final): Linear(in_features=1000, out_features=2)\n)\n{'lr': 0.1, 'epochs': 1000, 'batch_size': 64}\nNum Params: 4009002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain acc 0.866666666667\nTest acc 0.80625"
  },
  {
    "objectID": "posts/entropy/index.html",
    "href": "posts/entropy/index.html",
    "title": "Entropy and Maximum Entropy",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/entropy/index.html#information-and-entropy",
    "href": "posts/entropy/index.html#information-and-entropy",
    "title": "Entropy and Maximum Entropy",
    "section": "Information and Entropy",
    "text": "Information and Entropy\nImagine tossing a coin. If I knew the exact physics of the coin, the initial conditions of the tossing, I could predict deterministically how the coin would land. But usually, without knowing anything, I say by symmetry or by “long-run” experince that the odds of getting heads are 50%.\nThis reflects my minimal knowledge about this system. Without knowing anything else about the universe…the physics, the humidity, the weighting of the coin, etc..assuming a probability of 0.5 of getting heads is the most conservative thing I could do.\nYou can think of this as a situation with minimal information content.\nIt is a very interesting situation, however, when you think about this from the perspective of the multiple ways you could get half the coin tosses in a long run of coin tosses come up heads. There is only one way in which you can get all coin tosses come up heads. There are \\(n \\choose n/2\\) ways on the other hand in which you can get half heads! You can think of this situation being one of more states or events being consistent with a probability of heads of 0.5 than with a probability of 1.\nBy the same token, an election with a win probability of 0.99 isnt that interesting. A lot of information went into getting this probability presumably: polls, economic modelling, etc. But to get such certainty implies a greater determinism-to-randomness ratio in the process.\nOne can think of information as the reduction in uncertainty from learning an outcome\nClearly we need a measure of uncertainty so that we can quantify how much it has decreased.\n\nDesiderata for a measure of uncertainty\n\nmust be continuous so that there are no jumps\nmust be additive across events or states, and must increase as the number of events/states increases"
  },
  {
    "objectID": "posts/entropy/index.html#entropy-measures-uncertainty",
    "href": "posts/entropy/index.html#entropy-measures-uncertainty",
    "title": "Entropy and Maximum Entropy",
    "section": "Entropy measures uncertainty",
    "text": "Entropy measures uncertainty\nA function that satisfies these desiderata is the information entropy:\n\\[H(p) = - E_p[log(p)] = - \\int p(x) log(p(x))dx \\,\\,\\,OR\\, - \\sum_i p_i log(p_i) \\]\nThus the entropy is the average log probability of an event…\n\nExample of the coin toss or Bernoulli variable\n\\[H(p) = - E_p[log(p)] = - p*log(p) - (1-p)*log(1-p)\\]\nFor \\(p=0\\) or \\(p=1\\) we must use L’Hospital’s rule: if we have the division of two limits as \\(0/0\\) or \\(\\infty/\\infty\\) then differentiate both the numerator and denominator and try again:\n\\[\\lim_{p \\to 0}  \\frac{log(p)}{1/p} =  \\lim_{p \\to 0}  \\frac{1/p}{-1/p^2} = 0\\]\n\nimport math\np = np.linspace(0,1,100)\ndef h(p):\n    if p==1.:\n        ent = 0\n    elif p==0.:\n        ent = 0\n    else:\n        ent = - (p*math.log(p) + (1-p)* math.log(1-p))\n    return ent\nplt.plot(p, [h(pr) for pr in p]);\nplt.axvline(0.5, 0, 1,'r')\n\n\n\n\n\n\n\n\nThus you can see there is maximal uncertainty at 0.5."
  },
  {
    "objectID": "posts/entropy/index.html#thermodynamic-notion-of-entropy",
    "href": "posts/entropy/index.html#thermodynamic-notion-of-entropy",
    "title": "Entropy and Maximum Entropy",
    "section": "Thermodynamic notion of Entropy",
    "text": "Thermodynamic notion of Entropy\nImagine dividing \\(N\\) objects amongst \\(M\\) bins. One can think of this as stone tossing, where we toss N stones and see in which bin they land up. There is a distribution for this, \\(\\{p_i\\}\\), of-course, so lets see what it is.\nThere are \\(N\\) ways to fill the first bin, \\(N-1\\) ways to fill the second, \\(N-2\\) ways to fill the third, and so on…thus \\(N!\\) ways. Since we dont distinguish the arrangement of objects in each bin we must divide bu the factorial of the bin amounts. If we then assume a uniform chance of landing in each bucket, then we just get the nultinomial distribution:\n\\[P(n_1, n_2, ..., n_M) = \\frac{N!}{\\prod_{i} n_i!} \\prod_i (\\frac{1}{M})^{n_i} = \\frac{N!}{\\prod_{i} n_i!} \\left(\\frac{1}{M}\\right)^N\\]\n\\[ W =  \\frac{N!}{\\prod_{i} n_i!} \\]\nis called the multiplicity and the entropy is then defined as:\n\\[H = \\frac{1}{N} log(W)\\] which is:\n\\[\\frac{1}{N}log(P(n_i, n_2, ...,n_M))\\]\nwith a constant term removed.\n\\[H = \\frac{1}{N} log(N!) - \\frac{1}{N} \\sum_i log(n_i!)\\].\nUsing Stirling’s approximation \\(log(N!) \\sim Nlog(N) -N\\) as \\(N \\to \\infty\\) and where the fractions \\(n_i/N\\) are held fixed:\n\\[ H =  \\frac{1}{N}\\left( N log(N) - N - \\sum_i (n_i log(n_i) - n_i)\\right)\\]\n\\[ = log(N) -1 -\\frac{1}{N} \\sum_i (Np_i log(Np_i) - Np_i) = log(N) -1 - \\sum_i \\left(p_i(log(N) + log(p_i)) - p_i\\right)\\]\nThus\n\\[H = -\\sum_i p_i log(p_i)\\]\nIf the probabilities of landing in each bucket are not equal, ie not uniform, then we can show:\n\\[\\frac{1}{N}log(P(n_i, n_2, ...,n_M)) = -\\sum_i p_i log(\\frac{p_i}{q_i})\\]\nThis definition has origins in statistical mechanics. Entropy was first introduced in thermodynamics and then later interpreted as a measure of disorder: how many events or states can a system constrained to have a given enrgy have. A physicist calls a particular arrangement \\(\\{n_i\\} = (m_1, n_2, n_3,...,n_M)\\) a microstate and the overall distribution of \\(\\{p_i\\}\\), here the multinomial , a macrostate, with \\(W\\) calledthe weight."
  },
  {
    "objectID": "posts/entropy/index.html#maximum-entropy-maxent",
    "href": "posts/entropy/index.html#maximum-entropy-maxent",
    "title": "Entropy and Maximum Entropy",
    "section": "Maximum Entropy (maxent)",
    "text": "Maximum Entropy (maxent)\nMaximum entropy is the notion of finding distributions consistent with constraints and the current state of our knowledge . In other words, what would be the least surprising distribution? The one with the least additional assumptions?\nWe can maximize\n\\[H = -\\sum_i p_i log(p_i)\\]\nin the case of the ball and bin model above, by considering the langrange-multiplier enhanced, constraint enforcing entropy\n\\[H = -\\sum_i p_i log(p_i) + \\lambda \\left( \\sum_i p(x_i) - 1 \\right)\\]\n\\[\\frac{\\partial H}{\\partial p_j} = 0 \\implies -(1+log(p_j)) + \\lambda = 0\\]\nThis means that the \\(p_j\\)’s are all equal and thus must be \\(\\frac{1}{M}\\): thus the distribution with all \\(p\\)s equal maximizes entropy.\nThe distribution that can happen in the most ways is the one with the highest entropy, as we can see above.\n\nNormal as maxent\nThe origin story of the gaussian itself is that many small effects add up to produce them. It is exactly the “many” aspect os these that makes the gaussian a maxent distribution. For every sequence that produces an unbalanced outcome(like a long string of heads), there are many more ways of producing a balanced outcome. In otherwords, there are so many microstates of the system that can produce the “peak” macrostates.\nThis is a plot from McElreath of a bunch of generalized normal distributions. with same mean and variance. The Gaussuan has the highest entropy, as we shall prove below.\n\n\n\nGeneralized normal distributions (left) and their entropy as a function of shape parameter (right). The Gaussian (shape=2) has maximum entropy among distributions with fixed variance.\n\n\nIf you think about entropy increasing as we make a distribution flatter, you realize that the shape must come about because finite and equal variance puts a limit on how wide the distribution can be.\n\\[\\renewcommand{kld}{D_{KL}}\\]\nFor a gaussian\n\\[p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(x - \\mu)^2/2\\sigma^2}\\]\n\\[H(p) = E_p[log(p)] = E_p[-\\frac{1}{2}log(2\\pi\\sigma^2) - (x - \\mu)^2/2\\sigma^2]\\]\n\\[ =  -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}E_p[(x - \\mu)^2] = -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2} = \\frac{1}{2}log(2\\pi e \\sigma^2)\\]\nNo other distribution \\(q\\) can have higher entropy than this, provided they share the same variance and mean.\nTo see this consider (note change in order, we are considering \\(\\kld(q, p)\\):\n\\[\\kld(q, p) = E_q[log(q/p)] = H(q,p) - H(q)\\]\n\\[H(q,p) = E_q[log(p)] = E_q[-\\frac{1}{2}log(2\\pi\\sigma^2) - (x - \\mu)^2/2\\sigma^2] \\\\= -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}E_q[(x - \\mu)^2]\\]\nThe second expectation here is the variance \\(\\s\nigma^2\\) on the assumption that \\(E_q[x] = \\mu\\).\nThus\n\\[H(q,p) =  -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2} =  -\\frac{1}{2}log(2\\pi e \\sigma^2) = H(p)\\]\nNow as we have shown \\(\\kld(q,p) &gt;=0\\). This means that \\(H(q,p) - H(q) &gt;= 0\\). Which then means that \\(H(p) - H(q) &gt;= 0\\) or \\(H(p) &gt;= H(q)\\). This means that the Gaussian has the highest entropy of any distribution with the same mean and variance.\nSee http://www.math.uconn.edu/~kconrad/blurbs/analysis/entropypost.pdf for details on maxent for distributions.\n\n\nBinomial as Maxent\nInformation entropy increases as a probability distribution becomes more even. We saw that with the thermodynamic idea of entropy and the multinomial distribution.\nConsider the situation when:\n\nonly two outcomes (unordered) are possible.\nthe process generating the outcomes is invariant in time, ie the expected value remains constant (over temporal or other subsequences)\n\nThen it turns out that for these constraints, the maximum entropy distribution is the binomial. The binomial basically spreads probability out as evenly and conservatively as possible, making sure that outcomes that have many more ways they can happen have more probability mass. Basically the binomial figures the number of ways any possible sequence of data can be realized, which is what entropy does. Thus it turns out that likelihoods derived by such counting turn out to be maximum entropy likelihoods.\n\\[H(q,p) &gt;= H(q) \\implies -E_q[log(p)] &gt;= -E_q[log(q)]\\]\nFor binomial parameter \\(\\lambda/n\\):\n\\[ H(q, p) = - \\sum_i q_i log(p_i) = -\\sum_i q_i \\left(log \\left(\\frac{\\lambda}{n}\\right)^{x_i}  + log \\left(\\frac{n-\\lambda}{n}\\right)^{n - x_i} \\right)\\]\n\\[ =  - \\sum_i q_i \\left( x_i log\\left(\\frac{\\lambda}{n}\\right) + (n - x_i) log \\left(\\frac{n-\\lambda}{n}\\right)\\right)\\]\n\\[ =  - \\sum_i q_i  \\left( x_i log \\left(\\frac{\\lambda}{n-\\lambda}\\right)  + n log \\left(\\frac{n-\\lambda}{\\lambda}\\right) \\right)\\]\n\\[ H (q, p) =  - n log \\left(\\frac{n-\\lambda}{\\lambda}\\right) -  log\\left(\\frac{\\lambda}{n-\\lambda}\\right)E_q[x]\\]\nNow, if \\(E_q[x] = \\lambda\\), our invariant expectation, we have \\(H(q,p) = H(p)\\) as we get the same formula if we substitute \\(q=p\\) to get the entropy of the binomial. In other words, \\(H(p) &gt;= H(q)\\) and we have shown the binomial has maximum entropy amongst discrete distributions with two outcomes and fixed expectations."
  },
  {
    "objectID": "posts/entropy/index.html#the-importance-of-maxent",
    "href": "posts/entropy/index.html#the-importance-of-maxent",
    "title": "Entropy and Maximum Entropy",
    "section": "The importance of maxent",
    "text": "The importance of maxent\nThe most common distributions used as likelihoods (and priors) in modeling are those in the exponential family. The exponential family can be defined as having pmf or pdf:\n\\[p(x|\\theta) =  \\frac{1}{Z(\\theta)} h(x) e^{\\theta^T\\phi(x)}\\]\nWhere \\(Z(\\theta)\\), also called the partition function, is the normalization.\nFor example, the univariate Gaussian Distribution can be obtained with:\n\\[\n\\begin{eqnarray}\n\\theta &=& \\begin{pmatrix}\\mu/\\sigma^2 \\\\-1/2\\sigma^2\\end{pmatrix}\\\\\n\\phi(x) &=&  \\begin{pmatrix}x \\\\x^2\\end{pmatrix}\\\\\nZ(\\mu, \\sigma^2) &=& \\sigma\\sqrt{2\\pi} e^{\\mu^2/2\\sigma^2}\\\\\nh(x) &=& 1\n\\end{eqnarray}\n\\]\nEach member of the exponential family turns out to be a maximum entropy distribution subject to different constraints. These distributions are then used as likelihoods.\n\n\n\nRelationships among members of the exponential family: Gamma, Normal, Binomial, and Poisson distributions arise as limiting cases of the Exponential distribution.\n\n\nFor example, the gamma distribution, which we shall see later, is maximum entropy amongst all distributions with the same mean and same average logarithm. The poisson distribution, used for low event rates, is maxent under similar conditions as the binomial as it is a special case of the binomial. The exponential distribution is maxent among all non-negative continuous distributions with the same average inter-event displacement. (In our births example, the inter-birth time).\nWe’ll talk more about these distributions when we encounter them, and when we talk about generalized linear models.\nBut here is the critical point. We will often choose a maximum entropy distribution as a likelihood . Information entropy ennumerates the number of ways a distribution can arise, after having fixed some assumptions. Thus, in choosing a MAXENT distribution as a likelihood, we choose a distribution that once the constraints has been met, does not contain any additional assumptions. It is thus the most conservative distribution we could choose consistent with our constraints."
  },
  {
    "objectID": "posts/hmcexplore/index.html",
    "href": "posts/hmcexplore/index.html",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\nimport seaborn as sns\n\nfrom IPython.core.display import Image\n\nimport pymc3 as pm"
  },
  {
    "objectID": "posts/hmcexplore/index.html#looking-for-the-perfect-mcmc",
    "href": "posts/hmcexplore/index.html#looking-for-the-perfect-mcmc",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Looking for the perfect MCMC?",
    "text": "Looking for the perfect MCMC?\nThe problem with MH is that is sensitive to the step size, Gibbs we need to know how to draw from the conditionals. What we want is the ability to adjust the step size but at the same time preserving the properties of MCMC, namely being able to sample the whole space and at the same time converge to the target distribution. We also would like to have no burnin, no autocorrelation, guarantee mixing and convergence very fast and no tunable parameters (and a pony).\nIn other words, we’d like to explore the typical-set surface smoothly. To do this we must first characterize the surface, something we can do via a gradient. To do this we must identify the equation of the typical set surface so that we can find the gradient which is perpendicular to the surface. And once we do that, we are not done, as the gradient points towards regions of higher density (modes) from the surface of the typical set.\nTo do this imagine a sliver \\(dP = p(q)dq\\) thats in the typical set. If \\(dq\\) (we are using \\(q\\) instead of \\(x\\)) is small enough, then we can consider the typical set as a collection of foliates \\(\\{q_i\\}\\) each of constant probability density \\(p(q)=c_i\\) where \\(c_i\\) is a constant. Thus there are n such foliates, or “orbits”, or level sets. Now we know that the gradient is perpendicular to such level-sets and we can use it to characterize these sets."
  },
  {
    "objectID": "posts/hmcexplore/index.html#mechanics-to-the-rescue",
    "href": "posts/hmcexplore/index.html#mechanics-to-the-rescue",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Mechanics to the rescue",
    "text": "Mechanics to the rescue\nWe can make our usual connection to the energy of a physical system by tentatively identifying the energy \\(E = - log(p(q))\\). This is what we did to find distributions in Metropolis and the inverse \\(p(q) = e^{-E(q)/T}\\) the minima of functions in simulated annealing. There we proposed a move using a proposal distribution, creating a random walk.\nWe dont want to do that here, preferring something that will move us smoothly along a level set. We use our newly acquired knowledge of data-augmentation and gibbs-sampling from an augmented distribution instead.\nBut first let us make a change of purpose and notation. Lets write now the potential energy as:\n\\[V(q) = - log\\, p(q)\\]\nsuch that\n\\[p(q) = e^{-V(q)}.\\]\n\nfun = lambda q: sp.stats.norm.pdf(q) \n\nqq = np.linspace(-5,5,100)\nplt.plot(qq, fun(qq))\n\n\n\n\n\n\n\n\n\nV = lambda q: -np.log(fun(q))\nplt.plot(qq, V(qq))\n\n\n\n\n\n\n\n\nThe basic idea is to add a momentum variable \\(p\\) for each \\(q\\) in our probability density, adding a kinetic energy term to the potential energy to create a total energy, and thus creating a joint pdf \\(p(p,q)\\).\nHow would this work? And why momentum? Lets think about a rocket (or a satellite with thrusters it can fire) in orbit around the earth\n\n\n\nA satellite in orbit: with the right momentum, it follows a constant-energy trajectory around the Earth, analogous to HMC traversing level sets. From Betancourt.\n\n\nIf this rocket had no velocity, it would simply fall down to the earth because it would not be able to counterbalance earth’s gravitational potential (the equivalent of the energy we formulated above and have used in simulated annealing). On the other hand, if it had too much velocity, it would escape earth’s gravity and take off to mars or similar.\nIf we add just the right amount of momentum, it will exactly counterbalance the gravitational force, and the satellite will continue to move in its orbit. And this is an orbit of minimum energy (and therefore constant energy, since a system at minumum energy wont lift from it unless kicked(perhaps stochastically) to do so). Thus the satellite will move exactly along a level-curve of the energy level, in a direction exactly perpendicular to the gradient. Such motion is called conservative, as it conserves energy. In mechanics, the conserving of energy is related to the time-independence and thus time irreversibility of motion: is there is no explicit dependence on time in the equations of motion (there always is implicit dependence as positions and momenta depend on time), energy is conserved."
  },
  {
    "objectID": "posts/hmcexplore/index.html#data-augmentation",
    "href": "posts/hmcexplore/index.html#data-augmentation",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nRecall that the basic idea behind Data Augmentation is to resolve difficulty in simulate from \\(p(x)\\) using the traditional methods by constructing a joint pdf such that:\n\\[ \\int p(x,y)\\, dy = p(x)  \\]\nand simulating from the associated conditional pdfs, \\(p(x|y)\\) and \\(p(y|x)\\) is easy.\nThe DA algorithm is based on this joint pdf, and we marginalize over the augmenting variable \\(y\\) to obtain samples for \\(p(x)\\).\nThe simplest form of DA algorithm takes the following form:\n\nDraw \\(Y\\sim P_{Y|X}(.|x)\\) and call the observed value y\nDraw \\(X_{n+1} \\sim P_{X|Y}(.|y)\\)\n\nAfter disregarding the \\(Y_i\\) in our samples in making a \\(X\\) histogram, we have samples \\(X_i \\sim P(x)\\).\nTo achieve our goal we introduce a new variable as we did for DA and Slice Sampling we now call \\(p\\). As we explained in DA the new joint distribution\n\\[p(q) = \\int p(q,p) \\, dp \\]\nAn easy way to achieve this is if \\(p(q,p)=p(q)p(p \\vert q)\\). And since we can choose our joint distribution as we like, we choose (the reasons will be obvious soon).\n\\[ p(q,p) = e^{-V(q)-K(q,p)} = e^{-V(q)} e^{-p^2/2m}\\]\nWhere \\(m\\) is just a constant thus \\(p(p)\\) is Gaussian. The kinetic energy if \\(p\\) is the momentum is \\(K(q, p)=p^2/2m\\). This kinetic energy \\(K(p,q)\\) has a mass parameter \\(m(q)\\)(one can consider generalizations of the kinetic energy term from this version we have from introductory physics).\nThe choice of a kinetic energy term then is the choice of a conditional probability distribution over the “augmented” momentum which ensures that\n\\[\\int dp p(p, q) = \\int dp p(p \\vert q) p(q) = p(q) \\int p(p \\vert q) dp = p(q).\\]\nThus the key to moving a sampler along a probability level curve is to give the sampler momentum and thus kinetic energy via the augmented momemtum variable. In other words, we must carry out an augmentation with an additional momentum which leaves the energy Hamiltonian\n\\[H(p, q) = \\frac{p^2}{2m} +  V(q) = E_i,\\]\nand thus:\n\\[H(p,q) = -log(p(p,q)) = -log p(p \\vert q) - log p(q)\\]\nwith \\(E_i\\) constants (constant energies) for each level-set foliate and where the potential energy \\(V(q) = -log(p(q))\\) replaces the energy term we had earlier in simulated annealing.\nThus it is critical to realize that we are now looking at level curve foliates of \\(p(p,q)\\), not those of \\(p(q)\\). Our real aim is to explore the latter. This joint distribution is called the canonical distribution\nWith a quadratic in \\(p\\) and if \\(V(q) = \\frac{1}{2}q^2\\) our distribution is gaussian and the level sets are ellipses of constant energy, as illustrated below, in a space called phase space, which is constructed by plotting the co-ordinates against the momenta: that is, it is just the space in which our augmented joint-distribution lives.\n\n\n\nHamiltonian level sets in phase space: constant-energy elliptical orbits in (p,q) for a Gaussian target. From Betancourt."
  },
  {
    "objectID": "posts/hmcexplore/index.html#hamiltonian-mechanics",
    "href": "posts/hmcexplore/index.html#hamiltonian-mechanics",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Hamiltonian Mechanics",
    "text": "Hamiltonian Mechanics\nThe game now is to sample from this two-N-dimensional distribution and marginalize over the momenta to get the distribution from the \\(q\\). To carry out this sampling, we’ll use the physics equations of motion in the Hamiltonian Formalism (thus leading to the name Hamiltonian Monte Carlo) to “glide” over a level set. Given a Hamiltonian H, the Hamiltonian equations of motion are as follows:\n\\[\n\\begin{eqnarray}\n\\frac{dp}{dt}  &=& -\\frac {\\partial H}{\\partial q} \\\\\n\\frac{dq}{dt}  &=& \\frac {\\partial H}{\\partial p}\n\\end{eqnarray}\n\\]\nIf we have\n\\[H=p^2/2m + V(q)\\]\nthen \\(\\frac{dp}{dt} = -\\frac {\\partial H}{\\partial q} = -\\frac {\\partial V}{\\partial q} = Force\\) is newton’s law, and the other equation sats that velocity is momentum divided by mass.\n\nAn example\nHere is an example of a harmonic oscillator with mass 1 and spring-constant 1\n\nq_t = lambda t: 4. * np.cos(t)\np_t = lambda t: -4. * np.sin(t)\nH_x_r_t = lambda t: q_t(t)**2/2. + p_t(t)**2/2.\n\nt = np.linspace(0,2*np.pi,100)\n\nplt.figure(figsize=[12,4])\nplt.subplot(1,2,1)\nplt.plot(t, q_t(t), label='q')\nplt.plot(t, p_t(t), label='p')\nplt.plot(t, H_x_r_t(t), label='H')\nplt.xlabel('t')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(q_t(t), p_t(t), '.')\nplt.axis('equal')\nplt.xlabel('q(t)')\nplt.ylabel('p(t)')\n\nplt.show()\n\nprint(\"min and max of H: \", np.min( H_x_r_t(t)), np.max( H_x_r_t(t)))\n\n\n\n\n\n\n\n\nmin and max of H:  8.0 8.0\n\n\n\n\nThe Hamiltonian is conserved\nNotice that in the example above the Hamiltonian is conserved. While the \\(q\\) amd \\(p\\) have time dependencies, the \\(H\\) does not.\nIf the Hamiltonian H doesn’t have a functional dependence on time we see that\n\\[ \\frac{dH}{dt} = \\sum_i \\left[\\frac {\\partial H}{\\partial q_i}\\frac{dq_i}{dt} + \\frac {\\partial H}{\\partial p_i}\\frac{dp_i}{dt} \\right] + \\frac{\\partial H}{\\partial t} \\]\nNotice that I have explicitly indexed the co-ordinates and momenta to reflect the dimensionality of the prolem.\nSubstituting in the Hamiltonian equations of motion we can see: \\[ \\frac{dH}{dt} = \\sum_i \\left[\\frac {\\partial H}{\\partial q_i}\\frac {\\partial H}{\\partial p_i} + (\\frac {\\partial H}{\\partial p_i})(-\\frac {\\partial H}{\\partial q_i}) \\right] + \\frac{\\partial H}{\\partial t} \\]\nSo as long as the Hamiltonian $ H({q_i, p_i}, t)$ depends only on $ {q_i, p_i } $ then \\(\\frac{\\partial H}{\\partial t} = 0\\) and\n\\[ \\frac{dH}{dt} = 0\\ \\]\nand thus $ H(t + t) = H(t) , t $.\nThis time independence is crucial to reversibility: we cannot pick a direction of time in which this system is actually being run.\n\n\nReversibility\nConsider a transformation \\(T_s\\) from \\((q,p) \\to (q',p')\\) where the primed quantities are just the variables at a “later” time \\(t' = t + s\\). This mapping is 1-1 and thus has an inverse \\(T_{-s}\\). This can be obtained by simply negating time in Hamilton’s equations:\n\\[\n\\begin{eqnarray}\n\\frac{dp}{d(-t)}  &=& -\\frac {\\partial H}{\\partial q} \\\\\n\\frac{dq}{d(-t)}  &=& \\frac {\\partial H}{\\partial p}\n\\end{eqnarray}\n\\]\nThis changes the equations of motion and would not be kosher, but we can preserve the equation, by merely transforming \\(p \\to -p\\) and then\n\\[\n\\begin{eqnarray}\n\\frac{d(-p)}{d(-t)}  &=& -\\frac {\\partial H}{\\partial q} \\\\\n\\frac{dq}{d(-t)}  &=& \\frac {\\partial H}{\\partial (-p)}\n\\end{eqnarray}\n\\]\nand we have the old equations back. Notice that the momentum transformation does not change the Hamiltonian as long as the kinetic energy is symmetric in \\(p\\).\nIn other words this is what we need is this: to reverse \\(T_{s}\\), flip the momentum, run Hamiltonian equations backwords in time until you get back to the original position and momentum in phase space at time t, and then flip the momentum again so it is pointing in the right direction (otherwise you would be going the wrong direction on re-running time forward…that is u want to make the tranformation which preserved the Hamiltonian equations above, twice.).\nIn our oscillator example above, \\(T_{-s}\\) is simply a rotation by \\(s\\) radians counterclockwise in phase space, undoing the clockwise rotation created by \\(T_s\\), and then fliiping the direction of rotation again.\nThis is like in the superman movie! It will be critical in crafting a reversible Metropolis-Hastings proposal soon.\n\n\nVolume in phase space is conserved\nNot only does no-explicit-time dynamics conserve the Hamiltonian, it also conserves a volume element in phase space. One can see this by If the determinant of the Jacobian of this transformation is 1, the volume is preserved.\nThe transformation \\(T_{s}\\) for small change \\(s = \\delta\\) can be written as:\n\\[ T_{\\delta} = \\left( \\begin{array}{c} q \\\\ p\\end{array} \\right) + \\delta \\left( \\begin{array}{c} \\frac{dq}{dt} \\\\ \\frac{dp}{dt}\\end{array} \\right)  + O(\\delta^2)\\]\nThe jacobian of this transformation is: \\[\n\\left [\n\\begin{array}{c}\n1 + \\delta \\frac{\\partial^2H}{\\partial q\\partial p} & \\delta \\frac{\\partial^2H}{\\partial p^2} \\\\\n\\delta \\frac{\\partial^2H}{\\partial q^2} &  1 - \\delta \\frac{\\partial^2H}{\\partial p\\partial q}\n\\end{array}\n\\right ]\n\\]\nand thus the determinant is\n\\[ 1 +  O(\\delta^2).\\]\nSince we can compose the time transformation \\(s\\) out of many such data, out Hamiltonian is volume preserving!\nThus as our system evolves, any contraction or expansion in position space must be compensated by a respective expansion or compression in momentum space.\nAs a result of this, the momenta we augment our distribution with must be dual to our pdf’s parameters, transforming in the opposite way so that phase space volumes are invariant.\n\n\nThe microcanonical distribution\nWe have expanded our pdf’s parameter space by auxillary momenta. We have lift our target distribution in \\(q\\) space to a joint probability distribution on phase space called the canonical distribution. In this phase space, we can explore the joint typical set by integrating Hamiltonian equations for a time.\nSince we will consider time-implicit Hamiltonians, these flows will be volume preserving and reversible, and indeed stick to a level surface of energy.\nIndeed we are traveling along constant \\(E\\) or \\(p(p,q)\\) foliates. This allows us to consider a related distribution to the canonical distribution, the microcanonical distribution (the first term in the product below). The canonical distribution can be written as a product of this microcanonical distribution and a marginal energy distribution:\n\\[p(q,p) = p(\\theta_E \\vert E) p(E)\\]\nwhere \\(\\theta_E\\) indexes the position on the level set: ie a co-ordinate for example along the ellipses in our diagram above.\nThe microcanonical distribution thus gives us the states for a given energy (or joint probability) while the marginal tells us the probability of a given level set in the typical set itself.\nThis decomposition of the problem tells us how to explore the entire typical set. We integrate Hamilton’s equations for a while to explore the microcanonical distribution on a given level set."
  },
  {
    "objectID": "posts/hmcexplore/index.html#stochasticity",
    "href": "posts/hmcexplore/index.html#stochasticity",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Stochasticity",
    "text": "Stochasticity\nBut this then leaves us with the problem of having to go from one level-set to another: after all, we wish to explore the entire typical set. We thus need to stochastically explore the marginal energy distribution to go from one level-set to the other.\n\nSampling the momentum\nThe idea for this once again comes from our mechanical satellite analog. Think of a rocket in a partivcular orbit firing its thrusters or letting momentum slip. It can achieve a different orbit.\nThus the solution to exploring the marginal distribution is simple..after exploring a given level set for a while, we resample the momentum, and off to another level-set we go, as illustrated below:\n\n\n\nMomentum resampling in HMC: after gliding along one energy level set, resample momentum to jump to a different orbit. From Betancourt.\n\n\nLet \\(p(E \\vert q)\\) as the transition distribution of energies induced by a momentum resampling using \\(p(p \\vert q) = - log\\, K(p,q)\\) at a given position \\(q\\). Our efficiency of this stochastic exploration over level sets then depends on how narrow this induced distribution is with respect to the marginal energy distribution:\n\n\n\nEnergy distributions: the marginal energy pi(E) versus the microcanonical distribution pi(E|q). Good sampling requires their overlap. From Betancourt.\n\n\nIf the transition distribution is narrow compared to the marginal energy distribution, then the random walk amongst level sets will proceed slowly. But if it matches the marginal energy distribution, we will generate nearly independent samples from the marginal energy distribution very efficiently.\nThus we will draw \\(p\\) from a distribution that is determined by the distribution of momentum, i.e. \\(p \\sim  N(0,\\sqrt{M})\\) for example, and attempt to explore the level sets."
  },
  {
    "objectID": "posts/hmcexplore/index.html#tuning",
    "href": "posts/hmcexplore/index.html#tuning",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Tuning",
    "text": "Tuning\nIt should be clear now that HMC needs to be tuned in at-least two ways:\n\nwhat is our choice of kinetic energy and how does this impact the stochastic exploration of the marginal distribution\nhow long shall we integrate along a certain level set\n\n\nChoice of Kinetic Energy\nIn theory we can choose any Kinetic Energy function K(p) that we deem useful. The ideal kinetic energy would interact with the target distribution to make microcanonical exploration as easy and uniform as possible and marginal exploration well matched by the transition distribution.\nIn practice we often use\n\\[K(p) = p'M^{-1}p \\]\nso that $K(p) = _i $ and M is diagonal square matrix with elements $ 1/m_i $. This would allow us to particularize to various parameters.\nThis is a euclidean-gaussian choice: euclidean as it corresponds to a euclidean norm on the co-ordinate space and gaussian as the conditional distribution it induces is Gaussian. Gaussian choices perform well in higher dimensions as marginal energy distributions become a convolution of more and more parameters and a kind of central-limit theorem holds.\nIf one were to set the inverse mass matrix to the covariance of the target distribution then one would maximally decorrelate the target distribution. This can be seen thus: apply the transformation \\(p \\to \\sqrt{M^{-1}}p\\) which simplifies the kinetic energy but transforms the potential via \\(q \\to \\sqrt{M}q\\) due to the duality and volume preservation of \\(q,p\\) phase space. Then you are effectively dividing \\(q\\) by the squareoot of its distributions covariance and carrying out our decorrelation.\nWe can do this by computing a target covariance in a warm-up (this is not burn-in) phase of our sampler and then setting the kinetic energy appropriately to make an optimized chain.\n\n\nChoice of integration times\nChoosing a kinetic energy defines the shape of the level sets. Now the game is to glide along any one of these. How long should we do it? If we do it too short, we are not taking advantage of the coherent Hamiltonian updates and are back to our drunk random walk. If we do it too long, the topological compactness of these level sets in well defined problems will mean we revisit old points in phase space, yielding slower convergence.\nWhat we need to do is to find the point at which the orbital expectations converge to the spatial expectations..a sort of ergodicity. Integrating beyond this regime brings diminishing returns.\nThis time can be represented with two parameters, \\(L\\), the number of iterations for which we run the Hamiltonian dynamics, and \\(\\epsilon\\) which is the (small) length of time each iteration is run. We say iterations as to solve Hamiltonian differential equations we will need to carry out a numerical solution of these equations with a small time-grid size. Then the total integration time is \\(L \\times \\epsilon\\). These are parameters we need to play with, for this and other reasons (see below).\nBut in general, no single optimal integration time will hold…this depends on which orbit in which level set we are. In general, for a distribution more heavy tailed than a Gaussian, as we move to higher energy level sets in the tails, we will need more time to integrate over there (this can be shown analytically in 1-D$). Thus if we used a static integration time everywhere, the tails might be very poorly explored (and presumably more of the typical set is in the tails of these distributuions than for thin tailed ones.\nThus in general we will want to identify the integration time dynamically. This is the idea behind the No U-Turn Sampler (NUTS) which is the default in both pymc3 and Stan: the basic idea is that when the Hamiltonian flows starts to double back on itself and retrace its steps, we resample momentum again."
  },
  {
    "objectID": "posts/hmcexplore/index.html#simulating-hamiltonian-systems-discretization",
    "href": "posts/hmcexplore/index.html#simulating-hamiltonian-systems-discretization",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "Simulating Hamiltonian Systems: Discretization",
    "text": "Simulating Hamiltonian Systems: Discretization\nUnfortunately we can’t simulate Hamiltonian systems exactly and we have to rely on discretization! What does that mean? Well in the derivations above, we assumed that we could solve the differential equations in question exactly. Since we’re going to be sampling from our Hamiltonian dynamics we need to discretize our systems in to small timesteps of size $ $.\nThere are a number of discretization methods available for simulating dynamics systems. It turns out that all of them aren’t equally useful. Some of the properties of the Let’s start with the simplest, Euler discretization. In all of our discretization examples we’ll make the following assumptions:\n\nthe Hamiltonian has the form H(q, p) = U(q)+K(p)\nK(p) = $p’M^{-1}p $ so that $K(p) = _i $ and M is square with diagonal elements $ 1/m_i $\n\nLet’s start with the most basic discretization, Euler’s method.\n\nEuler Discretization\nEuler discretization involves directly updating the momentum and position at each time step. The algorithm is as follows:\n\n$p_i(t + ) = p_i(t) - _{q(t)} $\n$q_i(t + ) = q_i(t) + $\n\nSee code and plot below for harmonic oscillator, $ H = U(q) + K(p) = + $. Notice that the volume is not preserved as the trajectory we plot for the harmonic oscillator, which should be an elipse, diverges to infinity and the Hamiltonian is not conserved as we’d expect it to be. Something is wrong with this discretization.\n\nU = lambda q: q**2/2\nK = lambda p:  (p**2)/2\ndUdq= lambda q: q\n\ndef euler(p0, q0, L, epsilon):\n    \n    ps = [p0]\n    qs = [q0]\n    \n    pprev = p0\n    qprev = q0\n    \n\n\n    # alternate full steps for position and momentum\n    for i in range(L):\n        p = pprev - epsilon*dUdq(qprev)\n        ps.append(p)\n        q = qprev + epsilon*pprev\n        qs.append(q)\n        \n        pprev = p\n        qprev = q\n\n\n    \n    return (ps, qs)\n\n\neps,eqs = euler(1,0, 55, .3)\nh = [U(q) + K(p) for q, p in zip(eqs, eps)]\n\nplt.figure(figsize=[12,8])\nplt.subplot(1,2,1)\nplt.plot(eqs,eps, 'b.')\nplt.xlabel('q')\nplt.ylabel('p')\nplt.subplot(1,2,2)\nplt.plot(h)\nplt.ylabel('H')\n\n\n\n\n\n\n\n\nMost ODE solvers, like the one we just wrote, suffer from drift:\n\n\n\nNumerical integration error: the exact trajectory (red) versus the leapfrog approximation (green) in a vector field. From Neal.\n\n\nAs we solve longer, error adds coherently, and our trajectory diverges from the true trajectory. This is because our discrete transformations like the Euler one above, do not preserve volume elements: the determinant of the Jacobian is larger than 1. The critical reason behind this is that we update both \\(p\\) and \\(q\\) simultaneously, accumulating error fast.\nIf you think about the Jacobian, then the diagonal terms are 1. But if you update both \\(p\\) and \\(q\\) at once, the off-diagonals are non-zero for small but finite \\(\\epsilon\\) (in other words u cant make \\(\\epsilon \\to 0\\)). Thus volume is not preserved.\nWhat we want are symplectic integrators, which preserve phase space volume elements.\n\n\nBeing symplectic: the Leapfrog method\nAny method that has only shear transforms: ie something that changes only one thing at a time, will work. The leapfrog method is one such idea, involving stage-wise updating the momentum and position. The algorithm is as follows:\n\n$p_i(t + ) = p_i(t) - _{q(t)} $\n$q_i(t + ) = q_i(t) + $\n$p_i(t + ) = p_i(t+) - _{q(t+ )} $\n\nIf we do this repeatedly, the first and last steps can be combined as you can see in the code below.\nNotice that volume is preserved and the trajectory is very stable. In addition the discretization is reversible. The leapfrog method (named because updating momentum and position leapfrog each other) is a symplectic discretization.\n\nU = lambda q: (q)**2/2.\nK = lambda p:  (p**2)/2.\ndUdq= lambda q: q\n\ndef leapfrog(p0, q0, L, epsilon):\n    # keep all the epsilon steps\n    ps = [p0]\n    qs = [q0]\n    \n    # starting position\n    p = p0\n    q = q0\n    \n    # Make a half step for momentum at the beginning\n    p = p - epsilon*dUdq(q)/2.0\n\n    # alternate full steps for position and momentum\n    for i in range(L):\n        q = q + epsilon*p\n        qs.append(q)\n        if (i != L-1):\n            p = p - epsilon*dUdq(q)\n            ps.append(p)\n\n    #make a half step at the end\n    p = p - epsilon*dUdq(q)/2.\n    ps.append(p)\n\n    return (ps, qs)\n\n\n\nlps,lqs = leapfrog(4.,0., 1000, .1 )\nh = [U(q) + K(p) for q, p in zip(lqs, lps)]\n\nprint(\"min and max of leapfrog H: \", np.min(h), np.max(h))\n\nplt.figure(figsize=[12,4])\nplt.subplot(1,2,1)\nplt.plot(lqs,lps, 'b.')\nplt.plot(x_t(t), r_t(t), 'r.')\nplt.xlabel('x(t)')\nplt.ylabel('y(t)')\nplt.axis('equal')\n\nplt.subplot(1,2,2)\nplt.plot(h)\n\nplt.show()\n\nmin and max of leapfrog H:  7.6190477193 8.42105060531\n\n\n\n\n\n\n\n\n\nOne thing to note, see the diagram and code above, is that even though the discretization is symplectic it (like the Euler modification) doesn’t preserve the Hamiltonian perfectly. That’s because of the approximation errors incumbant in discretization. The error is very stable and the value of the Hamiltonian at each step oscillates around the true value.\nBecause of this exact reversibility is lost. There is no reason to sweat though, superman to the rescue! When we sample from the potential in our HMC, we will use the acceptance probability to balance out the error in H.\n\n\nThe Acceptance Probability\nAt the beginning of each sampling step we choose a new \\(p\\) and our current \\(q\\) and then run the Leapfrog algorithm for \\(L\\) steps of size $ $. The new \\(q_L\\) and \\(p_L\\) at the end of our Leapfrog steps is our new proposed values, with our proposal being\n\\[Q(q', p' \\vert q, p) = \\delta(q' - q_L) \\delta (p' - p_L).\\]\nIn other words, only propose something which gives us the integration sanctioned values..deterministic!\nThis would seam to be the end of the story, but remember that even our symplectic integrator has errors. Errors make the Hamiltonian flow irreversible. In other words we can only propose states going forwards in time: the proposal probability for the time-reversed transition is 0 and the acceptance ratio undefined.\nBut we know how to make the Hamiltonian transition reversible! Superman to the rescue. We augment the numerical integration with a momentum flip so that we now consider the transition:\n\\[(q,p) \\to (q_L, -p_L)\\]\nwith proposal:\n\\[Q(q', p' \\vert q, p) = \\delta(q' - q_L) \\delta (p' + p_L).\\]\nThis leads us to accept with probability\n\\[A = \\min[1, \\frac{p(q_L, -p_L)\\delta(q_L -q_L)\\delta(-p_L + p_L)}{p(q,p)\\delta(q-q)\\delta(p-p)}] = \\min[1, \\frac{p(q_L, -p_L)}{p(q,p)}]\\]\nand thus:\n\\[A= \\min[1, \\exp(-U(q_L)+U(q)-K(p_L)+K(p)] \\]\nThis is exactly the kind of acceptance ratio we have seen in simulated annealing and metropolis, and stems from our definition of the joint(canonical) distribution.\nBut the critical thing with HMC is that our time evolution is on a level set. So our acceptance probability is always close to 1, and we have a very efficient sampler. (You might have thought you could pick up a sample after each leapfrog update since it is coherent evolution of Hamiltonian equations, but then you would have to do a momentum reversal and acceptance comparision many many times).\nThe momentum reversal could be left out if you are not within a more complex sampling scheme like HMC within gibbs since you will be resampling anyways. But if you are updating both a discrete parameter and a continuous parameter, you will want to reverse the sign so that you are using the correct \\(p\\) when sampling from the conditionals.\n\n\n\nReversibility in leapfrog integration: negating momentum at the endpoint and reintegrating returns to the starting point. From Neal.\n\n\nIn general we’ll want to sum over all such points in the orbit, since we want time averages to represent a sample from the microcanonical distribution."
  },
  {
    "objectID": "posts/hmcexplore/index.html#the-hmc-algorithm",
    "href": "posts/hmcexplore/index.html#the-hmc-algorithm",
    "title": "Exploring Hamiltonian Monte Carlo",
    "section": "The HMC Algorithm",
    "text": "The HMC Algorithm\nNow that we have the tools in place, let’s describe the HMC algorithm. We’re going to assume as above a Kinetic Energy $ K(p) = $. The algorithm is as follows\n\nfor i=1:N_samples\n\nDraw $ p N(0,M) $\nSet $ q_{c} = q^{(i)} $ where the subscript \\(c\\) stands for current\n$ p_c = p $\nUpdate momentum before going into LeapFrog stage: $ p^* = p_c - $\n\nLeapFrog to get new proposals. For j=1:L\n\n$ q^{} = q^{} + p $\nif not the last step, $ p = p - U(q) $\n\nComplete leapfrog: $ p = p - $\n$ p^{*}= -p $\n$ U_c = U(q_c), ,,, K_c = $\n$ U^{} = U(q^), ,,, K^{*} = $\n$ r \\rm{Unif}(0,1) $\nif $ r &lt; e{(U_{c}-U{}+K_{c}-K^{})} $\n* accept \\(q_i = q^*\\) * otherwise reject\n\n\n\nA simple implementation\nWe implement the HMC algorithm below with a gaussian Kinetic energy\n\n#constants\n\ndef HMC(U,K,dUdq,N,q_0, p_0, epsilon=0.01, L=100):\n    p_mu=0.\n    p_sig=1.\n    \n    current_q = q_0\n    current_p = p_0\n    \n    H = np.zeros(N)\n    qall = np.zeros(N)\n    accept=0\n    for j in range(N):\n    \n        q = current_q\n        p = current_p\n        \n        #draw a new p\n        p = np.random.normal(p_mu, p_sig)\n        \n        current_p=p\n        \n        # leap frog\n        \n        # Make a half step for momentum at the beginning\n        p = p - epsilon*dUdq(q)/2.0\n        \n        \n        # alternate full steps for position and momentum\n        for i in range(L):\n            q = q + epsilon*p\n            if (i != L-1):\n                p = p - epsilon*dUdq(q)\n    \n        #make a half step at the end\n        p = p - epsilon*dUdq(q)/2.\n\n        # negate the momentum\n        p= -p;\n        current_U = U(current_q)\n        current_K = K(current_p)\n\n        proposed_U = U(q)\n        proposed_K = K(p)\n        A=np.exp( current_U-proposed_U+current_K-proposed_K)\n    \n        # accept/reject\n        if np.random.rand() &lt; A:\n            current_q = q\n            qall[j]=q\n            accept+=1\n        else:\n            qall[j] = current_q\n        \n        H[j] = U(current_q)+K(current_p)\n    print(\"accept=\",accept/np.double(N))\n    return H, qall\n\n\n\n# functions\nU = lambda q: q**2/2.\nK = lambda p:  (p**2)/2.\ndUdq= lambda q: q\n\n\n\nH, qall= HMC(U=U,K=K,dUdq=dUdq,N=10000,q_0=0, p_0=-4, epsilon=0.01, L=200)\nplt.hist(qall, bins=50, normed=True)\nx = np.linspace(-4,4,100)\nplt.plot(x, sp.stats.norm.pdf(x),'r')\nplt.show()\n\naccept= 1.0\n\n\n\n\n\n\n\n\n\nWe compare it to a MH sampler with the same number of steps\n\ndef MH_simple(p, n, sig, x0):\n    x_prev = x0\n    x=[]\n    k=1\n    i=0\n    while i&lt;n:\n        x_star = np.random.normal(x_prev, sig)\n        P_star = p(x_star)\n        P_prev = p(x_prev)\n        U =  np.random.uniform()\n        \n        A =  P_star/P_prev\n        if U &lt; A:\n            x.append(x_star)\n            i = i + 1\n            x_prev = x_star\n            k +=1\n        else :\n            x.append(x_prev)\n            x_prev = x[i]  \n            i = i + 1\n            \n    print(\"accept=\",k/np.double(n))\n    return x\n\n\nsamples_mh = MH_simple(p=P, n=10000, sig=4.0, x0=0)\n\nplt.subplot(1,2,1)\nplt.hist(qall, bins=50, normed=True)\nx = np.linspace(-4,4,100)\nplt.plot(x, sp.stats.norm.pdf(x),'r')\n\nplt.subplot(1,2,2)\nplt.hist(samples_mh, bins=50, normed=True)\nx = np.linspace(-4,4,100)\nplt.plot(x, sp.stats.norm.pdf(x),'r')\nplt.show()\n\naccept= 0.3002\n\n\n\n\n\n\n\n\n\nHere we see that the MH acceptance ration is much lower and the correlation much higher!\n\ndef corrplot(trace,  maxlags=100):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\ncorrplot(qall)\nplt.title('hmc');\n\n\n\n\n\n\n\n\n\ncorrplot(samples_mh)\nplt.title('mh');"
  },
  {
    "objectID": "posts/logisticbp/index.html",
    "href": "posts/logisticbp/index.html",
    "title": "Logistic Regression and Backpropagation",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]\n\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]"
  },
  {
    "objectID": "posts/logisticbp/index.html#logistic-regression-mle",
    "href": "posts/logisticbp/index.html#logistic-regression-mle",
    "title": "Logistic Regression and Backpropagation",
    "section": "Logistic Regression MLE",
    "text": "Logistic Regression MLE\nLogistic regression if one of the well known supervized learning algorithms used for classification.\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the ‘1’ samples from the ‘0’ samples, just like in the diagram above. In other words, we wish to find the “regression” line which divides the samples. Now, a line has the form \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) in 2-dimensions. On one side of this line we have\n\\[w_1 x_1 + w_2 x_2 + w_0 \\ge 0,\\]\nand on the other side we have\n\\[w_1 x_1 + w_2 x_2 + w_0 &lt; 0.\\]\nOur classification rule then becomes:\n\\[\n\\begin{eqnarray}\ny = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &if& \\v{w}\\cdot\\v{x} &lt; 0\n\\end{eqnarray}\n\\]\nwhere \\(\\v{x}\\) is the vector \\(\\{1,x_1, x_2,...,x_n\\}\\) where we have also generalized to more than 2 features.\nWhat hypotheses \\(h\\) can we use to achieve this? One way to do so is to use the sigmoid function:\n\\[h(z) = \\frac{1}{1 + e^{-z}}.\\]\nNotice that at \\(z=0\\) this function has the value 0.5. If \\(z &gt; 0\\), \\(h &gt; 0.5\\) and as \\(z \\to \\infty\\), \\(h \\to 1\\). If \\(z &lt; 0\\), \\(h &lt; 0.5\\) and as \\(z \\to -\\infty\\), \\(h \\to 0\\). As long as we identify any value of \\(y &gt; 0.5\\) as 1, and any \\(y &lt; 0.5\\) as 0, we can achieve what we wished above.\nThis function is plotted below:\n\nh = lambda z: 1./(1+np.exp(-z))\nzs=np.arange(-5,5,0.1)\nplt.plot(zs, h(zs), alpha=0.5);\n\n\n\n\n\n\n\n\nSo we then come up with our rule by identifying:\n\\[z = \\v{w}\\cdot\\v{x}.\\]\nThen \\(h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\ge 0\\) and \\(h(\\v{w}\\cdot\\v{x}) \\lt 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\lt 0\\), and:\n\\[\n\\begin{eqnarray}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray}\n\\]\nWe said above that if \\(h &gt; 0.5\\) we ought to identify the sample with \\(y=1\\)? One way of thinking about this is to identify \\(h(\\v{w}\\cdot\\v{x})\\) with the probability that the sample is a ‘1’ (\\(y=1\\)). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a ‘1’ is \\(\\ge 0.5\\).\nSo suppose we say then that the probability of \\(y=1\\) for a given \\(\\v{x}\\) is given by \\(h(\\v{w}\\cdot\\v{x})\\)?\nThen, the conditional probabilities of \\(y=1\\) or \\(y=0\\) given a particular sample’s features \\(\\v{x}\\) are:\n\\[\\begin{eqnarray}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray}\\]\nThese two can be written together as\n\\[P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} \\]\nThen multiplying over the samples we get the probability of the training \\(y\\) given \\(\\v{w}\\) and the \\(\\v{x}\\):\n\\[P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\]\nWhy use probabilities? Earlier, we talked about how the regression function \\(f(x)\\) never gives us the \\(y\\) exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently.\nWe said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(P(x,y)\\). Thus we need to model \\(y\\) at a given \\(x\\), written as \\(P(y \\mid x)\\), and since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) ,\\]\nand can obtain our joint probability (\\(P(x, y))\\).\nIndeed its important to realize that a particular sample can be thought of as a draw from some “true” probability distribution. If for example the probability of classifying a sample point as a ‘0’ was 0.1, and it turns out that the sample point was actually a ‘0’, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a ‘0’! But, of-course its more unlikely than its likely, and having good probabilities means that we’ll be likely right most of the time, which is what we want to achieve in classification.\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a ‘1’.\nNow if we maximize \\[P(y \\mid \\v{x},\\v{w})\\], we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This maximum likelihood estimation maximises the likelihood of the sample y,\n\\[\\like = P(y \\mid \\v{x},\\v{w}).\\]\nAgain, we can equivalently maximize\n\\[\\loglike = log(P(y \\mid \\v{x},\\v{w}))\\]\nsince the natural logarithm \\(log\\) is a monotonic function. This is known as maximizing the log-likelihood.\n\\[\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).\\]\nThus\n\\[\\begin{eqnarray}\n\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x_i})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x_i})) \\right )\n\\end{eqnarray}\\]\nThe negative of this log likelihood (henceforth abbreviated NLL), is also called the cross-entropy, for reasons that will become clearer soon.\n\\[ NLL = - \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\\]\nWe can calculate the gradient of this cost function, and the hessian as well.\n\\[\\nabla_{\\v{w}} NLL = \\sum_i \\v{x_i}^T (p_i - y_i) = \\v{X}^T \\cdot ( \\v{p} - \\v{w} )\\]\n\\[H = \\v{X}^T diag(p_i (1 - p_i))\\v{X}\\] which is positive definite, making the cross-entropy loss convex with a global minimum.\nLogistic regression can be represented by the following diagram. This diagram uses the language of “units”. A linear unit is followed by a non-linear sigmoidal squashing unit, or more precisely a log-sigmoidal squashing unit which is then used to construct the cross-entropy loss.\n\n\n\nLogistic regression as a computational graph: input passes through a linear layer and sigmoid activation to produce the negative log-likelihood cost."
  },
  {
    "objectID": "posts/logisticbp/index.html#softmax-formulation",
    "href": "posts/logisticbp/index.html#softmax-formulation",
    "title": "Logistic Regression and Backpropagation",
    "section": "Softmax formulation",
    "text": "Softmax formulation\nThe softmax formulation of logistic regression comes from the desire togeneralize logistic regression to the multinomial case, that is more than 2 classes. Its instructive to see the two-class problem formulated in softmax as well.\nThe basic idea is to identify the probabilities \\(p_i\\) and \\(1-p_i\\) as two separate probabilities which are constrained to add to 1. That is\n\\[p_{1i} = p_i ; p_{2i} = 1 - p_i. \\]\nThen, the function \\(h\\) which is used to supply this probability can be reformulated as well:\n\\[p_{1i} = \\frac{e^{\\v{w_1} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}\\]\nand\n\\[p_{2i} = \\frac{e^{\\v{w_2} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}\\]\nThe constraint that these probabilities add to 1 is clearly satisfied, but notice that we now have double the number of parameters we had before. We can obtain the old interpretation of \\(p_{1i}\\) by multiplying both the numerator and denominator by \\(e^{-\\v{w_1} \\cdot \\v{x}}\\).\nThis then identifies \\(\\v{w} = \\v{w_1} - \\v{w_2}\\). In general, we can see that we can always translate the coefficients by a fixed amount \\(\\psi\\) without any change. This leads to a lack of “identification” of the parameters in the softmax formalism, which leads to problems for inference that we must fix. On the plus side, it also suggests a way to calculate softmax in a stable way (see the log-sum-exp trick). We’ll tackle both of these later.\nThe softmax formulation of logistic regression can be illustrated using the following diagram, where we now have 2 linear units. These units are now fed into a nonlinear log-softmax unit (which requires outputs of both the linear units) to produce two log-softmax outputs which is then fed to the NLL loss (cross-entropy).\n\n\n\nMulticlass logistic regression: two linear units feed into a softmax layer, producing class probabilities for the negative log-likelihood cost.\n\n\nIn this formalism, we can write the likelihood and thus NLL more succintly:\n\\[\\like = \\prod_i p_{1i}^{\\mathbb1_1(y_i)} p_{2i}^{\\mathbb1_2(y_i)}\\]\n\\[NLL = -\\sum_i \\left( \\mathbb1_1(y_i) log(p_{1i}) + \\mathbb1_2(y_i) log(p_{2i}) \\right)\\]\nWe are now left with 2 gradients (or a combined gradient if you combine both \\(\\v{w_1}\\) and \\(\\v{w_2}\\) into one vector:\n\\[\\frac{\\partial NLL}{\\partial \\v{w_1}} = -\\sum_i \\v{x_i} (y_i - p_{1i})\\]\nand\n\\[\\frac{\\partial NLL}{\\partial \\v{w_2}} = -\\sum_i \\v{x_i} (y_i - p_{2i})\\]"
  },
  {
    "objectID": "posts/logisticbp/index.html#layer-structure",
    "href": "posts/logisticbp/index.html#layer-structure",
    "title": "Logistic Regression and Backpropagation",
    "section": "Layer Structure",
    "text": "Layer Structure\nWriting the \\(NLL\\) slightly differently suggests a layer structure:\n\\[NLL = -\\sum_i \\left( \\mathbb1_1(y_i) log(SM_1(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x})) + \\mathbb1_2(y_i) log(SM_2(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x})) \\right)\\]\n\\[NLL = -\\sum_i \\left( \\mathbb1_1(y_i) LSM_1(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x}) + \\mathbb1_2(y_i) LSM_2(\\v{w_1} \\cdot \\v{x}, \\v{w_2} \\cdot \\v{x}) \\right)\\]\nwhere \\(SM_1 = \\frac{e^{\\v{w_1} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}\\) puts the first argument in the numerator. Ditto for \\(LSM_1\\) which is simply \\(log(SM_1)\\).\nThe layer structure suggested is captured in the diagram below. There are 4 layers, which we shall generally label using the notation \\(\\v{z}^l\\) where the vector on \\(z\\) indicates multiple values and the \\(l\\) indicates the number of the layer (it is NOT a power).\n\n\n\nThe computational graph with intermediate variables z labeled at each layer, preparing for the backpropagation derivation.\n\n\nFirst, the input:\n\\[\\v{z}^1 = \\v{x_i}\\]\nNotice here that we are writing this data-point by data-point. We will follow this structure for everything but the cost function. The dimension of this \\(\\v{z}^1\\) depends on the number of features \\(\\v{x_i}\\) has.\nThen, the second layer.\n\\[\\v{z}^2 = (z^2_1, z^2_2) = (\\v{w_1} \\cdot \\v{x_i}, \\v{w_2} \\cdot \\v{x_i}) = (\\v{w_1} \\cdot \\v{z^1_i}, \\v{w_2} \\cdot \\v{z^1_i})\\]\nThe dimension of \\(\\v{z}^2\\) is 2 corresponding to the two linear layers.\nThen the third layer is also two dimensional, corresponding to the 2 log-softmax functions we have.\n\\[\\v{z}^3 = (z^3_1, z^3_2) = \\left( LSM_1(z^2_1, z^2_2), LSM_2(z^2_1, z^2_2) \\right)\\]\nFinally, the fourth layer is just the cost layer, and is actually a scalar.\n\\[z^4 = NLL(\\v{z}^3) = NLL(z^3_1, z^3_2) = - \\sum_i \\left( \\mathbb1_1(y_i)z^3_1(i) + \\mathbb1_2(y_i)z^3_1(i) \\right)\\]\nNotice how these expressions are different from the fully expanded expressions we had earlier. Here each layer only depends on the previous layer. We shall utilize this structure soon to make our lives easier. Our tool to do this is backpropagation, which is just an example of Reverse Mode differentiation, which as can be surmised from the above structure, is a matter of taking derivatives by substitution, but in a particular order."
  },
  {
    "objectID": "posts/logisticbp/index.html#reverse-mode-differentiation",
    "href": "posts/logisticbp/index.html#reverse-mode-differentiation",
    "title": "Logistic Regression and Backpropagation",
    "section": "Reverse Mode Differentiation",
    "text": "Reverse Mode Differentiation\nWe wont go into many details, but the key observation is this. An operation like finding the loss in our logistic regression problem can be considered as an exercise in function composition, where the last function (\\(z^4\\), or the NLL cost) is a scalar. In other words, we are wanting to calculate:\n\\[Cost = f^{Loss}(\\v{f}^3(\\v{f}^2(\\v{f}^1(\\v{x}))))\\]\nwhere the vectorial function or the \\(\\v{x}\\) is a short form notation for both data and parameters. for this expression. Now, for gradient-descent, when i want to calculate the derivative or gradient with respect to data/parameters \\(\\v{x}\\), i can write:\n\\[\\nabla_{\\v{x}} Cost = \\frac{\\partial f^{Loss}}{\\partial \\v{f}^3}\\,\\frac{\\partial \\v{f}^3}{\\partial \\v{f}^2}\\,\\frac{\\partial \\v{f}^2}{\\partial \\v{f}^1}\\frac{\\partial \\v{f}^1}{\\partial \\v{x}}\\]\nNow, based on the observation that the first term in the above product is a vector while the second is a (Jacobian!) matrix, we can consider rewriting the product in this fashion:\n\\[\\nabla_{\\v{x}} Cost = (((\\frac{\\partial f^{Loss}}{\\partial \\v{f}^3}\\,\\frac{\\partial \\v{f}^3}{\\partial \\v{f}^2})\\,\\frac{\\partial \\v{f}^2}{\\partial \\v{f}^1})\\,\\frac{\\partial \\v{f}^1}{\\partial \\v{x}})\\]\nThis way of writing things always provides us with a vector times a matrix giving us a vector and saves a huge amount of memory, especially on large problems. This is the key idea of reverse mode auto diff.\nBackpropagation falls easily out of this. We add a “cost layer” to \\(z^4\\). The derivative of this layer with respect to \\(z^4\\) will always be 1. We then propagate this derivative back."
  },
  {
    "objectID": "posts/logisticbp/index.html#backpropagation",
    "href": "posts/logisticbp/index.html#backpropagation",
    "title": "Logistic Regression and Backpropagation",
    "section": "Backpropagation",
    "text": "Backpropagation\nEverything comes together now. Let us illustrate with a diagram:\n\n\n\nForward and backward passes through a three-layer network, with error signals δ propagating backward.\n\n\nWe can consider our calculations to now consist of two phases: a forward phase, and a backward phase.\nIn the forward phase, all we are doing is function composition, in the old fashioned way, starting from the bottom-most \\(\\vec{x_i}\\) later, and moving up the layer cake to the cost. (Notice how this is just .forward in pytorch.)\nRULE1: FORWARD\n\\[\\v{z}^{l+1} = \\v{f}^l (\\v{z}^l)\\]\nIn the reverse or backward phase, we now propagate the derivatives backward through the layer cake (pytoych: .backward). By derivatives we mean:\n\\[\\v{\\delta^l} = \\frac{\\partial C}{\\partial \\v{z}^l}\\]\nor (u for unit}\n\\[\\delta^l_u = \\frac{\\partial C}{\\partial z^l_u}\\].\nAnd the formula for this now is very simple, as we will do it iteratively as in reverse-mode differentiation.\nRULE2: BACKWARD\n\\[\\delta^l_u = \\frac{\\partial C}{\\partial z^l_u} = \\sum_v \\frac{\\partial C}{\\partial z^{l+1}_v} \\, \\frac{\\partial z^{l+1}_v}{\\partial z^l_u} = \\sum_v \\delta^{l+1}_v \\, \\frac{\\partial z^{l+1}_v}{\\partial z^l_u} \\]\nOnce again the first term in the product is obviously a vector, and the second term a matrix, so we can recursively get the derivatives all the way down. So this gives us the compositional derivative at any depth we want. In particular we will start with\n\\[\\delta^3_u = \\frac{\\partial z^{4}}{\\partial z^3_u} = \\frac{\\partial C}{\\partial z^3_u}\\]\nwhich is now simply a derivative of the NLL with respect to “dummy” variables representing the LSM functions.\nOne formula remains: the derivatives with respect to any parameter in any layer. In our case we have the \\(\\vec{w}\\) parameters at level-2, but these could be parameters at a higher level as well. For them we have\nRULE 3: Parameters\n\\[\\frac{\\partial C}{\\partial \\theta^l} = \\sum_u \\frac{\\partial C}{\\partial z^{l+1}_u} \\, \\frac{\\partial z^{l+1}_u}{\\partial \\theta^l} = \\sum_u \\delta^{l+1}_u \\frac{\\partial z^{l+1}_u}{\\partial \\theta^l}\\]\nAnother recursion! Uses the derivatives we calculate in the backward pass. Both those derivates and these are used to fill the variable.grad parts of the various parameters in pytorch. This is also the reason for the strange choice to hold the gradients along with the variables rather than with the cost: we back propagate those gradients into the variables, so to speak."
  },
  {
    "objectID": "posts/logisticbp/index.html#coding-a-layer",
    "href": "posts/logisticbp/index.html#coding-a-layer",
    "title": "Logistic Regression and Backpropagation",
    "section": "Coding a layer",
    "text": "Coding a layer\nThis extreme modularity suggests that we can define our own layers (or recursively, even our own combination of layers). even though we wont want to do this for logistic regression, this functionality is useful in defining Artificial Neural Network architectures.\nWhat we must specifically provide is a way to implement the 3 rules for a layer: how to get output \\(z\\) given input \\(z\\), how to get reverse output \\(\\delta\\)s given input ones, and how to differentiate the cost with respect to any local parameters.\n\n\n\nA generic layer l receives activations z from below and error signals δ from above, producing parameter gradients.\n\n\nSuch modularity allows for lots of experimentation."
  },
  {
    "objectID": "posts/sufstatexch/index.html",
    "href": "posts/sufstatexch/index.html",
    "title": "Sufficient Statistics and Exchangeability",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pylab as plt \nimport seaborn as sn\n\nfrom scipy.stats import norm\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/sufstatexch/index.html#sufficient-statistics-and-the-exponential-family",
    "href": "posts/sufstatexch/index.html#sufficient-statistics-and-the-exponential-family",
    "title": "Sufficient Statistics and Exchangeability",
    "section": "Sufficient Statistics and the Exponential Family",
    "text": "Sufficient Statistics and the Exponential Family\nProbability distributions that belong to an exponential family have natural conjugate prior distributions. The form of the exponential family is:\n\\[p(y_i \\vert \\theta) = f(y_i)g(\\theta) e^{\\phi(\\theta)^{T} u(y_i)}.\\]\nThus the likelihood corresponding to n i.i.d. points is:\n\\[ p(y \\vert \\theta) = \\left(\\prod_{i=1}^{n} f(y_i) \\right) g(\\theta)^n \\,\\, \\exp\\left(\\phi(\\theta)\\sum_{i=1}^{n} u(y_i)\\right)\\]\nNow notice that the product of y -dependent stuff in front is irrelavant as far as sampling goes: it does not interact with \\(\\theta\\) in any way! If I wanted the actual value of the likelihood it would be important to model it well. But if all I want is to use this expression in a samples generator, I dont care. This kind of observation will bve critical for us as we sample from ever more complex models: indeed isolating such dependencies is at the cornerstone of the gibbs method.\nThus one can say, that for all n and y, this has a fixed form as a functio of \\(\\theta\\):\n\\[ p(y \\vert \\theta)  \\propto g(\\theta)^n \\, e^{\\phi (\\theta)^T   t(y)}\\]\nwhere \\(t(y) = \\sum_{i=1}^{n} u(y_i)\\) is said to be a sufficient statistic for \\(\\theta\\) , because the likelihood for theta “depends” on y only through \\(t(y)\\).\nIn general the exponential families are the only classes of distributions that have natural conjugate prior distributions, since, apart from some special cases, they are the only distributions having a fixed number of sufficient statistics for all \\(n\\).\nThis family includes exponential, poisson, gamma, beta, pareto, binomial, gaussian…."
  },
  {
    "objectID": "posts/sufstatexch/index.html#an-example-with-poissons-and-gammas",
    "href": "posts/sufstatexch/index.html#an-example-with-poissons-and-gammas",
    "title": "Sufficient Statistics and Exchangeability",
    "section": "An example with Poissons and Gammas",
    "text": "An example with Poissons and Gammas\nConsider some data gathered in the 1990s on educational attainment. The data consists of 155 women who were 40 years old. We are interested in the birth rate of women with a college degree and women without. We are told that 111 women without college degrees have 217 children, while 44 women with college degrees have 66 children.\nLet \\(Y_{1,1}, \\ldots, Y_{n_1,1}\\) denote the number of children for the \\(n_1\\) women without college degrees and \\(Y_{1,2}, \\ldots, Y_{n_2,2}\\) be the data for \\(n_2\\) women with college degrees.\n\nExchangeability\nLets assume that the number of children of a women in any one of these classes can me modelled as coming from ONE birth rate (we dont know anything about their individual situations so we treat each woman as interchangeable or exchageable with another within the same class). This is the basis for the IID assumption that we generally use.\nAnother way to think about it, is that the in-class likelihood for these women is invariant to a permutation of variables. If we assume a Poisson likelihood (low counts) for the number of births for each woman, we have, for each woman:\n\\[Y_{i,1} \\sim Poisson(\\theta_{1}),  Y_{i,2} \\sim Poisson(\\theta_{2})\\]\nThen, the likelihood for the first population is:\n\\[ p(Y_{1,1}, \\ldots, Y_{n_1,1}  \\vert  \\theta_1)  = \\prod_{i=1}^{n_1} p(Y_{i,1} \\vert \\theta_1) =  \\prod_{i=1}^{n_1}  \\frac{1}{Y_{i,1} !} \\theta_1^{Y_{i,1}} e^{-\\theta_1}\n= c(Y_{1,1}, \\ldots, Y_{n_1,1}) \\,\\, (n_{1}\\theta_{1})^{\\sum Y_{i,1}} e^{-n_1 \\theta_1}\n  \\sim Poisson(n_1 \\theta_1) \\]\nand similarly\n\\[ Y_{1,2}, \\ldots, Y_{n_1,2}  \\vert  \\theta_2 \\sim Poisson(n_2\\theta_2) \\]\n** The distributions are still poisson **\n\n\nObtaining the Posterior\nThe posterior is a simple product of two sub-posteriors:\n\\[p(\\theta_1 \\vert  Y_{1,1}, \\ldots, Y_{n_1,1} )* p(\\theta_2 \\vert  Y_{1,2}, \\ldots, Y_{n_2,2} ) ,\\] which, given independent priors on\n\\(\\theta_1\\) and \\(\\theta_2\\), is:\n\\[c_1(n_1, y_1, \\ldots, y_{n_1}) \\,\\, (n_{1}\\theta_{1})^{\\sum Y_{i,1}} e^{-n_1 \\theta_1}\\, p(\\theta_1) \\times c_2(n_2, y_1, \\ldots, y_{n_2}) \\,\\, (n_{2}\\theta_{2})^{\\sum Y_{i,2}} e^{-n_2 \\theta_2}  \\, p(\\theta_2) \\]\nThe quantity \\(\\sum Y_i\\) contains all the information about \\(\\theta\\) and thus \\(\\sum Y_i\\) is sufficient statistics. Indeed all you need is the total number of children in each class of mom as far as making any inferences about the \\(\\theta_{1 or 2}\\) are concerned.\nSo as long as we dont need the exact value of the likelihood, we are go ob treating the likelihood as a\nFor our example we have \\(n_1 =111\\), \\(\\sum_i^{n_1} Y_{i,1} =217\\) and \\(n_2=44\\), \\(\\sum_i^{n_2} Y_{i,2} =66\\).\n\n\nCongugate Priors\nLets now choose priors. A class of priors is said to be conjugate for a sampling distribution \\(p(y_1, \\ldots, y_n \\vert  \\theta)\\) if the posterior is also in the class.\nFor the Poisson :\n\\[ p(Y_1, \\ldots, y_n \\vert  \\theta)  \\sim  \\theta^{\\sum Y_i} e^{-n \\theta} \\]\nKeeping the same functional form means our conjugate class has to include terms like \\(\\theta^{c_1} e^{-c_2 \\theta}\\).\nThis is a known family known as Gamma distributions. In the shape-rate parametrization (see wikipedia)\n\\[p(\\theta) =  \\rm{Gamma}(\\theta, a, b) = \\frac{b^a}{\\Gamma(a)} \\theta^{a-1} e^{-b \\theta} \\]\nIf \\(p(\\theta) =  \\rm{Gamma}(\\theta, a, b)\\) and $ p(Y_1 , Y_n ) \\rm{Gamma}(_{1,2}, a=2, b=1) $. The mean and variance of gamma distributions are known\n\\[ E[\\theta] = a/b, var[\\theta] = a/b^2 .\\]\nSo the mean of the gamma is roughly a notion of your belief of prior kids to moms. Here we say 2.\n\nfrom scipy.stats import gamma\nxxx=np.linspace(0,10,100)\nplt.plot(xxx, gamma.pdf(xxx, 3, scale=1), label=\"3 kids 1 mom\");\nplt.plot(xxx, gamma.pdf(xxx, 2, scale=1), label=\"2 kids 1 mom\");\nplt.plot(xxx, gamma.pdf(xxx, 1, scale=1), label=\"1 kid 1 mom\");\nplt.plot(xxx, gamma.pdf(xxx, 1, scale=1/3), label=\"1 kids 3 moms\");\nplt.legend();\n\n\n\n\n\n\n\n\n\n\nOur Posteriors\n\\[ p(\\theta_1 \\vert n_1 = 111,  \\sum_i^{n_1} Y_{i,1}=217 ) \\sim  \\rm{Gamma}(\\theta_1, 2+217, 1+111) =  \\rm{Gamma}(\\theta_1, 219, 112) \\]\n\\[ p(\\theta_2 \\vert n_2 = 44,  \\sum_i^{n_2} Y_{i,2}=66 ) \\sim  \\rm{Gamma}(\\theta_2, 2+66, 1+44) =  \\rm{Gamma}(\\theta_2, 68, 45) \\]\nThe mean of our posterior is then a ratio of posterior kids to moms:\n\\[ E[\\theta] = (a + \\sum y_i)/(b + N), var[\\theta] = (a + \\sum y_i)/(b + N)^2 .\\]\nIn this case 219/112 and 68/45 which is not very sensitive to our prior as you might expect.\n\n219/112, 68/45\n\n(1.9553571428571428, 1.511111111111111)\n\n\nWe can calculate and plot the posterior predictives. We do that here for \\(\\theta_1\\) and \\(\\theta_2\\). We also show (lack of) sensitivity to the prior by considering a wierd prior with a=20, b=2.\n\nfrom scipy.stats import gamma\na = 2 # Gamma prior, a,b values \nb = 1 \n\nn1 = 111\nsy1 = 217  # sum of y1\nn2 = 44 \nsy2=66     #sum of y2\nN=1000\n\n\n# ACTUAL VALUES \n# posterior mean \n(a+sy1)/(b+n1) \n(a+sy2)/(b+n2)\n\n# EXACT POSTERIORS\n\n\ntheta1=gamma.rvs(a+sy1, scale=1.0/( b+n1), size=N)\nq=plt.hist(theta1, 50,linewidth=1.5,normed=True, histtype='step',   label=u'posterior for theta1')\ntheta2 = gamma.rvs(a+sy2,scale= 1./(b+n2), size=N)\nq=plt.hist(theta2, 50, linewidth=1,histtype='step', alpha=1.0,normed=True,   label=u'posterior for theta2') \n\n\n\nth_prior = gamma.rvs(2.0, 1.0, size=N);\nplt.hist(th_prior, 50,linewidth=1, histtype='step',alpha=1.0, normed=True,   label=u'prior') \n\n#just for theta1, try a wierd pri\nth_priorwierd = gamma.rvs(20.0, 1.0, size=N);\ntheta1wierd=gamma.rvs(20+sy1, scale=1.0/( 2+n1), size=N)\nplt.hist(th_priorwierd, 50,linewidth=1, histtype='step',alpha=1.0, normed=True,   label=u'prior 20,2') \nplt.hist(theta1wierd, 50, linewidth=1,histtype='step', alpha=1.0,normed=True,   label=u'posterior for theta1 with 20,2') \n\n\n\n\n#plt.xlim( [0,8])\nplt.xlabel('\\theta')\n\n\n\n# ## MONTE CARLO APPROACH - REJECTION METHOD \n\n# a =2.0 \n# b = 1.0 \n# prior = lambda theta:  gamma.pdf(theta, a ,b)\n# pdf_s1 = lambda theta: prior(theta)*  poisson.pmf(sy1, n1*theta)\n# pdf_s2 = lambda theta:  prior(theta)* poisson.pmf(sy2, n2*theta)\n\n\nplt.xlim([0,8])\nplt.legend()\n\n\n## Finally we can do inference as we wish\n\n\n\n\n\n\n\n\nThe mean birth-rates can be calculated from the samples, as can the variances, which are also given us by the formulae from above:\n\\[ E[\\theta] = (a + \\sum y_i)/(b + N), var[\\theta] = (a + \\sum y_i)/(b + N)^2 .\\]\n\nnp.mean(theta1), np.var(theta1)\n\n(1.9516881521791478, 0.018527204185785785)\n\n\n\nnp.mean(theta2), np.var(theta2)\n\n(1.5037252100213609, 0.034220717257786061)\n\n\nIts easy to get the posterior birth-rate difference from the samples\n\nnp.mean(theta1 - theta2)\n\n(0.43687373794997003, -0.43687373794997003)\n\n\n\n\nPosterior predictives\nRemember that the posterior predictive is the following integral\n\\[p(y^{*} \\vert D) = \\int d\\theta p(y^{*} \\vert \\theta) p(\\theta \\vert D)\\]\nFrom the perspective of sampling, all we have to do is to first draw the thetas from the posterior, then draw y’s from the likelihood, and histogram the likelihood. This is the same logic as marginal posteriors, with the addition of the fact that we must draw y from the likelihood once we drew \\(\\theta\\). You might think that we have to draw multiple \\(y\\)s at a theta, but this is already taken care of for us because of the nature of sampling. We already have multiple \\(\\theta\\)a in a bin.\n\nfrom scipy.stats import poisson\npostpred1 = poisson.rvs(theta1)\npostpred2 = poisson.rvs(theta2)\n\n\nplt.hist(postpred1, alpha=0.4, align=\"left\");\nplt.hist(postpred2, alpha=0.4, align=\"left\");\n\n\n\n\n\n\n\n\nIt turns out that the distribution characterizing the posterior predictive is a negative binomial (see wikipedia, this requires some manipulations of gamma functions which we shall not reproduce here). The mean of the posterior predictive distribution is the same as that of the posterior\n\\[ E[y^*] = \\frac{(a + \\sum y_i)}{(b + N)}, var[y^*] = \\frac{(a + \\sum y_i)}{(b + N)^2} (N + b + 1) .\\]\n\nnp.mean(postpred1), np.var(postpred1)\n\n(1.976, 1.8554239999999997)\n\n\n\nnp.mean(postpred2), np.var(postpred2)\n\n(1.502, 1.5719960000000002)\n\n\nNotice that the error on the posterior predictive is much larger, even with the same means. The reason for this is that in the posterior predictive, you are smearing out the posterior error. At each point in the posterior, there is the smearing associated with the sampling distribution for \\(y^* \\sim \\theta\\), and thus the posterior predictive is conservative.\n\nnp.mean(postpred1 - postpred2)\n\n0.47399999999999998\n\n\nWhy bother with the posterior predictive?\n\nyou might want to make predictions\nmodel checking: is the model kosher?\n\nThere are multiple ways of accomplishing the latter (all of which we shall see).\n\nFuture observations could be compared to the posterior predictive.\ncross-validation could be used for this purpose to calculate a prediction error\njust plotting posterior predictives can be useful since sometimes a visual inspection of simulation data gives away the fact that it looks nothing like actual data."
  },
  {
    "objectID": "posts/regularization/index.html",
    "href": "posts/regularization/index.html",
    "title": "Regularization",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/regularization/index.html#revisiting-the-model",
    "href": "posts/regularization/index.html#revisiting-the-model",
    "title": "Regularization",
    "section": "Revisiting the model",
    "text": "Revisiting the model\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\n\ndffull=pd.read_csv(\"data/religion.csv\")\ndffull.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\n\nx=dffull.rfrac.values\nf=dffull.promney.values\n\n\ndf = pd.read_csv(\"data/noisysample.csv\")\ndf.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\nfrom sklearn.cross_validation import train_test_split\ndatasize=df.shape[0]\n#split dataset using the index, as we have x,f, and y that we want to split.\nitrain,itest = train_test_split(range(30),train_size=24, test_size=6)\nxtrain= df.x[itrain].values\nftrain = df.f[itrain].values\nytrain = df.y[itrain].values\nxtest= df.x[itest].values\nftest = df.f[itest].values\nytest = df.y[itest].values\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\ndef make_features(train_set, test_set, degrees):\n    traintestlist=[]\n    for d in degrees:\n        traintestdict={}\n        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n        traintestlist.append(traintestdict)\n    return traintestlist\n\n\ndegrees=range(21)\ntraintestlists=make_features(xtrain, xtest, degrees)"
  },
  {
    "objectID": "posts/regularization/index.html#constraining-parameters-by-penalty",
    "href": "posts/regularization/index.html#constraining-parameters-by-penalty",
    "title": "Regularization",
    "section": "Constraining parameters by penalty",
    "text": "Constraining parameters by penalty\nUpto now we have focussed on finding the polynomial with the right degree of complexity \\(d^*\\) given the data that we have.\nLet us now ask a different question: if we are going to fit the data with an expressive model such as 20th order polynomials, how can we regularize or smooth or restrict the choices of the kinds of 20th order polynomials that we allow in our fits. In other words, we are again trying to bring down the complexity of the hypothesis space, but by a different tack: a tack which prefers smooth polynomials over wiggly ones.\nThat is, if we want to fit with a 20th order polynomial, ok, lets fit with it, but lets reduce the size of, or limit the functions in \\(\\cal{H}_{20}\\) that we allow.\nIn a sense we have already done this, havent we? When we compared \\(\\cal{H_{1}}\\) over \\(\\cal{H_{20}}\\), we imposed a hard constraint by setting all polynomial co-efficients for terms higher than \\(x\\) to 0. In other words you can think of a line as a 20th degree polynomial with many 0 coefficients. Why not have the math machinery do this for you than do it by hand, and use the data to figure out how to make the cut.\nWe do this by a soft constraint by setting:\n\\[\\sum_{i=0}^j a_i^2 &lt; C.\\]\nThis setting is called the Ridge.\nThis ensures that the coefficients dont get too high, which makes sure we dont get wildly behaving pilynomials with high coefficients. If we set\n\\[\\sum_{i=0}^j | a_i | &lt; C,\\]\nit can be shown that some coefficients will be set to exactly 0. This is called the Lasso.\nIt turns out that we can do this by adding a term to the empirical risk that we minimize on the training data for \\(\\cal{H}_j\\) (seeing why is beyond the scope here but google on lagrange multipliers and the dual problem):\n\\[\\cal{R}(h_j) =  \\sum_{y_i \\in \\cal{D}} (y_i - h_j(x_i))^2 +\\alpha \\sum_{i=0}^j a_i^2.\\]\nThis new risk takes the empirical risk and adds a “penalty term” to it to minimize overfitting. The term is proportional to the sum of the squares of the coefficients and is positive, so it will keep their values down\nThe entire structure is similar to what we did to find the optimal \\(d=*\\), with \\(\\alpha\\) being the analog of \\(d\\). And thus we can use the same validation and cross-validation technology that we developed to estimate \\(d\\).\nThis technique is called regularization or shrinkage as it takes the coefficients \\(a_i\\) towards smaller sizes. As you have seen earlier, for polynomials this corresponds to choosing smooth functions over wiggly functions. When \\(\\alpha=0\\) we have the regular polynomial regression problem, and if we are using 20th order polynomials we will wildly overfit. We are in the high variance zone. The problem with a non-zero \\(\\alpha\\) is called ridge regression. As \\(\\alpha\\) increases, the importance of the penalty term increases at the expense of the ERM term, and we are pushed to increase the smoothness of the polynomials. When \\(\\alpha\\) becomes very large the penalty term dominates and we get into the high bias zone. Thus \\(\\alpha\\) acts as a complexity parameter just like \\(d\\) did, with high complexity being \\(\\alpha \\to 0\\).\n\nA toy example\nEven though we are not doing any proofs, let us illustrate the concept of regularization using a line fit to a sine wave. We fit a straight line to 3 points, choosing 100 such sets of 3 points from the data set.\n\nxs=np.arange(-1.,1.,0.01)\nff = lambda x: np.sin(np.pi*x)\nffxs=ff(xs)\naxes=make_simple_plot()\nc0=sns.color_palette()[0]\nc1=sns.color_palette()[1]\naxes[0].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\naxes[1].plot(xs, ff(xs), alpha=0.9, lw=3, color=c0)\nfrom sklearn.linear_model import Ridge\nD=np.empty((100,3), dtype=\"int\")\nprint(D.shape)\nfor i in range(100):\n    D[i,:] = np.random.choice(200, replace=False, size=3)\nfor i in range(100):\n    choices = D[i,:]\n    #regular fit\n    p1=np.polyfit(xs[choices], ffxs[choices],1)\n    #ridge fit\n    est = Ridge(alpha=1.0)\n    est.fit(xs[choices].reshape(-1,1), ffxs[choices])\n    axes[0].plot(xs, np.polyval(p1, xs), color=c1, alpha=0.2)\n    axes[1].plot(xs, est.predict(xs.reshape(-1,1)), color=c1, alpha=0.2)\naxes[0].set_title(\"Unregularized\");\naxes[1].set_title(\"Regularized with $\\\\alpha=1.0$\");\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n(100, 3)\n\n\n\n\n\n\n\n\n\nIn the left panel we plot unregularized straight line fits. The plot is hairy, since choosing 3 points from 200 between -1 and 1 dosent constrain the lines much at all. On the right panel, we plot the output of Ridge regression with \\(\\alpha=1\\). This corresponds to adding a term to the empirical risk of \\(\\alpha\\, (a_0^2 + a_1^2)\\) where \\(a_0\\) and \\(a_1\\) are the intercept and slope of the line respectively. Notice that the lines are much more constrained in this second plot. The penalty term has regularized the values of the intercept and slope, and forced the intercept to be closer to 0 and the lines to be flatter.\n\n\nContrast with complexity parameter validation\nNotice that in regularization, we are adding a term to the training error, once \\(\\alpha\\) is defined. It is this term that is estimated..\nWhen we were fitting for the degree of the polynomial, there was no explicit added term, we just fit the regular model. But there, as with regularization, the choice of the hyperparameter was made by comparing validation risks which were calculated by taking the parameters found with fixed hyperparameters on the training set."
  },
  {
    "objectID": "posts/regularization/index.html#regularization-of-the-romney-model-with-cross-validation",
    "href": "posts/regularization/index.html#regularization-of-the-romney-model-with-cross-validation",
    "title": "Regularization",
    "section": "Regularization of the Romney model with Cross-Validation",
    "text": "Regularization of the Romney model with Cross-Validation\n\ndef plot_functions(est, ax, df, alpha, xtest, Xtest, xtrain, ytrain):\n    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n    ax.plot(df.x, df.f, color='k', label='f')\n    ax.plot(xtrain, ytrain, 's', label=\"training\", alpha=0.4)\n    ax.plot(xtest, ytest, 's', label=\"testing\", alpha=0.6)\n    transx=np.arange(0,1.1,0.01)\n    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n    ax.plot(transx, est.predict(transX),  '.', label=\"alpha = %s\" % str(alpha))\n    ax.set_ylim((0, 1))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='lower right')\n    \ndef plot_coefficients(est, ax, alpha):\n    coef = est.coef_.ravel()\n    ax.semilogy(np.abs(coef), marker='o', label=\"alpha = %s\" % str(alpha))\n    ax.set_ylim((1e-1, 1e15))\n    ax.set_ylabel('abs(coefficient)')\n    ax.set_xlabel('coefficients')\n    ax.legend(loc='upper left')\n\nLets now go back to the Romney voting model and see what regularization does to the fits in that model. The addition of a penalty term to the risk or error causes us to choose a smaller subset of the entire set of complex \\(\\cal{H_{20}}\\) polynomials. This is shown in the diagram below where the balance between bias and variance occurs at some subset \\(S_{\\*}\\) of the set of 20th order polynomials indexed by \\(\\alpha_{\\*}\\) (there is an error on the diagram, the 13 there should actually be a 20).\n\n\n\nBias-variance tradeoff controlled by regularization strength alpha\n\n\nLets see what some of the \\(\\alpha\\)s do. The diagram below trains on the entire training set, for given values of \\(\\alpha\\), minimizing the penalty-term-added training error.\n\nfig, rows = plt.subplots(5, 2, figsize=(12, 16))\nd=20\nalphas = [0.0, 1e-5, 1e-3, 1, 10]\nXtrain = traintestlists[d]['train']\nXtest = traintestlists[d]['test']\nfor i, alpha in enumerate(alphas):\n    l,r=rows[i]\n    est = Ridge(alpha=alpha)\n    est.fit(Xtrain, ytrain)\n    plot_functions(est, l, df, alpha, xtest, Xtest, xtrain, ytrain )\n    plot_coefficients(est, r, alpha)\n\n\n\n\n\n\n\n\nAs you can see, as we increase \\(\\alpha\\) from 0 to 1, we start out overfitting, then doing well, and then, our fits, develop a mind of their own irrespective of data, as the penalty term dominates the risk.\nLets use cross-validation to figure what this critical \\(\\alpha_*\\) is. To do this we use the concept of a meta-estimator from scikit-learn. As the API paper puts it:\n\nIn scikit-learn, model selection is supported in two distinct meta-estimators, GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n\nThe concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\nclf = Ridge()\nparameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\ngridclassifier=GridSearchCV(clf, param_grid=parameters, cv=4, scoring=\"mean_squared_error\")\nThe GridSearchCV replaces the manual iteration over thefolds using KFolds and the averaging we did previously, doint it all for us. It takes a parameter grid in the shape of a dictionary as input, and sets \\(\\alpha\\) to the appropriate parameter values one by one. It then trains the model, cross-validation fashion, and gets the error. Finally it compares the errors for the different \\(\\alpha\\)’s, and picks the best choice model.\n\nfrom sklearn.metrics import make_scorer\n#, 1e-6, 1e-5, 1e-3, 1.0\nfrom sklearn.grid_search import GridSearchCV\ndef cv_optimize_ridge(X, y, n_folds=4):\n    clf = Ridge()\n    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n    #the scoring parameter below is the default one in ridge, but you can use a different one\n    #in the cross-validation phase if you want.\n    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=\"mean_squared_error\")\n    gs.fit(X, y)\n    return gs\n\n\nfitmodel = cv_optimize_ridge(Xtrain, ytrain, n_folds=4)\n\n\nfitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_, fitmodel.grid_scores_\n\n(Ridge(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=None,\n    normalize=False, random_state=None, solver='auto', tol=0.001),\n {'alpha': 0.0005},\n -0.005863476137886476,\n [mean: -0.01156, std: 0.00816, params: {'alpha': 1e-08},\n  mean: -0.00643, std: 0.00350, params: {'alpha': 1e-06},\n  mean: -0.00618, std: 0.00355, params: {'alpha': 1e-05},\n  mean: -0.00596, std: 0.00358, params: {'alpha': 5e-05},\n  mean: -0.00589, std: 0.00369, params: {'alpha': 0.0001},\n  mean: -0.00586, std: 0.00424, params: {'alpha': 0.0005},\n  mean: -0.00592, std: 0.00446, params: {'alpha': 0.001},\n  mean: -0.00587, std: 0.00458, params: {'alpha': 0.01},\n  mean: -0.00606, std: 0.00406, params: {'alpha': 0.1},\n  mean: -0.01280, std: 0.00548, params: {'alpha': 1.0}])\n\n\nOur best model occurs for \\(\\alpha=0.01\\). We also output the mean cross-validation error at different \\(\\alpha\\) (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error).\nWe refit the estimator on old training set, and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0.\n\nalphawechoose = fitmodel.best_params_['alpha']\nclf = Ridge(alpha=alphawechoose).fit(Xtrain,ytrain)\n\n\ndef plot_functions_onall(est, ax, df, alpha, xtrain, ytrain, Xtrain, xtest, ytest):\n    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n    ax.plot(df.x, df.f, color='k', label='f')\n    ax.plot(xtrain, ytrain, 's', alpha=0.4, label=\"train\")\n    ax.plot(xtest, ytest, 's', alpha=0.6, label=\"test\")\n    transx=np.arange(0,1.1,0.01)\n    transX = PolynomialFeatures(20).fit_transform(transx.reshape(-1,1))\n    ax.plot(transx, est.predict(transX), '.', alpha=0.6, label=\"alpha = %s\" % str(alpha))\n    #print est.predict(transX)\n    ax.set_ylim((0, 1))\n    ax.set_xlim((0, 1))\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.legend(loc='lower right')\n\n\nfig, rows = plt.subplots(1, 2, figsize=(12, 5))\nl,r=rows\nplot_functions_onall(clf, l, df, alphawechoose, xtrain, ytrain, Xtrain, xtest, ytest)\nplot_coefficients(clf, r, alphawechoose)\n\n\n\n\n\n\n\n\nAs we can see, the best fit model is now chosen from the entire set of 20th order polynomials, and a non-zero hyperparameter \\(\\alpha\\) that we fit for ensures that only smooth models amonst these polynomials are chosen, by setting most of the polynomial coefficients to something close to 0 (Lasso sets them exactly to 0)."
  },
  {
    "objectID": "posts/votingforcongress/index.html",
    "href": "posts/votingforcongress/index.html",
    "title": "Some Data Analysis about Congress",
    "section": "",
    "text": "PredictWise congressional voting visualization\n\n\n\nimport pandas as pd\n\n\ntbl = pd.read_html(\"https://www.presidency.ucsb.edu/statistics/data/seats-congress-gainedlost-the-presidents-party-mid-term-elections\")\n\n\ndf = tbl[0]\ndf.columns = df.columns.to_flat_index()\ndf\n\n\n\n\n\n\n\n\n(Unnamed: 0_level_0, Year)\n(Unnamed: 1_level_0, Lame Duck?)\n(Unnamed: 2_level_0, President)\n(Unnamed: 3_level_0, President'sParty)\n(President's Job Approval Percentage (Gallup) As of:, Early Aug)\n(President's Job Approval Percentage (Gallup) As of:, Late Aug)\n(President's Job Approval Percentage (Gallup) As of:, Early Sep)\n(President's Job Approval Percentage (Gallup) As of:, Late Sep)\n(President's Job Approval Percentage (Gallup) As of:, Early Oct)\n(President's Job Approval Percentage (Gallup) As of:, Late Oct)\n(President's Party, House Seatsto Defend)\n(President's Party, Senate Seatsto Defend)\n(Seat Change, President's Party, House Seats)\n(Seat Change, President's Party, Senate Seats)\n\n\n\n\n0\n1934\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n--\n313\n14\n+9\n+9\n\n\n1\n1938\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n60\n334\n27\n-81\n-7\n\n\n2\n1942\nNaN\nFranklin D. Roosevelt\nD\n74\n--\n74\n--\n--\n--\n267\n25\n-46\n-9\n\n\n3\n1946\nNaN\nHarry S. Truman\nD\n--\n--\n33\n--\n--\n27\n244\n21\n-45\n-12\n\n\n4\n1950\nLD*\nHarry S. Truman\nD\nnd\n43\n35\n35\n43\n41\n263\n21\n-29\n-6\n\n\n5\n1954\nNaN\nDwight D. Eisenhower\nR\n67\n62\n--\n66\n62\n--\n221\n11\n-18\n-1\n\n\n6\n1958\nLD\nDwight D. Eisenhower\nR\n58\n56\n56\n54\n57\n--\n203\n20\n-48\n-13\n\n\n7\n1962\nNaN\nJohn F. Kennedy\nD\n--\n67\n--\n63\n--\n61\n264\n18\n-4\n+3\n\n\n8\n1966\n†\nLyndon B. Johnson\nD\n51\n47\n--\n--\n44\n44\n295\n21\n-47\n-4\n\n\n9\n1970\nNaN\nRichard Nixon\nR\n55\n55\n57\n51\n58\n--\n192\n7\n-12\n+2\n\n\n10\n1974\n±\nGerald R. Ford (Nixon)\nR\n71\n--\n66\n50\n53\n--\n192\n15\n-48\n-5\n\n\n11\n1978\nNaN\nJimmy Carter\nD\n43\n43\n48\n--\n49\n45\n292\n14\n-15\n-3\n\n\n12\n1982\nNaN\nRonald Reagan\nR\n41\n42\n--\n42\n--\n42\n192\n12\n-26\n+1\n\n\n13\n1986\nLD\nRonald Reagan\nR\n--\n64\n--\n63\n64\n--\n181\n22\n-5\n-8\n\n\n14\n1990\nNaN\nGeorge Bush\nR\n75\n73\n54\n--\n--\n57\n175\n17\n-8\n-1\n\n\n15\n1994\nNaN\nWilliam J. Clinton\nD\n43\n40\n40\n44\n43\n48\n258\n17\n-52\n-8\n\n\n16\n1998\nLD\nWilliam J. Clinton\nD\n65\n62\n63\n66\n65\n65\n207\n18\n+5\n0\n\n\n17\n2002\nNaN\nGeorge W. Bush\nR\n--\n66\n66\n66\n68\n67\n220\n20\n+8\n+2\n\n\n18\n2006\nLD\nGeorge W. Bush\nR\n37\n42\n39\n44\n37\n37\n233\n15\n-30\n-6\n\n\n19\n2010\nNaN\nBarack Obama\nD\n44\n44\n45\n45\n45\n45\n257\n15\n-63\n-6\n\n\n20\n2014\nLD\nBarack Obama\nD\n42\n42\n41\n43\n42\n41\n201\n20\n-13\n-9\n\n\n21\n2018\nNaN\nDonald J. Trump\nR\n41\n41\n39\n41\n44\n44\n241\n9\n-40\n+2\n\n\n22\n2022\nNaN\nJoseph R. Biden\nD\n38\n44\n44\n42\n42\nNaN\n222\n14\nTBD\nTBD"
  },
  {
    "objectID": "posts/metropolis/index.html",
    "href": "posts/metropolis/index.html",
    "title": "From Annealing to Metropolis",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nAs we have seen, in Simulated Annealing, we reduce the temperature while accepting proposlas which increase the energy (the function we want to minimize).\nThe probability of accepting a proposal is proportional to\n\\[A=\\exp{ ( -\\Delta f/kT)}\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the temperature.\nThis happens because, in simulated annealing, our physics analogy is that we are looking at a system in equilibrium, with a Boltzmann distribution of energies:\n\\(p(X=i) = \\frac{1}{Z(T)} \\exp{ \\left( \\frac{-E_i}{kT}\\right) }\\)$\nwhere\n\\[Z(T) = \\sum_j \\exp{ \\left( \\frac{ -E_j}{kT}\\right)}\\]\nis the normalization constant of the distribution, also called the partition function.\n\nf = lambda x: x**2 + 4*np.sin(2*x)\nxs = np.linspace(-10.,10.,1000)\nplt.plot(xs, f(xs));\nplt.xlabel('x')\nplt.ylabel('f');\n\n\n\n\n\n\n\n\nSimulated Annealing works by us making a proposal to change the current state, and then calculating the new energy of the state. We mentioned earlier that this works because simulated annealing produces a set of homogeneous markov chains, one at each temperature. We will explore this idea soon, but intuitively speaking, we can see that Simulated Annealing samples the boltzmann distribution by using our proposal “distribution”.(It is critical that this proposal is symmetric, or supports what we have been calling detailed balance, which in our physical analogy corresponds to isothermal reversibility.\nThis was the argument we made then, promising we would explain it later:\nThis detailed balance condition ensures that the sequence of \\(\\{x_t\\}\\) generated by simulated annealing is a stationary markov chain with the boltzmann distribution as the stationary distribution of the chain as \\(t \\to \\infty\\).\nBy stationary distribution, we mean an unchanging one. We’ll define this carefully in the notes on Markov Chains.\nOr, in physics words, you are in equilibrium .\nIf you think of our example in terms of the Boltzmann probability distribution as the stationary distribution, then you can identify\n\\[p_{T}(x) = e^{-f(x)/T}\\]\nas the probability distribution we get down to. Specifically let \\(p(x) = p_{1}(x)\\). Then \\(f(x) = - log(p(x))\\).\nBut even more interesting:\n\\[P_{T}(x) = P(x)^{1/T}\\]\nand so you get a peakier and peakier distribution as \\(T \\to 0\\) around the global minimum (the globality and the exponentiation ensures that this peak is favored over the rest in the dfunction \\(f\\)). You can see this in the diagram below. As \\(T \\to 0\\), we get towards a delta function at the optimum and get a global minimum instead of a distribution.\n\nimport functools\ndistx = lambda g, x: np.e**(-g(x))\ndxf = functools.partial(distx, f)\noutx = np.linspace(-10, 10,1000)\nimport scipy.integrate as integrate\nO=20\nplt.plot(outx, dxf(outx)/integrate.quad(dxf,-O, O)[0]);\nA=integrate.quad(lambda x: dxf(x)**1.2,-O, O)[0]\nplt.plot(outx, (dxf(outx)**1.2)/A);\nB=integrate.quad(lambda x: dxf(x)**2.4,-O, O)[0]\nplt.plot(outx, (dxf(outx)**2.4)/B);\nC=integrate.quad(lambda x: dxf(x)**10,-O, O)[0]\nplt.plot(outx, (dxf(outx)**10)/C);\n\n\nplt.xlim([-5,5])\nplt.xlabel('x')\nplt.xlabel('p(x)')\nplt.title(\"distribution corresponding to function f\")\n\n\n\n\n\n\n\n\n\n\n\nLets turn the question on its head. Suppose we wanted to sample from a distribution \\(p(x)\\) (corresponding to a minimization of energy \\(-log(p(x))\\)).\nIn this case, we can now use a symmetric proposal to sample from \\(p(x)\\) (this will keep things “reversible”). The criterion for the proposal being able to reach any other point in a finite series of operations is called irreducibility or the notion that we wont get any disconnected sets. Clearly we need our proposal to be able to go everywhere on the support of the distribution…\nSo we stick to one temperature \\(T=1\\), and sample from \\(p(x)\\) using the same techniques we learnt in simulated annealing. We:\n\nuse a proposal distribution to propose a step.\nThen we calculate the pdf at that step, and compare it to the one at the previous step.\nIf the probability increased (energy decreased) we accept. If probability decreased (energy increased) we accept some of the time, based on the ratio of the new probability to the old one.\nWe accumulate our samples, as we are now trying to sample a distribution rather than find its “global maximum”.\n\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples\n\n\n\n\n\nxxx= np.linspace(-10,10,1000)\nplt.plot(xxx, norm.pdf(xxx), 'r'); \n\n\n\n\n\n\n\n\n\nfrom scipy.stats import uniform\ndef propmaker(delta):\n    rv = uniform(-delta, 2*delta)\n    return rv\nuni = propmaker(0.5)\ndef uniprop(xprev):\n    return xprev+uni.rvs()\n\n\nnorm.pdf(0.1)\n\n0.39695254747701181\n\n\n\nsamps = metropolis(norm.pdf, uniprop, 100000, 0.0)\n\n\n# plot our sample histogram\nplt.hist(samps,bins=80, alpha=0.4, label=u'MCMC distribution', normed=True) \n\n#plot the true function\nxx= np.linspace(0,1,100)\nplt.plot(xxx, norm.pdf(xxx), 'r', label=u'True distribution') \nplt.legend()\n\nplt.show()\nprint(\"starting point was \", 0.0)\n\n\n\n\n\n\n\n\nstarting point was  0.0\n\n\n\n\n\nWe’ve learnt how to do the inverse transform and how to use rejection sampling with a majorizing function. So why not use these methods to sample a distribution?\nIt can be hard to find a majorizing \\(g(x)\\) and this gets even harder for multidimensional \\(g(x)\\)…and to boot less efficient as you leave more and more space out. Also, our general ideais to compute expectations as sample averages, and majorizing in multiple dimensions can have us spending a lot of time in tails of distributions.\nAlso note that the integrals being calculated in these expectations are of the type:\n\\[E_f[g] = \\int dV f(x) g(x).\\]\nIn multiple domensions, volume elements get smaller and smaller…the curse of dimensionality. This can be seen in the diagram :\n\n\n\nCurse of dimensionality: the inner volume shrinks exponentially relative to the outer volume as dimensions increase.\n\n\n(image from Betancourt)\nwhere the centre-partitions combination to an integral goes from 1/3rd to 1/27th. Now suppose the mode of the distibution is contained in this partition: then its contribution to the integral is going down with dimensions.\nAs the centre volume decreases, the outer volume increases, but this is in distribution tails, so we dont get much of a contribution from there either:\n\n\n\nThe typical set in increasing dimensions: probability mass concentrates in a thin shell between the center and outer boundary. From Betancourt.\n\n\n(image from Betancourt)\n(the slivers outside increase).\nIt is the neighborhood between these extremes, called the typical set which our sampler must explore well. And to get a good rejection sampling majorizer for this becomes hard.\nWe’ll come back to this picture later…\n\n\n\n\nf = lambda x: 6*x*(1-x)\n\n\nxxx= np.linspace(-1,2,100)\nplt.plot(xxx, f(xxx), 'r') \nplt.axvline(0, 0,1, color=\"gray\")\nplt.axvline(1, 0,1, color=\"gray\")\nplt.axhline(0, 0,1, color=\"gray\");\n\n\n\n\n\n\n\n\nWe wish to consider the support [0,1]. We could truncate our “distribution” beyond these. But it does not matter, even though we use a normal proposal whichcan propose negative and gretar-than-one \\(x\\) values.\nWhat happens if the proposal proposes a number outside of [0,1]? Notice then that our pdf is negative(ie it is not a pdf. (we could have defined it as 0 as well). Then in the metropolis acceptance formula, we are trying to check if a uniform is less than a negative or 0 number and we will not accept. This does however mean that we will need a longer set of samples than otherwise…\n\ndef prop(x):\n    return np.random.normal(x, 0.6)\n\n\nprop(0.1)\n\n-0.38115925548141294\n\n\n\nx0=np.random.uniform()\nsamps = metropolis(f, prop, 1000000, x0)\n\n\n# plot our sample histogram\nplt.hist(samps,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) \nsomesamps=samps[0::100000]\nfor i,s in enumerate(somesamps):\n    xs=np.linspace(s-1, s+1, 100)\n    plt.plot(xs, norm.pdf(xs,s,0.6),'k', lw=i/5)\n#plot the true function\nxx= np.linspace(0,1,100)\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend()\n\nplt.show()\nprint(\"starting point was \", x0)\n\n\n\n\n\n\n\n\nstarting point was  0.16855486023328337\n\n\nWhat happens if we just reject samples from the normal proposal outside the range we are interested in? This seems like a legitimate thing to do: we are mixing rejection sampling with our Metropolis. However, you may realize that such a proposal has the problem of asymmetry: by rejecting in one direction we destroythe symmetry needed for “detailed balance” or equilibrium to hold."
  },
  {
    "objectID": "posts/metropolis/index.html#simulated-annealing",
    "href": "posts/metropolis/index.html#simulated-annealing",
    "title": "From Annealing to Metropolis",
    "section": "",
    "text": "As we have seen, in Simulated Annealing, we reduce the temperature while accepting proposlas which increase the energy (the function we want to minimize).\nThe probability of accepting a proposal is proportional to\n\\[A=\\exp{ ( -\\Delta f/kT)}\\]\nwhere \\(k\\) is the Boltzmann constant and \\(T\\) is the temperature.\nThis happens because, in simulated annealing, our physics analogy is that we are looking at a system in equilibrium, with a Boltzmann distribution of energies:\n\\(p(X=i) = \\frac{1}{Z(T)} \\exp{ \\left( \\frac{-E_i}{kT}\\right) }\\)$\nwhere\n\\[Z(T) = \\sum_j \\exp{ \\left( \\frac{ -E_j}{kT}\\right)}\\]\nis the normalization constant of the distribution, also called the partition function.\n\nf = lambda x: x**2 + 4*np.sin(2*x)\nxs = np.linspace(-10.,10.,1000)\nplt.plot(xs, f(xs));\nplt.xlabel('x')\nplt.ylabel('f');\n\n\n\n\n\n\n\n\nSimulated Annealing works by us making a proposal to change the current state, and then calculating the new energy of the state. We mentioned earlier that this works because simulated annealing produces a set of homogeneous markov chains, one at each temperature. We will explore this idea soon, but intuitively speaking, we can see that Simulated Annealing samples the boltzmann distribution by using our proposal “distribution”.(It is critical that this proposal is symmetric, or supports what we have been calling detailed balance, which in our physical analogy corresponds to isothermal reversibility.\nThis was the argument we made then, promising we would explain it later:\nThis detailed balance condition ensures that the sequence of \\(\\{x_t\\}\\) generated by simulated annealing is a stationary markov chain with the boltzmann distribution as the stationary distribution of the chain as \\(t \\to \\infty\\).\nBy stationary distribution, we mean an unchanging one. We’ll define this carefully in the notes on Markov Chains.\nOr, in physics words, you are in equilibrium .\nIf you think of our example in terms of the Boltzmann probability distribution as the stationary distribution, then you can identify\n\\[p_{T}(x) = e^{-f(x)/T}\\]\nas the probability distribution we get down to. Specifically let \\(p(x) = p_{1}(x)\\). Then \\(f(x) = - log(p(x))\\).\nBut even more interesting:\n\\[P_{T}(x) = P(x)^{1/T}\\]\nand so you get a peakier and peakier distribution as \\(T \\to 0\\) around the global minimum (the globality and the exponentiation ensures that this peak is favored over the rest in the dfunction \\(f\\)). You can see this in the diagram below. As \\(T \\to 0\\), we get towards a delta function at the optimum and get a global minimum instead of a distribution.\n\nimport functools\ndistx = lambda g, x: np.e**(-g(x))\ndxf = functools.partial(distx, f)\noutx = np.linspace(-10, 10,1000)\nimport scipy.integrate as integrate\nO=20\nplt.plot(outx, dxf(outx)/integrate.quad(dxf,-O, O)[0]);\nA=integrate.quad(lambda x: dxf(x)**1.2,-O, O)[0]\nplt.plot(outx, (dxf(outx)**1.2)/A);\nB=integrate.quad(lambda x: dxf(x)**2.4,-O, O)[0]\nplt.plot(outx, (dxf(outx)**2.4)/B);\nC=integrate.quad(lambda x: dxf(x)**10,-O, O)[0]\nplt.plot(outx, (dxf(outx)**10)/C);\n\n\nplt.xlim([-5,5])\nplt.xlabel('x')\nplt.xlabel('p(x)')\nplt.title(\"distribution corresponding to function f\")"
  },
  {
    "objectID": "posts/metropolis/index.html#motivating-metropolis",
    "href": "posts/metropolis/index.html#motivating-metropolis",
    "title": "From Annealing to Metropolis",
    "section": "",
    "text": "Lets turn the question on its head. Suppose we wanted to sample from a distribution \\(p(x)\\) (corresponding to a minimization of energy \\(-log(p(x))\\)).\nIn this case, we can now use a symmetric proposal to sample from \\(p(x)\\) (this will keep things “reversible”). The criterion for the proposal being able to reach any other point in a finite series of operations is called irreducibility or the notion that we wont get any disconnected sets. Clearly we need our proposal to be able to go everywhere on the support of the distribution…\nSo we stick to one temperature \\(T=1\\), and sample from \\(p(x)\\) using the same techniques we learnt in simulated annealing. We:\n\nuse a proposal distribution to propose a step.\nThen we calculate the pdf at that step, and compare it to the one at the previous step.\nIf the probability increased (energy decreased) we accept. If probability decreased (energy increased) we accept some of the time, based on the ratio of the new probability to the old one.\nWe accumulate our samples, as we are now trying to sample a distribution rather than find its “global maximum”.\n\n\ndef metropolis(p, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        if np.random.uniform() &lt; min(1, pdfratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples"
  },
  {
    "objectID": "posts/metropolis/index.html#example-sampling-a-gaussian",
    "href": "posts/metropolis/index.html#example-sampling-a-gaussian",
    "title": "From Annealing to Metropolis",
    "section": "",
    "text": "xxx= np.linspace(-10,10,1000)\nplt.plot(xxx, norm.pdf(xxx), 'r'); \n\n\n\n\n\n\n\n\n\nfrom scipy.stats import uniform\ndef propmaker(delta):\n    rv = uniform(-delta, 2*delta)\n    return rv\nuni = propmaker(0.5)\ndef uniprop(xprev):\n    return xprev+uni.rvs()\n\n\nnorm.pdf(0.1)\n\n0.39695254747701181\n\n\n\nsamps = metropolis(norm.pdf, uniprop, 100000, 0.0)\n\n\n# plot our sample histogram\nplt.hist(samps,bins=80, alpha=0.4, label=u'MCMC distribution', normed=True) \n\n#plot the true function\nxx= np.linspace(0,1,100)\nplt.plot(xxx, norm.pdf(xxx), 'r', label=u'True distribution') \nplt.legend()\n\nplt.show()\nprint(\"starting point was \", 0.0)\n\n\n\n\n\n\n\n\nstarting point was  0.0"
  },
  {
    "objectID": "posts/metropolis/index.html#motivating-metropolis-why-not-rejection-sampling-etc",
    "href": "posts/metropolis/index.html#motivating-metropolis-why-not-rejection-sampling-etc",
    "title": "From Annealing to Metropolis",
    "section": "",
    "text": "We’ve learnt how to do the inverse transform and how to use rejection sampling with a majorizing function. So why not use these methods to sample a distribution?\nIt can be hard to find a majorizing \\(g(x)\\) and this gets even harder for multidimensional \\(g(x)\\)…and to boot less efficient as you leave more and more space out. Also, our general ideais to compute expectations as sample averages, and majorizing in multiple dimensions can have us spending a lot of time in tails of distributions.\nAlso note that the integrals being calculated in these expectations are of the type:\n\\[E_f[g] = \\int dV f(x) g(x).\\]\nIn multiple domensions, volume elements get smaller and smaller…the curse of dimensionality. This can be seen in the diagram :\n\n\n\nCurse of dimensionality: the inner volume shrinks exponentially relative to the outer volume as dimensions increase.\n\n\n(image from Betancourt)\nwhere the centre-partitions combination to an integral goes from 1/3rd to 1/27th. Now suppose the mode of the distibution is contained in this partition: then its contribution to the integral is going down with dimensions.\nAs the centre volume decreases, the outer volume increases, but this is in distribution tails, so we dont get much of a contribution from there either:\n\n\n\nThe typical set in increasing dimensions: probability mass concentrates in a thin shell between the center and outer boundary. From Betancourt.\n\n\n(image from Betancourt)\n(the slivers outside increase).\nIt is the neighborhood between these extremes, called the typical set which our sampler must explore well. And to get a good rejection sampling majorizer for this becomes hard.\nWe’ll come back to this picture later…"
  },
  {
    "objectID": "posts/metropolis/index.html#another-example",
    "href": "posts/metropolis/index.html#another-example",
    "title": "From Annealing to Metropolis",
    "section": "",
    "text": "f = lambda x: 6*x*(1-x)\n\n\nxxx= np.linspace(-1,2,100)\nplt.plot(xxx, f(xxx), 'r') \nplt.axvline(0, 0,1, color=\"gray\")\nplt.axvline(1, 0,1, color=\"gray\")\nplt.axhline(0, 0,1, color=\"gray\");\n\n\n\n\n\n\n\n\nWe wish to consider the support [0,1]. We could truncate our “distribution” beyond these. But it does not matter, even though we use a normal proposal whichcan propose negative and gretar-than-one \\(x\\) values.\nWhat happens if the proposal proposes a number outside of [0,1]? Notice then that our pdf is negative(ie it is not a pdf. (we could have defined it as 0 as well). Then in the metropolis acceptance formula, we are trying to check if a uniform is less than a negative or 0 number and we will not accept. This does however mean that we will need a longer set of samples than otherwise…\n\ndef prop(x):\n    return np.random.normal(x, 0.6)\n\n\nprop(0.1)\n\n-0.38115925548141294\n\n\n\nx0=np.random.uniform()\nsamps = metropolis(f, prop, 1000000, x0)\n\n\n# plot our sample histogram\nplt.hist(samps,bins=100, alpha=0.4, label=u'MCMC distribution', normed=True) \nsomesamps=samps[0::100000]\nfor i,s in enumerate(somesamps):\n    xs=np.linspace(s-1, s+1, 100)\n    plt.plot(xs, norm.pdf(xs,s,0.6),'k', lw=i/5)\n#plot the true function\nxx= np.linspace(0,1,100)\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend()\n\nplt.show()\nprint(\"starting point was \", x0)\n\n\n\n\n\n\n\n\nstarting point was  0.16855486023328337\n\n\nWhat happens if we just reject samples from the normal proposal outside the range we are interested in? This seems like a legitimate thing to do: we are mixing rejection sampling with our Metropolis. However, you may realize that such a proposal has the problem of asymmetry: by rejecting in one direction we destroythe symmetry needed for “detailed balance” or equilibrium to hold."
  },
  {
    "objectID": "posts/noisylearning/index.html",
    "href": "posts/noisylearning/index.html",
    "title": "Learning With Noise",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\ndef make_simple_plot():\n    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$y$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([-2,2])\n    axes[1].set_ylim([-2,2])\n    plt.tight_layout();\n    return axes\ndef make_plot():\n    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n    axes[0].set_ylabel(\"$p_R$\")\n    axes[0].set_xlabel(\"$x$\")\n    axes[1].set_xlabel(\"$x$\")\n    axes[1].set_yticklabels([])\n    axes[0].set_ylim([0,1])\n    axes[1].set_ylim([0,1])\n    axes[0].set_xlim([0,1])\n    axes[1].set_xlim([0,1])\n    plt.tight_layout();\n    return axes"
  },
  {
    "objectID": "posts/noisylearning/index.html#revisiting-our-model",
    "href": "posts/noisylearning/index.html#revisiting-our-model",
    "title": "Learning With Noise",
    "section": "Revisiting our model",
    "text": "Revisiting our model\nLet us revisit our model from noiseless_learning.\nLet \\(x\\) be the fraction of religious people in a county and \\(y\\) be the probability of voting for Romney as a function of \\(x\\). In other words \\(y_i\\) is data that pollsters have taken which tells us their estimate of people voting for Romney and \\(x_i\\) is the fraction of religious people in county \\(i\\). Because poll samples are finite, there is a margin of error on each data point or county \\(i\\), but we will ignore that for now.\nLet us assume that we have a “population” of 200 counties \\(x\\):\n\ndf=pd.read_csv(\"data/religion.csv\")\ndf.head()\n\n\n\n\n\n\n\npromney\nrfrac\n\n\n\n\n0\n0.047790\n0.00\n\n\n1\n0.051199\n0.01\n\n\n2\n0.054799\n0.02\n\n\n3\n0.058596\n0.03\n\n\n4\n0.062597\n0.04\n\n\n\n\n\n\n\nLets suppose now that the Lord came by and told us that the points in the plot below captures \\(f(x)\\) exactly.\n\nx=df.rfrac.values\nf=df.promney.values\nplt.plot(x,f,'.', alpha=0.3)\n\n\n\n\n\n\n\n\nNotice that our sampling of \\(x\\) is not quite uniform: there are more points around \\(x\\) of 0.7.\nNow, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample \\(\\cal{D}\\) of 30 data points. Such data is called in-sample data. Contrastingly, the entire population of data points is also called out-of-sample data.\n\ndfsample = pd.read_csv(\"data/noisysample.csv\")\nindexes=dfsample.i.values\n\n\ndfpop = pd.read_csv(\"data/noisypopulation.csv\")\ndfpop.head()\n\n\n\n\n\n\n\nf\nx\ny\n\n\n\n\n0\n0.047790\n0.00\n0.011307\n\n\n1\n0.051199\n0.01\n0.010000\n\n\n2\n0.054799\n0.02\n0.007237\n\n\n3\n0.058596\n0.03\n0.000056\n\n\n4\n0.062597\n0.04\n0.010000\n\n\n\n\n\n\n\n\nsamplex = x[indexes]\nsamplef = f[indexes]\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\naxes[1].plot(x,f, 'r.', alpha=0.2, label=\"population\");\naxes[1].plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n\n\n\n\n\n\n\n\nStatement of the learning problem.\nLet us restate the learning problem from noiseless learning.\nIf we find a hypothesis \\(g\\) that minimizes the cost or risk over the training set (our sample), this hypothesis might do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nIn other words, we hope the empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small."
  },
  {
    "objectID": "posts/noisylearning/index.html#why-go-out-of-sample",
    "href": "posts/noisylearning/index.html#why-go-out-of-sample",
    "title": "Learning With Noise",
    "section": "Why go out-of-sample",
    "text": "Why go out-of-sample\nClearly we want tolearn something about a population, a population we dont have access to. So far we have stayed within the constraints of our sample and just like we did in the case of monte-carlo, might want to draw all our conclusions from this sample.\nThis is a bad idea.\nYou probably noticed that I used weasel words like “might” and “hope” in the last section when saying that representative sampling in both training and test sets combined with ERM is what we need to learn a model. Let me give you a very simple counterexample: a prefect memorizer.\nSuppose I construct a model which memorizes all the data points in the training set. Then its emprical risk is zero by definition, but it has no way of predicting anything on a test set. Thus it might as well choose the value at a new point randomly, and will perform very poorly. The process of interpolating a curve from points is precisely this memorization\n\nStochastic Noise\nWe saw before that \\(g_{20}\\) did a very good job in capturing the curves of the population. However, note that the data obtained from \\(f\\), our target, was still quite smooth. Most real-world data sets are not smooth at all, because of various effects such as measurement errors, other co-variates, and so on. Such stochastic noise plays havoc with our fits, as we shall see soon.\nStochastic noise bedevils almost every data set known to humans, and happens for many different reasons.\nConsider for example two customers of a bank with identical credit histories and salaries. One defaults on their mortgage, and the other does not. In this case we have identical \\(x = (credit, salary)\\) for these two customers, but different \\(y\\), which is a variable that is 1 if the customer defaulted and 0 otherwise. The true \\(y\\) here might be a function of other co-variates, such as marital strife, sickness of parents, etc. But, as the bank, we might not have this information. So we get different \\(y\\) for different customers at the information \\(x\\) that we possess.\nA similar thing might be happen in the election example, where we have modelled the probability of voting for romney as a function of religiousness of the county. There are many other variables we might not have measured, such as the majority race in that county. But, we have not measured this information. Thus, in counties with high religiousness fraction \\(x\\) we might have more noise than in others. Consider for example two counties, one with \\(x=0.8\\) fraction of self-identified religious people in the county, and another with \\(x=0.82\\). Based on historical trends, if the first county was mostly white, the fraction of those claiming they would vote for Romney might be larger than in a second, mostly black county. Thus you might have two very \\(y\\)’s next to each other on our graphs.\nIt gets worse. When pollsters estimate the number of people voting for a particular candidate, they only use finite samples of voters. So there is a 4-6% polling error in any of these estimates. This “sampling noise” adds to the noisiness of the \\(y\\)’s.\nIndeed, we wish to estimate a function \\(f(x)\\) so that the values \\(y_i\\) come from the function \\(f\\). Since we are trying to estimate f with data from only some counties, and furthermore, our estimates of the population behaviour in these counties will be noisy, our estimate wont be the “god given” or “real” f, but rather some noisy estimate of it.\n\ndfsample.head()\n\n\n\n\n\n\n\nf\ni\nx\ny\n\n\n\n\n0\n0.075881\n7\n0.07\n0.138973\n\n\n1\n0.085865\n9\n0.09\n0.050510\n\n\n2\n0.096800\n11\n0.11\n0.183821\n\n\n3\n0.184060\n23\n0.23\n0.057621\n\n\n4\n0.285470\n33\n0.33\n0.358174\n\n\n\n\n\n\n\n\ndfsample.shape\n\n(30, 4)\n\n\n\nx = dfpop.x.values\nf = dfpop.f.values\ny = dfpop.y.values\n\n\nplt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\nplt.plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"in-sample y (observed)\");\nplt.plot(x, y, '.', alpha=0.6, label=\"population y\");\nplt.xlabel('$x$');\nplt.ylabel('$p_R$')\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nIn the figure above, one can see the scatter of the \\(y\\) population about the curve of \\(f\\). The errors of the 30 observation points (“in-sample”) are shown as squares. One can see that observations next to each other can now be fairly different, as we descibed above."
  },
  {
    "objectID": "posts/noisylearning/index.html#fitting-a-noisy-model",
    "href": "posts/noisylearning/index.html#fitting-a-noisy-model",
    "title": "Learning With Noise",
    "section": "Fitting a noisy model",
    "text": "Fitting a noisy model\nLet us now try and fit the noisy data we simulated above, both using straight lines (\\(\\cal{H_1}\\)), and 20th order polynomials(\\(\\cal{H_{20}}\\)).\nWhat we have done is introduced a noisy target \\(y\\), so that\n\\[y = f(x) + \\epsilon\\,,\\]\nwhere \\(\\epsilon\\) is a random noise term that represents the stochastic noise.\n\nDescribing things probabilistically\nAnother way to think about a noisy \\(y\\) is to imagine that our data is generated from a joint probability distribution \\(P(x,y)\\). In our earlier case with no stochastic noise, once you knew \\(x\\), if I were to give you \\(f(x)\\), you could give me \\(y\\) exactly. This is now not possible because of the noise \\(\\epsilon\\): we dont know exactly how much noise we have at any given \\(x\\). Thus we need to model \\(y\\) at a given \\(x\\), \\(P(y \\mid x)\\), as well using a probability distribution. Since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) .\\]\n\n\n\nThe noisy learning framework: target distribution P(y|x) replaces deterministic f(x)\n\n\nNow the entire learning problem can be cast as a problem in probability density estimation: if we can estimate \\(P(x,y)\\) and take actions based on that estimate thanks to our risk or error functional, we are done.\nWe now fit in both \\(\\cal{H_1}\\) and \\(\\cal{H_{20}}\\) to find the best fit straight line and best fit 20th order polynomial respectively.\n\ng1 = np.poly1d(np.polyfit(x[indexes],f[indexes],1))\ng20 = np.poly1d(np.polyfit(x[indexes],f[indexes],20))\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\ng1noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],1))\ng20noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],20))\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'r-', alpha=0.6, label=\"f\");\naxes[1].plot(x,f, 'r-', alpha=0.6, label=\"f\");\naxes[0].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\naxes[1].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\naxes[0].plot(x,g1(x),  'b--', alpha=0.6, label=\"$g_1 (no noise)$\");\naxes[0].plot(x,g1noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_1 (noisy)$\");\naxes[1].plot(x,g20(x),  'b--', alpha=0.6, label=\"$g_10 (no noise)$\");\naxes[1].plot(x,g20noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_{10}$ (noisy)\");\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n\n\n\n\n\n\n\nThe results are (to put it mildly) very interesting.\nLets look at the figure on the left first. The noise changes the best fit line by a little but not by much. The best fit line still does a very poor job of capturing the variation in the data.\nThe best fit 20th order polynomial, in the presence of noise, is very interesting. It tries to follow all the curves of the observations..in other words, it tries to fit the noise. This is a disaster, as you can see if you plot the population (out-of-sample) points on the plot as well:\n\nplt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\nplt.plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\nplt.plot(x,y,  '.', alpha=0.6, label=\"population y\");\nplt.plot(x,g20noisy(x), 'b:', alpha=0.6, label=\"$g_{10}$ (noisy)\");\nplt.ylim([0,1])\nplt.legend(loc=4);\n\n\n\n\n\n\n\n\nWhoa. The best-fit 20th order polynomial does a reasonable job fitting the in-sample data, and is even well behaved in the middle where we have a lot of in-sample data points. But at places with less in-sample data points, the polynomial wiggles maniacally.\nThis fitting to the noise is a danger you will encounter again and again in learning. Its called overfitting. So, \\(\\cal{H_{20}}\\) which seemed to be such a good candidate hypothesis space in the absence of noise, ceases to be one. The take away lesson from this is that we must further ensure that our model does not fit the noise.\nLets make a plot similar to the one we made for the deterministic noise earlier, and compare the error in the new \\(g_1\\) and \\(g_{20}\\) fits on the noisy data.\n\nplt.plot(x, ((g1noisy(x)-f)**2), lw=3, label=\"$g_1$\")\nplt.plot(x, ((g20noisy(x)-f)**2), lw=3,label=\"$g_{20}$\");\nplt.plot(x, [1]*x.shape[0], \"k:\", label=\"noise\", alpha=0.2);\nfor i in indexes[:-1]:\n    plt.axvline(x[i], 0, 1, color='r', alpha=0.1)\nplt.axvline(x[indexes[-1]], 0, 1, color='r', alpha=0.1, label=\"samples\")\nplt.xlabel(\"$x$\")\nplt.ylabel(\"population error\")\nplt.yscale(\"log\")\nplt.legend(loc=4);\nplt.title(\"Noisy Data\");\n\n\n\n\n\n\n\n\n\\(g_1\\) now, for the most part, has a lower error! So you’d be better off by having chosen a set of models with much more bias (the straight lines, \\(\\cal{H}_1\\)) than a more complex model set (\\(\\cal{H}_{20}\\)) in the case of noisy data.\n\n\nThe Variance of your model\nThis tendency of a more complex model to overfit, by having enough freedom to fit the noise, is described by something called high variance. What is variance?\nVariance, simply put, is the “error-bar” or spread in models that would be learnt by training on different data sets \\(\\cal{D_1}, \\cal{D_2},...\\) drawn from the population. Now, this seems like a circular concept, as in real-life, you do not have access to the population. But since we simulated our data here anyways, we do, and so let us see what happens if we choose different 30 points randomly from our population of 200, and fit models in both \\(\\cal{H_1}\\) and \\(\\cal{H_{20}}\\) to them. We do this on 200 sets of randomly chosen (from the population) data sets of 30 points each and plot the best fit models in noth hypothesis spaces for all 200 sets.\n\ndef gen(degree, nsims, size, x, out):\n    outpoly=[]\n    for i in range(nsims):\n        indexes=np.sort(np.random.choice(x.shape[0], size=size, replace=False))\n        pc=np.polyfit(x[indexes], out[indexes], degree)\n        p=np.poly1d(pc)\n        outpoly.append(p)\n    return outpoly\n\n\npolys1 = gen(1, 200, 30,x, y);\npolys20 = gen(20, 200, 30,x, y);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/numpy/lib/polynomial.py:595: RankWarning: Polyfit may be poorly conditioned\n  warnings.warn(msg, RankWarning)\n\n\n\naxes=make_plot()\naxes[0].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\naxes[1].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\naxes[0].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\naxes[1].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\naxes[0].plot(x, y, '.', alpha=0.6, label=\"population y\");\naxes[1].plot(x, y, '.', alpha=0.6, label=\"population y\");\nc=sns.color_palette()[2]\nfor i,p in enumerate(polys1[:-1]):\n    axes[0].plot(x,p(x), alpha=0.05, c=c)\naxes[0].plot(x,polys1[-1](x), alpha=0.05, c=c,label=\"$g_1$ from different samples\")\nfor i,p in enumerate(polys20[:-1]):\n    axes[1].plot(x,p(x), alpha=0.05, c=c)\naxes[1].plot(x,polys20[-1](x), alpha=0.05, c=c, label=\"$g_{10}$ from different samples\")\naxes[0].legend(loc=4);\naxes[1].legend(loc=4);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\nOn the left panel, you see the 200 best fit straight lines, each a fit on a different 30 point training sets from the 200 point population. The best-fit lines bunch together, even if they dont quite capture \\(f\\) (the thick red line) or the data (squares) terribly well.\nOn the right panel, we see the same with best-fit models chosen from \\(\\cal{H}_{20}\\). It is a diaster. While most of the models still band around the central trend of the real curve \\(f\\) and data \\(y\\) (and you still see the waves corresponding to all too wiggly 20th order polynomials), a substantial amount of models veer off into all kinds of noisy hair all over the plot. This is variance: the the predictions at any given \\(x\\) are all over the place.\nThe variance can be seen in a different way by plotting the coefficients of the polynomial fit. Below we plot the coefficients of the fit in \\(\\cal{H}_1\\). The variance is barely 0.2 about the mean for both co-efficients.\n\npdict1={}\npdict20={}\nfor i in reversed(range(2)):\n    pdict1[i]=[]\n    for j, p in enumerate(polys1):\n        pdict1[i].append(p.c[i])\nfor i in reversed(range(21)):\n    pdict20[i]=[]\n    for j, p in enumerate(polys20):\n        pdict20[i].append(p.c[i]) \ndf1=pd.DataFrame(pdict1)\ndf20=pd.DataFrame(pdict20)\nfig = plt.figure(figsize=(14, 5)) \nfrom matplotlib import gridspec\ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 10]) \naxes = [plt.subplot(gs[0]), plt.subplot(gs[1])]\naxes[0].set_ylabel(\"value\")\naxes[0].set_xlabel(\"coeffs\")\naxes[1].set_xlabel(\"coeffs\")\nplt.tight_layout();\nsns.violinplot(df1, ax=axes[0]);\nsns.violinplot(df20, ax=axes[1]);\naxes[0].set_yscale(\"symlog\");\naxes[1].set_yscale(\"symlog\");\naxes[0].set_ylim([-1e12, 1e12]);\naxes[1].set_ylim([-1e12, 1e12]);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/seaborn/categorical.py:1791: UserWarning: The violinplot API has been changed. Attempting to adjust your arguments for the new API (which might not work). Please update your code. See the version 0.6 release notes for more info.\n  warnings.warn(msg, UserWarning)\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\nIn the right panel we plot the coefficients of the fit in \\(\\cal{H}_{20}\\). This is why we use the word “variance”: the spread in the values of the middle coefficients about their means (dashed lines) is of the order \\(10^{10}\\) (the vertical height of the bulges), with huge outliers!! The 20th order polynomial fits are a disaster!"
  },
  {
    "objectID": "posts/noisylearning/index.html#bias-and-variance",
    "href": "posts/noisylearning/index.html#bias-and-variance",
    "title": "Learning With Noise",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nWe have so far informally described two different concepts: bias and variance. Bias is deterministic error, the kind of error you get when your model is not expressive enough to describe the data. Variance describes the opposite problem, where it is too expressive.\nEvery model has some bias and some variance. Clearly, you dont want either to dominate, which is something we’ll worry about soon.\nLet us mathematically understand what bias and variance are, so that we can use these terms more precisely from now onwards. Follow carefully to see how our calculation mimics the code/process we used above.\n\\[\\renewcommand{\\gcald}{g_{\\cal D}}\\] \\[\\renewcommand{\\ecald}{E_{\\cal{D}}}\\]\nWe had from noiseless learning:\n\\[R_{out}(h) =  E_{p(x)}[(h(x) - f(x))^2] = \\int dx p(x)  (h(x) - f(x))^2 .\\]\nIn the presence of noise \\(\\epsilon\\) which we shall assume to be 0-mean, variance \\(\\sigma^2\\) noise, we have \\(y = f(x) + \\epsilon\\) and the above formula becomes:\n\\[R_{out}(h) =  E_{p(x)}[(h(x) - y)^2] = \\int dx p(x)  (h(x) - f(x) - \\epsilon)^2 .\\]\nNow let us use Empirical Risk minimization to fit on our training set. We come up with a best fit hypothesis \\(h = \\gcald\\), where \\(\\cal{D}\\) is our training sample.\n\\[R_{out}(\\gcald) =  E_{p(x)}[(\\gcald(x) - f(x) - \\epsilon)^2] \\]\nLet us compute the expectation of this quantity with respect to the sampling distribution obtained by choosing different samples from the population. Note that we cant really do this if we have been only given one training set, but in this document, we have had access to the population and can thus experiment.\nDefine:\n\\[\\langle  R \\rangle = E_{\\cal{D}} [R_{out}(\\gcald)] =  E_{\\cal{D}}E_{p(x)}[(\\gcald(x) - f(x) - \\epsilon)^2] \\]\n\\[\n\\begin{eqnarray*}\n=& E_{p(x)}\\ecald[(\\gcald(x) - f(x) - \\epsilon)^2]\\\\\n=& E_{p(x)}[\\ecald[\\gcald^2] +  f^2 + \\epsilon^2 - 2\\,f\\,\\ecald[\\gcald]]\n\\end{eqnarray*}\n\\]\nDefine:\n\\[ \\bar{g} = \\ecald[\\gcald] = (1/M)\\sum_{\\cal{D}} \\gcald\\]\nas the average “g” over all the fits (M of them) on the different samples, so that we can write, adding and subtracting \\(\\bar{g}^2\\):\n\\[\\langle  R \\rangle =  E_{p(x)}[\\ecald[\\gcald^2] - \\bar{g}^2 +  f^2 - 2\\,f\\,\\bar{g} + \\bar{g}^2 + \\epsilon^2 ] = E_{p(x)}[\\ecald[(\\gcald - \\bar{g})^2]  +  (f - \\bar{g})^2 + \\epsilon^2 ]\\]\nThus:\n\\[\\langle  R \\rangle =  E_{p(x)}[\\ecald[(\\gcald - \\bar{g})^2]] + E_{p(x)}[(f - \\bar{g})^2] + \\sigma^2\\]\nThe first term here is called the variance, and captures the squared error of the various fit g’s from the average g, or in other words, the hairiness. The second term is called the bias, and tells us, how far the average g is from the original f this data came from. Finally the third term is the stochastic noise, the minimum error that this model will always have.\nNote that if we set the stochastic noise to 0 we get back the noiseless model we started out with. So even in a noiseless model, we do have bias and variance. This is because we still have sampling noise in such a model, and this is one of the sources of variance."
  },
  {
    "objectID": "posts/noisylearning/index.html#so-far",
    "href": "posts/noisylearning/index.html#so-far",
    "title": "Learning With Noise",
    "section": "So far?",
    "text": "So far?\n\nyou have learnt the basic formulation of the learning problem, the concept of a hypothesis space, and a strategy using minimization of distance (called cost or risk) to find the best fit model for the target function from this hypothesis space.\nYou learned the effect of noise on this fit, and the issues that crop up in learning target functions from data, chiefly the problem of overfitting to this noise.\n\nThe process of learning has two parts:\n\nFit for a model by minimizing the in-sample risk\nHope that the in-sample risk approximates the out-of-sample risk well.\n\nMathematically, we are saying that:\n\\[\n\\begin{eqnarray*}\nA &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\nB &:& R_{out} (g) \\approx R_{\\cal{D}}(g)\n\\end{eqnarray*}\n\\]\nWell, we are scientists. Just hoping does not befit us. But we only have a sample. What are we to do? We can model the in-sample risk and out-of-sample risk. And we can use a test set to estimate our out of sample risk, as we see here."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', False)\nimport seaborn as sns\nsns.set_style(\"white\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/probability/index.html#what-is-probability",
    "href": "posts/probability/index.html#what-is-probability",
    "title": "Probability",
    "section": "What is probability?",
    "text": "What is probability?\nSuppose you were to flip a coin. Then you expect not to be able to say whether the next toss would yield a heads or a tails. You might tell a friend that the odds of getting a heads is equal to to the odds of getting a tails, and that both are \\(1/2\\).\nThis intuitive notion of odds is a probability. It comes about because of our physical model of the world: say that because of our faith in the U.S. Mint, we might be willing to, without having seen any tosses, say that the coin is fair. In other words, there are two choices, both of which are equally likely.\n\nProbability from Symmetry\nConsider another example. If we were tossing a ‘fair’ six-sided dice, we may thus equivalently say that the odds of the dice falling on any one of its sides is \\(1/6\\). Indeed if there are \\(C\\) different equally likely possibilities, we’d expect that the probability of any one particular outcome would be \\(1/C\\).\nThe examples of the coin as well as the dice illustrate the notion of probability springing from symmetry. Here we think of probability of of the number 4 on the dice as the ratio:\n\\[\\frac{Number\\: of\\: cases\\: for\\: number\\: 4}{number\\: of\\: possibilities} = \\frac{1}{6},\\] assuming equally likely possibilities.\nIn other words, the symmetry refers to the notion that when there are multiple ways for an event to happen, and that then we have an intuitive model of fairness between these ways that tells us that none of these are any more likely than the other.\n\n\nProbability from a model\nThus one might think of symmetry as providing a model. There are also other kinds of models.\nThink of an event like an election, say a presidential election. You cant exactly run multiple trials of the election: its a one-off event. But you still want to talk about the likelyhood of a candidate winning. However people do make models of elections, based on inputs such as race, age, income, sampling polls, etc. They assign likeyhoods of candidates winning and run large numbers of simulations of the election, making predictions based on that. Forecasters like Nate Silver, Sam Wang, And Drew Linzer, made incredibly successfull predictions of the 2012 elections.\nOr consider what a weather forecaster means when he or she says there is a 90% chance of rain today. Presumably, this conclusion has been made from many computer simulations which take in the weather conditions known in the past, and propagated using physics to the current day. The simulations give different results based on the uncertainty in the measurement of past weather, and the inability of the physics to capture the phenomenon exactly (all physics is some approximation to the natural world). But 90% of these simulations show rain.\nIn all of these cases, there is either a model (a fair coin, an election forecasting model, a weather differential equation), or an experiment ( a large number of coin tosses) that is used to estimate a probability, or the odds, of an event \\(E\\) occuring.\n\nCombining models and observations\nIn all of these cases, probability is something we speak of, for observations we are to make in the future. And it is something we assign, based on the model or belief of the world we have, or on the basis of past observations that we have made, or that we might even imagine that we would make.\nConsider some additional examples. You might ask the probability of the Yankees winning the next baseball game against the Red Sox. Or you might ask for the probability of a launch failure for the next missile protecting Tel-Aviv. These are not academic questions: lots of betting money and lives depend upon them respectively. In both cases there is some past data, and some other inputs such as say, weather conditions, which might be used to construct a model, which is then used to predict the fate of the next game or launch.\nThey key takeaway is this: for some reasons, and possibly using some data, we have constructed a model of the universe. In other words, we have combined prior beliefs and past frequencies respectively. This notion of such combination is yet another notion of probability, called the Bayesian notion of probability. And we can now use this model to make predictions, such us the future odds of a particular event happening.\n\n\n\nProbability from frequency\nConsider doing a large number of coin flips. You would do, or imagine doing, a large number of flips or trials \\(N\\), and finding the number of times you got heads \\(N_H\\). Then the probability of getting heads would be \\[\\frac{N_H}{N}.\\]\nThis is the notion of probability as a relative frequency: if there are multiple ways an event like the tossing of a coin can happen, lets look at multiple trials of the event and see the fraction of times one or other of these ways happened.\nThis jibes with our general notion of probability from symmetry: indeed you can think of it as an experimental verificaltion of a symmetry based model.\n\nSimulating the results of the model\nWe dont have a coin right now. So let us simulate this process on a computer. To do this we will use a form of the random number generator built into numpy. In particular, we will use the function np.random.choice, which will with equal probability for all items pick an item from a list (thus if the list is of size 6, it will pick one of the six list items each time, with a probability 1/6).\n\ndef throw_a_coin(N):\n    return np.random.choice(['H','T'], size=N)\nthrows=throw_a_coin(40)\nprint(\"Throws:\",\" \".join(throws))\nprint(\"Number of Heads:\", np.sum(throws=='H'))\nprint(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/40.)\n\nThrows: T H T H T H H H H H T H H H T H T H T H T H H H T H H H H T T T T H T T H T T T\nNumber of Heads: 22\np1 = Number of Heads/Total Throws: 0.55\n\n\nNotice that you do not necessarily get 20 heads.\nNow say that we run the entire process again, a second replication to obtain a second sample. Then we ask the same question: what is the fraction of heads we get this time? Lets call the odds of heads in sample 2, then, \\(p_2\\):\n\ndef make_throws(N):\n    throws=throw_a_coin(N)\n    if N &lt;= 100:\n        print(\"Throws:\",\" \".join(throws))\n    else:\n        print(\"First 100 Throws:\",\" \".join(throws[:100]))\n    print(\"Number of Heads:\", np.sum(throws=='H'))\n    print(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/N)\nmake_throws(40)\n\nThrows: H T H T H H H H T H H H T T T H T H H H H T H H T T H H T H T H T H H H T T H H\nNumber of Heads: 25\np1 = Number of Heads/Total Throws: 0.625\n\n\nLet’s do many more trials\n\nmake_throws(1000)\n\nFirst 100 Throws: H H H T H H T T T H T H H H H T T T H H H H H H T H T T T H T H T T T H H T H T H H H H T H T H T T H H T T T T T T H T H H T H T H T H T H T T H T H H H T H T T T H H T H T H T H T H H H H T T T T H\nNumber of Heads: 521\np1 = Number of Heads/Total Throws: 0.521\n\n\nAnd even more:\n\nmake_throws(10000)\n\nFirst 100 Throws: H T T T H T T T H T T T T H T T H T H H H H T H T H T T H T H T H H H H T T H H H H T H H H H T H T T T H T H H T T T H T H T H T T T H T T T T H T H H H H H T T H H H T T H H H H H H T H T H T T T H\nNumber of Heads: 5047\np1 = Number of Heads/Total Throws: 0.5047\n\n\nAs you can see, the larger number of trials we do, the closer we seem to get to half the tosses showing up heads. Lets see this more systematically:\n\ntrials=np.arange(0, 40000, 1000)\nplt.plot(trials, [np.sum(throw_a_coin(j)=='H')/np.float(j) for j in trials], 'o-', alpha=0.2);\nplt.axhline(0.5, 0, 1, color='r');\nplt.xlabel('number of trials');\nplt.ylabel('probability of heads from simulation');\nplt.title('frequentist probability of heads');\n\n\n\n\n\n\n\n\nThus, the true odds fluctuate about their long-run value of 0.5, in accordance with the model of a fair coin (which we encoded in our simulation by having np.random.choice choose between two possibilities with equal probability), with the fluctuations becoming much smaller (we shall talk a lot more about this later in the book). These fluctations are what give rise to probability distributions.\nEach finite length run is called a sample, which has been obtained from the generative model of our fair coin. Its called generative as we can use the model to generate, using simulation, a set of samples we can play with to understand a model. Such simulation from a model is a key technique which we will come back to again and again in learning from data."
  },
  {
    "objectID": "posts/probability/index.html#the-rules-of-probability",
    "href": "posts/probability/index.html#the-rules-of-probability",
    "title": "Probability",
    "section": "The rules of probability",
    "text": "The rules of probability\nWe have seen multiple notions of probability so far. One might assign probabilities based on symmetry, for eg, 2 sides of a fair coin, or six sides of a fair dice. One might assign probabilities based on doing an experiment. such as the long run number of heads in many coin flips. One might assign probabilities based on beliefs; and one might even assign probabilities to events that have no chance of repeating, such as the 2012 presidential election, or the probability of rain between 2pm and 6pm today.\nThus, the very definition of probability seems to be wishy-washy and subjective. Thus you might wonder how you might work with such probabilities. For this, we turn to the rules of probability.\nThe rules dont care where our probabilities come from, as to how we estimated them, as long as they behave in intuitively sensible ways.\nConsider an example:\nE is the event of getting a heads in a first coin toss, and F is the same for a second coin toss. Here \\(\\Omega\\), the set of all possibilities that can happen when you toss two coins is \\(\\{HH, HT, TH, TT\\}\\). Since E only specifies that the first toss is heads, \\(E=\\{HT, HH\\}\\). Similarly \\(F= {HH, TH}\\) The set of all events that are not E then is \\(\\tilde{E} = {TH, TT}\\).\nThese sets, along with some others are captured in the venn diagram below:\n\n\n\nVenn diagram for the 2-coin-toss sample space\n\n\n\nThe Multiply/And/Intersection Formula for independent events\nIf E and F are independent events, the probability of both events happening together \\(P(EF)\\) or \\(P(E \\cap F)\\) (read as E and F or E intersection F, respectively) is the multiplication of the individual probabilities.\n\\[ P(EF) = P(E) P(F) .\\]\nIf you made the two independent coin tosses in our example, and you had a fair coin, the probability of both coming up heads is \\((1/2)*(1/2) = 1/4\\). This makes intuitive sense: half the time the first coin comes up heads, and then 1/2 the time the second coin comes up heads, so its 1/4 of the times that both come up heads.\n\n\nThe Plus/Or/Union Formula\nWe can now ask the question, what is \\(P(E+F)\\), the odds of E alone, F alone, or both together. Translated into English, we are asking, whats the probability that only the first toss was heads, or only the second toss was heads, or that both came up heads? Or in other words, what are the odds of at least one heads? The answer to this question is given by the rule:\n\\[P(E+F) = P(E) + P(F) - P(EF),\\]\nthe “plus” formula, where E+F, read as E or F (also \\(E \\cup F\\), reads as E union F) means “E alone, F alone, or both together”. This rule is a hard one to understand and has a lot of notation, so lets examine it in some detail.\nThere are four ways that these two tosses can arrange themselves, as illustrated by this diagram, adapted from the probability chapter in Feynman’s lectures on Physics..you should read it!.\n\n\n\nTree diagram for 2 coin flips\n\n\nWe can have a HH, HT, TH, or TT. In three out of 4 of these cases, either the first toss was heads, or the second was heads. Thus \\(P(E+F)=3/4\\).\nThe formula says, add the odds that “the first toss was a heads, without worrying about the second one (1/2), to the probability that the second toss was a heads, without worrying about the first one” (1/2). Since this double counts the situation where both are heads; subtract that (1/4):\n\\[\\begin{eqnarray}\nP(E+F) \\, & = &\\, P(E) + P(F) - P(EF)\\\\\n\\frac{3}{4} \\, & = &\\, \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4}\n\\end{eqnarray}\\]\nArmed with these two formulas, we can tackle the world of conditional and marginal probabilities, and Bayes theorem!\n\n\nFormally summarizing the rules\nIf \\(X\\) and \\(Y\\) are two events and \\(p(X)\\) is the probability of the event \\(X\\) to happen. $X^- $ is the complement of \\(X\\), the event which is all the occurrences which are not in \\(X\\). \\(X+Y\\) is the union of \\(X\\) and \\(Y\\); \\(X,Y\\) is the intersection of \\(X\\) and \\(Y\\). (Both \\(X+Y\\) and \\(X,Y\\) are also events.)\n\n\nThe very fundamental rules of probability:\n\n\\(p(X) &gt;=0\\); probability must be non-negative\n\\(0 ≤ p(X) ≤ 1 \\;\\) \\(X\\) has probability range from 0 to 1.\n\\(p(X)+p(X^-)=1 \\;\\) \\(X\\) must either happen or not happen. These last two aximoms can be thought of as saying that the probabilities if all events put tohether must sum to 1.\n\\(p(X+Y)=p(X)+p(Y)−p(X,Y) \\;\\) \\(X\\) can happen and \\(Y\\) can happen but we must subtract the cases that are happening together so we do not over-count."
  },
  {
    "objectID": "posts/probability/index.html#random-variables",
    "href": "posts/probability/index.html#random-variables",
    "title": "Probability",
    "section": "Random Variables",
    "text": "Random Variables\nTo link the notion of events such as \\(E\\) and collections of events, or probability spaces \\(\\Omega\\) to data, we must introduce the concept of random variables. The following definition is taken from Larry Wasserman’s All of Stats.\nDefinition. A random variable is a mapping\n\\[ X: \\Omega \\rightarrow \\mathbb{R}\\]\nthat assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\). \\(\\Omega\\) is the sample space. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. Subsets of \\(\\Omega\\) are called Events. Say \\(\\omega = HHTTTTHTT\\) then \\(X(\\omega) = 3\\) if defined as number of heads in the sequence \\(\\omega\\).\nWe will assign a real number P(A) to every event A, called the probability of A. We also call P a probability distribution or a probability measure. To qualify as a probability, P must satisfy the three axioms (non-negative, \\(P(\\Omega)=1\\), disjoint probs add)."
  },
  {
    "objectID": "posts/probability/index.html#marginals-and-conditionals-and-bayes-theorem",
    "href": "posts/probability/index.html#marginals-and-conditionals-and-bayes-theorem",
    "title": "Probability",
    "section": "Marginals and conditionals, and Bayes Theorem",
    "text": "Marginals and conditionals, and Bayes Theorem\nThe diagram below taken from Bishop may be used to illustrate the concepts of conditionals and marginals. Consider two random variables, \\(X\\), which takes the values \\({x_i}\\) where \\(i = 1,...,M\\), and \\(Y\\), which takes the values \\({y_j}\\) where \\(j = 1,...,L\\). The number of instances for which \\(X = x_i\\) and \\(Y = y_j\\) is \\(n_{ij}\\). The number of points in column i where \\(X=x_i\\) is \\(c_i\\), and for the row where \\(Y = y_j\\) is \\(r_j\\).\n\n\n\nJoint probability table notation\n\n\nThen the joint probability of having \\(p(X = x_i, Y= y_j)\\) is in the asymptotic limit of large numbers in the frequency sense of probability \\(n_{ij}/N\\) where is the total number of instances. The \\(X\\) marginal, \\(p(X=x_i)\\) can be obtained by summing instances in all the cells in the i’th column:\n\\[p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)\\]\nLets consider next only those instances for which \\(X=x_i\\). This means that we are limiting our analysis to the ith row. Then, we write the conditional probability of \\(Y = y_j\\) given \\(X = x_i\\) as \\(p(Y = y_j \\mid X = x_i)\\). This is the asymptotic fraction of these instances where \\(Y = y_j\\) and is obtained by dividing the instances in the cell by those in the comumn as\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i}.\\]\nA little algebraic rearrangement gives:\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i} = \\frac{n_{ij}}{N} / \\frac{c_i}{N},\\]\nor:\n\\[p(Y = y_j \\mid X = x_i) \\times p(X=x_i) =  p(X=x_i, Y=y_j).\\]\nThis is the product rule of probability with conditionals involved.\nLet us simplify the notation by dropping the \\(X=\\) and \\(Y=\\).\nThen we can write the marginal probability of x as a sum over the joint distribution of x and y where we sum over all possibilities of y,\n\\[p(x) = \\sum_y p(x,y) \\].\nWe can rewrite a joint distribution as a product of a conditional and marginal probability,\n\\[ p(x,y) = p(y\\mid x) p(x) \\]\nThe product rule is applied repeatedly to give expressions for the joint probability involving more than two variables. For example, the joint distribution over three variables can be factorized into a product of conditional probabilities:\n\\[ p(x,y,z) = p(x|y,z) \\, p(y,z) = p(x |y,z) \\, p(y|z) p(z) \\]\n\nBayes rule\nObserve that\n\\[ p(x,y) = p(y\\mid x) p(x) = P(x\\mid y)p(y).\\]\nGiven the product rule one can derive the Bayes rule, which plays a central role in a lot of the things we will be talking:\n\\[ p(y\\mid x) = \\frac{p(x\\mid y) \\, p(y) }{p(x)} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x,y')} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x\\mid y')p(y')}\\]\n\n\nIndependence\nTwo variables are said to be independent if their joint distribution factorizes into a product of two marginal probabilities:\n\\[ p(x,y) = p(x) \\, p(y) \\]\nAnother consequence of independence is that if \\(x\\) and \\(y\\) are independent, the conditional probability of \\(x\\) given \\(y\\) is just the probability of \\(x\\):\n\\[ p(x|y) = p(x) \\]\nIn other words, by conditioning on a particular \\(y\\), we have learned nothing about \\(x\\) because of independence. Two variables \\(x\\) and \\(y\\) and said to be conditionally independent of \\(z\\) if the following holds:\n\\[ p(x,y|z) = p(x|z) p(y|z) \\]\nTherefore, if we learn about z, x and y become independent. Another way to write that \\(x\\) and \\(y\\) are conditionally independent of \\(z\\) is\n\\[ p(x| z, y) = p(x|z) \\]\nIn other words, if we condition on \\(z\\), and now also learn about \\(y\\), this is not going to change the probability of \\(x\\). It is important to realize that conditional independence between \\(x\\) and \\(y\\) does not imply independence between \\(x\\) and \\(y\\).\n\n\nApplication of Bayes Theorem\n\nSally Clark, a lawyer who lost her first son at 11 weeks and her second at 8 weeks, was convicted in 1999. A prominent pediatrician, Sir Roy Meadow, had testified for the prosecution about Sudden Infant Death Syndrome, known as SIDS in the U.S. and cot death in Britain. Citing a government study, Meadow said the incidence of one SIDS death was one in 8,500 in a family like Clark’s–stable, affluent, nonsmoking, with a mother more than 26 years old.\n\n\nThen, despite the fact that some families are predisposed to SIDS, Meadow assumed erroneously that each sibling’s death occurred independently of the other. Multiplying 8,500 by 8,500, he calculated that the chance of two children dying in a family like Sally Clark’s was so rare–one in 73 million–that they must have been murdered.\n\n(from http://www.mcgrayne.com/disc.htm)\np(child 1 dying of sids) = 1/8500\n\nP(child 2 dying of sids) = 1/100\n\nFirst, we look at natural causes of sudden infant death. The chance of one random infant dying from SIDS was about 1 in 1,300 during this period in Britain. Meadow’s argument was flawed and produced a much slimmer chance of natural death. The estimated odds of a second SIDS death in the same family was much larger, perhaps one in 100, because family members can share a common environmental or genetic propensity for SIDS.\n\n\nSecond, we turn to the hypothesis that the babies were murdered. Only about 30 children out of 650,000 annual births in England, Scotland, and Wales were known to have been murdered by their mothers. The number of double murders must be much lower, estimated as 10 times less likely.\n\np(S2 = both children dying of sids) =  0.000007\np(notS2 = not both dying of sids) =  0.999993\n\nData: both children died unexpectedly\nSo now ask, whats:\np(data | S2) = 1\np(data | notS2) = ? both died but not SIDS. Murder? =  30/650000    × 1/10 = 0.000005\nWe want to calculate the “posterior probability”:\np(S2 | data) = P(data | S2) P(S2) /(P(data | S2) P(S2) + P(data|notS2)P(notS2))\n= 1*0.000007/(1*0.000007 + 0.000005*0.999993)\n=0.58\n58% chance of having died from SIDS!\nSally Clark spent 3 years in jail."
  },
  {
    "objectID": "posts/normalreg/index.html",
    "href": "posts/normalreg/index.html",
    "title": "From the Normal Model to Regression",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nThe example we use here is described in McElreath’s book, and our discussion mostly follows the one there, in sections 4.3 and 4.4."
  },
  {
    "objectID": "posts/normalreg/index.html#howells-data",
    "href": "posts/normalreg/index.html#howells-data",
    "title": "From the Normal Model to Regression",
    "section": "Howell’s data",
    "text": "Howell’s data\nThese are census data for the Dobe area !Kung San (https://en.wikipedia.org/wiki/%C7%83Kung_people). Nancy Howell conducted detailed quantitative studies of this Kalahari foraging population in the 1960s.\n\ndf = pd.read_csv('assets/Howell1.csv', sep=';', header=0)\ndf.head()\n\n\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n0\n151.765\n47.825606\n63.0\n1\n\n\n1\n139.700\n36.485807\n63.0\n0\n\n\n2\n136.525\n31.864838\n65.0\n0\n\n\n3\n156.845\n53.041915\n41.0\n1\n\n\n4\n145.415\n41.276872\n51.0\n0\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n539\n145.415\n31.127751\n17.0\n1\n\n\n540\n162.560\n52.163080\n31.0\n1\n\n\n541\n156.210\n54.062496\n21.0\n0\n\n\n542\n71.120\n8.051258\n0.0\n1\n\n\n543\n158.750\n52.531624\n68.0\n1\n\n\n\n\n\n\n\n\nplt.hist(df.height, bins=30);\n\n\n\n\n\n\n\n\nWe get rid of the kids and only look at the heights of the adults.\n\ndf2 = df[df.age &gt;= 18]\nplt.hist(df2.height, bins=30);"
  },
  {
    "objectID": "posts/normalreg/index.html#model-for-heights",
    "href": "posts/normalreg/index.html#model-for-heights",
    "title": "From the Normal Model to Regression",
    "section": "Model for heights",
    "text": "Model for heights\nWe will now get relatively formal in specifying our models.\nWe will use a Normal model, \\(h \\sim N(\\mu, \\sigma)\\), and assume that the priors are independent. That is \\(p(\\mu, \\sigma) = p(\\mu \\vert \\sigma) p(\\sigma) = p(\\mu)p(\\sigma)\\).\nOur model is:\n\\[\nh \\sim N(\\mu, \\sigma)\\\\\n\\mu \\sim Normal(148, 20)\\\\\n\\sigma = Std. dev.\n\\]\n\nfrom scipy.stats import norm\nY = df2.height.values\n#Data Quantities\nsig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\nmu_data = np.mean(Y)\nn = len(Y)\nprint(\"sigma\", sig, \"mu\", mu_data, \"n\", n)\n\nsigma 7.73132668454 mu 154.597092614 n 352\n\n\n\nplt.hist(Y, bins=30, alpha=0.5);\n\n\n\n\n\n\n\n\n\n# Prior mean\nmu_prior = 148\n# prior std\ntau = 20\n\n\nkappa = sig**2 / tau**2\nsig_post =np.sqrt(1./( 1./tau**2 + n/sig**2));\n# posterior mean\nmu_post = kappa / (kappa + n) *mu_prior + n/(kappa+n)* mu_data\nprint(\"mu post\", mu_post, \"sig_post\", sig_post)\n\nmu post 154.594293158 sig_post 0.41199365493\n\n\n\n#samples\nN = 15000\ntheta_prior = np.random.normal(loc=mu_prior, scale=tau, size=N);\ntheta_post = np.random.normal(loc=mu_post, scale=sig_post, size=N);\n\n\nplt.hist(theta_post, bins=30, alpha=0.9, label=\"posterior\");\nplt.hist(theta_prior, bins=30, alpha=0.2, label=\"prior\");\n#plt.xlim([10, 30])\nplt.legend();\n\n\n\n\n\n\n\n\n\nY_postpred = np.random.normal(loc=mu_post, scale=np.sqrt(sig_post**2 + sig**2), size=N);\n\n\nY_postpred_sample = np.random.normal(loc=theta_post, scale=sig);\n\n\nplt.hist(Y_postpred, bins=100, alpha=0.2);\nplt.hist(Y_postpred_sample, bins=100, alpha=0.2);\nplt.hist(np.random.choice(Y, replace=True, size=N), bins=100, alpha=0.5);"
  },
  {
    "objectID": "posts/normalreg/index.html#regression-adding-a-predictor",
    "href": "posts/normalreg/index.html#regression-adding-a-predictor",
    "title": "From the Normal Model to Regression",
    "section": "Regression: adding a predictor",
    "text": "Regression: adding a predictor\n\nplt.plot(df2.height, df2.weight, '.');\n\n\n\n\n\n\n\n\nSo lets write our model out now:\n\\[\nh \\sim N(\\mu, \\sigma)\\\\\n\\mu = intercept + slope \\times weight\\\\\nintercept \\sim N(150, 100)\\\\\nslope \\sim N(0, 10)\\\\\n\\sigma = std. dev,\n\\]\nWhy should you not use a uniform prior on a slope?\n\nminweight = df2.weight.min()\nmaxweight = df2.weight.max()\nminheight = df2.height.min()\nmaxheight = df2.height.max()\n\n\nfrom scipy.stats import norm\nfrom scipy.stats import multivariate_normal\ndef cplot(f, ax=None, lims=None):\n    if not ax:\n        plt.figure()\n        ax=plt.gca()\n    if lims:\n        xx,yy=np.mgrid[lims[0]:lims[1]:lims[2], lims[3]:lims[4]:lims[5]]\n    else:\n        xx,yy=np.mgrid[0:300:1,-15:15:.1]\n    pos = np.empty(xx.shape + (2,))\n    pos[:, :, 0] = xx\n    pos[:, :, 1] = yy\n    ax.contourf(xx, yy, f(pos))\n    #data = [x, y]\n    return ax\ndef plotSampleLines(mu, sigma, numberOfLines, dataPoints=None, ax=None):\n    #Plot the specified number of lines of the form y = w0 + w1*x in [-1,1]x[-1,1] by\n    # drawing w0, w1 from a bivariate normal distribution with specified values\n    # for mu = mean and sigma = covariance Matrix. Also plot the data points as\n    # blue circles. \n    #print \"datap\",dataPoints\n    if not ax:\n        plt.figure()\n        ax=plt.gca()\n    for i in range(numberOfLines):\n        w = np.random.multivariate_normal(mu,sigma)\n        func = lambda x: w[0] + w[1]*x\n        xx=np.array([minweight, maxweight])\n        ax.plot(xx,func(xx),'r', alpha=0.05)\n    if dataPoints:\n        ax.scatter(dataPoints[0],dataPoints[1], alpha=0.4, s=10)\n    #ax.set_xlim([minweight,maxweight])\n    #ax.set_ylim([minheight,maxheight])\n\n\n\npriorMean = np.array([150, 0])\npriorPrecision=2.0\npriorCovariance = np.array([[100*100, 0],[0, 10*10]])\npriorPDF = lambda w: multivariate_normal.pdf(w,mean=priorMean,cov=priorCovariance)\npriorPDF([1,2])\n\n5.1409768989960456e-05\n\n\n\ncplot(priorPDF);\n\n\n\n\n\n\n\n\n\nplotSampleLines(priorMean,priorCovariance,50)\n\n\n\n\n\n\n\n\n\nlikelihoodPrecision = 1./(sig*sig)\n\n\nPosterior\nWe can now continue with the standard Bayesian formalism\n\\[\n\\begin{eqnarray}\np(\\bf w| \\bf y,X) &\\propto& p(\\bf y | X, \\bf w) \\, p(\\bf w) \\nonumber \\\\\n                       &\\propto& \\exp{ \\left(- \\frac{1}{2 \\sigma_n^2}(\\bf y-X^T \\bf w)^T(\\bf y - X^T \\bf w) \\right)}\n                        \\exp{\\left( -\\frac{1}{2} \\bf w^T \\Sigma^{-1} \\bf w \\right)}  \\nonumber \\\\\n\\end{eqnarray}\n\\]\nIn the next step we `complete the square’ and obtain\n\\[\\begin{equation}\np(\\bf w| \\bf y,X)  \\propto  \\exp \\left( -\\frac{1}{2} (\\bf w - \\bar{\\bf w})^T  (\\frac{1}{\\sigma_n^2} X X^T + \\Sigma^{-1})(\\bf w - \\bar{\\bf w} )  \\right)\n\\end{equation}\\]\nThis is a Gaussian with inverse-covariance\n\\[A= \\sigma_n^{-2}XX^T +\\Sigma^{-1}\\]\nwhere the new mean is\n\\[\\bar{\\bf w} = A^{-1}\\Sigma^{-1}{\\bf w_0} + \\sigma_n^{-2}( A^{-1} X^T \\bf y )\\]\nTo make predictions for a test case we average over all possible parameter predictive distribution values, weighted by their posterior probability. This is in contrast to non Bayesian schemes, where a single parameter is typically chosen by some criterion.\n\n# Given the mean = priorMu and covarianceMatrix = priorSigma of a prior\n# Gaussian distribution over regression parameters; observed data, x\n# and y; and the likelihood precision, generate the posterior\n# distribution, postW via Bayesian updating and return the updated values\n# for mu and sigma. xtrain is a design matrix whose first column is the all\n# ones vector.\ndef update(x,y,likelihoodPrecision,priorMu,priorCovariance): \n    postCovInv  = np.linalg.inv(priorCovariance) + likelihoodPrecision*np.dot(x.T,x)\n    postCovariance = np.linalg.inv(postCovInv)\n    postMu = np.dot(np.dot(postCovariance,np.linalg.inv(priorCovariance)),priorMu) + likelihoodPrecision*np.dot(postCovariance,np.dot(x.T,y))\n    postW = lambda w: multivariate_normal.pdf(w,postMu,postCovariance)\n    return postW, postMu, postCovariance\n\n\ndesign = np.concatenate([np.ones(n).reshape(-1,1), df2.weight.values.reshape(-1,1)], axis=1)\nresponse = df2.height.values\n\n\n# For each iteration plot  the\n# posterior over the first i data points and sample lines whose\n# parameters are drawn from the corresponding posterior. \nfig, axes=plt.subplots(figsize=(12,6), nrows=1, ncols=2);\nmu = priorMean\ncov = priorCovariance\npostW,mu,cov = update(design,response,likelihoodPrecision,mu,cov)\ncplot(postW, axes[0], lims=[107, 122, 0.1, 0.7, 1.1, 0.01])\nplotSampleLines(mu, cov,50, (df2.weight.values,df2.height.values), axes[1])\n\n\n\n\n\n\n\n\n\n\nLets get the posteriors “at each point”\n\nweightgrid = np.arange(-20, 100)\ntest_design = np.concatenate([np.ones(len(weightgrid)).reshape(-1,1), weightgrid.reshape(-1,1)], axis=1)\n\n\nw = np.random.multivariate_normal(mu,cov, 1000) #1000 samples\nw[:,0].shape\n\n(1000,)\n\n\n\nsns.distplot(w[:,0] + w[:,1] * 55) # the weight=55 posterior\n\n\n\n\n\n\n\n\n\nmu_pred = np.zeros((len(weightgrid), 1000))\nfor i, weight in enumerate(weightgrid):\n    mu_pred[i, :] = w[:,0] + w[:,1] * weight\n\npost_means = np.mean(mu_pred, axis=1)\npost_stds = np.std(mu_pred, axis=1)\n\n(120,)\n\n\n\nwith sns.plotting_context('poster'):\n    plt.scatter(df2.weight, df2.height, c='b', alpha=0.9, s=10)\n    plt.plot(weightgrid, post_means, 'r')\n    #plt.fill_between(weightgrid, mu_hpd[:,0], mu_hpd[:,1], color='r', alpha=0.5)\n    plt.fill_between(weightgrid, post_means - 1.96*post_stds, ppmeans + 1.96*post_stds, color='red', alpha=0.4)\n\n\n    plt.xlabel('weight')\n    plt.ylabel('height')\n\n\n\n\n\n\n\n\n\nOops, what happened here? Our correlations in parameters are huge! But the regression lines do make some sense. Lets look at the posterior predictive.\n\n\nPosterior Predictive Distribution\nThus the predictive distribution at some \\(x^{*}\\) is given by averaging the output of all possible linear models w.r.t. the posterior\n\\[\n\\begin{eqnarray}\np(y^{*} | x^{*}, {\\bf x,y}) &=& \\int p({\\bf y}^{*}| {\\bf x}^{*}, {\\bf w} ) p(\\bf w| X, y)dw \\nonumber \\\\\n                                    &=& {\\cal N} \\left(y \\vert \\bar{\\bf w}^{T}x^{*}, \\sigma_n^2 + x^{*^T}A^{-1}x^{*} \\right),\n\\end{eqnarray}\n\\]\nwhich is again Gaussian, with a mean given by the posterior mean multiplied by the test input and the variance is a quadratic form of the test input with the posterior covariance matrix, showing that the predictive uncertainties grow with the magnitude of the test input, as one would expect for a linear model.\n\nppmeans = np.empty(len(weightgrid))\nppsigs = np.empty(len(weightgrid))\nt2 = np.empty(len(weightgrid))\n\n\n\nfor i, tp in enumerate(test_design):\n    ppmeans[i] = mu @ tp\n    ppsigs[i] = np.sqrt(sig*sig + tp@cov@tp)\n    t2[i] = np.sqrt(tp@cov@tp)\n\n\nweightgrid[75]\n\n55\n\n\n\nplt.hist(w[:,0] + w[:,1] * 55, alpha=0.8)\nplt.hist(norm.rvs(ppmeans[75], ppsigs[75], 1000), alpha=0.5)\n\n(array([   6.,   18.,   62.,  125.,  263.,  239.,  176.,   92.,   16.,    3.]),\n array([ 136.32163732,  141.63972751,  146.9578177 ,  152.27590789,\n         157.59399808,  162.91208827,  168.23017846,  173.54826866,\n         178.86635885,  184.18444904,  189.50253923]),\n &lt;a list of 10 Patch objects&gt;)\n\n\n\n\n\n\n\n\n\n\nwith sns.plotting_context('poster'):\n    plt.scatter(df2.weight, df2.height, c='b', alpha=0.9, s=10)\n    plt.plot(weightgrid, ppmeans, 'r')\n    #plt.fill_between(weightgrid, mu_hpd[:,0], mu_hpd[:,1], color='r', alpha=0.5)\n    plt.fill_between(weightgrid, ppmeans - 1.96*ppsigs, ppmeans + 1.96*ppsigs, color='green', alpha=0.4)\n\n\n    plt.xlabel('weight')\n    plt.ylabel('height')\n\n\n\n\n\n\n\n\nHowever, by including the \\(\\mu\\) as a deterministic in our traces we only get to see the traces at existing data points. If we want the traces on a grid of weights, we’ll have to explivitly plug in the intercept and slope traces in the regression formula"
  },
  {
    "objectID": "posts/distributions.html",
    "href": "posts/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Remember that a Random Variable is a mapping $ X: $ that assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\) in a sample space \\(\\Omega\\). The definitions below are taken from Larry Wasserman’s All of Statistics."
  },
  {
    "objectID": "posts/distributions.html#cumulative-distribution-function",
    "href": "posts/distributions.html#cumulative-distribution-function",
    "title": "Distributions",
    "section": "Cumulative distribution Function",
    "text": "Cumulative distribution Function\nThe cumulative distribution function, or the CDF, is a function\n\\[F_X : \\mathbb{R} → [0, 1] \\],\ndefined by\n\\[F_X (x) = p(X \\le x).\\]\nA note on notation: \\(X\\) is a random variable while \\(x\\) is a particular value of the random variable.\nLet \\(X\\) be the random variable representing the number of heads in two coin tosses. Then \\(x\\) can take on values 0, 1 and 2. The CDF for this random variable can be drawn thus (taken from All of Stats):\n\n\n\nCDF of the 2-coin-toss distribution\n\n\nNotice that this function is right-continuous and defined for all \\(x\\), even if $x $does not take real values in-between the integers."
  },
  {
    "objectID": "posts/distributions.html#probability-mass-and-distribution-function",
    "href": "posts/distributions.html#probability-mass-and-distribution-function",
    "title": "Distributions",
    "section": "Probability Mass and Distribution Function",
    "text": "Probability Mass and Distribution Function\n\\(X\\) is called a discrete random variable if it takes countably many values \\(\\{x_1, x_2,…\\}\\). We define the probability function or the probability mass function (pmf) for X by:\n\\[f_X(x) = p(X=x)\\]\n\\(f_X\\) is a probability.\nThe pmf for the number of heads in two coin tosses (taken from All of Stats) looks like this:\n\n\n\nPMF of the 2-coin-toss distribution\n\n\nOn the other hand, a random variable is called a continuous random variable if there exists a function \\(f_X\\) such that \\(f_X (x) \\ge 0\\) for all x, \\(\\int_{-\\infty}^{\\infty} f_X (x) dx = 1\\) and for every a ≤ b,\n\\[p(a &lt; X &lt; b) = \\int_{a}^{b} f_X (x) dx\\]\nThe function \\(f_X\\) is called the probability density function (pdf). We have the CDF:\n\\[F_X (x) = \\int_{-\\infty}^{x}f_X (t) dt \\]\nand \\(f_X (x) = \\frac{d F_X (x)}{dx}\\) at all points x at which \\(F_X\\) is differentiable.\nContinuous variables are confusing. Note:\n\n\\(p(X=x) = 0\\) for every \\(x\\). You cant think of \\(f_X(x)\\) as \\(p(X=x)\\). This holds only for discretes. You can only get probabilities from a pdf by integrating, if only over a very small paty of the space.\nA pdf can be bigger than 1 unlike a probability mass function, since probability masses represent actual probabilities.\n\n\nA continuous example: the Uniform Distribution\nSuppose that X has pdf \\[\nf_X (x) =\n\\begin{cases}\n1 & \\text{for } 0 \\leq x\\leq 1\\\\\n    0             & \\text{otherwise.}\n\\end{cases}\n\\] A random variable with this density is said to have a Uniform (0,1) distribution. This is meant to capture the idea of choosing a point at random between 0 and 1. The cdf is given by: \\[\nF_X (x) =\n\\begin{cases}\n0 & x \\le 0\\\\\nx & 0 \\leq x \\leq 1\\\\\n1 & x &gt; 1.\n\\end{cases}\n\\] and can be visualized as so (again from All of Stats):\n\n\n\nCDF of the uniform distribution\n\n\n\n\nA discrete example: the Bernoulli Distribution\nThe Bernoulli Distribution represents the distribution a coin flip. Let the random variable \\(X\\) represent such a coin flip, where \\(X=1\\) is heads, and \\(X=0\\) is tails. Let us further say that the probability of heads is \\(p\\) (\\(p=0.5\\) is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p)\\]\nwhich is to be read as \\(X\\) has distribution \\(Bernoulli(p)\\). The pmf or probability function associated with the Bernoulli distribution is \\[\nf(x) =\n\\begin{cases}\n1 - p & x = 0\\\\\np & x = 1.\n\\end{cases}\n\\]\nfor p in the range 0 to 1. This pmf may be written as\n\\[f(x) = p^x (1-p)^{1-x}\\]\nfor x in the set {0,1}.\n\\(p\\) is called a parameter of the Bernoulli distribution."
  },
  {
    "objectID": "posts/distributions.html#conditional-and-marginal-distributions",
    "href": "posts/distributions.html#conditional-and-marginal-distributions",
    "title": "Distributions",
    "section": "Conditional and Marginal Distributions",
    "text": "Conditional and Marginal Distributions\nMarginal mass functions are defined in analog to probabilities. Thus:\n\\[f_X(x) = p(X=x) =  \\sum_y f(x, y);\\,\\, f_Y(y) = p(Y=y) = \\sum_x f(x,y).\\]\nSimilarly, marginal densities are defined using integrals:\n\\[f_X(x) = \\int dy f(x,y);\\,\\, f_Y(y) = \\int dx f(x,y).\\]\nNotice there is no interpretation of the marginal densities in the continuous case as probabilities. An example here if \\(f(x,y) = e^{-(x+y)}\\) defined on the positive quadrant. The marginal is an exponential defined on the positive part of the line.\nConditional mass function is similarly, just a conditional probability. So:\n\\[f_{X \\mid Y}(x \\mid y) = p(X=x \\mid Y=y) = \\frac{p(X=x, Y=y)}{p(Y=y)} = \\frac{f_{XY}(x,y)}{f_Y(y)}\\]\nThe similar formula for continuous densities might be suspected to a bit more complex, because we are conditioning on the event \\(Y=y\\) which strictly speaking has 0 probability. But it can be proved that the same formula holds for densities with some additional requirements and interpretation:\n\\[f_{X \\mid Y}(x \\mid y)  = \\frac{f_{XY}(x,y)}{f_Y(y)},\\]\nwhere we must assume that \\(f_Y(y) &gt; 0\\). Then we have the interpretation that for some event A:\n\\[p(X \\in A \\mid Y=y) = \\int_{x \\in A} f_{X \\mid Y}(x,y) dx.\\]\nAn example of this is the uniform distribution on the unit square. Suppose then that \\(y=0.3\\). Then the conditional density is a uniform density on the line between 0 and 1 at \\(y=0.3\\)."
  },
  {
    "objectID": "posts/distrib-example/index.html",
    "href": "posts/distrib-example/index.html",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\nIn the last section, we made a simple simulation of a coin-toss on the computer from a fair-coin model which associated equal probability with heads and tails. Let us consider another model here, a table of probabilities that PredictWise made on October 2, 2012 for the US presidential elections. PredictWise aggregated polling data and, for each state, estimated the probability that the Obama or Romney would win. Here are those estimated probabilities:\npredictwise = pd.read_csv('assets/predictwise.csv').set_index('States')\npredictwise.head()\n\n\n\n\n\n\n\nObama\nRomney\nVotes\n\n\nStates\n\n\n\n\n\n\n\nAlabama\n0.000\n1.000\n9\n\n\nAlaska\n0.000\n1.000\n3\n\n\nArizona\n0.062\n0.938\n11\n\n\nArkansas\n0.000\n1.000\n6\n\n\nCalifornia\n1.000\n0.000\n55\nSay you toss a coin and have a model which says that the probability of heads is 0.5 (you have figured this out from symmetry, or physics, or something). Still, there will be sequences of flips in which more or less than half the flips are heads. These fluctuations induce a distribution on the number of heads (say k) in N coin tosses (this is a binomial distribution).\nSimilarly, here, if the probability of Romney winning in Arizona is 0.938, it means that if somehow, there were 10000 replications (as if we were running the election in 10000 parallel universes) with an election each, Romney would win in 9380 of those Arizonas on the average across the replications. And there would be some replications with Romney winning more, and some with less. We can run these simulated universes or replications on a computer though not in real life."
  },
  {
    "objectID": "posts/distrib-example/index.html#simulating-a-simple-election-model",
    "href": "posts/distrib-example/index.html#simulating-a-simple-election-model",
    "title": "Distributions Example: Elections",
    "section": "Simulating a simple election model",
    "text": "Simulating a simple election model\nTo do this, we will assume that the outcome in each state is the result of an independent coin flip whose probability of coming up Obama is given by the Predictwise state-wise win probabilities. Lets write a function simulate_election that uses this predictive model to simulate the outcome of the election given a table of probabilities.\n\nBernoulli Random Variables (in scipy.stats)\nThe Bernoulli Distribution represents the distribution for coin flips. Let the random variable X represent such a coin flip, where X=1 is heads, and X=0 is tails. Let us further say that the probability of heads is p (p=0.5 is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p),\\]\nwhich is to be read as X has distribution Bernoulli(p). The probability distribution function (pdf) or probability mass function associated with the Bernoulli distribution is\n\\[\\begin{eqnarray}\nP(X = 1) &=& p \\\\\nP(X = 0) &=& 1 - p\n\\end{eqnarray}\\]\nfor p in the range 0 to 1. The pdf, or the probability that random variable \\(X=x\\) may thus be written as\n\\[P(X=x) = p^x(1-p)^{1-x}\\]\nfor x in the set {0,1}.\nThe Predictwise probability of Obama winning in each state is a Bernoulli Parameter. You can think of it as a different loaded coin being tossed in each state, and thus there is a bernoulli distribution for each state\nNote: some of the code, and ALL of the visual style for the distribution plots below was shamelessly stolen from https://gist.github.com/mattions/6113437/ .\n\nfrom scipy.stats import bernoulli\n#bernoulli random variable\nbrv=bernoulli(p=0.3)\nprint(brv.rvs(size=20))\nevent_space=[0,1]\nplt.figure(figsize=(12,8))\ncolors=sns.color_palette()\nfor i, p in enumerate([0.1, 0.2, 0.5, 0.7]):\n    ax = plt.subplot(1, 4, i+1)\n    plt.bar(event_space, bernoulli.pmf(event_space, p), label=p, color=colors[i], alpha=0.5)\n    plt.plot(event_space, bernoulli.cdf(event_space, p), color=colors[i], alpha=0.5)\n\n    ax.xaxis.set_ticks(event_space)\n   \n    plt.ylim((0,1))\n    plt.legend(loc=0)\n    if i == 0:\n        plt.ylabel(\"PDF at $k$\")\nplt.tight_layout()\n\n[1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0]\n\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\n\n\nRunning the simulation using the Uniform distribution\nIn the code below, each column simulates a single outcome from the 50 states + DC by choosing a random number between 0 and 1. Obama wins that simulation if the random number is \\(&lt;\\) the win probability. If he wins that simulation, we add in the electoral votes for that state, otherwise we dont. We do this n_sim times and return a list of total Obama electoral votes in each simulation.\n\ndef simulate_election(model, n_sim):\n    simulations = np.random.uniform(size=(51, n_sim))\n    obama_votes = (simulations &lt; model.Obama.values.reshape(-1, 1)) * model.Votes.values.reshape(-1, 1)\n    #summing over rows gives the total electoral votes for each simulation\n    return obama_votes.sum(axis=0)\n\nThe first thing to pick up on here is that np.random.uniform gives you a random number between 0 and 1, uniformly. In other words, the number is equally likely to be between 0 and 0.1, 0.1 and 0.2, and so on. This is a very intuitive idea, but it is formalized by the notion of the Uniform Distribution.\nWe then say:\n\\[X \\sim Uniform([0,1),\\]\nwhich is to be read as X has distribution Uniform([0,1]). The probability distribution function (pdf) associated with the Uniform distribution is\n\\[\\begin{eqnarray}\nP(X = x) &=& 1 \\, for \\, x \\in [0,1] \\\\\nP(X = x) &=& 0 \\, for \\, x \\notin [0,1]\n\\end{eqnarray}\\]\nWhat assigning the vote to Obama when the random variable drawn from the Uniform distribution is less than the Predictwise probability of Obama winning (which is a Bernoulli Parameter) does for us is this: if we have a large number of simulations and \\(p_{Obama}=0.7\\) , then 70% of the time, the random numbes drawn will be below 0.7. And then, assigning those as Obama wins will hew to the frequentist notion of probability of the Obama win. But remember, of course, that in 30% of the simulations, Obama wont win, and this will induce fluctuations and a distribution on the total number of electoral college votes that Obama gets. And this is what we see in the histogram below.\nThe following code takes the necessary probabilities for the Predictwise data, and runs 10000 simulations. If you think of this in terms of our coins, think of it as having 51 biased coins, one for each state, and tossing them 10,000 times each.\nWe use the results to compute the number of simulations, according to this predictive model, that Obama wins the election (i.e., the probability that he receives 269 or more electoral college votes)\n\nresult = simulate_election(predictwise, 10000)\nprint((result &gt;= 269).sum())\n\n9955\n\n\n\nresult\n\narray([303, 326, 329, ..., 332, 281, 324])\n\n\nThere are roughly only 50 simulations in which Romney wins the election!"
  },
  {
    "objectID": "posts/distrib-example/index.html#displaying-the-prediction",
    "href": "posts/distrib-example/index.html#displaying-the-prediction",
    "title": "Distributions Example: Elections",
    "section": "Displaying the prediction",
    "text": "Displaying the prediction\nNow, lets visualize the simulation. We will build a histogram from the result of simulate_election. We will normalize the histogram by dividing the frequency of a vote tally by the number of simulations. We’ll overplot the “victory threshold” of 269 votes as a vertical black line and the result (Obama winning 332 votes) as a vertical red line.\nWe also compute the number of votes at the 5th and 95th quantiles, which we call the spread, and display it (this is an estimate of the outcome’s uncertainty). By 5th quantile we mean that if we ordered the number of votes Obama gets in each simulation in increasing order, the 5th quantile is the number below which 5% of the simulations lie.\nWe also display the probability of an Obama victory\n\ndef plot_simulation(simulation):    \n    plt.hist(simulation, bins=np.arange(200, 538, 1), \n             label='simulations', align='left', normed=True)\n    plt.axvline(332, 0, .5, color='r', label='Actual Outcome')\n    plt.axvline(269, 0, .5, color='k', label='Victory Threshold')\n    p05 = np.percentile(simulation, 5.)\n    p95 = np.percentile(simulation, 95.)\n    iq = int(p95 - p05)\n    pwin = ((simulation &gt;= 269).mean() * 100)\n    plt.title(\"Chance of Obama Victory: %0.2f%%, Spread: %d votes\" % (pwin, iq))\n    plt.legend(frameon=False, loc='upper left')\n    plt.xlabel(\"Obama Electoral College Votes\")\n    plt.ylabel(\"Probability\")\n    sns.despine()\n\n\nplot_simulation(result)\n\n\n\n\n\n\n\n\nThe model created by combining the probabilities we obtained from Predictwise with the simulation of a biased coin flip corresponding to the win probability in each states leads us to obtain a histogram of election outcomes. We are plotting the probabilities of a prediction, so we call this distribution over outcomes the predictive distribution. Simulating from our model and plotting a histogram allows us to visualize this predictive distribution. In general, such a set of probabilities is called a probability mass function."
  },
  {
    "objectID": "posts/distrib-example/index.html#empirical-distribution",
    "href": "posts/distrib-example/index.html#empirical-distribution",
    "title": "Distributions Example: Elections",
    "section": "Empirical Distribution",
    "text": "Empirical Distribution\nThis is an empirical Probability Mass Function.\nLets summarize: the way the mass function arose here that we did ran 10,000 tosses (for each state), and depending on the value, assigned the state to Obama or Romney, and then summed up the electoral votes over the states.\nThere is a second, very useful question, we can ask of any such probability mass or probability density: what is the probability that a random variable is less than some value. In other words: \\(P(X &lt; x)\\). This is also a probability distribution and is called the Cumulative Distribution Function, or CDF (sometimes just called the distribution, as opposed to the density, or mass function). Its obtained by “summing” the probability density function for all \\(X\\) less than \\(x\\).\n\nCDF = lambda x: np.float(np.sum(result &lt; x))/result.shape[0]\nfor votes in [200, 300, 320, 340, 360, 400, 500]:\n    print(\"Obama Win CDF at votes=\", votes, \" is \", CDF(votes))\n\nObama Win CDF at votes= 200  is  0.0\nObama Win CDF at votes= 300  is  0.1447\nObama Win CDF at votes= 320  is  0.4439\nObama Win CDF at votes= 340  is  0.839\nObama Win CDF at votes= 360  is  0.9979\nObama Win CDF at votes= 400  is  1.0\nObama Win CDF at votes= 500  is  1.0\n\n\n\nvotelist=np.arange(0, 540, 5)\nplt.plot(votelist, [CDF(v) for v in votelist], '.-');\nplt.xlim([200,400])\nplt.ylim([-0.1,1.1])\nplt.xlabel(\"votes for Obama\")\nplt.ylabel(\"probability of Obama win\");"
  },
  {
    "objectID": "posts/distrib-example/index.html#binomial-distribution",
    "href": "posts/distrib-example/index.html#binomial-distribution",
    "title": "Distributions Example: Elections",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nLet us consider a population of coinflips, n of them to be precise, \\(x_1,x_2,...,x_n\\). The distribution of coin flips is the binomial distribution. By this we mean that each coin flip represents a bernoulli random variable (or comes from a bernoulli distribution) with \\(p=0.5\\).\nAt this point, you might want to ask the question, what is the probability of obtaining \\(k\\) heads in \\(n\\) flips of the coin. We have seen this before, when we flipped 2 coins. What happens when when we flip 3?\n(This diagram is taken from the Feynman Lectures on Physics, volume 1. The chapter on probability is http://www.feynmanlectures.caltech.edu/I_06.html) \nWe draw a possibilities diagram like we did with the 2 coin flips, and see that there are different probabilities associated with the events of 0, 1,2, and 3 heads with 1 and 2 heads being the most likely. The probability of each of these events is given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k; n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\n\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\n\n\n\n\n\n\nApplying the CLT to elections: Binomial distribution in the large n, large k limit\nConsider the binomial distribution Binomial(n,k, p) in the limit of large n. The number of successes k in n trials can be regarded as the sum of n IID Bernoulli variables with values 1 or 0. Call these \\(x_i\\).\nThen:\n\\[S_n = \\frac{1}{n} \\sum_i x_i .\\]\nThe CLT tells us then that for large n, we have:\n\\[S_n \\sim N(p, \\frac{p(1-p)}{n}),\\]\nsince the mean of a Bernoulli is \\(p\\), and its variance \\(p*(1-p)\\).\nThis means that we can replace the binomial distribution at large n by a gaussian where k is now a continuous variable, and whose mean is the mean of the binomial \\(np\\) (\\(nS_n\\), since the binomial distribution is on the sum, not on the average) and whose variance is \\(np(1-p)\\).\nThe accuracy of this approximation depends on the variance. A large variance makes for a broad distribution spanning many discrete k, thus justifying the transition from a discrete to a continuous distribution.\nThis approximation is used a lot in studying elections. For example, suppose I told you that I’d polled 1000 people in Ohio and found that 600 would vote Democratic, and 400 republican. Imagine that this 1000 is a “sample” drawn from the voting “population” of Ohio. Assume then that these are 1000 independent bernoulli trials with p=600/1000 = 0.6. Then we can say that, from the CLT, the mean of the sampling distribution of the mean of the bernoulli or is 0.6 (equivalently the binomial’s mean is 600), with a variance of \\(0.6*0.4/1000 = 0.00024\\) (equivalently the binomials variance is 240). Thus the standard deviation is 0.015 for a mean of 0.6, or 1.5% on a mean of 60% voting Democratic. This 1.5% if part of what pollsters quote as the margin of error of a candidates winning; they often include other factors such as errors in polling methodology.\n\n\nGallup Party Affiliation Poll\nEarlier we had used the Predictwise probabilities from Octover 12th to create a predictive model for the elections. This time we will try to estimate our own win probabilities to plug into our predictive model.\nWe will start with a simple forecast model. We will try to predict the outcome of the election based the estimated proportion of people in each state who identify with one one political party or the other.\nGallup measures the political leaning of each state, based on asking random people which party they identify or affiliate with. Here’s the data they collected from January-June of 2012:\n\ngallup_2012=pd.read_csv(\"assets/g12.csv\").set_index('State')\ngallup_2012[\"Unknown\"] = 100 - gallup_2012.Democrat - gallup_2012.Republican\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\n\n\nState\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n\n\n\n\n\n\n\nEach row lists a state, the percent of surveyed individuals who identify as Democrat/Republican, the percent whose identification is unknown or who haven’t made an affiliation yet, the margin between Democrats and Republicans (Dem_Adv: the percentage identifying as Democrats minus the percentage identifying as Republicans), and the number N of people surveyed.\nThe most obvious source of error in the Gallup data is the finite sample size – Gallup did not poll everybody in America, and thus the party affilitions are subject to sampling errors. How much uncertainty does this introduce? Lets estimate the sampling error using the definition of the standard error (we use N-1 rather than N; see the sample error section in the page on the CLT).\n\ngallup_2012[\"SE_percentage\"]=100.0*np.sqrt((gallup_2012.Democrat/100.)*((100. - gallup_2012.Democrat)/100.)/(gallup_2012.N -1))\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\nSE_percentage\n\n\nState\n\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n0.849059\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n2.395543\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n0.744384\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n1.082971\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n0.392658\n\n\n\n\n\n\n\nOn their webpage discussing these data, Gallup notes that the sampling error for the states is between 3 and 6%, with it being 3% for most states. This is more than what we find, so lets go with what Gallup says.\nWe now use Gallup’s estimate of 3% to build a Gallup model with some uncertainty. We will, using the CLT, assume that the sampling distribution of the Obama win percentage is a gaussian with mean the democrat percentage and standard error the sampling error of 3%.\nWe’ll build the model in the function uncertain_gallup_model, and return a forecast where the probability of an Obama victory is given by the probability that a sample from the Dem_Adv Gaussian is positive.\nTo do this we simply need to find the area under the curve of a Gaussian that is on the positive side of the x-axis. The probability that a sample from a Gaussian with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) exceeds a threhold \\(z\\) can be found using the the Cumulative Distribution Function of a Gaussian:\n\\[\nCDF(z) = \\frac1{2}\\left(1 + {\\mathrm erf}\\left(\\frac{z - \\mu}{\\sqrt{2 \\sigma^2}}\\right)\\right)\n\\]\n\nfrom scipy.special import erf\ndef uncertain_gallup_model(gallup):\n    sigma = 3\n    prob =  .5 * (1 + erf(gallup.Dem_Adv / np.sqrt(2 * sigma**2)))\n    return pd.DataFrame(dict(Obama=prob), index=gallup.index)\n\n\nmodel = uncertain_gallup_model(gallup_2012)\nmodel = model.join(predictwise.Votes)\n\n\nprediction = simulate_election(model, 10000)\nplot_simulation(prediction)\n\n\n\n\n\n\n\n\n\n\nMultiple Pollsters\nIf one has results from multiple pollsters, one can now treat them as independent samples from the voting population. Now we use the CLT again. Then the average from these samples will approach the average in the population, with the sample means distributed normally around it. So we can average the averages of the samples to get the population mean, and estimate the variance around this population mean as well."
  },
  {
    "objectID": "posts/boxloop.html",
    "href": "posts/boxloop.html",
    "title": "Box’s Loop",
    "section": "",
    "text": "In the 1960’s, the great statistician Box, along with his collaborators, formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box’s loop by Blei et. al., 1, and illustrated in the diagram (taken from the above linked paper) below:\n1 Blei, David M. “Build, compute, critique, repeat: Data analysis with latent variable models.” Annual Review of Statistics and Its Application 1 (2014): 203-232.\n\n\nBox’s loop: Build, Infer, Criticize, Apply\n\n\nBox himself focussed on the scientific method, but the loop is applicable at large to other examples of probabilistic modelling, such as the building of an information retrieval or recommendation system, exploratory data analysis, etc, etc\nWe:\n\nfirst build a model. This is as much as an art as a science if we are of the philosophical bent that we desire explainability. We bring in domain experts.\nWe compute a model using the observed data.\nWe then critique our model, studying how they succeed or fail and how they predict future data or on held out sets.\nIf we are satisfied with the performance of our model we apply it in the context of a predictive or explanatory system. If we are not, we go back to 1.\n\nIf we are Bayesians, we compute the posterior distribution (the distribution of the parameters conditioned on the data) of the (hidden) parameters of the model. Here we assume that the data is fixed and our stochasticity is in the parameters.\nIf we are Frequentists, we assume our data is a sample from a population and compute the parameters of our models abd confidence intervals for those parameters. Here we assume that the data is stochastic as in we could get multiple different samplkes, but that the parameter is fixed and given.\nWe could have mis-specified our model. It might be too simple or too complex. If so we go back to (1) and try again with another model specification."
  },
  {
    "objectID": "posts/montecarlointegrals/index.html",
    "href": "posts/montecarlointegrals/index.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#the-basic-idea",
    "href": "posts/montecarlointegrals/index.html#the-basic-idea",
    "title": "Monte Carlo Integration",
    "section": "The basic idea",
    "text": "The basic idea\nLet us formalize the basic idea behind Monte Carlo Integration in 1-D.\nConsider the definite integral:\n\\[ I = \\int_{a}^{b} f(x) \\, dx \\]\nConsider:\n\\[ J = \\int_{a}^{b} f(x) U_{ab}(x) \\, dx \\]\nIf \\(V\\) is the support of the uniform distribution on a to b then the pdf \\[ U_{ab}(x) = \\frac{1}{V} = \\frac{1}{b-a}\\]\nThen from LOTUS and the law of large numbers:\n\\[J = \\frac{1}{V}  \\int_{a}^{b} f(x) \\, dx  =  \\frac{I}{V} = E_{U}[f] = \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nor\n\\[I  = V  \\times \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nPractically speaking, our estimate will only be as exact as the number of samples we draw, but more on this soon..\n\nExample.\n**Calculate the integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx. $**\nWe know from calculus that the anti-derivative is \\[ x^3/3 + 4\\sin(x) -4x\\cos(x). \\]\nTo solve this using MC, we simply draw \\(N\\) random numbers from 2 to 3 and then take the average of all the values \\(f(x)=x^2 + 4 \\, x \\,\\sin(x)\\) and normalized over the volume; this case the volume is 1 (3-2=1).\n\ndef f(x):\n    return x**2 + 4*x*np.sin(x) \n\ndef intf(x): \n    return x**3/3.0+4.0*np.sin(x) - 4.0*x*np.cos(x) \n\n\na = 2;    \nb = 3; \n\n# use N draws \nN= 10000\n\nX = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \nY =f(X)   # CALCULATE THE f(x) \nV = b-a\nImc= V * np.sum(Y)/ N;\n\nexactval=intf(b)-intf(a)\n\nprint(\"Monte Carlo estimation=\",Imc, \"Exact number=\", intf(b)-intf(a))\n\nMonte Carlo estimation= 11.8120823531 Exact number= 11.8113589251\n\n\n\n\nMutlidimensional integral.\nThat is nice but how about a multidimensional case?\nLet us calculate the two dimensional integral \\(I=\\int \\int f(x, y) dx dy\\) where \\(f(x,y) = x^2 +y^2\\) over the region deﬁned by the condition \\(x^2 +y^2 ≤ 1\\)\nIn other words we are talking about a uniform distribution on the unit circle\n\nfmd = lambda x,y: x*x + y*y\n\n\n# use N draws \nN= 8000\nX= np.random.uniform(low=-1, high=1, size=N) \nY= np.random.uniform(low=-1, high=1, size=N) \nZ=fmd(X, Y)   # CALCULATE THE f(x) \n\nR = X**2 + Y**2\nV = np.pi*1.0*1.0\nN = np.sum(R&lt;1)\nsumsamples = np.sum(Z[R&lt;1])\n\nprint(\"I=\",V*sumsamples/N, \"actual\", np.pi/2.0) #actual value (change to polar to calculate)\n\nI= 1.56308724855 actual 1.5707963267948966"
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#monte-carlo-as-a-function-of-number-of-samples",
    "href": "posts/montecarlointegrals/index.html#monte-carlo-as-a-function-of-number-of-samples",
    "title": "Monte Carlo Integration",
    "section": "Monte-Carlo as a function of number of samples",
    "text": "Monte-Carlo as a function of number of samples\nHow does the accuracy depends on the number of points(samples)? Lets try the same 1-D integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx $ as a function of the number of points.\n\nImc=np.zeros(1000)\nNa = np.linspace(0,1000,1000)\n\nexactval= intf(b)-intf(a)\n\nfor N in np.arange(0,1000):\n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[N]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.plot(Na[10:],np.sqrt((Imc[10:]-exactval)**2), alpha=0.7)\nplt.plot(Na[10:], 1/np.sqrt(Na[10:]), 'r')\nplt.xlabel(\"N\")\nplt.ylabel(\"sqrt((Imc-ExactValue)$^2$)\")\n\n# \n\n\n\n\n\n\n\n\nObviously this depends on the number of \\(N\\) as \\(1/\\sqrt{N}\\)."
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#errors-in-mc",
    "href": "posts/montecarlointegrals/index.html#errors-in-mc",
    "title": "Monte Carlo Integration",
    "section": "Errors in MC",
    "text": "Errors in MC\nMonte Carlo methods yield approximate answers whose accuracy depends on the number of draws. So far, we have used our knowledge of the exact value to determine that the error in the Monte Carlo method approaches zero as approximately \\(1/\\sqrt{N}\\) for large \\(N\\), where \\(N\\) is the number of trials.\nBut in the usual case, the exact answer is unknown. Why do this otherwise?\nSo, lets repeat the same evaluation \\(m\\) times and check the variance of the estimate.\n\n# multiple MC estimations\nm=1000\nN=10000\nImc=np.zeros(m)\n\n\nfor i in np.arange(m):\n    \n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[i]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.hist(Imc, bins=30)\nplt.xlabel(\"Imc\")\nprint(np.mean(Imc), np.std(Imc))\n\n11.8114651823 0.00398497853806\n\n\n\n\n\n\n\n\n\nThis looks like our telltale Normal distribution.\nThis is not surprising\n\nEstimating the error in MC integration using the CLT.\nWe know from the CLT that if \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\), and that if \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\).\nThen,\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nThis is true regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nThe sums\n\\[S_n(f) = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\]\nare exactly what we want to calculate for Monte-Carlo Integration(due to the LOTUS) and correspond to the random variable f(X) where X is uniformly distributed on the support.\nWhatever the original variance of f(X) might be, we can see that the variance of the sampling distribution of the mean goes down as \\(1/n\\) and thus the standard error goes down as \\(1/\\sqrt{n}\\) as we discovered when we compared it to the exact value as well.\nWhy is this important?\n\n\nComparing to standard integration techniques\nWhat if we changed the dimensionality of the integral? The formula for \\(S_n\\) does not change, we just replace \\(g(x_i)\\) by \\(g(x_i, y_i, z_i...)\\). Thus, the CLT still holds and the error still scales as \\(\\frac{1}{\\sqrt{n}}\\).\nOn the other hand, if we divide the \\(a, b\\)-interval into \\(N\\) steps and use some regular integration routine, what is the error? Consider the midpoint rule as illustrated in this diagram from Wikipedia:\n\n\n\nRectangle rule for numerical integration\n\n\nThe basic idea is that the function value at the midpoint of the interval is used as the height of the approximating rectangle. In general, the differing methods consist of choosing different \\(x_i\\) below..with left being at the left end, right being at the right end. \\[I(est) = \\sum_i f(x_i)\\Delta x_i = \\frac{b-a}{n} \\sum_i f(x_i)\\]\nThe error on the estimation of the integral can be shown to decrease as \\(\\frac{1}{n^2}\\). The basic reason for this can be understood on a taylor series expansion of the function to second order. When you integrate on the sub-interval, the linear term vanishes while the quadratic term becomes cubic in \\(\\Delta x\\). So the local error goes as \\(\\frac{1}{n^3}\\) and thus the global as \\(\\frac{1}{n^2}\\).\nMonte-Carlo if clearly not competitive with the midpoint method in 1-D. Its actually not even competitive with left or right rectangle methods.\nThe trapeziod rule uses a line between the sub-interval points while the Simpsons rule uses a quadratic.\nThese integrations can be generalized to multiple dimensions, and the rule for these\n\nleft or right rule: \\(\\propto 1/n\\)\nMidpoint rule: \\(\\propto 1/n^2\\)\nTrapezoid: \\(\\propto 1/n^2\\)\nSimpson: \\(\\propto 1/n^4\\)\n\nwhere \\(n=N^{1/d}\\). MC becomes better than the Simpson method only in 8 dimensions.."
  },
  {
    "objectID": "posts/globemodel/index.html",
    "href": "posts/globemodel/index.html",
    "title": "The Beta-Binomial Globe Model",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/globemodel/index.html#formulation-of-the-problem",
    "href": "posts/globemodel/index.html#formulation-of-the-problem",
    "title": "The Beta-Binomial Globe Model",
    "section": "Formulation of the problem",
    "text": "Formulation of the problem\nThis problem, taken from McElreath’s book, involves a seal (or a well trained human) tossing a globe, catching it on the nose, and noting down if the globe came down on water or land.\nThe seal tells us that the first 9 samples were:\nWLWWWLWLW.\nWe wish to understand the evolution of belief in the fraction of water on earth as the seal tosses the globe.\nSuppose \\(\\theta\\) is the true fraction of water covering the globe. Our data story if that \\(\\theta\\) then is the probability of the nose landing on water, with each throw or toss of the globe being independent.\nNow we build a probabilistic model for the problem, which we shall use to guide a process of Bayesian updating of the model as data comes in.\n\\[\\cal{L} = p(n,k|\\theta) = Binom(n,k, \\theta)=\\frac{n!}{k! (n-k)! } \\, \\theta^k \\, (1-\\theta)^{(n-k)} \\]\nSince our seal hasnt really seen any water or land, (strange, I know), it assigns equal probabilities, ie uniform probability to any value of \\(\\theta\\).\nThis is our prior information\nFor reasons of conjugacy we choose as prior the beta distribution, with \\(Beta(1,1)\\) being the uniform prior."
  },
  {
    "objectID": "posts/globemodel/index.html#choosing-a-prior-and-posterior",
    "href": "posts/globemodel/index.html#choosing-a-prior-and-posterior",
    "title": "The Beta-Binomial Globe Model",
    "section": "Choosing a prior and posterior",
    "text": "Choosing a prior and posterior\nThe mean of \\(Beta(\\alpha, \\beta)\\) is \\(\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\) while the variance is\n\\[V=\\mu (1- \\mu)/(\\alpha + \\beta + 1)\\]\n\nfrom scipy.stats import beta\nx=np.linspace(0., 1., 100)\nplt.plot(x, beta.pdf(x, 1, 1));\nplt.plot(x, beta.pdf(x, 1, 9));\nplt.plot(x, beta.pdf(x, 1.2, 9));\nplt.plot(x, beta.pdf(x, 2, 18));\n\n\n\n\n\n\n\n\nWe shall choose \\(\\alpha=1\\) and \\(\\beta=1\\) to be uniform.\n\\[ p(\\theta) = {\\rm Beta}(\\theta,\\alpha, \\beta) = \\frac{\\theta^{\\alpha-1} (1-x)^{\\beta-1} }{B(\\alpha, \\beta)} \\] where \\(B(\\alpha, \\beta)\\) is independent of \\(\\theta\\) and it is the normalization factor.\nFrom Bayes theorem, the posterior for \\(\\theta\\) is\n\\[ p(\\theta|D) \\propto  p(\\theta) \\, p(n,k|\\theta)  =  Binom(n,k, \\theta) \\,  {\\rm Beta}(\\theta,\\alpha, \\beta)  \\]\nwhich can be shown to be\n\\[{\\rm Beta}(\\theta, \\alpha+k, \\beta+n-k)\\]\n\nfrom scipy.stats import beta, binom\n\nplt.figure(figsize=( 15, 18))\n\nprior_params = np.array( [1.,1.] )  # FLAT \n\nx = np.linspace(0.00, 1, 125)\ndatastring = \"WLWWWLWLW\"\ndata=[]\nfor c in datastring:\n    data.append(1*(c=='W'))\ndata=np.array(data)\nprint(data)\nchoices=['Land','Water']\n\n\nfor i,v in enumerate(data):\n    plt.subplot(9,1,i+1)\n    prior_pdf = beta.pdf( x, *prior_params)\n    if v==1:\n        water = [1,0]\n    else:\n        water = [0,1]\n    posterior_params = prior_params + np.array( water )    # posteriors beta parameters\n    posterior_pdf = beta.pdf( x, *posterior_params)  # the posterior \n    prior_params = posterior_params\n    plt.plot( x,prior_pdf, label = r\"prior for this step\", lw =1, color =\"#348ABD\" )\n    plt.plot( x, posterior_pdf, label = \"posterior for this step\", lw= 3, color =\"#A60628\" )\n    plt.fill_between( x, 0, prior_pdf, color =\"#348ABD\", alpha = 0.15) \n    plt.fill_between( x, 0, posterior_pdf, color =\"#A60628\", alpha = 0.15) \n    \n    plt.legend(title = \"N=%d, %s\"%(i, choices[v]));\n    #plt.ylim( 0, 10)#\n\n[1 0 1 1 1 0 1 0 1]"
  },
  {
    "objectID": "posts/globemodel/index.html#interrogating-the-posterior",
    "href": "posts/globemodel/index.html#interrogating-the-posterior",
    "title": "The Beta-Binomial Globe Model",
    "section": "Interrogating the posterior",
    "text": "Interrogating the posterior\nSince we can sample from the posterior now after 9 observations, lets do so!\n\nsamples = beta.rvs(*posterior_params, size=10000)\nplt.hist(samples, bins=50, normed=True);\nsns.kdeplot(samples);\n\n//anaconda/envs/py35/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\nNow we can calculate all sorts of stuff.\nThe probability that the amount of water is less than 50%\n\nnp.mean(samples &lt; 0.5)\n\n0.17299999999999999\n\n\nThe probability by which we get 80% of the samples.\n\nnp.percentile(samples, 80)\n\n0.76255263476156399\n\n\nYou might try and find a credible interval. This, unlike the wierd definition of confidence intervals, is exactly what you think it is, the amount of probability mass between certain percentages, like the middle 80%\n\nnp.percentile(samples, [10, 90])\n\narray([ 0.44604094,  0.81516349])\n\n\nYou can make various point estimates: mean, median\n\nnp.mean(samples), np.median(samples), np.percentile(samples, 50) #last 2 are same\n\n(0.63787343440335842, 0.6473143052303143, 0.6473143052303143)\n\n\nA particularly important and useful point estimate is the MAP, or “maximum a-posteriori” estimate, the value of the parameter at which the pdf (num-samples) reach a maximum.\n\nsampleshisto = np.histogram(samples, bins=50)\n\n(array([  2,   3,   4,   7,  13,   9,  23,  27,  37,  53,  58,  57,  74,\n         94, 124, 152, 170, 216, 215, 224, 224, 269, 310, 308, 341, 335,\n        371, 405, 434, 419, 455, 474, 407, 427, 425, 380, 360, 332, 307,\n        297, 262, 202, 194, 152, 138,  90,  55,  35,  23,   7]),\n array([ 0.1684931 ,  0.18443135,  0.20036959,  0.21630783,  0.23224608,\n         0.24818432,  0.26412256,  0.2800608 ,  0.29599905,  0.31193729,\n         0.32787553,  0.34381378,  0.35975202,  0.37569026,  0.39162851,\n         0.40756675,  0.42350499,  0.43944324,  0.45538148,  0.47131972,\n         0.48725797,  0.50319621,  0.51913445,  0.5350727 ,  0.55101094,\n         0.56694918,  0.58288743,  0.59882567,  0.61476391,  0.63070215,\n         0.6466404 ,  0.66257864,  0.67851688,  0.69445513,  0.71039337,\n         0.72633161,  0.74226986,  0.7582081 ,  0.77414634,  0.79008459,\n         0.80602283,  0.82196107,  0.83789932,  0.85383756,  0.8697758 ,\n         0.88571405,  0.90165229,  0.91759053,  0.93352878,  0.94946702,\n         0.96540526]))\n\n\n\nmaxcountindex = np.argmax(sampleshisto[0])\nmapvalue = sampleshisto[1][maxcountindex]\nprint(maxcountindex, mapvalue)\n\n31 0.662578641304\n\n\nA principled way to get these point estimates is a loss function. This is the subject of decision theory, and we shall come to it soon. Different losses correspond to different well known point estimates, as we shall see.\nBut as a quick idea of this, consider the squared error decision loss:\n\\[R(t) = E_{p(\\theta \\vert D)}[(\\theta -t)^2] = \\int d\\theta  (\\theta -t)^2  p(\\theta \\vert D)\\]\n\\[\\frac{dR(t)}{dt} = 0 \\implies  \\int  d\\theta -2(\\theta -t)p(\\theta \\vert D) = 0\\]\nor\n\\[ t= \\int d\\theta \\theta\\,p(\\theta \\vert D) \\]\nor the mean of the posterior.\nWe can see this with some quick computation:\n\nmse = [np.mean((xi-samples)**2) for xi in x]\nplt.plot(x, mse);\nprint(\"Mean\",np.mean(samples));\n\nMean 0.634941511888"
  },
  {
    "objectID": "posts/globemodel/index.html#obtaining-the-posterior-predictive",
    "href": "posts/globemodel/index.html#obtaining-the-posterior-predictive",
    "title": "The Beta-Binomial Globe Model",
    "section": "Obtaining the posterior predictive",
    "text": "Obtaining the posterior predictive\nIts easy to sample from any one probability to get the sampling distribution at a particular \\(\\theta\\)\n\npoint3samps = np.random.binomial( len(data), 0.3, size=10000);\npoint7samps = np.random.binomial( len(data), 0.7, size=10000);\nplt.hist(point3samps, lw=3, alpha=0.5, histtype=\"stepfilled\", bins=np.arange(11));\nplt.hist(point7samps, lw=3, alpha=0.3,histtype=\"stepfilled\", bins=np.arange(11));\n\n\n\n\n\n\n\n\nThe posterior predictive:\n\\[p(y^{*} \\vert D) = \\int d\\theta p(y^{*} \\vert \\theta) p(\\theta \\vert D)\\]\nseems to be a complex integral. But if you parse it, its not so complex. This diagram from McElreath helps:\n\n\n\nThe posterior predictive distribution as a mixture: each parameter value implies a sampling distribution, weighted by the posterior probability, producing the marginal prediction. From McElreath, Statistical Rethinking.\n\n\nA similar risk-minimization holds for the posterior-predictive so that\n\\[y_{min mse} = \\int  dy \\, y \\, p(y \\vert D)\\]\nwhich is indeed what we would use in a regression scenario…\n\nPlug-in Approximation\nAlso, often, people will use the plug-in approximation by putting the posterior mean or MAP value\n\\[p(\\theta \\vert D) = \\delta(\\theta - \\theta_{MAP})\\]\nand then simply drawing the posterior predictive from :\n\\[p(y^{*} \\vert D) = p(y^{*} \\vert \\theta_{MAP})\\]\n(the same thing could be done for \\(\\theta_{mean}\\)).\n\npluginpreds = np.random.binomial( len(data), mapvalue, size = len(samples))\n\n\nplt.hist(pluginpreds, bins=np.arange(11));\n\n\n\n\n\n\n\n\nThis approximation is just sampling from the likelihood(sampling distribution), at a posterior-obtained value of \\(\\theta\\). It might be useful if the posterior is an expensive MCMC and the MAP is easier to find by optimization, and can be used in conjunction with quadratic (gaussian) approximations to the posterior, as we will see in variational inference. But for now we have all the samples, and it would be inane not to use them…\n\n\nThe posterior predictive from sampling\nBut really from the perspective of sampling, all we have to do is to first draw the thetas from the posterior, then draw y’s from the likelihood, and histogram the likelihood. This is the same logic as marginal posteriors, with the addition of the fact that we must draw y from the likelihood once we drew \\(\\theta\\). You might think that we have to draw multiple \\(y\\)s at a theta, but this is already taken care of for us because of the nature of sampling. We already have multiple \\(\\theta\\)a in a bin.\n\npostpred = np.random.binomial( len(data), samples);\n\n\npostpred\n\narray([5, 5, 7, ..., 7, 5, 8])\n\n\n\nsamples.shape, postpred.shape\n\n((10000,), (10000,))\n\n\n\nplt.hist(postpred, bins=np.arange(11), alpha=0.5, align=\"left\", label=\"predictive\")\nplt.hist(pluginpreds, bins=np.arange(11), alpha=0.2, align=\"left\", label=\"plug-in (MAP)\")\nplt.title('Posterior predictive')\nplt.xlabel('k')\nplt.legend()\n\n\n\n\n\n\n\n\nYou can interrogate the posterior-predictive, or simulated samples in other ways, asking about the longest run of water tosses, or the number of times the water/land switched. This is left as an exercise. In particular, you will find that the number of switches is not consistent with what you see in our data. This might lead you to question our model…always a good thing..but note that we have very little data as yet to go on"
  },
  {
    "objectID": "posts/nnreg/index.html",
    "href": "posts/nnreg/index.html",
    "title": "How Sigmoids Combine",
    "section": "",
    "text": "How sigmoids combine\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as fn\nfrom torch.autograd import Variable\nclass MLRegP(nn.Module):\n    def __init__(self, input_dim, hidden_dim, nonlinearity = fn.tanh, additional_hidden_wide=0):\n        super(MLRegP, self).__init__()\n        self.fc_initial = nn.Linear(input_dim, hidden_dim)\n        self.fc_mid = nn.ModuleList()\n        self.additional_hidden_wide = additional_hidden_wide\n        for i in range(self.additional_hidden_wide):\n            self.fc_mid.append(nn.Linear(hidden_dim, hidden_dim))\n        self.fc_final = nn.Linear(hidden_dim, 1)\n        self.nonlinearity = nonlinearity\n\n    def forward(self, x):\n        x = self.fc_initial(x)\n        out_init = self.nonlinearity(x)\n        x = self.nonlinearity(x)\n        for i in range(self.additional_hidden_wide):\n            x = self.fc_mid[i](x)\n            x = self.nonlinearity(x)\n        out_final = self.fc_final(x)\n        return out_final, x, out_init\n\n\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.arange(0, 2*np.pi, 0.01)\ny = np.sin(x) + 0.1*np.random.normal(size=x.shape[0])\nxgrid=x\nygrid=y\nplt.plot(x,y, '.', alpha=0.2);\n\n\n\n\n\n\n\n\n\nxgrid.shape\n\n(629,)\n\n\n\nfrom sklearn.linear_model import LinearRegression\nest = LinearRegression().fit(x.reshape(-1,1), y)\nplt.plot(x,y, '.', alpha=0.2);\nplt.plot(x,est.predict(x.reshape(-1,1)), 'k-', lw=3, alpha=0.2);\n\n\n\n\n\n\n\n\n\nxdata = Variable(torch.Tensor(xgrid))\nydata = Variable(torch.Tensor(ygrid))\n\n\nimport torch.utils.data\ndataset = torch.utils.data.TensorDataset(torch.from_numpy(xgrid.reshape(-1,1)), torch.from_numpy(ygrid))\nloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n\n\ndataset.data_tensor.shape, dataset.target_tensor.shape\n\n(torch.Size([629, 1]), torch.Size([629]))\n\n\n\ndef run_model(model, epochs):\n    criterion = nn.MSELoss()\n    lr, epochs, batch_size = 1e-1 , epochs , 64\n    optimizer = torch.optim.SGD(model.parameters(), lr = lr )\n    accum=[]\n    for k in range(epochs):\n        localaccum = []\n        for localx, localy in iter(loader):\n            localx = Variable(localx.float())\n            localy = Variable(localy.float())\n            output, _, _ = model.forward(localx)\n            loss = criterion(output, localy)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            localaccum.append(loss.data[0])\n        accum.append((np.mean(localaccum), np.std(localaccum)))\n    return accum\n\n\ninput dim 1, 2 hidden layers width 2, linear output\n\nmodel = MLRegP(1, 2, nonlinearity=fn.sigmoid, additional_hidden_wide=1)\n\n\nprint(model)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=2)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=2, out_features=2)\n  )\n  (fc_final): Linear(in_features=2, out_features=1)\n)\n\n\n\naccum = run_model(model, 2000)\n\n\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nplt.plot([a[0]+a[1] for a in accum]);\nplt.plot([a[0]-a[1] for a in accum]);\nplt.xlim(0, 1000)\n\n\n\n\n\n\n\n\n\nfinaloutput, init_output, mid_output = model.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n#plt.xticks([])\n#plt.yticks([])\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nio.shape\n\n(629, 2)\n\n\n\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 2 hidden layers width 4, linear output\n\nmodel2 = MLRegP(1, 4, nonlinearity=fn.sigmoid, additional_hidden_wide=1)\naccum = run_model(model2, 4000)\n\n\nprint(model2)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=4)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=4, out_features=4)\n  )\n  (fc_final): Linear(in_features=4, out_features=1)\n)\n\n\n\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nfinaloutput, init_output, mid_output = model2.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nio.shape\n\n(629, 4)\n\n\n\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 2 hidden layers width 8, linear output\n\nmodel3 = MLRegP(1, 8, nonlinearity=fn.sigmoid, additional_hidden_wide=1)\naccum = run_model(model3, 4000)\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nprint(model3)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=8)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=8, out_features=8)\n  )\n  (fc_final): Linear(in_features=8, out_features=1)\n)\n\n\n\nfinaloutput, init_output, mid_output = model3.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 3 hidden layers width 4, linear output\n\nmodel4 = MLRegP(1, 4, nonlinearity=fn.sigmoid, additional_hidden_wide=2)\naccum = run_model(model4, 4000)\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nprint(model4)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=4)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=4, out_features=4)\n    (1): Linear(in_features=4, out_features=4)\n  )\n  (fc_final): Linear(in_features=4, out_features=1)\n)\n\n\n\nfinaloutput, init_output, mid_output = model4.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 3 hidden layers width 2, linear output\n\nmodel5 = MLRegP(1, 2, nonlinearity=fn.sigmoid, additional_hidden_wide=2)\naccum = run_model(model5, 4000)\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nprint(model5)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=2)\n  (fc_mid): ModuleList(\n    (0): Linear(in_features=2, out_features=2)\n    (1): Linear(in_features=2, out_features=2)\n  )\n  (fc_final): Linear(in_features=2, out_features=1)\n)\n\n\n\nfinaloutput, init_output, mid_output = model5.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 1 hidden layers width 2, linear output\n\nmodel6 = MLRegP(1, 2, nonlinearity=fn.sigmoid, additional_hidden_wide=0)\naccum = run_model(model6, 4000)\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nprint(model6)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=2)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=2, out_features=1)\n)\n\n\n\nfinaloutput, init_output, mid_output = model6.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 1 hidden layers width 1, linear output\n\nmodel7 = MLRegP(1, 1, nonlinearity=fn.sigmoid, additional_hidden_wide=0)\naccum = run_model(model7, 4000)\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nprint(model7)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=1)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=1, out_features=1)\n)\n\n\n\nfinaloutput, init_output, mid_output = model7.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)\n\n\n\n\n\n\n\n\n\n\ninput dim 1, 1 hidden layers width 16, linear output\n\nmodel8 = MLRegP(1, 16, nonlinearity=fn.sigmoid, additional_hidden_wide=0)\naccum = run_model(model8, 4000)\nplt.plot([a[0] for a in accum]);\n\n\n\n\n\n\n\n\n\nprint(model8)\n\nMLRegP(\n  (fc_initial): Linear(in_features=1, out_features=16)\n  (fc_mid): ModuleList(\n  )\n  (fc_final): Linear(in_features=16, out_features=1)\n)\n\n\n\nfinaloutput, init_output, mid_output = model8.forward(xdata.view(-1,1))\nplt.plot(xgrid, ygrid, '.')\nplt.plot(xgrid, finaloutput.data.numpy(), lw=3, color=\"r\")\nplt.title(\"input dim 1, 1 hidden layers width 16, linear output\");\n\n\n\n\n\n\n\n\n\nio = mid_output.data.numpy()\nplt.plot(xgrid, ygrid, '.', alpha=0.2)\nfor j in range(io.shape[1]):\n    plt.plot(xgrid, io[:, j], lw=2)"
  },
  {
    "objectID": "posts/typesoflearning/index.html",
    "href": "posts/typesoflearning/index.html",
    "title": "Mixture Models and Types of Learning",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\nimport seaborn as sns\nfrom IPython.display import Image\n\n\\[\\newcommand{\\isum}{\\sum_{i}}\\] \\[\\newcommand{\\zsum}{\\sum_{k=1}^{K}}\\] \\[\\newcommand{\\zsumi}{\\sum_{\\{z_i\\}}}\\]\n\n\nIt is common to assume that observations are correlated due to some common “cause”. Hierarchical bayesian models are an example where we assume that information flows between observations through a tied-together set of higher level hyper-parameters.\nWe can also construct models with ‘hidden’ or ‘augmented’ variables, also known as latent variable models, which may or may not correlate with a cause. Since such models often have fewer parameters than observations, they are useful in modelling many problems.\nAn example of a hidden model is the mixture model. A distribution \\(p(x \\vert \\{\\theta_{k}\\})\\) is a mixture of \\(K\\) component distributions \\(p_1, p_2,... p_K\\) if:\n\\[p(x \\vert \\{\\theta_{k}\\}) = \\sum_k \\lambda_k p_{k}(x \\vert \\theta_k)\\]\nwith the \\(\\lambda_k\\) being mixing weights, \\(\\lambda_k &gt; 0\\), \\(\\zsum \\lambda_k = 1\\).\nThe \\(p_k\\)’s can be completely arbitrary, but we usually assume they are from the same family, like Gaussians with different centers and variances, or Poissons with different means.\nWe have already seen an example of this with the zero-inflated poisson distribution where\nthe likelihood of observing 0 is:\n\\[\\cal{L}(y=0) = p + (1-p) e^{-\\lambda},\\]\nand the Likelihood of a non-zero \\(y\\) is:\n\\[\\cal{L}(y \\ne 0) = (1-p) \\frac{\\lambda^y e^{-\\lambda}}{y!}\\]\nThis model can be described by this diagram, taken from Mc-Elreath\n\n\n\nZero-inflated monks model: a mixture process where monks either drink (observe y=0) or work (observe y&gt;0), with the resulting zero-inflated distribution of manuscripts completed. From McElreath.\n\n\nThe way to generate a new observation from such a distribution thus would be the following:\n\\[Z \\sim Categorical(\\lambda_1,\\lambda_2,...,\\lambda_K)\\]\nwhere \\(Z\\) says which component X is drawn from (we could write this \\(Z \\sim Categorical(\\bar{\\lambda})\\)). Thus \\(\\lambda_j\\) is the probability that the hidden class variable \\(z\\) is \\(j\\). Then:\n\\[X \\sim p_{z}(x \\vert \\theta_z)\\]\nThus we can see the general structure above:\n\\[p(x  \\vert  \\theta) = \\sum_z p(z)p(x  \\vert  z, \\theta)\\]\nwhere \\(\\theta = \\{ \\theta_k \\}\\) is the collection of distribution parameters.\nWhats going on? Pick a bitmask according to the Categorical (the analog of the bernoulli, a multinomial with n=1). Where you find the 1, thats the distribution from which you make a draw.\n\nfrom scipy.stats import multinomial\nmultinomial.rvs(1,[0.6,0.1, 0.3], size=10)\n\narray([[1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0]])\n\n\n\n\n\nThe Gaussian mixture model or GMM is the most widely used mixture distribution. In this model, each base distribution is a multivariate Gaussian with mean \\(\\mu_k\\) and covariance matrix \\(\\Sigma_k\\). Thus the model has the form\n\\[p(x \\vert  \\{\\theta_{k}\\}) = \\zsum \\lambda_k N(x \\vert \\mu_k , \\Sigma_k ) \\]\nThus each mixture component can be thought of as represented by a different set of eliptical contours, and we add these to create our overall density.\n\n#In 1-D\n# True parameter values\nmu_true = np.array([2, 5, 10])\nsigma_true = np.array([0.6, 0.8, 0.5])\nlambda_true = np.array([.4, .2, .4])\nn = 10000\n\n# Simulate from each distribution according to mixing proportion lambda\nz = multinomial.rvs(1, lambda_true, size=n)\n\n\nx=np.array([np.random.normal(mu_true[i.astype('bool')][0], sigma_true[i.astype('bool')][0]) for i in z])\n\n\nsns.distplot(x, bins=100);\n\n\n\n\n\n\n\n\n\n\n\nIn supervised learning, we have some training data of \\(N\\) data points, each with \\(m\\) features. The big idea is that on the training set, we have a label associated with each data point. Here the label is \\(z\\).\nFor a feature vector x, we use Bayes rule to express the posterior of the class-conditional as:\n\\[p(z = c \\vert x, \\theta) = \\frac{p(z = c  \\vert  \\theta)p(x  \\vert  z = c, \\theta)}{ \\sum_{c′} p(z = c′  \\vert  \\theta) p(x  \\vert  z = c′, \\theta)}\\]\nThis is a generative classifier, since it specifies how to generate the data using the class-conditional density \\(p(x \\vert z = c, \\theta)\\) and the class prior \\(p(z = c\\vert \\theta)\\).\nNotice that even though we are talking about priors and posteriors here, I am carrying along \\(\\theta\\). This is because the models here are just probabilistic models, and we havent chosen a frequentist or bayesian modelling paradigm yet. We are just specifying the model, and the priors and posteriors we are talking about here are simply those from bayes theorem.\nAn alternative approach is to directly fit the class posterior, \\(p(z = c \\vert x, \\theta)\\); this is known as a discriminative classifier. For example, a Gaussian Mixture model or Naive bayes is a generative classifier whose discriminative counterpart is the logistic regression.\nThe supervised learning case is the one in which where hidden variables \\(z\\) are known on the training set. So the supervized case is one in which the model is not a hidden variables model at all.\n\n\nSuppose we have input data \\(x\\) that are continuous-valued random variables, and \\(z\\) labels. We call \\(p(x,z \\vert \\theta)\\) the full-data likelihood. Since we have both \\(z\\) and \\(x\\) on our training set, this is the likelihood we will want to maximize.\nLet us see how to obtain it.\nOur model is (limiting ourselves to two gaussians for simplicity):\n\\[ Z \\sim \\rm{Bernoulli}(\\lambda) \\] \\[ X \\vert z=0 \\sim {\\cal N}(\\mu_0, \\Sigma_0) \\] \\[ X \\vert z=1 \\sim {\\cal N}(\\mu_1, \\Sigma_1) \\]\nThe distributions in details are \\[ p(z) = \\lambda^z (1-\\lambda)^{1-z}\\] \\[ p(x \\vert z=0) = \\frac{1}{(2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu_0)^T \\,\\Sigma_0^{-1}(x-\\mu_0) \\right) \\] \\[ p(x \\vert z=1) = \\frac{1}{(2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu_1)^T \\,\\Sigma_1^{-1}(x-\\mu_1) \\right) \\]\nwhere the parameters of the model \\(\\lambda\\), \\(\\Sigma_0\\), \\(\\Sigma_1\\), \\(\\mu_0\\) and \\(\\mu_1\\) are to be determined. From here on, for simplicity of exposition we will use one covariance matrix for both Gaussians, \\(\\Sigma\\). This is called tieing the covariances and leads to a model in the literature called Linear Discriminant Analysis.\nThe full-data log-likelihood of the data is given\n\\[\n\\begin{eqnarray}\nl(x,z \\vert  \\lambda,\\mu_0, \\mu_1, \\Sigma) &=& \\log \\prod_{i=1}^{m} p(x_i,z_i \\vert  \\lambda, \\mu_0, \\mu_1, \\Sigma) \\nonumber \\\\\n          &=& \\sum_{i=1}^{m} \\log \\left[p(x_i \\vert z_i,  \\mu_0, \\mu_1, \\Sigma) \\,p(z_i \\vert  \\lambda) \\right]  \\nonumber \\\\\n          &=& \\sum_{i=1}^{m} \\log p(x_i \\vert z_i,  \\mu_0, \\mu_1, \\Sigma) + \\sum_{i=1}^{m}  \\log p(z_i \\vert  \\lambda)   \\nonumber   \\\\      \n    &=&  -\\sum_{i=1}^{m} \\log ((2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}) - \\frac{1}{2} \\sum_{i=1}^{m}  (x-\\mu_{z_i})^T \\,\\Sigma^{-1}(x-\\mu_{z_i})   \\nonumber \\\\\n        & & \\quad \\quad +\\sum_{i=1}^{m} \\left[ z_i \\, \\log \\lambda + (1-z_i) \\log(1-\\lambda) \\right]\n\\end{eqnarray}\n\\]\nTaking derivatives with respect to \\(\\lambda\\), \\(\\Sigma\\), \\(\\mu_0\\) and \\(\\mu_1\\) and setting them to zero we get\n\\[\n\\begin{eqnarray}\n   \\lambda & = &\\frac{1}{m}  \\sum_{i=1}^{m}  \\delta_{z_i,1} \\nonumber  \\\\\n   \\mu_0 &=& \\frac{ \\sum_{i=1}^{m}  \\delta_{z_i,0} \\, x_i  }{ \\sum_{i=1}^{m}   \\delta_{z_i,0}}\\nonumber  \\\\\n    \\mu_1 &=& \\frac{ \\sum_{i=1}^{m}  \\delta_{z_i,1} \\, x_i  }{ \\sum_{i=1}^{m}   \\delta_{z_i,1}}\\nonumber  \\\\\n\\Sigma &=&\\frac{1}{m}   \\sum_{i=1}^{m}  (x_i-\\mu_{z_i})   (x_i-\\mu_{z_i})^{T}\n\\end{eqnarray}\n\\]\nThis gives us the obvious result, namely \\(\\lambda\\) is nothing more but the fraction of objects with label \\(z=1\\) and the total number of objects, \\(\\mu\\)’s are the mean within the class and \\(\\Sigma\\) is the the covariance matrix for each group. This analysis is called “Gaussian Discriminant Analysis” or GDA, here specifically LDA as we tied the covariance matrices.\nLets do this in code. First we simulate some data:\n\n#In 1-D\n# True parameter values\nmu_true = np.array([2, 5])\nsigma_true = np.array([0.6, 0.6])\nlambda_true = np.array([.4, .6])\nn = 10000\n\n# Simulate from each distribution according to mixing proportion lambda_true\nfrom scipy.stats import binom\nz = binom.rvs(1, lambda_true[0], size=n)\nx=np.array([np.random.normal(mu_true[i], sigma_true[i]) for i in z])\nsns.distplot(x, bins=100);\n\n\n\n\n\n\n\n\nNow we split into a training set and a test set.\n\n#the z's are the classes in the supervised learning\n#the 'feature' is the x position of the sample\nfrom sklearn.model_selection import train_test_split\nztrain, ztest, xtrain, xtest = train_test_split(z,x)\n\n\nztrain.shape, xtrain.shape\n\n((7500,), (7500,))\n\n\n\nplt.hist(xtrain, bins=20);\n\n\n\n\n\n\n\n\n\nlambda_train=np.mean(ztrain)\nmu0_train = np.sum(xtrain[ztrain==0])/(np.sum(ztrain==0))\nmu1_train = np.sum(xtrain[ztrain==1])/(np.sum(ztrain==1))\nxmus=np.array([mu0_train if z==0 else mu1_train for z in ztrain])\nxdiffs = xtrain - xmus\nsigma_train=np.sqrt(np.dot(xdiffs, xdiffs)/xtrain.shape[0])\nprint(lambda_train, mu0_train, mu1_train, sigma_train)\n\n0.407066666667 1.99814996529 5.0176232719 0.599407546657\n\n\nWe can use the log likelihood at a given x as a classifier: assign the class ‘0’ or ‘1’ depending upon which probability \\(p(x_j \\vert \\lambda, z, \\Sigma)\\) is larger. Note that this is JUST the \\(x\\) likelihood, because we want to compare probabilities for fixed \\(z\\)s.\n\\[log\\,p(x_j \\vert \\lambda, z, \\Sigma) = -\\sum_{i=1}^{m} \\log ((2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}) - \\frac{1}{2} \\sum_{i=1}^{m}  (x-\\mu_{z_i})^T \\,\\Sigma^{-1}(x-\\mu_{z_i})  \\]\nThe first term of the likelihood does not matter since it is independent of \\(z\\), therefore:\n\ndef loglikdiff(x):\n    for0= - (x-mu0_train)*(x-mu0_train)/(2.0*sigma_train*sigma_train) \n    for0 = for0 + np.log(1.-lambda_train)\n    for1 = - (x-mu1_train)*(x-mu1_train)/(2.0*sigma_train*sigma_train) \n    for1 = for1 + np.log(lambda_train)\n    return 1*(for1 - for0 &gt;= 0)\n\n\npred = np.array([loglikdiff(test_x) for test_x in xtest])\nprint(\"correct classification rate\", np.mean(ztest == pred))\n\ncorrect classification rate 0.9944\n\n\n\n\n\n\nIn unsupervised learning, we do not know the class labels. We wish to generate these labels automatically from the data. An example of an unsupervised model is clustering. In the context of mixture models we then do not know what the components of the mixture model are, i.e. what the parameters of the components and their admixture (\\(\\lambda\\)s) are. Indeed, we might not even know how many components we have!!\nIn this case, to carry out the clustering, we first fit the mixture model, and then compute \\(p(z_i = k  \\vert  x_i, \\theta)\\), which represents the posterior probability that point i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be computed as before using Bayes rule as follows:\n\\[p(z_k = c \\vert x_i, \\theta) = \\frac{p(z_k = c  \\vert  \\theta)p(x_i  \\vert  z_k = c, \\theta)}\n{ \\sum_{_c′} p(z_k = c′  \\vert  \\theta) p(x_i  \\vert  z_k = c′, \\theta)}\\]\nThis is called soft clustering. K-means is a hard-clustering analog where you associate a data point with a cluster rather than simply computing probabilities for the association.\nThe process is identical to the computations performed before in the supervised learning, except at training time: here we never observe \\(z_k\\) for any samples, whereas before with the generative GDA classifier, we did observe \\(z_k\\) on the training set.\nHow many clusters? The best number will generalize best to future data, something we can use cross-validation or other techniques to find.\nPut differently, unsupervised learning is a density estimation problem for \\(p(x)\\).\n\\[p(x) = \\sum_c p(x \\vert z) p(z).\\]\nIn other words we discover the marginal p(x) through the generative model \\(p(x \\vert z)\\). This is also very useful in discovering outliers in our data.\n\n\nSo let us turn our attention to the case where we do not know the labels \\(z\\).\nSuppose we are given a data set \\(\\{x_1,\\ldots, x_m\\}\\) but not given the labels \\(z\\). The model consists of \\(k\\) Gaussians. In other words our model assumes that each \\(x_i\\) was generated by randomly choosing \\(z_i\\) from \\(\\{1, \\ldots, k\\}\\), and then \\(x_i\\) is drawn from one of the \\(k\\) Gaussians depending on \\(z_i\\).\nWe wish to compute either the maximum likelihood estimate for this model, \\(p(\\{x_{i}\\} \\vert \\theta)\\). The goal is to model the joint distribution \\(p(\\{x_i\\}, \\{z_i\\})=p(\\{x_i\\} \\vert \\{z_i\\}) \\, p(\\{z_i\\})\\) where \\(z_i \\sim \\rm{Categorical}(\\lambda)\\), and \\(\\lambda = \\{\\lambda_j\\}\\).\nAs in our definition of mixture models \\(\\lambda_j\\ge0\\) and\n\\[ \\sum_j^k \\lambda_i = 1 \\]\nThe parameters \\(\\lambda_j\\) produce \\(p(z_i=j)\\) and then \\(x_i \\vert z_i=j \\sim {\\cal N}(\\mu_j, \\Sigma_j)\\).\nThe parameters of our problem are \\(\\lambda\\), \\(\\mu\\) and \\(\\Sigma\\). But because the labels \\(z\\) are hidden to us, we no longer have the full-data likelihood. So we estimate our parameters by minimizing the \\(x\\)-data log-likelihood:\n\\[\n\\begin{eqnarray}\nl(x \\vert  \\lambda, \\mu, \\Sigma) &=& \\sum_{i=1}^{m} \\log p(x_i \\vert  \\lambda,  \\mu ,\\Sigma)   \\nonumber \\\\\n     &=& \\sum_{i=1}^{m} \\log \\sum_z p(x_i \\vert  z_i,  \\mu , \\Sigma) \\, p(z_i \\vert  \\lambda)  \n\\end{eqnarray}\n\\]\nHowever, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we’ll find that it is not possible to find the maximum likelihood estimates Futhermore, we have to enforce constraints such as mixing weights summing to 1, covariance matrices being positive definite, etc.\nFor all of these reasons, its simpler, but not always faster to use an iterative algorithm called the EM algorithm to get the local maximum likelihood or MAP estimate. We shall learn this algorithm soon. But we can also set this problem up as a bayesian problem with reasonable priors on the parameters and try to use MCMC.\n\n\n\n\nIn unsupervized learning we are given samples from some unknown data distribution with density \\(p(x)\\). Our goal is to estimate this density or a known functional (like the risk) thereof. Supervized learning can be treated as estimating \\(p(x,c)\\) or a known functional of it. But there is usually no need to estimate the input distribution so estimating the complete density is wasteful, and we usually focus on estimating \\(p(c \\vert x)\\) discriminatively or generatively(via \\(p(x \\vert c) p(c)\\). Here \\(c\\) or \\(y\\) or \\(z\\) are the classes we are trying to estimate. In the unsupervized case we often estimate \\(\\sum_z p(x \\vert z) p(z) = p(x)\\) with latent (hidden) \\(z\\), which you may or may not wish to identify with classes.\nSemi-supervised learning is the situation in which we have some labels, but typically very few labels: not enough to form a good training set.\nIn semi-supervized learning we combine the two worlds. We write a joint likelihood for the supervised and unsupervised parts:\n\\[ l(\\{x_i\\},\\{x_j\\},\\{z_i\\} \\vert \\theta, \\lambda) = \\sum_i log \\, p(x_i, z_i \\vert \\lambda, \\theta) +  \\sum_j log \\, p(x_j \\vert \\lambda, \\theta) = \\sum_i log \\, p(z_i \\vert \\lambda) p(x_i \\vert z_,\\theta) + \\sum_j log \\, \\sum_z p(z_j \\vert \\lambda) p(x_j \\vert z_j,\\theta) \\]\nHere \\(i\\) ranges over the data points where we have labels, and \\(j\\) over the data points where we dont.\nIn a traditional classification-based machine learning scenario we might still split the data into a training and validation set. But the basic idea in semi-supervised learning is that there is structure in \\(p(x)\\) which might help us divine the conditionals, so what we would want to do, is include in the training set unlabelled points. The game then is that if there is geometric structure in \\(p(x)\\), some kind of cluster based foliation, then we can explot this."
  },
  {
    "objectID": "posts/typesoflearning/index.html#mixture-models",
    "href": "posts/typesoflearning/index.html#mixture-models",
    "title": "Mixture Models and Types of Learning",
    "section": "",
    "text": "It is common to assume that observations are correlated due to some common “cause”. Hierarchical bayesian models are an example where we assume that information flows between observations through a tied-together set of higher level hyper-parameters.\nWe can also construct models with ‘hidden’ or ‘augmented’ variables, also known as latent variable models, which may or may not correlate with a cause. Since such models often have fewer parameters than observations, they are useful in modelling many problems.\nAn example of a hidden model is the mixture model. A distribution \\(p(x \\vert \\{\\theta_{k}\\})\\) is a mixture of \\(K\\) component distributions \\(p_1, p_2,... p_K\\) if:\n\\[p(x \\vert \\{\\theta_{k}\\}) = \\sum_k \\lambda_k p_{k}(x \\vert \\theta_k)\\]\nwith the \\(\\lambda_k\\) being mixing weights, \\(\\lambda_k &gt; 0\\), \\(\\zsum \\lambda_k = 1\\).\nThe \\(p_k\\)’s can be completely arbitrary, but we usually assume they are from the same family, like Gaussians with different centers and variances, or Poissons with different means.\nWe have already seen an example of this with the zero-inflated poisson distribution where\nthe likelihood of observing 0 is:\n\\[\\cal{L}(y=0) = p + (1-p) e^{-\\lambda},\\]\nand the Likelihood of a non-zero \\(y\\) is:\n\\[\\cal{L}(y \\ne 0) = (1-p) \\frac{\\lambda^y e^{-\\lambda}}{y!}\\]\nThis model can be described by this diagram, taken from Mc-Elreath\n\n\n\nZero-inflated monks model: a mixture process where monks either drink (observe y=0) or work (observe y&gt;0), with the resulting zero-inflated distribution of manuscripts completed. From McElreath.\n\n\nThe way to generate a new observation from such a distribution thus would be the following:\n\\[Z \\sim Categorical(\\lambda_1,\\lambda_2,...,\\lambda_K)\\]\nwhere \\(Z\\) says which component X is drawn from (we could write this \\(Z \\sim Categorical(\\bar{\\lambda})\\)). Thus \\(\\lambda_j\\) is the probability that the hidden class variable \\(z\\) is \\(j\\). Then:\n\\[X \\sim p_{z}(x \\vert \\theta_z)\\]\nThus we can see the general structure above:\n\\[p(x  \\vert  \\theta) = \\sum_z p(z)p(x  \\vert  z, \\theta)\\]\nwhere \\(\\theta = \\{ \\theta_k \\}\\) is the collection of distribution parameters.\nWhats going on? Pick a bitmask according to the Categorical (the analog of the bernoulli, a multinomial with n=1). Where you find the 1, thats the distribution from which you make a draw.\n\nfrom scipy.stats import multinomial\nmultinomial.rvs(1,[0.6,0.1, 0.3], size=10)\n\narray([[1, 0, 0],\n       [1, 0, 0],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 0, 1],\n       [0, 1, 0],\n       [1, 0, 0],\n       [1, 0, 0],\n       [1, 0, 0]])"
  },
  {
    "objectID": "posts/typesoflearning/index.html#gaussian-mixture-models",
    "href": "posts/typesoflearning/index.html#gaussian-mixture-models",
    "title": "Mixture Models and Types of Learning",
    "section": "",
    "text": "The Gaussian mixture model or GMM is the most widely used mixture distribution. In this model, each base distribution is a multivariate Gaussian with mean \\(\\mu_k\\) and covariance matrix \\(\\Sigma_k\\). Thus the model has the form\n\\[p(x \\vert  \\{\\theta_{k}\\}) = \\zsum \\lambda_k N(x \\vert \\mu_k , \\Sigma_k ) \\]\nThus each mixture component can be thought of as represented by a different set of eliptical contours, and we add these to create our overall density.\n\n#In 1-D\n# True parameter values\nmu_true = np.array([2, 5, 10])\nsigma_true = np.array([0.6, 0.8, 0.5])\nlambda_true = np.array([.4, .2, .4])\nn = 10000\n\n# Simulate from each distribution according to mixing proportion lambda\nz = multinomial.rvs(1, lambda_true, size=n)\n\n\nx=np.array([np.random.normal(mu_true[i.astype('bool')][0], sigma_true[i.astype('bool')][0]) for i in z])\n\n\nsns.distplot(x, bins=100);"
  },
  {
    "objectID": "posts/typesoflearning/index.html#supervised-learning",
    "href": "posts/typesoflearning/index.html#supervised-learning",
    "title": "Mixture Models and Types of Learning",
    "section": "",
    "text": "In supervised learning, we have some training data of \\(N\\) data points, each with \\(m\\) features. The big idea is that on the training set, we have a label associated with each data point. Here the label is \\(z\\).\nFor a feature vector x, we use Bayes rule to express the posterior of the class-conditional as:\n\\[p(z = c \\vert x, \\theta) = \\frac{p(z = c  \\vert  \\theta)p(x  \\vert  z = c, \\theta)}{ \\sum_{c′} p(z = c′  \\vert  \\theta) p(x  \\vert  z = c′, \\theta)}\\]\nThis is a generative classifier, since it specifies how to generate the data using the class-conditional density \\(p(x \\vert z = c, \\theta)\\) and the class prior \\(p(z = c\\vert \\theta)\\).\nNotice that even though we are talking about priors and posteriors here, I am carrying along \\(\\theta\\). This is because the models here are just probabilistic models, and we havent chosen a frequentist or bayesian modelling paradigm yet. We are just specifying the model, and the priors and posteriors we are talking about here are simply those from bayes theorem.\nAn alternative approach is to directly fit the class posterior, \\(p(z = c \\vert x, \\theta)\\); this is known as a discriminative classifier. For example, a Gaussian Mixture model or Naive bayes is a generative classifier whose discriminative counterpart is the logistic regression.\nThe supervised learning case is the one in which where hidden variables \\(z\\) are known on the training set. So the supervized case is one in which the model is not a hidden variables model at all.\n\n\nSuppose we have input data \\(x\\) that are continuous-valued random variables, and \\(z\\) labels. We call \\(p(x,z \\vert \\theta)\\) the full-data likelihood. Since we have both \\(z\\) and \\(x\\) on our training set, this is the likelihood we will want to maximize.\nLet us see how to obtain it.\nOur model is (limiting ourselves to two gaussians for simplicity):\n\\[ Z \\sim \\rm{Bernoulli}(\\lambda) \\] \\[ X \\vert z=0 \\sim {\\cal N}(\\mu_0, \\Sigma_0) \\] \\[ X \\vert z=1 \\sim {\\cal N}(\\mu_1, \\Sigma_1) \\]\nThe distributions in details are \\[ p(z) = \\lambda^z (1-\\lambda)^{1-z}\\] \\[ p(x \\vert z=0) = \\frac{1}{(2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu_0)^T \\,\\Sigma_0^{-1}(x-\\mu_0) \\right) \\] \\[ p(x \\vert z=1) = \\frac{1}{(2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}} \\exp \\left( -\\frac{1}{2}(x-\\mu_1)^T \\,\\Sigma_1^{-1}(x-\\mu_1) \\right) \\]\nwhere the parameters of the model \\(\\lambda\\), \\(\\Sigma_0\\), \\(\\Sigma_1\\), \\(\\mu_0\\) and \\(\\mu_1\\) are to be determined. From here on, for simplicity of exposition we will use one covariance matrix for both Gaussians, \\(\\Sigma\\). This is called tieing the covariances and leads to a model in the literature called Linear Discriminant Analysis.\nThe full-data log-likelihood of the data is given\n\\[\n\\begin{eqnarray}\nl(x,z \\vert  \\lambda,\\mu_0, \\mu_1, \\Sigma) &=& \\log \\prod_{i=1}^{m} p(x_i,z_i \\vert  \\lambda, \\mu_0, \\mu_1, \\Sigma) \\nonumber \\\\\n          &=& \\sum_{i=1}^{m} \\log \\left[p(x_i \\vert z_i,  \\mu_0, \\mu_1, \\Sigma) \\,p(z_i \\vert  \\lambda) \\right]  \\nonumber \\\\\n          &=& \\sum_{i=1}^{m} \\log p(x_i \\vert z_i,  \\mu_0, \\mu_1, \\Sigma) + \\sum_{i=1}^{m}  \\log p(z_i \\vert  \\lambda)   \\nonumber   \\\\      \n    &=&  -\\sum_{i=1}^{m} \\log ((2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}) - \\frac{1}{2} \\sum_{i=1}^{m}  (x-\\mu_{z_i})^T \\,\\Sigma^{-1}(x-\\mu_{z_i})   \\nonumber \\\\\n        & & \\quad \\quad +\\sum_{i=1}^{m} \\left[ z_i \\, \\log \\lambda + (1-z_i) \\log(1-\\lambda) \\right]\n\\end{eqnarray}\n\\]\nTaking derivatives with respect to \\(\\lambda\\), \\(\\Sigma\\), \\(\\mu_0\\) and \\(\\mu_1\\) and setting them to zero we get\n\\[\n\\begin{eqnarray}\n   \\lambda & = &\\frac{1}{m}  \\sum_{i=1}^{m}  \\delta_{z_i,1} \\nonumber  \\\\\n   \\mu_0 &=& \\frac{ \\sum_{i=1}^{m}  \\delta_{z_i,0} \\, x_i  }{ \\sum_{i=1}^{m}   \\delta_{z_i,0}}\\nonumber  \\\\\n    \\mu_1 &=& \\frac{ \\sum_{i=1}^{m}  \\delta_{z_i,1} \\, x_i  }{ \\sum_{i=1}^{m}   \\delta_{z_i,1}}\\nonumber  \\\\\n\\Sigma &=&\\frac{1}{m}   \\sum_{i=1}^{m}  (x_i-\\mu_{z_i})   (x_i-\\mu_{z_i})^{T}\n\\end{eqnarray}\n\\]\nThis gives us the obvious result, namely \\(\\lambda\\) is nothing more but the fraction of objects with label \\(z=1\\) and the total number of objects, \\(\\mu\\)’s are the mean within the class and \\(\\Sigma\\) is the the covariance matrix for each group. This analysis is called “Gaussian Discriminant Analysis” or GDA, here specifically LDA as we tied the covariance matrices.\nLets do this in code. First we simulate some data:\n\n#In 1-D\n# True parameter values\nmu_true = np.array([2, 5])\nsigma_true = np.array([0.6, 0.6])\nlambda_true = np.array([.4, .6])\nn = 10000\n\n# Simulate from each distribution according to mixing proportion lambda_true\nfrom scipy.stats import binom\nz = binom.rvs(1, lambda_true[0], size=n)\nx=np.array([np.random.normal(mu_true[i], sigma_true[i]) for i in z])\nsns.distplot(x, bins=100);\n\n\n\n\n\n\n\n\nNow we split into a training set and a test set.\n\n#the z's are the classes in the supervised learning\n#the 'feature' is the x position of the sample\nfrom sklearn.model_selection import train_test_split\nztrain, ztest, xtrain, xtest = train_test_split(z,x)\n\n\nztrain.shape, xtrain.shape\n\n((7500,), (7500,))\n\n\n\nplt.hist(xtrain, bins=20);\n\n\n\n\n\n\n\n\n\nlambda_train=np.mean(ztrain)\nmu0_train = np.sum(xtrain[ztrain==0])/(np.sum(ztrain==0))\nmu1_train = np.sum(xtrain[ztrain==1])/(np.sum(ztrain==1))\nxmus=np.array([mu0_train if z==0 else mu1_train for z in ztrain])\nxdiffs = xtrain - xmus\nsigma_train=np.sqrt(np.dot(xdiffs, xdiffs)/xtrain.shape[0])\nprint(lambda_train, mu0_train, mu1_train, sigma_train)\n\n0.407066666667 1.99814996529 5.0176232719 0.599407546657\n\n\nWe can use the log likelihood at a given x as a classifier: assign the class ‘0’ or ‘1’ depending upon which probability \\(p(x_j \\vert \\lambda, z, \\Sigma)\\) is larger. Note that this is JUST the \\(x\\) likelihood, because we want to compare probabilities for fixed \\(z\\)s.\n\\[log\\,p(x_j \\vert \\lambda, z, \\Sigma) = -\\sum_{i=1}^{m} \\log ((2\\pi)^{n/2}  \\vert  \\Sigma \\vert ^{1/2}) - \\frac{1}{2} \\sum_{i=1}^{m}  (x-\\mu_{z_i})^T \\,\\Sigma^{-1}(x-\\mu_{z_i})  \\]\nThe first term of the likelihood does not matter since it is independent of \\(z\\), therefore:\n\ndef loglikdiff(x):\n    for0= - (x-mu0_train)*(x-mu0_train)/(2.0*sigma_train*sigma_train) \n    for0 = for0 + np.log(1.-lambda_train)\n    for1 = - (x-mu1_train)*(x-mu1_train)/(2.0*sigma_train*sigma_train) \n    for1 = for1 + np.log(lambda_train)\n    return 1*(for1 - for0 &gt;= 0)\n\n\npred = np.array([loglikdiff(test_x) for test_x in xtest])\nprint(\"correct classification rate\", np.mean(ztest == pred))\n\ncorrect classification rate 0.9944"
  },
  {
    "objectID": "posts/typesoflearning/index.html#unsupervised-learning-mixture-of-gaussians",
    "href": "posts/typesoflearning/index.html#unsupervised-learning-mixture-of-gaussians",
    "title": "Mixture Models and Types of Learning",
    "section": "",
    "text": "In unsupervised learning, we do not know the class labels. We wish to generate these labels automatically from the data. An example of an unsupervised model is clustering. In the context of mixture models we then do not know what the components of the mixture model are, i.e. what the parameters of the components and their admixture (\\(\\lambda\\)s) are. Indeed, we might not even know how many components we have!!\nIn this case, to carry out the clustering, we first fit the mixture model, and then compute \\(p(z_i = k  \\vert  x_i, \\theta)\\), which represents the posterior probability that point i belongs to cluster k. This is known as the responsibility of cluster k for point i, and can be computed as before using Bayes rule as follows:\n\\[p(z_k = c \\vert x_i, \\theta) = \\frac{p(z_k = c  \\vert  \\theta)p(x_i  \\vert  z_k = c, \\theta)}\n{ \\sum_{_c′} p(z_k = c′  \\vert  \\theta) p(x_i  \\vert  z_k = c′, \\theta)}\\]\nThis is called soft clustering. K-means is a hard-clustering analog where you associate a data point with a cluster rather than simply computing probabilities for the association.\nThe process is identical to the computations performed before in the supervised learning, except at training time: here we never observe \\(z_k\\) for any samples, whereas before with the generative GDA classifier, we did observe \\(z_k\\) on the training set.\nHow many clusters? The best number will generalize best to future data, something we can use cross-validation or other techniques to find.\nPut differently, unsupervised learning is a density estimation problem for \\(p(x)\\).\n\\[p(x) = \\sum_c p(x \\vert z) p(z).\\]\nIn other words we discover the marginal p(x) through the generative model \\(p(x \\vert z)\\). This is also very useful in discovering outliers in our data.\n\n\nSo let us turn our attention to the case where we do not know the labels \\(z\\).\nSuppose we are given a data set \\(\\{x_1,\\ldots, x_m\\}\\) but not given the labels \\(z\\). The model consists of \\(k\\) Gaussians. In other words our model assumes that each \\(x_i\\) was generated by randomly choosing \\(z_i\\) from \\(\\{1, \\ldots, k\\}\\), and then \\(x_i\\) is drawn from one of the \\(k\\) Gaussians depending on \\(z_i\\).\nWe wish to compute either the maximum likelihood estimate for this model, \\(p(\\{x_{i}\\} \\vert \\theta)\\). The goal is to model the joint distribution \\(p(\\{x_i\\}, \\{z_i\\})=p(\\{x_i\\} \\vert \\{z_i\\}) \\, p(\\{z_i\\})\\) where \\(z_i \\sim \\rm{Categorical}(\\lambda)\\), and \\(\\lambda = \\{\\lambda_j\\}\\).\nAs in our definition of mixture models \\(\\lambda_j\\ge0\\) and\n\\[ \\sum_j^k \\lambda_i = 1 \\]\nThe parameters \\(\\lambda_j\\) produce \\(p(z_i=j)\\) and then \\(x_i \\vert z_i=j \\sim {\\cal N}(\\mu_j, \\Sigma_j)\\).\nThe parameters of our problem are \\(\\lambda\\), \\(\\mu\\) and \\(\\Sigma\\). But because the labels \\(z\\) are hidden to us, we no longer have the full-data likelihood. So we estimate our parameters by minimizing the \\(x\\)-data log-likelihood:\n\\[\n\\begin{eqnarray}\nl(x \\vert  \\lambda, \\mu, \\Sigma) &=& \\sum_{i=1}^{m} \\log p(x_i \\vert  \\lambda,  \\mu ,\\Sigma)   \\nonumber \\\\\n     &=& \\sum_{i=1}^{m} \\log \\sum_z p(x_i \\vert  z_i,  \\mu , \\Sigma) \\, p(z_i \\vert  \\lambda)  \n\\end{eqnarray}\n\\]\nHowever, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we’ll find that it is not possible to find the maximum likelihood estimates Futhermore, we have to enforce constraints such as mixing weights summing to 1, covariance matrices being positive definite, etc.\nFor all of these reasons, its simpler, but not always faster to use an iterative algorithm called the EM algorithm to get the local maximum likelihood or MAP estimate. We shall learn this algorithm soon. But we can also set this problem up as a bayesian problem with reasonable priors on the parameters and try to use MCMC."
  },
  {
    "objectID": "posts/typesoflearning/index.html#semi-supervised",
    "href": "posts/typesoflearning/index.html#semi-supervised",
    "title": "Mixture Models and Types of Learning",
    "section": "",
    "text": "In unsupervized learning we are given samples from some unknown data distribution with density \\(p(x)\\). Our goal is to estimate this density or a known functional (like the risk) thereof. Supervized learning can be treated as estimating \\(p(x,c)\\) or a known functional of it. But there is usually no need to estimate the input distribution so estimating the complete density is wasteful, and we usually focus on estimating \\(p(c \\vert x)\\) discriminatively or generatively(via \\(p(x \\vert c) p(c)\\). Here \\(c\\) or \\(y\\) or \\(z\\) are the classes we are trying to estimate. In the unsupervized case we often estimate \\(\\sum_z p(x \\vert z) p(z) = p(x)\\) with latent (hidden) \\(z\\), which you may or may not wish to identify with classes.\nSemi-supervised learning is the situation in which we have some labels, but typically very few labels: not enough to form a good training set.\nIn semi-supervized learning we combine the two worlds. We write a joint likelihood for the supervised and unsupervised parts:\n\\[ l(\\{x_i\\},\\{x_j\\},\\{z_i\\} \\vert \\theta, \\lambda) = \\sum_i log \\, p(x_i, z_i \\vert \\lambda, \\theta) +  \\sum_j log \\, p(x_j \\vert \\lambda, \\theta) = \\sum_i log \\, p(z_i \\vert \\lambda) p(x_i \\vert z_,\\theta) + \\sum_j log \\, \\sum_z p(z_j \\vert \\lambda) p(x_j \\vert z_j,\\theta) \\]\nHere \\(i\\) ranges over the data points where we have labels, and \\(j\\) over the data points where we dont.\nIn a traditional classification-based machine learning scenario we might still split the data into a training and validation set. But the basic idea in semi-supervised learning is that there is structure in \\(p(x)\\) which might help us divine the conditionals, so what we would want to do, is include in the training set unlabelled points. The game then is that if there is geometric structure in \\(p(x)\\), some kind of cluster based foliation, then we can explot this."
  },
  {
    "objectID": "posts/lawoflargenumbers.html",
    "href": "posts/lawoflargenumbers.html",
    "title": "The LLN",
    "section": "",
    "text": "Suppose that you toss a fair coin and catch it to see if you got heads or tails. Then you have this intuition that while you might get a streak of several heads in a row, in the long run the heads and tails are balanced.\nThis is actually an example of a famous law: the Law of Large numbers (LLN), which states that if you have a random variable X with a mean, the average value of X over a sample of size N converges i.e. gets close and closer to this mean as N becomes larger and larger.\n\n\n\nThe LLN was first proved by Jakob Bernoulli in Ars Conjectandi, published posthumously by his nephew Niklaus Bernoulli, who appropriated entire passages of it for his treatise on law. It is the basis of much of modern statistics, including the Monte-Carlo method.\nLets parse the law. A random variable is one that can take multiple values, each with some probability. So if X represents the flip of a coin, it will take values Heads and Tails with some probability. We’ll assign Heads the value 1 and Tails the value 0.\nThe probabilities attatched to the values a random variable takes is called a distribution, or probability mass function (pmf). For a fair coin, the “Bernoulli” Distribution attaches the probabilities 0.5 to value 1 and 0.5 to value 0. These probabilities must add to 1.\n\n\n\nAn unfair coin thats more likely to land on heads might have a distribution where 0 has attached probability 0.4 and 1 has attached probability 0.6. In this case the mean µ of the distribution is 0.4 x 0 + 0.6 x 1 = 0.6.\n\n\n\nThis mean does not need to be one of the allowed values of the distribution (here 0 and 1). The mean here simply indicates whats more likely: 0.6 means that heads is more likely than tails. What is the mean in the case of the fair coin?\nNow let us simulate the case of the fair coin. We’ll toss a sample of N coins, or 1 coin N times, using the magic of numpy. We’ll find the average of these N tosses. This is the fraction of heads! We’ll plot this sample average against the sample size N.\n\n\n\nWe find that these sample averages are quite close to 0.5. And, as we increase the sample size N, these sample averages become super close to 0.5. Indeed, as N becomes infinite, the sample averages approach the mean µ=0.5. This is the Law of Large Numbers.\n\n\n\nThe LLN can be tautologically used to define the probability of a fair coin showing heads as the asymptotic (infinite N) sampling average. This is the frequentist definition of “sampling probability”, the population frequency µ.\nBut we might also treat the mean µ as an intrinsic fraction of heads, a “parameter” of the Bernoulli distribution. Where does it come from in the first place? The value µ can be thought of as an “inferential probability” derived from symmetry and lack of knowledge.\n\n\n\nIf you have a coin (2 sides, 2 possibilities), and no additional information about the coin and toss physics (thus fair), you would guess fraction µ=0.5 for heads. The LLN then says that sampling probabilities converge to this “inferential probability”.\n\nwh"
  },
  {
    "objectID": "posts/hierarch/index.html",
    "href": "posts/hierarch/index.html",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import beta\nfrom scipy.stats import distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nsns.set_style('whitegrid')\nsns.set_context('poster')\n\n\n\nThe below data is from tumors in female rats of type “F344” that recieve a particular drug, in 70 different experiments.\nThe first column is the number that get the tumor; the second is the total number or rats tested\n\ntumordata=\"\"\"0 20 \n0 20 \n0 20 \n0 20 \n0 20 \n0 20 \n0 20 \n0 19 \n0 19 \n0 19 \n0 19 \n0 18 \n0 18 \n0 17 \n1 20 \n1 20 \n1 20 \n1 20 \n1 19 \n1 19 \n1 18 \n1 18 \n3 27 \n2 25 \n2 24 \n2 23 \n2 20 \n2 20 \n2 20 \n2 20 \n2 20 \n2 20 \n1 10 \n5 49 \n2 19 \n5 46 \n2 17 \n7 49 \n7 47 \n3 20 \n3 20 \n2 13 \n9 48 \n10 50 \n4 20 \n4 20 \n4 20 \n4 20 \n4 20 \n4 20 \n4 20 \n10 48 \n4 19 \n4 19 \n4 19 \n5 22 \n11 46 \n12 49 \n5 20 \n5 20 \n6 23 \n5 19 \n6 22 \n6 20 \n6 20 \n6 20 \n16 52 \n15 46 \n15 47 \n9 24 \n\"\"\"\n\n\ntumortuples=[e.strip().split() for e in tumordata.split(\"\\n\")]\ntumory=np.array([np.int(e[0].strip()) for e in tumortuples if len(e) &gt; 0])\ntumorn=np.array([np.int(e[1].strip()) for e in tumortuples if len(e) &gt; 0])\ntumory, tumorn            \n\n(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,\n         1,  1,  1,  1,  1,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  5,\n         2,  5,  2,  7,  7,  3,  3,  2,  9, 10,  4,  4,  4,  4,  4,  4,  4,\n        10,  4,  4,  4,  5, 11, 12,  5,  5,  6,  5,  6,  6,  6,  6, 16, 15,\n        15,  9]),\n array([20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 20, 20, 20,\n        20, 19, 19, 18, 18, 27, 25, 24, 23, 20, 20, 20, 20, 20, 20, 10, 49,\n        19, 46, 17, 49, 47, 20, 20, 13, 48, 50, 20, 20, 20, 20, 20, 20, 20,\n        48, 19, 19, 19, 22, 46, 49, 20, 20, 23, 19, 22, 20, 20, 20, 52, 46,\n        47, 24]))\n\n\nNow, a 71st experiment is done and we are told that 4 out of 14 rats develop tumors. Our problem is to estimate the risk of tumor in the rats in the 71st experiment .\nThus we are considering the problem of estimating the tumor rate from a small experiment (no 71) and a prior constructed from previous experiments with similar structure. Mathematically, we consider the current and historical experiments as random samples from a common population.\n\ntumor_rat = [e[0]/e[1] for e in zip(tumory, tumorn)]\ntmean = np.mean(tumor_rat)\ntvar = np.var(tumor_rat)\ntmean, tvar\n\n(0.13600653889043893, 0.010557640623609196)\n\n\n\nplt.hist(tumor_rat);\n\n\n\n\n\n\n\n\n\n\n\nIn the \\(j\\)-th historical experiment, let the number of rats with tumors be \\(y_j\\) and the total number of rats be \\(n_j\\). Since the rats either have or dont have the tumor, it makes sense to use a Binomial Model for each experiment, assuming a sample size \\(n_j\\) and a probability \\(\\theta_j\\) that a rat has a tumor. For any one of the experiments\n\\[p(y_i|\\theta_i; n_i) =  Binom(n_i, y_i, \\theta_i) \\]\nand for all the data we have, using \\(Y=[y_1,\\ldots, y_{70}]\\) and \\(\\Theta = [\\theta_1, \\ldots, \\theta_{70}]\\) and I use the notation \\(\\{n_i\\} =[n_1, \\ldots, n_{70}]\\)\n\\[ p(Y|\\Theta; \\{n_i\\}) =  \\prod_{i=1}^{70}Binom(n_i, y_i, \\theta_i) \\]\nWe now need to choose a prior \\(p(\\Theta)\\).\n\n\nOur first thought might be to use a Beta (conjugate prior to Binomial) for each \\(\\theta_i\\), i.e.\n\\[ \\theta_i \\sim Beta(\\alpha_i, \\beta_i).\\]\n\\[p(\\Theta| \\{\\alpha_i\\}, \\{\\beta_i\\}) = \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha_{i}, \\beta_{i}),\\]\nwhere \\(\\alpha_i\\) and \\(\\beta_i\\) are what we called hyperparameters. Again I use the notation \\(\\{\\alpha_i\\}=[\\alpha_1, \\ldots, \\alpha_{70} ]\\) and similarly for \\(\\{beta\\}\\).\nHowever, we would then come up with a very overfit model with 210 parameters and not much data. Besides, there are likely outliers in some experiments, small sample sizes, etc.\nIn your standard bias-variance tradeoff view of things this is a model with a large amount of variance.\n\n\n\nThe other extreme is to assume that there is only one \\(\\theta\\) in the problem, and set an prior on it, making this a three parameter problem. This ignores any variation amongst the sampling units other than sampling variance. Thus such a model might thus be underfit, unable to capture the variations in the samples. This is likely a model with a large amount of variance\n\n\n\n\nLet us compromise and assume that the \\(\\theta_i\\)s are drawn from a “population distribution” given by a conjugate Beta prior \\(Beta(\\alpha, \\beta)\\) with parameters \\(\\alpha\\) and \\(\\beta\\) to describe this data.\n\\[ \\theta_i \\sim Beta(\\alpha, \\beta).\\]\n\\[p(\\Theta | \\alpha, \\beta) = \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha, \\beta).\\]\nThis structure is shown in the diagram below.\n\n\n\nHierarchical model in plate and unrolled form: hyperparameters (A,B) govern unit-level parameters theta_s, which generate observations Y_s.\n\n\n(image from http://seor.vse.gmu.edu/~klaskey/SYST664/Bayes_Unit7.pdf)\nSuch a model is called a hierarchical model, with observable outcomes modeled conditionally on certain parameters(\\(\\theta\\)s) which themselves are given a probabilistic specification in terms of further parameters (\\(\\alpha\\) and \\(\\beta\\)), known as hyperparameters.\n\n\nNow, to complete the story, we need to ask, where do \\(\\alpha\\) and \\(\\beta\\) come from? Why are we calling them hyperparameters? So far, in all the bayesian models we have created, we have assumed known values of the “hyperparameters” in the priors. The criteria for the values we have used have been to create either uninformative or weakly-informative(weakly-regularizing) priors.\nNow we wish to estimate the parameters of these priors themselves from the data. This seems to contravene the Bayesian idea. Why would we want to do this?\nThe key idea here is that some of our units (experiments in our example) are statistically more robust than others. The non-robust experiments may have smaller samples or outlier like behavior, for example. What we wish to do is to borrow strength from all the data as a whole through the estimation of the hyperparameters. In this sense, our procedure will help us create a regularized partial pooling model in which the “lower level” (closer to data) parameter (\\(\\theta\\)s) estimations are tied together by “upper level” parameters.\n\n\n\n\nAnother example comes from kidney cancer rates across counties in the US. Here is a map of counties with the highest kidney cancer rated in blue and the lowest kidney cancer rates in red:\n\n\n\nUS county-level kidney cancer mortality rates: red and blue counties show highest and lowest rates respectively, with extreme rates concentrated in low-population counties.\n\n\nIf you plot the rates against the population of the counties it gives away the story:\n\n\n\nKidney cancer mortality rate vs. county population: small counties show extreme variation (both high and low rates) that shrinks toward the mean as population increases.\n\n\nIt is hard to estimate rates in counties with low populations. 1 case maybe a rate too low or a rate too high.\nHere hierarchical models can also come to the rescue! By assuming that the rates are drawn from a common prior distribution with hyperparameters somehow estimated using all the data, we borrow statistical strength from more populated counties and give it to less populated counties, thus regularizing their rate estimates.\n\n\n\nOur first idea to this is to simply estimate these hyperparameters (\\(\\alpha\\) and \\(\\beta\\)) directly from the data. The idea here is simple. We find the posterior-predictive distribution, as a function of these upper level parameters. Lets call these parameters \\(\\eta\\) (in our case \\(\\eta = (\\alpha, \\beta)\\)).\nThen, for some “new” data \\(y^*\\):\n\\[p(y^* \\vert  D, \\eta) = \\int d\\theta \\, p(y^* \\vert \\theta) \\, p(\\theta \\vert D, \\eta)\\]\nWe notice that this looks like a likelihood with parameters \\(\\eta\\) and simply use maximum-likelihood with respect to \\(\\eta\\) to estimate these \\(\\eta\\) using our “data” \\(y^*\\). But note that, unlike in cross-validation in machine learning, where we have already determined \\(\\theta\\) on the training set, we have not determined \\(\\theta\\) yet here. Indeed, instead of optimizing on \\(\\theta\\), we have marginalized over them. So we can do the optimization to find \\(\\eta\\) directly on our training set, or sample, itself.\nThis method is called Emprical Bayes or Type-2 Maximum Likelihood.\nIn practice, we often match moments of the hyperparameter likelihood with our data. In our example, there are two parameters \\(\\alpha\\) and \\(\\beta\\) to be estimated. By computing the mean and the variance of the type-2 likelihood (the posterior predictive as a function of the hyperparameters) we can solve for both \\(\\alpha\\) and \\(\\beta\\). Sometimes we will use the prior instead: it depends on the meaning of either distribution.\n\n\n\n\n\nEmpirical Bayes approach: point estimates of the hyperparameters alpha and beta yield Beta posterior updates for each unit’s theta_s.\n\n\n(image from http://seor.vse.gmu.edu/~klaskey/SYST664/Bayes_Unit7.pdf)\nWe’ll insert point estimates from the method of moments, used on the prior distribution. Since the prior is the beta distribution, we need to find the mean and variance of it:\n\\[\\mu =  \\frac{\\alpha}{\\alpha + \\beta}\\]\nand\n\\[V  = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\]\nNote that there are different ratios for each experiment, and we are taking the average of these.\n\naplusb = tmean*(1-tmean)/tvar - 1\na_est=aplusb*tmean\nb_est=aplusb*(1.0 - tmean)\na_est, b_est\n\n(1.3777748392916778, 8.7524354471531129)\n\n\nWe can now use these to compute the posterior means for all the experiments.\nThe conditional posterior distribution for each of the \\(\\theta_i\\), given everything else is a Beta distribution itself (remember Beta is conjugate prior to Bionomial).\n\\(p(\\theta_i | y_i, n_i, \\alpha, \\beta) = Beta(\\alpha + y_i, \\beta + n_i - y_i)\\)\nThus the posterior mean is\n\\[\\bar{\\theta}_{post, i} = \\frac{\\alpha + y_i}{\\alpha + \\beta + n_i}\\]\n\npost_means = (a_est + tumory)/(a_est + b_est + tumorn)\n\n\nplt.plot(tumor_rat, post_means,'o')\nplt.plot([0,0.5],[0,0.5],'k-')\nplt.xlabel(\"observed rates\")\nplt.ylabel(\"posterior means under EB\");\n\n\n\n\n\n\n\n\nAs you can see, the posterior rates are shrunk towards flatness, which would correspond to complete pooling. The 45 degree line would be for completely unpooled estimate.\nNow, for the 71st experiment, we have 4 out of 14 rats having tumors. The posterior estimate for this would be\n\\[\\frac{\\alpha + y_{71}}{\\alpha + \\beta + n_{71}}\\]\n\n4/14, (4+a_est)/(14+a_est+b_est)\n\n(0.2857142857142857, 0.22286481449822493)\n\n\nSo we would revise our estimate downwards for this experiment.\n\n\n\n\nEmpirical bayes seems a nice procedure, but suffers from the problem that one is still carrying out an optimization, even if with respect to the hyperparameters \\(\\eta\\). And every optimization is a chance to overfit.\nWhat we would like to do is to use integration all the way.\nTo do that we need to specify a hyper-prior \\(p(\\eta)\\) (\\(p(\\alpha, \\beta)\\)) on these hyperparameters \\(\\eta\\) (\\(\\alpha, \\beta\\)). The idea of the hyper-prior is the same as that of a prior; except that the hyperpriors are priors on prior parameters. In other words, we have pushed things one level up in the hierarchy.\nWe could go turtles all the way and create hyper-hyper-priors on the hyper-hyper-parameters of the hyper-prior. However , we typically use uninformative hyperpriors instead: indeed this is one of the key strategies of hierarchical modelling: un-informative priors are pushed down the prior hierarchy.\nModeling in this way has two advantages: (a) as we shall see, it helps us develop a computational strategy to solve the problem which naturally relies on the structure of gibbs sampling, and (b) similar to empirical-bayes, it allows estimates of the probabilities of any one of the units (here, one of the experiments) to borrow strength from all the data as a whole through the finding of posterior distributions on the hyperparameters, instead of just fitting for them. In other words, if some of the experiments had lower sample sizes or other outlier like behavior, the procedure helps regularize this, with the additional smearing from the hyper-parameter posteriors mitigating any overfitting.\n\n\nWe write out a joint posterior distribution for the \\(\\theta\\)s, \\(\\alpha\\) and \\(\\beta\\).\n\\[p( \\theta_i, \\alpha, \\beta | y_i, n_i) \\propto p(\\alpha, \\beta) \\, p(\\theta_i | \\alpha, \\beta) \\, p(y_i | \\theta_i, n_i,\\alpha, \\beta)\\]\nor for the whole dataset:\n\\[ p( \\Theta, \\alpha, \\beta | Y, \\{n_i\\}) \\propto p(\\alpha, \\beta) \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha, \\beta) \\prod_{i=1}^{70} Binom(n_i, y_i, \\theta_i)\\]\nNote that this is a high dimensional problem: there are 72 parameters (70 \\(\\theta\\)s and \\(\\alpha , \\beta\\)).\nThe conditional posterior distribution for each of the \\(\\theta_i\\), given everything else is a Beta distribution itself (remember Beta is conjugate prior to Bionomial).\n\\(p(\\theta_i | y_i, n_i, \\alpha, \\beta) = Beta(\\alpha + y_i, \\beta + n_i - y_i)\\)\nFor each of \\(\\alpha\\) and \\(\\beta\\), given everything else, the posterior distributions can be shown to be:\n\\[p(\\alpha | Y, \\Theta ,\\beta ) \\propto p(\\alpha, \\beta) \\, \\left(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)}\\right)^N \\prod_{i=1}^{N} \\theta_i^{\\alpha}\\]\n\\[P(\\beta | Y, \\Theta ,\\alpha ) \\propto p(\\alpha, \\beta) \\, \\left(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\beta)}\\right)^N \\prod_{i=1}^{N} (1 - \\theta_i)^{\\beta}\\]\nNote: The conditional posteriors do depend on \\(Y\\) and \\(\\{n\\}\\) via the \\(\\theta\\)’s.\nNow notice something: if \\(\\alpha\\) and \\(\\beta\\) are fixed I cn easily sample from any of the 70 \\(\\theta_i\\)s. And I can do this in order. Thus we have a Gibbs step for the \\(\\theta_i\\)s.\nThe sampling for \\(\\alpha\\) and \\(\\beta\\) is a bit more complex. From the expressions, its clear that these do not split out in a simple fashion, leaving us known distributions that would allow for straightforward gibbs sampling. But we can use metropolis steps with normal proposals for both. So, when we sample for \\(\\alpha\\), we will propose a new value using a normal proposal, while holding all the \\(\\theta\\)s and \\(\\beta\\) constant at the old value.\nWe will write the sampler for this model in lab.\n\n\n\n\nHierarchical models provide a simple way to organize inference into a directed acyclic graph, with the observations layer at the bottom of a tree, the next layer being the intermediate parameters, and the upper layers being the hyper-parameters, as we have seen above. This graph structure allows us to sample conditionals easily.\nWhen we are sampling up the graph, a node depends only on its parents all the way to the root. So, to sample a \\(y_5\\) (predictive) one needs only \\(\\theta_5\\), \\(\\alpha\\), and \\(\\beta\\), and nothing else. This structure allows one to factorize a simple conditional structure and use a gibbs sampler upwards. When trying to go downwards, all we need to do is to use metropolis or metropolis hastings steps in a componentwise fashion.\nGibbs is easiest to use when one can directly sample from a conditional, as in the case of the \\(\\theta\\)s. But once stationarity has been reached, one can always sample using MH from a non-simple-to-sample conditional. This makes Gibbs very generally applicable and is the basis for software packages such as BUGS and JAGS.\n(one could combine multiple components into a block if it made sense..for example, to sample from a 3D gaussian one could alternatively sample from a 1-D gaussian and a 2-D gaussian).\nIn the case that we can use conjugacy like we did above, this simplifies even further, because the draws then are from members of the exponential family for which sampling is well established.\nHierarchical modelling also provides us a disciplined way to think about exchangeability in modelling, which is very important to understand as it guides us in setting up models correctly.\n\n\nThe iid assumption in statistics is an assumption that the values \\(y_i\\) that go into a density (or likelihood) are exchangeable. That is, the likelihood is invariant to the permutation of data indices. If one has covariates, we are then talking about the joint density \\(p(x,y)\\) or \\(p(x,y \\vert \\theta)\\) and it is \\((x,y)\\) thats assumed to be the unit of permutation.\nIn practice, ignorance implies exchangeability. Maximal exchangeability is indeed the argument underlying maximum entropy.\nIn hierarchical models, we use the notion of exchangeability at the level of ‘units’. By units we mean an observationally cohesive bunch of data. For example, our unit may be observations in a particular experiment, and then hierarchically, we might talk about exchangeability between different experiments. For our rats, the \\(y_j\\) were exchangeable since we had no additional information about experimental conditions. But if we knew that specific groups of experiments came from specific laboratories, we would now only assume partial exchangeability. We’d assume that experiments were interchangeable if they came from the same lab. So then we’d have lab specific \\(\\alpha\\) and \\(\\beta\\) parameters, and add another level of hierarchy to figure how these might themselves be drawn from some common distribution."
  },
  {
    "objectID": "posts/hierarch/index.html#an-example-rats-tumors-from-gelman-chapter-5",
    "href": "posts/hierarch/index.html#an-example-rats-tumors-from-gelman-chapter-5",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "The below data is from tumors in female rats of type “F344” that recieve a particular drug, in 70 different experiments.\nThe first column is the number that get the tumor; the second is the total number or rats tested\n\ntumordata=\"\"\"0 20 \n0 20 \n0 20 \n0 20 \n0 20 \n0 20 \n0 20 \n0 19 \n0 19 \n0 19 \n0 19 \n0 18 \n0 18 \n0 17 \n1 20 \n1 20 \n1 20 \n1 20 \n1 19 \n1 19 \n1 18 \n1 18 \n3 27 \n2 25 \n2 24 \n2 23 \n2 20 \n2 20 \n2 20 \n2 20 \n2 20 \n2 20 \n1 10 \n5 49 \n2 19 \n5 46 \n2 17 \n7 49 \n7 47 \n3 20 \n3 20 \n2 13 \n9 48 \n10 50 \n4 20 \n4 20 \n4 20 \n4 20 \n4 20 \n4 20 \n4 20 \n10 48 \n4 19 \n4 19 \n4 19 \n5 22 \n11 46 \n12 49 \n5 20 \n5 20 \n6 23 \n5 19 \n6 22 \n6 20 \n6 20 \n6 20 \n16 52 \n15 46 \n15 47 \n9 24 \n\"\"\"\n\n\ntumortuples=[e.strip().split() for e in tumordata.split(\"\\n\")]\ntumory=np.array([np.int(e[0].strip()) for e in tumortuples if len(e) &gt; 0])\ntumorn=np.array([np.int(e[1].strip()) for e in tumortuples if len(e) &gt; 0])\ntumory, tumorn            \n\n(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,\n         1,  1,  1,  1,  1,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  1,  5,\n         2,  5,  2,  7,  7,  3,  3,  2,  9, 10,  4,  4,  4,  4,  4,  4,  4,\n        10,  4,  4,  4,  5, 11, 12,  5,  5,  6,  5,  6,  6,  6,  6, 16, 15,\n        15,  9]),\n array([20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 18, 18, 17, 20, 20, 20,\n        20, 19, 19, 18, 18, 27, 25, 24, 23, 20, 20, 20, 20, 20, 20, 10, 49,\n        19, 46, 17, 49, 47, 20, 20, 13, 48, 50, 20, 20, 20, 20, 20, 20, 20,\n        48, 19, 19, 19, 22, 46, 49, 20, 20, 23, 19, 22, 20, 20, 20, 52, 46,\n        47, 24]))\n\n\nNow, a 71st experiment is done and we are told that 4 out of 14 rats develop tumors. Our problem is to estimate the risk of tumor in the rats in the 71st experiment .\nThus we are considering the problem of estimating the tumor rate from a small experiment (no 71) and a prior constructed from previous experiments with similar structure. Mathematically, we consider the current and historical experiments as random samples from a common population.\n\ntumor_rat = [e[0]/e[1] for e in zip(tumory, tumorn)]\ntmean = np.mean(tumor_rat)\ntvar = np.var(tumor_rat)\ntmean, tvar\n\n(0.13600653889043893, 0.010557640623609196)\n\n\n\nplt.hist(tumor_rat);"
  },
  {
    "objectID": "posts/hierarch/index.html#setting-up-the-model",
    "href": "posts/hierarch/index.html#setting-up-the-model",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "In the \\(j\\)-th historical experiment, let the number of rats with tumors be \\(y_j\\) and the total number of rats be \\(n_j\\). Since the rats either have or dont have the tumor, it makes sense to use a Binomial Model for each experiment, assuming a sample size \\(n_j\\) and a probability \\(\\theta_j\\) that a rat has a tumor. For any one of the experiments\n\\[p(y_i|\\theta_i; n_i) =  Binom(n_i, y_i, \\theta_i) \\]\nand for all the data we have, using \\(Y=[y_1,\\ldots, y_{70}]\\) and \\(\\Theta = [\\theta_1, \\ldots, \\theta_{70}]\\) and I use the notation \\(\\{n_i\\} =[n_1, \\ldots, n_{70}]\\)\n\\[ p(Y|\\Theta; \\{n_i\\}) =  \\prod_{i=1}^{70}Binom(n_i, y_i, \\theta_i) \\]\nWe now need to choose a prior \\(p(\\Theta)\\).\n\n\nOur first thought might be to use a Beta (conjugate prior to Binomial) for each \\(\\theta_i\\), i.e.\n\\[ \\theta_i \\sim Beta(\\alpha_i, \\beta_i).\\]\n\\[p(\\Theta| \\{\\alpha_i\\}, \\{\\beta_i\\}) = \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha_{i}, \\beta_{i}),\\]\nwhere \\(\\alpha_i\\) and \\(\\beta_i\\) are what we called hyperparameters. Again I use the notation \\(\\{\\alpha_i\\}=[\\alpha_1, \\ldots, \\alpha_{70} ]\\) and similarly for \\(\\{beta\\}\\).\nHowever, we would then come up with a very overfit model with 210 parameters and not much data. Besides, there are likely outliers in some experiments, small sample sizes, etc.\nIn your standard bias-variance tradeoff view of things this is a model with a large amount of variance.\n\n\n\nThe other extreme is to assume that there is only one \\(\\theta\\) in the problem, and set an prior on it, making this a three parameter problem. This ignores any variation amongst the sampling units other than sampling variance. Thus such a model might thus be underfit, unable to capture the variations in the samples. This is likely a model with a large amount of variance"
  },
  {
    "objectID": "posts/hierarch/index.html#partial-pooling-and-the-hierarchical-model",
    "href": "posts/hierarch/index.html#partial-pooling-and-the-hierarchical-model",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "Let us compromise and assume that the \\(\\theta_i\\)s are drawn from a “population distribution” given by a conjugate Beta prior \\(Beta(\\alpha, \\beta)\\) with parameters \\(\\alpha\\) and \\(\\beta\\) to describe this data.\n\\[ \\theta_i \\sim Beta(\\alpha, \\beta).\\]\n\\[p(\\Theta | \\alpha, \\beta) = \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha, \\beta).\\]\nThis structure is shown in the diagram below.\n\n\n\nHierarchical model in plate and unrolled form: hyperparameters (A,B) govern unit-level parameters theta_s, which generate observations Y_s.\n\n\n(image from http://seor.vse.gmu.edu/~klaskey/SYST664/Bayes_Unit7.pdf)\nSuch a model is called a hierarchical model, with observable outcomes modeled conditionally on certain parameters(\\(\\theta\\)s) which themselves are given a probabilistic specification in terms of further parameters (\\(\\alpha\\) and \\(\\beta\\)), known as hyperparameters.\n\n\nNow, to complete the story, we need to ask, where do \\(\\alpha\\) and \\(\\beta\\) come from? Why are we calling them hyperparameters? So far, in all the bayesian models we have created, we have assumed known values of the “hyperparameters” in the priors. The criteria for the values we have used have been to create either uninformative or weakly-informative(weakly-regularizing) priors.\nNow we wish to estimate the parameters of these priors themselves from the data. This seems to contravene the Bayesian idea. Why would we want to do this?\nThe key idea here is that some of our units (experiments in our example) are statistically more robust than others. The non-robust experiments may have smaller samples or outlier like behavior, for example. What we wish to do is to borrow strength from all the data as a whole through the estimation of the hyperparameters. In this sense, our procedure will help us create a regularized partial pooling model in which the “lower level” (closer to data) parameter (\\(\\theta\\)s) estimations are tied together by “upper level” parameters."
  },
  {
    "objectID": "posts/hierarch/index.html#another-example-kidney-cancer",
    "href": "posts/hierarch/index.html#another-example-kidney-cancer",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "Another example comes from kidney cancer rates across counties in the US. Here is a map of counties with the highest kidney cancer rated in blue and the lowest kidney cancer rates in red:\n\n\n\nUS county-level kidney cancer mortality rates: red and blue counties show highest and lowest rates respectively, with extreme rates concentrated in low-population counties.\n\n\nIf you plot the rates against the population of the counties it gives away the story:\n\n\n\nKidney cancer mortality rate vs. county population: small counties show extreme variation (both high and low rates) that shrinks toward the mean as population increases.\n\n\nIt is hard to estimate rates in counties with low populations. 1 case maybe a rate too low or a rate too high.\nHere hierarchical models can also come to the rescue! By assuming that the rates are drawn from a common prior distribution with hyperparameters somehow estimated using all the data, we borrow statistical strength from more populated counties and give it to less populated counties, thus regularizing their rate estimates."
  },
  {
    "objectID": "posts/hierarch/index.html#empirical-bayes",
    "href": "posts/hierarch/index.html#empirical-bayes",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "Our first idea to this is to simply estimate these hyperparameters (\\(\\alpha\\) and \\(\\beta\\)) directly from the data. The idea here is simple. We find the posterior-predictive distribution, as a function of these upper level parameters. Lets call these parameters \\(\\eta\\) (in our case \\(\\eta = (\\alpha, \\beta)\\)).\nThen, for some “new” data \\(y^*\\):\n\\[p(y^* \\vert  D, \\eta) = \\int d\\theta \\, p(y^* \\vert \\theta) \\, p(\\theta \\vert D, \\eta)\\]\nWe notice that this looks like a likelihood with parameters \\(\\eta\\) and simply use maximum-likelihood with respect to \\(\\eta\\) to estimate these \\(\\eta\\) using our “data” \\(y^*\\). But note that, unlike in cross-validation in machine learning, where we have already determined \\(\\theta\\) on the training set, we have not determined \\(\\theta\\) yet here. Indeed, instead of optimizing on \\(\\theta\\), we have marginalized over them. So we can do the optimization to find \\(\\eta\\) directly on our training set, or sample, itself.\nThis method is called Emprical Bayes or Type-2 Maximum Likelihood.\nIn practice, we often match moments of the hyperparameter likelihood with our data. In our example, there are two parameters \\(\\alpha\\) and \\(\\beta\\) to be estimated. By computing the mean and the variance of the type-2 likelihood (the posterior predictive as a function of the hyperparameters) we can solve for both \\(\\alpha\\) and \\(\\beta\\). Sometimes we will use the prior instead: it depends on the meaning of either distribution.\n\n\n\n\n\nEmpirical Bayes approach: point estimates of the hyperparameters alpha and beta yield Beta posterior updates for each unit’s theta_s.\n\n\n(image from http://seor.vse.gmu.edu/~klaskey/SYST664/Bayes_Unit7.pdf)\nWe’ll insert point estimates from the method of moments, used on the prior distribution. Since the prior is the beta distribution, we need to find the mean and variance of it:\n\\[\\mu =  \\frac{\\alpha}{\\alpha + \\beta}\\]\nand\n\\[V  = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\]\nNote that there are different ratios for each experiment, and we are taking the average of these.\n\naplusb = tmean*(1-tmean)/tvar - 1\na_est=aplusb*tmean\nb_est=aplusb*(1.0 - tmean)\na_est, b_est\n\n(1.3777748392916778, 8.7524354471531129)\n\n\nWe can now use these to compute the posterior means for all the experiments.\nThe conditional posterior distribution for each of the \\(\\theta_i\\), given everything else is a Beta distribution itself (remember Beta is conjugate prior to Bionomial).\n\\(p(\\theta_i | y_i, n_i, \\alpha, \\beta) = Beta(\\alpha + y_i, \\beta + n_i - y_i)\\)\nThus the posterior mean is\n\\[\\bar{\\theta}_{post, i} = \\frac{\\alpha + y_i}{\\alpha + \\beta + n_i}\\]\n\npost_means = (a_est + tumory)/(a_est + b_est + tumorn)\n\n\nplt.plot(tumor_rat, post_means,'o')\nplt.plot([0,0.5],[0,0.5],'k-')\nplt.xlabel(\"observed rates\")\nplt.ylabel(\"posterior means under EB\");\n\n\n\n\n\n\n\n\nAs you can see, the posterior rates are shrunk towards flatness, which would correspond to complete pooling. The 45 degree line would be for completely unpooled estimate.\nNow, for the 71st experiment, we have 4 out of 14 rats having tumors. The posterior estimate for this would be\n\\[\\frac{\\alpha + y_{71}}{\\alpha + \\beta + n_{71}}\\]\n\n4/14, (4+a_est)/(14+a_est+b_est)\n\n(0.2857142857142857, 0.22286481449822493)\n\n\nSo we would revise our estimate downwards for this experiment."
  },
  {
    "objectID": "posts/hierarch/index.html#a-fully-bayesian-treatment",
    "href": "posts/hierarch/index.html#a-fully-bayesian-treatment",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "Empirical bayes seems a nice procedure, but suffers from the problem that one is still carrying out an optimization, even if with respect to the hyperparameters \\(\\eta\\). And every optimization is a chance to overfit.\nWhat we would like to do is to use integration all the way.\nTo do that we need to specify a hyper-prior \\(p(\\eta)\\) (\\(p(\\alpha, \\beta)\\)) on these hyperparameters \\(\\eta\\) (\\(\\alpha, \\beta\\)). The idea of the hyper-prior is the same as that of a prior; except that the hyperpriors are priors on prior parameters. In other words, we have pushed things one level up in the hierarchy.\nWe could go turtles all the way and create hyper-hyper-priors on the hyper-hyper-parameters of the hyper-prior. However , we typically use uninformative hyperpriors instead: indeed this is one of the key strategies of hierarchical modelling: un-informative priors are pushed down the prior hierarchy.\nModeling in this way has two advantages: (a) as we shall see, it helps us develop a computational strategy to solve the problem which naturally relies on the structure of gibbs sampling, and (b) similar to empirical-bayes, it allows estimates of the probabilities of any one of the units (here, one of the experiments) to borrow strength from all the data as a whole through the finding of posterior distributions on the hyperparameters, instead of just fitting for them. In other words, if some of the experiments had lower sample sizes or other outlier like behavior, the procedure helps regularize this, with the additional smearing from the hyper-parameter posteriors mitigating any overfitting.\n\n\nWe write out a joint posterior distribution for the \\(\\theta\\)s, \\(\\alpha\\) and \\(\\beta\\).\n\\[p( \\theta_i, \\alpha, \\beta | y_i, n_i) \\propto p(\\alpha, \\beta) \\, p(\\theta_i | \\alpha, \\beta) \\, p(y_i | \\theta_i, n_i,\\alpha, \\beta)\\]\nor for the whole dataset:\n\\[ p( \\Theta, \\alpha, \\beta | Y, \\{n_i\\}) \\propto p(\\alpha, \\beta) \\prod_{i=1}^{70} Beta(\\theta_i, \\alpha, \\beta) \\prod_{i=1}^{70} Binom(n_i, y_i, \\theta_i)\\]\nNote that this is a high dimensional problem: there are 72 parameters (70 \\(\\theta\\)s and \\(\\alpha , \\beta\\)).\nThe conditional posterior distribution for each of the \\(\\theta_i\\), given everything else is a Beta distribution itself (remember Beta is conjugate prior to Bionomial).\n\\(p(\\theta_i | y_i, n_i, \\alpha, \\beta) = Beta(\\alpha + y_i, \\beta + n_i - y_i)\\)\nFor each of \\(\\alpha\\) and \\(\\beta\\), given everything else, the posterior distributions can be shown to be:\n\\[p(\\alpha | Y, \\Theta ,\\beta ) \\propto p(\\alpha, \\beta) \\, \\left(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)}\\right)^N \\prod_{i=1}^{N} \\theta_i^{\\alpha}\\]\n\\[P(\\beta | Y, \\Theta ,\\alpha ) \\propto p(\\alpha, \\beta) \\, \\left(\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\beta)}\\right)^N \\prod_{i=1}^{N} (1 - \\theta_i)^{\\beta}\\]\nNote: The conditional posteriors do depend on \\(Y\\) and \\(\\{n\\}\\) via the \\(\\theta\\)’s.\nNow notice something: if \\(\\alpha\\) and \\(\\beta\\) are fixed I cn easily sample from any of the 70 \\(\\theta_i\\)s. And I can do this in order. Thus we have a Gibbs step for the \\(\\theta_i\\)s.\nThe sampling for \\(\\alpha\\) and \\(\\beta\\) is a bit more complex. From the expressions, its clear that these do not split out in a simple fashion, leaving us known distributions that would allow for straightforward gibbs sampling. But we can use metropolis steps with normal proposals for both. So, when we sample for \\(\\alpha\\), we will propose a new value using a normal proposal, while holding all the \\(\\theta\\)s and \\(\\beta\\) constant at the old value.\nWe will write the sampler for this model in lab."
  },
  {
    "objectID": "posts/hierarch/index.html#why-are-hierarchical-models-useful",
    "href": "posts/hierarch/index.html#why-are-hierarchical-models-useful",
    "title": "Hierarchical Bayesian Models",
    "section": "",
    "text": "Hierarchical models provide a simple way to organize inference into a directed acyclic graph, with the observations layer at the bottom of a tree, the next layer being the intermediate parameters, and the upper layers being the hyper-parameters, as we have seen above. This graph structure allows us to sample conditionals easily.\nWhen we are sampling up the graph, a node depends only on its parents all the way to the root. So, to sample a \\(y_5\\) (predictive) one needs only \\(\\theta_5\\), \\(\\alpha\\), and \\(\\beta\\), and nothing else. This structure allows one to factorize a simple conditional structure and use a gibbs sampler upwards. When trying to go downwards, all we need to do is to use metropolis or metropolis hastings steps in a componentwise fashion.\nGibbs is easiest to use when one can directly sample from a conditional, as in the case of the \\(\\theta\\)s. But once stationarity has been reached, one can always sample using MH from a non-simple-to-sample conditional. This makes Gibbs very generally applicable and is the basis for software packages such as BUGS and JAGS.\n(one could combine multiple components into a block if it made sense..for example, to sample from a 3D gaussian one could alternatively sample from a 1-D gaussian and a 2-D gaussian).\nIn the case that we can use conjugacy like we did above, this simplifies even further, because the draws then are from members of the exponential family for which sampling is well established.\nHierarchical modelling also provides us a disciplined way to think about exchangeability in modelling, which is very important to understand as it guides us in setting up models correctly.\n\n\nThe iid assumption in statistics is an assumption that the values \\(y_i\\) that go into a density (or likelihood) are exchangeable. That is, the likelihood is invariant to the permutation of data indices. If one has covariates, we are then talking about the joint density \\(p(x,y)\\) or \\(p(x,y \\vert \\theta)\\) and it is \\((x,y)\\) thats assumed to be the unit of permutation.\nIn practice, ignorance implies exchangeability. Maximal exchangeability is indeed the argument underlying maximum entropy.\nIn hierarchical models, we use the notion of exchangeability at the level of ‘units’. By units we mean an observationally cohesive bunch of data. For example, our unit may be observations in a particular experiment, and then hierarchically, we might talk about exchangeability between different experiments. For our rats, the \\(y_j\\) were exchangeable since we had no additional information about experimental conditions. But if we knew that specific groups of experiments came from specific laboratories, we would now only assume partial exchangeability. We’d assume that experiments were interchangeable if they came from the same lab. So then we’d have lab specific \\(\\alpha\\) and \\(\\beta\\) parameters, and add another level of hierarchy to figure how these might themselves be drawn from some common distribution."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html",
    "href": "posts/basicmontecarlo/index.html",
    "title": "Basic Monte Carlo",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('talk')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#monte-carlo",
    "href": "posts/basicmontecarlo/index.html#monte-carlo",
    "title": "Basic Monte Carlo",
    "section": "Monte Carlo",
    "text": "Monte Carlo\nThe basic idea of a Monte Carlo Algorithm is to use randomness to solve what is often a deterministic problem. In this course, we’ll study their application in 3 different places: optimization, integration, and obtaining draws from a probability distribution. These uses are often intertwined: optimization is needed to find modes of distributions and integration to find expectations.\nWikipedia has a facinating bit of history on the subject, from which I quote:\n\nThe first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a Canfield solitaire laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than “abstract thinking” might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to John von Neumann, and we began to plan actual calculations. –Stanislaw Ulam\n\n\nBeing secret, the work of von Neumann and Ulam required a code name.[citation needed] A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name Monte Carlo, which refers to the Monte Carlo Casino in Monaco where Ulam’s uncle would borrow money from relatives to gamble."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#estimate-the-area-of-a-unit-circle",
    "href": "posts/basicmontecarlo/index.html#estimate-the-area-of-a-unit-circle",
    "title": "Basic Monte Carlo",
    "section": "Estimate the area of a unit circle",
    "text": "Estimate the area of a unit circle\nTo understand how randomness can be brought to bear upon solving deterministic problems, consider a very simple example: the value of \\(\\pi\\). If you could uniformly generate random numbers on a square, you could ask, how many of these numbers would fall inside a unit circle embedded in and touching the midpoints of the sides of the square. This ratio would be\n\\[\\frac{\\pi \\times 1^2}{2^2} = \\frac{\\pi}{4}.\\]\n\n#area of the bounding box\nbox_area = 4.0    \n\n#number of samples\nN_total = 10000.0 \n\n#drawing random points uniform between -1 and 1\nX = np.random.uniform(low=-1, high=1, size=N_total)  \nY = np.random.uniform(low=-1, high=1, size=N_total)   \n\n# calculate the distance of the points from the center \ndistance = np.sqrt(X**2+Y**2);  \n \n# check if point is inside the circle    \nis_point_inside = distance&lt;1.0\n\n# sum up the hits inside the circle\nN_inside=np.sum(is_point_inside)\n\n# estimate the circle area\ncircle_area = box_area * N_inside/N_total\n\n# some nice visualization\nplt.scatter(X,Y, c=is_point_inside, s=5.0, edgecolors='none', cmap=plt.cm.Paired)  \nplt.axis('equal')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# text output\nprint(\"Area of the circle = \", circle_area)\nprint(\"pi = \", np.pi)\n\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n\n\nArea of the circle =  3.1344\npi =  3.141592653589793\n\n\n\n\n\n\n\n\n\nIntuitively, one might expect our estimate of \\(\\pi\\) to get better as we draw more and more samples: we are covering the areas with samples much better when we do that.\nLets try to think about the mathematics in the intuition which tells us that we can calculate \\(\\pi\\) in this way.\nThe area of the circle C can be obtained by computing a double integral like so:\n\\[A = \\int_x \\int_y I_{\\in C}(x, y) dx dy = \\int \\int_{\\in C} dx dy \\]\nwhere \\(I_{\\in C} (x, y) = 1\\) if \\(x,y \\in C\\) and \\(I_{\\in C}(x) = 0\\) if \\(x,y \\notin C\\).\nThis is basically adding up all the small area elements inside the circle.\nRemember from the LOTUS:\n\\[E_f[I_{\\in C} (X,Y)] = \\int I_{\\in C} (X,Y) dF(X,Y) = \\int_{\\in C} dF(X,Y) = \\int \\int_{\\in C} f_{X,Y} (x,y) dx dy = p(X,Y \\in C)\\]\nand then we can use the law of large numbers to calculate this expectation and thus this probability.\nThe relationship of the expression all the way on the left to that all the way on the right is simply the law of large numbers we saw before. This is a distribution independent statement.\nBut the critical thing to notice is that:\n\\[\\int \\int_{\\in C} f_{X,Y} (x,y) dx dy  =  \\frac{1}{V} \\int \\int_{\\in C}  dx dy = E_f[I_{\\in C} (X,Y)]\\]\nonce we choose a uniform distribution. Here \\(V\\) is the support, the normalizing factor..here 4. The expectation from the law of large numbers comes from a sequence of identically distributed bernoullis (independent of \\(f\\) which here is uniform). All we have to do, is just like before in the law, count the frequency of samples inside."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#hit-or-miss-method",
    "href": "posts/basicmontecarlo/index.html#hit-or-miss-method",
    "title": "Basic Monte Carlo",
    "section": "Hit or miss method",
    "text": "Hit or miss method\nThis simple scenario of inside-or-outside can be used as a general (but poor, as missing increases exponentially with dimension) way to use the generation of samples to carry out integration\n\n\n\nBounding box for hit-or-miss Monte Carlo integration\n\n\nYou basically generate samples from a uniform distribution with support on the rectangle and see how many fall below \\(y(x)\\) at a specific x.\nThis is the basic idea behind rejection sampling"
  },
  {
    "objectID": "posts/jensens/index.html",
    "href": "posts/jensens/index.html",
    "title": "Convexity and Jensen’s Inequality",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/jensens/index.html#convexity",
    "href": "posts/jensens/index.html#convexity",
    "title": "Convexity and Jensen’s Inequality",
    "section": "Convexity",
    "text": "Convexity\nLet \\(f\\) be a function with domain the set of real numbers. If the second derivative is greater than zero for all \\(x\\in R\\) this function is convex.\nConsider the case of two random variables \\(x_1\\) and \\(x_2\\), as seen in the diagram below:\n\n\n\nJensen’s inequality: for a convex function, the weighted average of function values always lies above the function of the weighted average.\n\n\nDefnition Let f be a real valued function defined on an interval \\(I = [a, b]\\). \\(f\\) is said to be convex on I if \\(\\forall x_1, x_2 \\in I, \\lambda \\in [0, 1]\\),\n\\[\\begin{equation}\nf(\\lambda x_1 + (1 - \\lambda)\\,x_2) \\le \\lambda f(x_1) + (1- \\lambda)\\,f(x_2).\n\\end{equation}\\]\n\\(f\\) is said to be strictly convex if the inequality is strict. Intuitively, this definition states that the function falls below the straight line (the secant) from points \\((x_1, f(x_1))\\) to \\((x_2, f(x_2))\\). In other words, the equality is satisfied only for \\(\\lambda = 0\\) and \\(\\lambda = 1\\)."
  },
  {
    "objectID": "posts/jensens/index.html#jensens-inequality",
    "href": "posts/jensens/index.html#jensens-inequality",
    "title": "Convexity and Jensen’s Inequality",
    "section": "Jensen’s Inequality",
    "text": "Jensen’s Inequality\nLet \\(f\\) be a convex function defined on an interval \\(I\\). If \\(x_1,x_2,\\dots,x_n \\in I {\\rm and} \\lambda_1, \\lambda_2,\\ldots,\\lambda_n \\ge  0\\) with \\(\\sum^n_{i=1} \\lambda_i = 1\\),\n\\[\\begin{equation}\nf \\left( \\sum_{i=1}^n \\lambda_i \\,  x_i \\right) \\le  \\sum_{i=1}^n \\lambda_i f(x_i)\n\\end{equation}\\]\nProof: For \\(n = 1\\) this is trivial. The case \\(n = 2\\) corresponds to the definition of convexity (see above). To show that this is true for all natural numbers, we proceed by induction. Assume the theorem is true for some \\(n\\) then,\n\\[\n\\begin{eqnarray}\nf \\left( \\sum_{i=1}^{n+1} \\lambda_i \\,  x_i \\right) &=& f\\left( \\lambda_{n+1} x_{n+1} + \\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n        &=&  f\\left( \\lambda_{n+1} x_{n+1} + \\frac{(1-\\lambda_{n+1})}{(1-\\lambda_{n+1})}\\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n        & \\le & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1}) f \\left( \\frac{1}{(1-\\lambda_{n+1})} \\sum_{i=1}^n \\lambda_i \\,  x_i  \\right) \\nonumber \\\\\n        & = & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1}) f \\left( \\sum_{i=1}^n \\frac{\\lambda_i}{(1-\\lambda_{n+1})} \\,  x_i  \\right)  \\nonumber \\\\\n        & \\le & \\lambda_{n+1} f(x_{n+1}) + (1-\\lambda_{n+1})  \\sum_{i=1}^n \\frac{\\lambda_i}{(1-\\lambda_{n+1})} \\,  f(x_i)  \\nonumber \\\\\n        & =&  \\lambda_{n+1} f(x_{n+1}) + \\sum_{i=1}^n \\lambda_i f(x_i) \\nonumber \\\\\n        & =&  \\sum_{i=1}^{n+1}  \\lambda_i f(x_i)\n\\end{eqnarray}\n\\]\nBy interpreting the \\(\\lambda_i\\) as the probability distribution over a discrete variable \\(x\\) taking the values \\(\\{x_i\\}\\):\n\\[f(\\mathrm{E}[x]) \\le \\mathrm{E}[f(x)]\\]\nTheorem. Let \\(f\\) be a convex function, and \\(X\\) be a random variable, then\n\\[ E[f(X)] \\ge f(E[X]) \\]\nFurthermore, if \\(f\\) is stricly convex (i.e. \\(f''(x)&gt;0\\)), then \\(E[f(x)]=f(E[X])\\) only if \\(X=E[X]\\) with probability 1 (\\(X\\) is constant)."
  },
  {
    "objectID": "posts/markov/index.html",
    "href": "posts/markov/index.html",
    "title": "Markov Chains and MCMC",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nMarkov Chains are the first example of a stochastic process we will see in this class. The values in a Markov chain depend on the previous values (probabilistically), with the defining characteristic being that a given value is depenendent only on the immediate previous value.\nThis is certainly not IID (idenpendently and identically distributed) data, which is what we have been assuming so far and will generally assume in this course, unless specified otherwise…\nDefinition: A sequence of random variables taking values in a state space is called a Markov Chain if the probability of the next step only depends on the current state.\nUsing the notation of transition probabilities to define the probability of going from state \\(x\\) ot state \\(y\\) as \\(T(x \\vert y)\\), we can write this mathematically:\n\\[T(x_n \\vert x_{n-1}, x_{n-1}..., x_1) = T(x_n \\vert x_{n-1})\\]\n\n\n\n\n\nA chain is homogeneous at step \\(t\\) if the transition probabilities are independent of \\(t\\). Thus the evolution of the Markov chain only depends on the previous state with a fixed transition matrix.\n\n\n\nEvery state is accessible in a finite number of steps from another state. That is, there are no absorbing states. In other words, one eventually gets everywhere in the chain.\nConsider as an example surfing the web. We do want to reach all parts of the web so we dont want to be trapped into an subset.\n\n\n\nStates visited repeatedly are recurrent: positive recurrent if time-to-return is bounded and null recurrent otherwise. Harris recurrent if all states are visited infinitely often as \\(t \\to \\infty\\)\n\n\n\nThere are no deterministic loops. This would be bad in our web example as well as we would be stuck in a loop at some pages.\nIndeed pagerank has 2 components: one mirroring the true web which can have periods and reducibility, and the second a nose component which “regularizes” the web by adding some noise to the transition matrix for 0 elements.\n\n\n\nExamples of Markov chain properties: irreducible, reducible, and irreducible but periodic (period 3) chains.\n\n\n\n\n\nWe can finally give a formal definition of stationarity. A stationary Markov chain produces the same marginal distribution when multiplied by the transition matrix.\nThat is\n\\[sT = s\\]\nor\n\\[\\sum_i s_i T_{ij} = s_j\\]\nor in the case of a continuous state space, which are the ones we encounter in sampling, if the Transition kernel T is defined so that\n\\[\\int dx_i s(x_i) T(x_{i+1} \\vert x_i) = s_(x_{i+1})\\]\nthen\n\\[\\int dx s(x) T(y \\vert x) = \\int p(y, x) dx = s(y)\\]\nPagerank is indeed the stationary distribution of its transition matrix (each web-page is a new x).\n\n\n\nAperiodic, irreducible, positive Harris recurrent markov chains are ergodic, that is, in the limit of infinite (many) steps, the marginal distribution of the chain is the same. This means that if we take largely spaced about samples from a stationary markov chain, we can draw independent samples.\n\\[\\int g(x) f(x) dx  = \\frac{1}{N} \\sum_{j=B+1}^{B+N} g(x_j)\\]\nHere B is called the burin (which comes from the approach to stationarity after a while) and T is called the thinning (which comes from ergodicity). So we have this “ergodic” law of large numbers.\n\n\n\n\nA irreducible (goes everywhere) and aperiodic (no cycles) markov chain will converge to a stationary markov chain. It is the marginal distribution of this chain that we want to sample from, and which we do in metropolis (and for that matter, in simulated annealing).\nAs we can see above, to find stationary distribution, we need to solve an eigenvector proble. This can be hard. We can show that if we follow reversibility, we are golden…thus..\nA sufficient, but not necessary, condition to ensure that \\(s(x)\\) is the desired stationary distribution is the already seen reversibility condition, also known as detailed balance:\n\\[s(x) T(y \\vert x) = s(y) T(x \\vert y)\\]\nIf one sums both sides over \\(x\\)\n\\[\\int dx s(x) t(y \\vert x) = s(y) \\int dx T(x \\vert y)\\] which gives us back the stationarity condition from above.\nThus we want to design us samplers which satisfy detailed balance."
  },
  {
    "objectID": "posts/markov/index.html#markov-chains",
    "href": "posts/markov/index.html#markov-chains",
    "title": "Markov Chains and MCMC",
    "section": "",
    "text": "Markov Chains are the first example of a stochastic process we will see in this class. The values in a Markov chain depend on the previous values (probabilistically), with the defining characteristic being that a given value is depenendent only on the immediate previous value.\nThis is certainly not IID (idenpendently and identically distributed) data, which is what we have been assuming so far and will generally assume in this course, unless specified otherwise…\nDefinition: A sequence of random variables taking values in a state space is called a Markov Chain if the probability of the next step only depends on the current state.\nUsing the notation of transition probabilities to define the probability of going from state \\(x\\) ot state \\(y\\) as \\(T(x \\vert y)\\), we can write this mathematically:\n\\[T(x_n \\vert x_{n-1}, x_{n-1}..., x_1) = T(x_n \\vert x_{n-1})\\]"
  },
  {
    "objectID": "posts/markov/index.html#some-jargon",
    "href": "posts/markov/index.html#some-jargon",
    "title": "Markov Chains and MCMC",
    "section": "",
    "text": "A chain is homogeneous at step \\(t\\) if the transition probabilities are independent of \\(t\\). Thus the evolution of the Markov chain only depends on the previous state with a fixed transition matrix.\n\n\n\nEvery state is accessible in a finite number of steps from another state. That is, there are no absorbing states. In other words, one eventually gets everywhere in the chain.\nConsider as an example surfing the web. We do want to reach all parts of the web so we dont want to be trapped into an subset.\n\n\n\nStates visited repeatedly are recurrent: positive recurrent if time-to-return is bounded and null recurrent otherwise. Harris recurrent if all states are visited infinitely often as \\(t \\to \\infty\\)\n\n\n\nThere are no deterministic loops. This would be bad in our web example as well as we would be stuck in a loop at some pages.\nIndeed pagerank has 2 components: one mirroring the true web which can have periods and reducibility, and the second a nose component which “regularizes” the web by adding some noise to the transition matrix for 0 elements.\n\n\n\nExamples of Markov chain properties: irreducible, reducible, and irreducible but periodic (period 3) chains.\n\n\n\n\n\nWe can finally give a formal definition of stationarity. A stationary Markov chain produces the same marginal distribution when multiplied by the transition matrix.\nThat is\n\\[sT = s\\]\nor\n\\[\\sum_i s_i T_{ij} = s_j\\]\nor in the case of a continuous state space, which are the ones we encounter in sampling, if the Transition kernel T is defined so that\n\\[\\int dx_i s(x_i) T(x_{i+1} \\vert x_i) = s_(x_{i+1})\\]\nthen\n\\[\\int dx s(x) T(y \\vert x) = \\int p(y, x) dx = s(y)\\]\nPagerank is indeed the stationary distribution of its transition matrix (each web-page is a new x).\n\n\n\nAperiodic, irreducible, positive Harris recurrent markov chains are ergodic, that is, in the limit of infinite (many) steps, the marginal distribution of the chain is the same. This means that if we take largely spaced about samples from a stationary markov chain, we can draw independent samples.\n\\[\\int g(x) f(x) dx  = \\frac{1}{N} \\sum_{j=B+1}^{B+N} g(x_j)\\]\nHere B is called the burin (which comes from the approach to stationarity after a while) and T is called the thinning (which comes from ergodicity). So we have this “ergodic” law of large numbers."
  },
  {
    "objectID": "posts/markov/index.html#getting-a-stationary-distribution",
    "href": "posts/markov/index.html#getting-a-stationary-distribution",
    "title": "Markov Chains and MCMC",
    "section": "",
    "text": "A irreducible (goes everywhere) and aperiodic (no cycles) markov chain will converge to a stationary markov chain. It is the marginal distribution of this chain that we want to sample from, and which we do in metropolis (and for that matter, in simulated annealing).\nAs we can see above, to find stationary distribution, we need to solve an eigenvector proble. This can be hard. We can show that if we follow reversibility, we are golden…thus..\nA sufficient, but not necessary, condition to ensure that \\(s(x)\\) is the desired stationary distribution is the already seen reversibility condition, also known as detailed balance:\n\\[s(x) T(y \\vert x) = s(y) T(x \\vert y)\\]\nIf one sums both sides over \\(x\\)\n\\[\\int dx s(x) t(y \\vert x) = s(y) \\int dx T(x \\vert y)\\] which gives us back the stationarity condition from above.\nThus we want to design us samplers which satisfy detailed balance."
  },
  {
    "objectID": "posts/markov/index.html#back-to-metropolis",
    "href": "posts/markov/index.html#back-to-metropolis",
    "title": "Markov Chains and MCMC",
    "section": "Back to Metropolis",
    "text": "Back to Metropolis\nNow we can put our Metropolis sampler on a sure footing. And this foOting based on Markov Chain theory is why we call this a MCMC or Markov Chain Monte Carlo technique.\nOur game is this: we wish to find a markov chain whose stationary distribution is the distribution we need to sample from.\nAs long as we set up a sampling scheme that follows detailed balance we are ok.\n\\[ s(x_i)T( x_{i-1} \\vert x_i ) = s(x_{i-1}) T( x_i \\vert x_{i-1} )\\]\n\nProof of Detailed Balance\nThe transition matrix (or kernel) can be written in this form for Metropolis:\n\\[T(x_i \\vert x_{i-1}) = q(x_i \\vert x_{i-1})\\,A(x_i, x_{i-1}) +  \\delta(x_{i-1} - x_i)r(x_{i-1})\\]\nwhere\n\\[A(x_i, x_{i-1}) = min(1,  \\frac{s(x_i)}{s(x_{i-1})})\\]\nis the Metropolis acceptance probability (you propose the move and accept it) and\n\\[r(x_i) = \\int dy q(y \\vert x_i)(1 - A(y, x_i))\\] is the rejection term. In the event you dont move, this could have happened 2 ways: (a) you proposed to move to the same position and accepted it, or you proposes moves aevrywhere else but dont accept any of those proposals and stay where you are.\nLets parse the term above:\n\nthe transition probability has two terms\nthe first term is the probability of proposing a new \\(x_i\\) times the probability of accepting it\nthe second term is the probability \\(r(x_{i-1})\\) of rejecting it with the delta function setting it so that \\(x_i = x_{i-1}\\)\nthe integral adds over all the points \\(y\\) that might have been proposed and then rejected, so we stayed at \\(x_{i-1}\\)\n\n\\[s(x_i)T( x_{i-1} \\vert x_i ) =  s(x_i) q(x_{i-1} \\vert x_{i})\\,A(x_{i-1}, x_{i}) +  s(x_i) \\delta(x_{i} - x_{i-1})r(x_{i})\\]\n\\[s(x_{i-1})T( x_{i} \\vert x_{i-1} ) =  s(x_{i-1}) q(x_i \\vert x_{i-1})\\,A(x_i, x_{i-1}) +  s(x_{i-1})\\delta(x_{i-1} - x_i)r(x_{i-1})\\]\nThe second term in each expression is equal.\nAssume, without loss of generality that \\(s(x_i) &lt; s(x_{i-1})\\). Then the first term of the first expression gives us \\(s(x_i) q(x_{i-1} \\vert x_i)\\). The first term of the second expression gives us \\(s(x_{i}) q(x_i \\vert x_{i-1})\\). Since the proposals are symmetric, detailed balance holds.\nThus the Metropolis algorithm respects \\(s(x)\\) as the stationary distribution.\n\n\nIntuition\nAll this math boils down to the following intuition:\n\n\n\nMCMC intuition: a Markov chain converging toward a target distribution shown in green, with the stationary distribution on the right.\n\n\nInstead of sampling \\(s\\) (or \\(p\\)) (since we dont know how to do that) we sample \\(q\\) instead. Sampling from this distribution yields a new state, and a new proposal distribution from which to sample.\n\n\n\nProposal distributions (black curves) overlaid on the true target distribution (red) and the resulting MCMC histogram (blue).\n\n\nIn general such a markov chain would meander around. But the aperiodicity and irreducibility means that it will preserve the (stationary) target distribution and go for the typical set (red), no matter where it is applied.\nThe possibility of rejection in the Metropolis algorithm based on the throw of a random uniform makes the chain aperiodic. And if we want it to be irreducible, we need to make sure \\(q\\) can go everywhere that \\(p\\) can, or that the support of \\(q\\) includes everywhere the support of \\(p\\). Thus our Metropolis algorithm converges."
  },
  {
    "objectID": "posts/markov/index.html#onwards-to-metropolis-hastings",
    "href": "posts/markov/index.html#onwards-to-metropolis-hastings",
    "title": "Markov Chains and MCMC",
    "section": "Onwards to Metropolis-Hastings",
    "text": "Onwards to Metropolis-Hastings\nPrior to a full discussion of Metropolis Hastings, let us motivate the season why we may want to tinker with the Metropolis algorithm.\nIf you think about the \\(6x(1-x)\\) “distribution” which we dont want to support outside of [0,1], we realize, as seen in the diagram above, that a normal distribution as a proposal will still want to sample stuff in negative areas and \\(&gt;1\\). So we wastefully make acceptance probability comparisons.\nOut intuition may be to reject samples outside the support. But as we shall show later, this makes our proposal asymmetric and we need to deal with this. This is because stepping to a negative number and coming back are not symmetric: one is rejected.\nIf we do it without taking into account this asymmetry, we will actually be sampling from a different distribution as we shall show later.\nHowever, we may also want to sample from a asymmetric proposal like a beta function because its guaranteed to be positive. However a beta distribution is not symmetric.\nHere is the outline code for metropolis hastings.\n\ndef metropolis_hastings(p,q, qdraw, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    for i in range(nsamp):\n        x_star = qdraw(x_prev)\n        p_star = p(x_star)\n        p_prev = p(x_prev)\n        pdfratio = p_star/p_prev\n        proposalratio = q(x_prev, x_star)/q(x_star, x_prev)\n        if np.random.uniform() &lt; min(1, pdfratio*proposalratio):\n            samples[i] = x_star\n            x_prev = x_star\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples\n\nLook at the proposalratio term. Its a ratio of proposal pdfs. But its opposite to the ratio of the pdf that we want to sample from.\nWhat is the intuition behind this? Remember that because you dont know how to sample from \\(p\\), you are sampling from \\(q\\) instead. Now if q is asymmetric, you will have a greater chance of going in one direction than the other. Indeed you are more likely to get samples from the region where \\(q\\) (more precisely each new \\(q\\)) is high . This may not match with the \\(p\\) you want to sample from, and you want to correct this oversampling by multiplying by the proposal ration. This helps erase the memory of \\(q\\) a bit.\n\nfrom scipy.stats import beta\nf = lambda x: 6*x*(1-x)\nxx= np.linspace(0,1,100)\nfor s in np.linspace(0,1,10):\n    plt.plot(xx, beta.pdf(xx,40*s,10),'k', lw=1)\nplt.axhline(beta.pdf(0.2, 20, 10),0,1)\nplt.axhline(beta.pdf(0.5, 8, 10),0,1)\n\n\n#plot the true function\nplt.plot(xx, f(xx), 'r', label=u'True distribution') \nplt.legend();"
  },
  {
    "objectID": "posts/em/index.html",
    "href": "posts/em/index.html",
    "title": "The EM Algorithm",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nimport pymc3 as pm\n\n\\[\\newcommand{\\isum}{\\sum_{i}}\\] \\[\\newcommand{\\zsum}{\\sum_{k=1}^{K}}\\] \\[\\newcommand{\\zsumi}{\\sum_{\\{z_i\\}}}\\]\n\n\nThis example is taken from Efron and Hastie, Computer Age Statistical Inference.\nAssume we have data drawn from a bi-variate Normal\n\n\n\nBivariate normal model: the joint distribution of (x1, x2) with means, variances, and correlation parameter rho.\n\n\n\nsig1=1\nsig2=0.75\nmu1=1.85\nmu2=1\nrho=0.82\nmeans=np.array([mu1, mu2])\ncov = np.array([\n    [sig1**2, sig1*sig2*rho],\n    [sig2*sig1*rho, sig2**2]\n])\nmeans, cov\n\n(array([ 1.85,  1.  ]), array([[ 1.    ,  0.615 ],\n        [ 0.615 ,  0.5625]]))\n\n\nWe plot the samples below as blue circles. Now say we lose the y-values of the last-20 pieces of data. We are left with a missing data or hidden data or latent-variables problem. We plot both datasets below, with the y-values of the lost points set to 0\n\nsamples=np.random.multivariate_normal(means, cov, size=40)\n\n\nsamples_censored=np.copy(samples)\nsamples_censored[20:,1]=0\nplt.plot(samples[:,0], samples[:,1], 'o', alpha=0.8)\nplt.plot(samples_censored[:,0], samples_censored[:,1], 's', alpha=0.3)\n\n\n\n\n\n\n\n\nWe would use MLE if we had all the data.\n\n\n\nMaximum likelihood estimators for the bivariate normal parameters: sample means, standard deviations, and correlation.\n\n\n\nmu1 = lambda s: np.mean(s[:,0])\nmu2 = lambda s: np.mean(s[:,1])\ns1 = lambda s: np.std(s[:,0])\ns2 = lambda s: np.std(s[:,1])\nrho = lambda s: np.mean((s[:,0] - mu1(s))*(s[:,1] - mu2(s)))/(s1(s)*s2(s))\n\nBut we dont. So we shall follow an iterative process to find them.\n\nmu1s=[]\nmu2s=[]\ns1s=[]\ns2s=[]\nrhos=[]\n\nBur remember our data are missing in the y-direction. Assume 0 and go. Since we are using the MLE of the full-data likelihood, with this assumption we can use the MLE formulae. This is called M-step or maximization step since we used the MLE formulae.\n\nmu1s.append(mu1(samples_censored))\nmu2s.append(mu2(samples_censored))\ns1s.append(s1(samples_censored))\ns2s.append(s2(samples_censored))\nrhos.append(rho(samples_censored))\nmu1s,mu2s,s1s,s2s,rhos\n\n([1.7547657491036741],\n [0.44394554402533365],\n [1.2718969990037354],\n [0.77180100755764103],\n [0.52348014627786521])\n\n\nNow having new estimates of our parameters due to our (fake) y-data, lets go in the other direction. Using these parameters let us calculate new y-values.\nOne way we might do this is to replace the old-missing-y values with the means of these fixing the parameters of the multi-variate normal and the non-missing data. In other words:\n\\[E_{p(z \\vert \\theta, x)}[z]\\]\nwhere we have used the notation \\(z\\) to refer to the missing values.\nThis posterior distribution (in the sense of bayes theorem, not bayesian analysis) for the multi-variate gaussian is a gaussian..see wikipedia for the formulae\n\\[\\bar{y}(t+1) - \\hat{\\mu_2}(t) = \\hat{\\rho}(t)\\frac{\\hat{\\sigma_2}(t)}{\\hat{\\sigma_1}(t)} \\left( \\bar{x} - \\hat{\\mu_1}(t) \\right)\\]\n\ndef ynew(x, mu1, mu2, s1, s2, rho):\n    return mu2 + rho*(s2/s1)*(x - mu1)\n    \n\n\nnewys=ynew(samples_censored[20:,0], mu1s[0], mu2s[0], s1s[0], s2s[0], rhos[0])\nnewys\n\narray([-0.25149771,  0.71551635,  0.81675223,  0.85921124,  0.08168773,\n       -0.14712658,  0.7300238 ,  0.42993898,  0.51504132,  0.28024916,\n       -0.06713977,  0.44793596,  0.54224183,  1.30632573,  0.23563191,\n        0.5396655 ,  0.25878933,  0.36945243,  0.99094696,  0.53980901])\n\n\nThis is called the E-step as it computes an expectation for us. Lets run this iteratively and see if we converge.\n\nfor step in range(1,20):\n    samples_censored[20:,1] = newys\n    #M-step\n    mu1s.append(mu1(samples_censored))\n    mu2s.append(mu2(samples_censored))\n    s1s.append(s1(samples_censored))\n    s2s.append(s2(samples_censored))\n    rhos.append(rho(samples_censored))\n    #E-step\n    newys=ynew(samples_censored[20:,0], mu1s[step], mu2s[step], s1s[step], s2s[step], rhos[step])\ndf=pd.DataFrame.from_dict(dict(mu1=mu1s, mu2=mu2s, s1=s1s, s2=s2s, rho=rhos))\ndf\n\n\n\n\n\n\n\n\nmu1\nmu2\nrho\ns1\ns2\n\n\n\n\n0\n1.754766\n0.443946\n0.523480\n1.271897\n0.771801\n\n\n1\n1.754766\n0.673782\n0.822228\n1.271897\n0.718523\n\n\n2\n1.754766\n0.792335\n0.904408\n1.271897\n0.749225\n\n\n3\n1.754766\n0.853302\n0.925737\n1.271897\n0.775802\n\n\n4\n1.754766\n0.884575\n0.932120\n1.271897\n0.790958\n\n\n5\n1.754766\n0.900583\n0.934331\n1.271897\n0.798740\n\n\n6\n1.754766\n0.908762\n0.935193\n1.271897\n0.802589\n\n\n7\n1.754766\n0.912935\n0.935558\n1.271897\n0.804467\n\n\n8\n1.754766\n0.915062\n0.935723\n1.271897\n0.805378\n\n\n9\n1.754766\n0.916144\n0.935799\n1.271897\n0.805821\n\n\n10\n1.754766\n0.916695\n0.935835\n1.271897\n0.806036\n\n\n11\n1.754766\n0.916974\n0.935853\n1.271897\n0.806141\n\n\n12\n1.754766\n0.917116\n0.935861\n1.271897\n0.806192\n\n\n13\n1.754766\n0.917189\n0.935866\n1.271897\n0.806218\n\n\n14\n1.754766\n0.917225\n0.935868\n1.271897\n0.806230\n\n\n15\n1.754766\n0.917244\n0.935869\n1.271897\n0.806236\n\n\n16\n1.754766\n0.917253\n0.935869\n1.271897\n0.806239\n\n\n17\n1.754766\n0.917258\n0.935869\n1.271897\n0.806241\n\n\n18\n1.754766\n0.917260\n0.935870\n1.271897\n0.806242\n\n\n19\n1.754766\n0.917261\n0.935870\n1.271897\n0.806242\n\n\n\n\n\n\n\nVoila. We converge to stable values of our parameters.\nBut they may not be the ones we seeded the samples with. The Em algorithm is only good upto finding local minima, and a finite sample size also means that the minimum found can be slightly different.\n\n\n\n** Expectation-maximization (EM)** method is an iterative method for maximizing difficult likelihood (or posterior) problems. It was first introduced by Dempster, Laird, and Rubin (1977).\nEM recognizes that if the data were fully observed, then ML/ MAP estimates would be easy to compute. It thus alternates between inferring the missing values given the parameters (E step), and then optimizing the parameters given the “filled in” data (M step). The idea is to find a lower-bound on the log-likelihood \\(\\ell\\) (E-step) and the optimize the lower-bound (M-step).\nWe want to use EM wherever we dont know how to optimize simply the x-data likelihood.\nThe EM algorithm is naturally useful for missing data, truncated data, censored data and grouped data problems. But it is also useful at places where you have clusters like mixture models, and you are working in an unsupervised or semi-supervised mode. Basically, any place you can set up a latent variable model or a data augmentation procedure, EM is useful.\nThere are applications in astronomical image analysis, genetics, engineering, etc\nThe basic idea is:\ncalculate MLE estimates for the incomplete data problem by using the complete-data likelihood. To create complete data, augment the observed data with manufactured data\nSorta like, just assign points to clusters to start with and iterate.\nThen, at each iteration, replace the augmented data by its conditional expectation given current observed data and parameter estimates. (E-step) Maximize the full-data likelihood to do the M-step.\n\n\n\nTo understand why EM works we will start with our old friend the KL-divergence. All images in this section are stolen from Bishop\nWe cast the problem as a MLE problem, but in general any problem in which there is a \\(\\theta, x, z\\) triad will work.\nWe start with:\n\\[p(x) = \\sum_z p(x, z)\\]\nSpecifically, lets cast these as likelihoods\n\\[p(x \\vert \\theta) = \\sum_z p(x, z \\vert \\theta)\\]\nwhere the \\(x\\) and \\(z\\) range over the multiple points in your data set.\nThen \\[\\ell( x \\vert \\theta) = log\\, p(x \\vert \\theta) = log\\, \\sum_z p(x, z \\vert \\theta)\\]\nAssume \\(z\\) has some normalized distribution:\n\\[z \\sim q(z)\\].\nWe wish to compute conditional expectations of the type:\n\\[E_{p(z \\vert x, \\theta)}[z]\\]\nbut we dont know this “posterior”. Lets however say that we somehow know \\(q\\) and thus can define our loss function as the KL-divergence between \\(q\\) and the posterior \\(p(z \\vert x, \\theta)\\), henceforth abbreviated \\(p\\).\nThen (notice new notation to be consistent with Bishop):\n\\[ \\mathrm{KL}(q \\vert\\vert p) = D_{KL}(q, p) = E_q[log \\frac{q}{p}] = -E_q[log \\frac{p}{q}]\\]\n\\[D_{KL}(q, p) = -E_q[log \\frac{p(x, z \\vert \\theta)}{q\\,p(x \\vert \\theta)}]\\]\nwhere we have used Bayes Theorem.\nNow at first blush this does not seem to have bought us anything as we dont know the full data likelihood and we dont know how to optimize the x-data likelihood. But lets persevere:\n\\[D_{KL}(q, p) = - \\left( E_q[log \\frac{p(x, z \\vert \\theta)}{q}] - E_q[log\\,p(x \\vert \\theta)] \\right)\\]\nThe second term does not depend on \\(q\\), and \\(q\\) is normalized so the expectation just gives 1 and we can rewrite the equation in terns of the x-data log likelihoood thus:\n\\[log\\,p(x \\vert \\theta) = E_q[log \\frac{p(x, z \\vert \\theta)}{q}] + D_{KL}(q, p)\\]\nIf we define the ELBO or Evidence Lower bound as:\n\\[\\mathcal{L}(q, \\theta) = E_q[log \\frac{p(x, z \\vert \\theta)}{q}]\\]\n, then\n\\(log\\,p(x \\vert \\theta)\\) = ELBO + KL-divergence\na situation made clear in the diagram below where the ELBO goues upto the blue line and the divergence from the blue to the red. Be careful with the p’s, the one on the right is for the x-data and the one on the left for the latent-variable posterior.\n\n\n\nDecomposition of the log marginal likelihood into the ELBO and KL divergence between the variational distribution q and the true posterior p. From Bishop.\n\n\nNow recall that the Kullback Liebler divergence is 0 only if the distributions as a function of \\(z\\) are the same at every poiny; it is otherwise ALWAYS greater than 0. This tells us that the quantity \\(\\mathcal{L}(q, \\theta)\\), is ALWAYS smaller than or equal to the log-likelihood of \\(p(x  \\vert  \\theta)\\), as illustrated above. In other words, \\(\\mathcal{L}(q, \\theta)\\) is a lower bound on the log-likelihood. This is why its called the ELBO, with the “evidence” aspect of it coming from Variational calculus (next lecture).\nThe ELBO itself is the expected value of the full-data log-likelihood minus the entropy of \\(q\\):\n\\[\\mathcal{L}(q, \\theta) = E_q[log \\frac{p(x, z \\vert \\theta)}{q}] = E_q[log p(x, z \\vert \\theta)] - E_q[log\\, q]\\]\n\n\nThese observations set up the EM algorithm for us. If we choose, in the E-step, at some (possibly initial) value of the parameters \\(\\theta_{old}\\),\n\\[q(z) = p(z  \\vert  x, \\theta_{old}),\\]\nwe then set the Kullback Liebler divergence to 0, and thus \\(\\mathcal{L}(q, \\theta)\\) to the log-likelihood at \\(\\theta_{old}\\), and maximizing the lower bound.\nUsing this missing data posterior, conditioned on observed data, and \\(\\theta_{old}\\), we compute the expectation of the missing data with respect to the posterior and use it later.\n\n\n\nThe E-step of EM: setting q equal to the posterior makes KL(q||p)=0, raising the ELBO to match the log likelihood. From Bishop.\n\n\n\n\n\nSince after the E-step, the lower bound touches the log-likelihood, any maximization of this ELBO from its current value with respect to \\(\\theta\\) will also “push up” on the likelihood itself. Thus M step guaranteedly modifies the parameters \\(\\theta\\) to increase (or keep same) the likelihood of the observed data.\nThus we hold now the distribution \\(q(z)\\) fixed at the hidden variable posterior calculated at \\(\\theta_{old}\\), and maximize \\(\\mathcal{L}(q, \\theta)\\) with respect to \\(\\theta\\) to obtain new parameter values \\(\\theta_{new}\\). This is a regular maximization.\nThe distribution \\(q\\), calculated as it is at \\(\\theta_{old}\\) will not in general equal the new posterior distribution \\(p(z \\vert x,\\theta_{new})\\), and hence there will be a nonzero KL divergence. Thus the increase in the log-likelihood will be greater than the increase in the lower bound \\(\\mathcal{L}\\), as illustrated below.\nThe M in “M-step” and “EM” stands for “maximization”.\n\n\n\nThe M-step of EM: holding q fixed and maximizing the ELBO with respect to theta increases the log likelihood by at least as much. From Bishop.\n\n\n\n\n\nNote that since \\(\\mathcal{L}\\) is maximized with respect to \\(\\theta\\), one can equivalently maximize the expectation of the full-data log likelihood \\(\\mathrm{E_q[\\ell( x,z  \\vert  \\theta)]}\\) in the M-step since the difference is purely a function of \\(q\\). Furthermore, if the joint distribution \\(p(x, z \\vert  \\theta)\\) is a member of the exponential family, the log-likelihood will have a particularly simple form and will lead to a much simpler maximization than that of the incomple-data log-likelihood \\(p(x \\vert \\theta)\\).\nWe now set \\(\\theta_{old} = \\theta_{new}\\) and repeat the process. This EM algorithm is presented and illustrated below:\n\n\n\nEM convergence: the red curve shows log p(X|theta), the green curve shows the ELBO, and each EM iteration moves from theta_old to theta_new, monotonically increasing the likelihood. From Bishop.\n\n\n\nWe start with the log-likelihood \\(p(x  \\vert  \\theta)\\)(red curve) and the initial guess \\(\\theta_{old}\\) of the parameter values\nUntil convergence (the \\(\\theta\\) values dont change too much):\n\nE-step: Evaluate the hidden variable posterior \\(q(z, \\theta_{old}) = p(z  \\vert  x, \\theta_{old})\\) which gives rise to a lower bound function of \\(\\theta\\): \\(\\mathcal{L}(q(z, \\theta_{old}), \\theta)\\)(blue curve) whose value equals the value of \\(p(x  \\vert  \\theta)\\) at \\(\\theta_{old}\\).\nM-step: maximize the lower bound function with respect to \\(\\theta\\) to get \\(\\theta_{new}\\).\nSet \\(\\theta_{old} = \\theta_{new}\\)\n\n\nOne iteration more is illustrated above, where the subsequent E-step constructs a new lower-bound function that is tangential to the log-likelihood at \\(\\theta_{new}\\), and whose value at \\(\\theta_{new}\\) is higher than the lower bound at \\(\\theta_{old}\\) from the previous step.\nThus\n\\[\\ell(\\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t}) = \\ell(\\theta_t)\\]\nThe first equality follows since \\(\\mathcal{L}\\) is a lower bound on \\(\\ell\\), the second from the M-step’s maximization of \\(\\mathcal{L}\\), and the last from the vanishing of the KL-divergence after the E-step. As a consequence, you must observe monotonic increase of the observed-data log likelihood \\(\\ell\\) across iterations. This is a powerful debugging tool for your code.\nNote that as shown above, since each EM iteration can only improve the likelihood, you are guaranteeing convergence to a local maximum. Because it IS local , you must try some different initial values of \\(\\theta_{old}\\) and take the one that gives you the largest \\(\\ell\\).\n\n\n\n\nI often find it confusing as to what the indices are actually doing if I dont write them out explicitly. So lets visit the EM derivation once more, focussing on mixtures, and explicitly writing out indices. The derivation does not need mixtures, but I find it helpful to imagine that we are fitting such a model.\nSuppose we have an estimation problem in which we have data consising of \\(m\\) independent examples \\(\\{x_1,\\ldots,x_m\\}\\) . The goal is to fit the parameters of the model, where the log-likelihood is given by \\[\\begin{eqnarray}\n\\ell(x  \\vert  \\theta)&=& \\log \\prod_{i=1}^{m} p(x_i \\vert  \\theta) =   \\sum_{i=1}^{m} \\log \\,p(x_i \\vert  \\theta)  \\\\\n   &=& \\sum_{i=1}^{m} \\log \\zsumi \\,p(x_i,z \\vert  \\theta)  \\\\\n\\end{eqnarray}\\]\nwhere the \\(z\\) are the latent random variables. If \\(z\\) were observed then the maximum likelihood estimation would be easy.\nIndeed then, let us start with the full data log-likelihood,\n\\[\\ell(x, z  \\vert  \\theta) = \\sum_{i=1}^{m}  \\log \\,p(x_i, z_i  \\vert  \\theta),\\]\nwhich is the log-likelihood we’d calculate if we knew all the \\(z_i\\). But we do not know thse, so lets assume the \\(\\{z_i\\}\\) have some normalized distribution \\(q(z)\\), and calculate the expected value of the full data log likelihood with respect to this distribution:\n\\[\\begin{eqnarray}\n\\mathrm{E_q[\\ell( x,z  \\vert  \\theta)]}  &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,p(x_i, z_i  \\vert  \\theta)\\\\\n    &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i  \\vert  \\theta)}{q_{i}(z_i)} +  \\sum_i \\zsumi q_{i}(z_i) \\log \\,q_{i}(z_i)\n\\end{eqnarray}\\]\nThe second term only involves \\(q\\) and is independent of \\(\\theta\\). Looking only at the first term inside the i-summation:\n\\[\\begin{eqnarray}\n\\mathcal{L}(i, q, \\theta) &=&  \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i  \\vert  \\theta)}{q_{i}(z_i)} \\\\\n&=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i \\vert  x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i  \\vert  \\theta)\\right)\n\\end{eqnarray}\\]\nwe can see that, since \\(\\zsumi q_i(z_i) = 1\\):\n\\[\\begin{eqnarray}\n\\mathcal{L}(i, q, \\theta) &=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i \\vert  x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i  \\vert  \\theta)\\right)\\\\\n    &=& -\\mathrm{KL}\\left(q_i  \\vert  \\vert  p_i \\right) + \\log p(x_i  \\vert  \\theta)\\\\\n\\end{eqnarray}\\]\nwhere \\(\\mathrm{KL}\\) is the Kullback-Leibler divergence between \\(q(x)\\) and the hidden variable posterior distribution \\(p(z \\vert x,\\theta)\\) at the poin \\(i\\).\nSince the sum over the data-points of the second term is just the log-likelihood we desire, it can then can be written as:\n\\[\\begin{eqnarray}\n\\ell(x  \\vert  \\theta) &=& \\sum_i \\left(\\mathcal{L}(i, q, \\theta) +\\mathrm{KL}\\left(q_i   \\vert  \\vert  p_i \\right)\\right)\\\\\n&=& \\mathcal{L}(q, \\theta) + \\mathrm{KL}\\left(q  \\vert  \\vert  p \\right)\n\\end{eqnarray}\\]\nwhere we are defining:\n\\[\\mathrm{KL}(q  \\vert  \\vert  p) = \\sum_i \\mathrm{KL}\\left(q_i   \\vert  \\vert  p_i \\right)\\]\nas the sum of the KL-divergence at each data point, and \\(\\mathcal{L}(q, \\theta)\\) as the sum of \\(\\mathcal{L}\\) at each data point.\n\n\n\nWe dont know how to solve for the MLE of the unsupervised problem. The EM algorithm comes to the rescue. As described above here is the algorithm:\n\nRepeat until convergence\nE-step: For each \\(i,j\\) calculate\n\n\\[ w_{i,j} = q_i(z_i=j)=p(z_i=j \\vert  x_i, \\lambda, \\mu, \\Sigma) \\]\n\nM-step: We need to maximize, with respect to our parameters the\n\n\\[\n\\begin{eqnarray}\n\\mathcal{L} &=& \\sum_i \\sum_{z_i} q_i(z_i) \\log \\frac{p(x_i,z_i  \\vert \\lambda, \\mu, \\Sigma)}{q_i(z_i)} \\nonumber \\\\\n\\mathcal{L} &=& \\sum_i \\sum_{j=i}^{k}  q_i(z_i=j) \\log \\frac{p(x_i \\vert z_i=j , \\mu, \\Sigma) p(z_i=j \\vert \\lambda)}{q_i(z_i=j)} \\\\\n\\mathcal{L} & =&  \\sum_{i=1}^{m} \\sum_{j=i}^{k} w_{i,j}  \\log \\left[   \\frac{ \\frac{1}{ (2\\pi)^{n/2} \\vert \\Sigma_j \\vert ^{1/2}} \\exp \\left(    -\\frac{1}{2}(x_i-\\mu_j)^T \\Sigma_j^{-1} (x_i-\\mu_j) \\right)  \\, \\lambda_j   }{w_{i,j}}\\right]\n\\end{eqnarray}\n\\]\nTaking the derivatives yields the following updating formulas:\n\\[\n\\begin{eqnarray}\n\\lambda_j &=& \\frac{1}{m} \\sum_{i=1}^m w_{i,j} \\nonumber \\\\\n\\mu_j&=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, x_i}{ \\sum_{i=1}^m  w_{i,j}} \\nonumber \\\\\n\\Sigma_j &=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, (x_i-\\mu_j)(x_i-\\mu_j)^T}{ \\sum_{i=1}^m  w_{i,j}}\n\\end{eqnarray}\n\\]\nTo calculate the E-step we basically calculating the posterior of the \\(z\\)’s given the \\(x\\)’s and the current estimate of our parameters. We can use Bayes rule\n\\[ w_{i,j}= p(z_i=j \\vert  x_i, \\lambda, \\mu, \\Sigma) = \\frac{p( x_i \\vert  z_i=j,  \\mu, \\Sigma)\\, p(z_i=j \\vert \\lambda)}{\\sum_{l=1}^k p(x_i  \\vert  z_i=l,  \\mu, \\Sigma) \\, p(z_i=l \\vert \\lambda)} \\]\nWhere \\(p(x_i  \\vert  z_i =j,  \\mu, \\Sigma)\\) is the density of the Gaussian with mean \\(\\mu_j\\) and covariance \\(\\Sigma_j\\) at \\(x_i\\) and \\(p(z_i=j \\vert  \\lambda)\\) is simply \\(\\lambda_j\\). If we to compare these formulas in the M-step with the ones we found in GDA we can see that are very similar except that instead of using \\(\\delta\\) functions we use the \\(w\\)’s. Thus the EM algorithm corresponds here to a weighted maximum likelihood and the weights are interpreted as the ‘probability’ of coming from that Gaussian instead of the deterministic \\(\\delta\\) functions. Thus we have achived a soft clustering (as opposed to k-means in the unsupervised case and classification in the supervised case).\n\n#In 1-D\n# True parameter values\nmu_true = [2, 5]\nsigma_true = [0.6, 0.6]\nlambda_true = .4\nn = 1000\n\n# Simulate from each distribution according to mixing proportion psi\nz = np.random.binomial(1, lambda_true, n)\nx = np.array([np.random.normal(mu_true[i], sigma_true[i]) for i in z])\n\nplt.hist(x, bins=20);\n\n\n\n\n\n\n\n\n\n#from Bios366 lecture notes\nfrom scipy.stats.distributions import norm\n\ndef Estep(x, mu, sigma, lam):\n    a = lam * norm.pdf(x, mu[0], sigma[0])\n    b = (1. - lam) * norm.pdf(x, mu[1], sigma[1])\n    return b / (a + b)\n\n\n\ndef Mstep(x, w):\n    lam = np.mean(1.-w) \n    \n    mu = [np.sum((1-w) * x)/np.sum(1-w), np.sum(w * x)/np.sum(w)]\n    \n    sigma = [np.sqrt(np.sum((1-w) * (x - mu[0])**2)/np.sum(1-w)), \n             np.sqrt(np.sum(w * (x - mu[1])**2)/np.sum(w))]\n    \n    return mu, sigma, lam\n\n\nprint(lambda_true, mu_true, sigma_true)\n# Initialize values\nmu = np.random.normal(4, 10, size=2)\nsigma = np.random.uniform(0, 5, size=2)\nlam = np.random.random()\nprint(\"Initials, mu:\", mu)\nprint(\"Initials, sigma:\", sigma)\nprint(\"Initials, lam:\", lam)\n\n# Stopping criterion\ncrit = 1e-15\n\n# Convergence flag\nconverged = False\n\n# Loop until converged\niterations=1\n\n\nwhile not converged:\n    # E-step\n    if np.isnan(mu[0]) or np.isnan(mu[1]) or np.isnan(sigma[0]) or np.isnan(sigma[1]):\n        print(\"Singularity!\")\n        break\n        \n    w = Estep(x, mu, sigma, lam)\n\n    # M-step\n    mu_new, sigma_new, lam_new = Mstep(x, w)\n    \n    # Check convergence\n    converged = ((np.abs(lam_new - lam) &lt; crit) \n                 & np.all(np.abs((np.array(mu_new) - np.array(mu)) &lt; crit))\n                 & np.all(np.abs((np.array(sigma_new) - np.array(sigma)) &lt; crit)))\n    mu, sigma, lam = mu_new, sigma_new, lam_new\n    iterations +=1           \n\nprint(\"Iterations\", iterations)\nprint('A: N({0:.4f}, {1:.4f})\\nB: N({2:.4f}, {3:.4f})\\nlam: {4:.4f}'.format(\n                        mu_new[0], sigma_new[0], mu_new[1], sigma_new[1], lam_new))\n\n0.4 [2, 5] [0.6, 0.6]\nInitials, mu: [  0.72500527 -20.77035111]\nInitials, sigma: [ 4.59386658  3.6262629 ]\nInitials, lam: 0.6261111131564271\nIterations 95\nA: N(5.0083, 0.6288)\nB: N(2.0261, 0.5936)\nlam: 0.4116\n\n\n\n\n\nWe have motivated the EM algorithm using mixture models and missing data, but that is not its only place of use.\nSince MLE’s can overfit, we often prefer to use MAP estimation. EM is a perfectly reasonable method for MAP estimation in mixture models; you just need to multiply in the prior.\nBasically the EM algorithm has a similar setup to the data augmentation problem and can be used in any problem which has a similar structure. Suppose for example you have two parameters \\(\\phi\\) and \\(\\gamma\\) in a posterior estimation, with daya \\(y\\). Say that we’d like to estimate the posterior \\(p(\\phi  \\vert  y)\\). It may be relatively hard to estimate this, but suppose we can work with \\(p(\\phi  \\vert  \\gamma, y)\\) and \\(p(\\gamma  \\vert  \\phi, y)\\). Then you can use the structure of the EM algorithm to estimate the marginal posterior of any one parameter. Start with:\n\\[log p(\\phi  \\vert  y) = log p(\\gamma, \\phi  \\vert  y) - log p(\\gamma  \\vert  \\phi, y)\\]\nNotice the similarity of this to the above expressions with \\(\\phi\\) as \\(x\\), \\(y\\) as \\(\\theta\\), and \\(\\gamma\\) as \\(z\\). Thus the same derivations apply toany problem with this structure.\nThis structure can also be used in type-2 likelihood or emprical bayes estimation."
  },
  {
    "objectID": "posts/em/index.html#a-toy-problem",
    "href": "posts/em/index.html#a-toy-problem",
    "title": "The EM Algorithm",
    "section": "",
    "text": "This example is taken from Efron and Hastie, Computer Age Statistical Inference.\nAssume we have data drawn from a bi-variate Normal\n\n\n\nBivariate normal model: the joint distribution of (x1, x2) with means, variances, and correlation parameter rho.\n\n\n\nsig1=1\nsig2=0.75\nmu1=1.85\nmu2=1\nrho=0.82\nmeans=np.array([mu1, mu2])\ncov = np.array([\n    [sig1**2, sig1*sig2*rho],\n    [sig2*sig1*rho, sig2**2]\n])\nmeans, cov\n\n(array([ 1.85,  1.  ]), array([[ 1.    ,  0.615 ],\n        [ 0.615 ,  0.5625]]))\n\n\nWe plot the samples below as blue circles. Now say we lose the y-values of the last-20 pieces of data. We are left with a missing data or hidden data or latent-variables problem. We plot both datasets below, with the y-values of the lost points set to 0\n\nsamples=np.random.multivariate_normal(means, cov, size=40)\n\n\nsamples_censored=np.copy(samples)\nsamples_censored[20:,1]=0\nplt.plot(samples[:,0], samples[:,1], 'o', alpha=0.8)\nplt.plot(samples_censored[:,0], samples_censored[:,1], 's', alpha=0.3)\n\n\n\n\n\n\n\n\nWe would use MLE if we had all the data.\n\n\n\nMaximum likelihood estimators for the bivariate normal parameters: sample means, standard deviations, and correlation.\n\n\n\nmu1 = lambda s: np.mean(s[:,0])\nmu2 = lambda s: np.mean(s[:,1])\ns1 = lambda s: np.std(s[:,0])\ns2 = lambda s: np.std(s[:,1])\nrho = lambda s: np.mean((s[:,0] - mu1(s))*(s[:,1] - mu2(s)))/(s1(s)*s2(s))\n\nBut we dont. So we shall follow an iterative process to find them.\n\nmu1s=[]\nmu2s=[]\ns1s=[]\ns2s=[]\nrhos=[]\n\nBur remember our data are missing in the y-direction. Assume 0 and go. Since we are using the MLE of the full-data likelihood, with this assumption we can use the MLE formulae. This is called M-step or maximization step since we used the MLE formulae.\n\nmu1s.append(mu1(samples_censored))\nmu2s.append(mu2(samples_censored))\ns1s.append(s1(samples_censored))\ns2s.append(s2(samples_censored))\nrhos.append(rho(samples_censored))\nmu1s,mu2s,s1s,s2s,rhos\n\n([1.7547657491036741],\n [0.44394554402533365],\n [1.2718969990037354],\n [0.77180100755764103],\n [0.52348014627786521])\n\n\nNow having new estimates of our parameters due to our (fake) y-data, lets go in the other direction. Using these parameters let us calculate new y-values.\nOne way we might do this is to replace the old-missing-y values with the means of these fixing the parameters of the multi-variate normal and the non-missing data. In other words:\n\\[E_{p(z \\vert \\theta, x)}[z]\\]\nwhere we have used the notation \\(z\\) to refer to the missing values.\nThis posterior distribution (in the sense of bayes theorem, not bayesian analysis) for the multi-variate gaussian is a gaussian..see wikipedia for the formulae\n\\[\\bar{y}(t+1) - \\hat{\\mu_2}(t) = \\hat{\\rho}(t)\\frac{\\hat{\\sigma_2}(t)}{\\hat{\\sigma_1}(t)} \\left( \\bar{x} - \\hat{\\mu_1}(t) \\right)\\]\n\ndef ynew(x, mu1, mu2, s1, s2, rho):\n    return mu2 + rho*(s2/s1)*(x - mu1)\n    \n\n\nnewys=ynew(samples_censored[20:,0], mu1s[0], mu2s[0], s1s[0], s2s[0], rhos[0])\nnewys\n\narray([-0.25149771,  0.71551635,  0.81675223,  0.85921124,  0.08168773,\n       -0.14712658,  0.7300238 ,  0.42993898,  0.51504132,  0.28024916,\n       -0.06713977,  0.44793596,  0.54224183,  1.30632573,  0.23563191,\n        0.5396655 ,  0.25878933,  0.36945243,  0.99094696,  0.53980901])\n\n\nThis is called the E-step as it computes an expectation for us. Lets run this iteratively and see if we converge.\n\nfor step in range(1,20):\n    samples_censored[20:,1] = newys\n    #M-step\n    mu1s.append(mu1(samples_censored))\n    mu2s.append(mu2(samples_censored))\n    s1s.append(s1(samples_censored))\n    s2s.append(s2(samples_censored))\n    rhos.append(rho(samples_censored))\n    #E-step\n    newys=ynew(samples_censored[20:,0], mu1s[step], mu2s[step], s1s[step], s2s[step], rhos[step])\ndf=pd.DataFrame.from_dict(dict(mu1=mu1s, mu2=mu2s, s1=s1s, s2=s2s, rho=rhos))\ndf\n\n\n\n\n\n\n\n\nmu1\nmu2\nrho\ns1\ns2\n\n\n\n\n0\n1.754766\n0.443946\n0.523480\n1.271897\n0.771801\n\n\n1\n1.754766\n0.673782\n0.822228\n1.271897\n0.718523\n\n\n2\n1.754766\n0.792335\n0.904408\n1.271897\n0.749225\n\n\n3\n1.754766\n0.853302\n0.925737\n1.271897\n0.775802\n\n\n4\n1.754766\n0.884575\n0.932120\n1.271897\n0.790958\n\n\n5\n1.754766\n0.900583\n0.934331\n1.271897\n0.798740\n\n\n6\n1.754766\n0.908762\n0.935193\n1.271897\n0.802589\n\n\n7\n1.754766\n0.912935\n0.935558\n1.271897\n0.804467\n\n\n8\n1.754766\n0.915062\n0.935723\n1.271897\n0.805378\n\n\n9\n1.754766\n0.916144\n0.935799\n1.271897\n0.805821\n\n\n10\n1.754766\n0.916695\n0.935835\n1.271897\n0.806036\n\n\n11\n1.754766\n0.916974\n0.935853\n1.271897\n0.806141\n\n\n12\n1.754766\n0.917116\n0.935861\n1.271897\n0.806192\n\n\n13\n1.754766\n0.917189\n0.935866\n1.271897\n0.806218\n\n\n14\n1.754766\n0.917225\n0.935868\n1.271897\n0.806230\n\n\n15\n1.754766\n0.917244\n0.935869\n1.271897\n0.806236\n\n\n16\n1.754766\n0.917253\n0.935869\n1.271897\n0.806239\n\n\n17\n1.754766\n0.917258\n0.935869\n1.271897\n0.806241\n\n\n18\n1.754766\n0.917260\n0.935870\n1.271897\n0.806242\n\n\n19\n1.754766\n0.917261\n0.935870\n1.271897\n0.806242\n\n\n\n\n\n\n\nVoila. We converge to stable values of our parameters.\nBut they may not be the ones we seeded the samples with. The Em algorithm is only good upto finding local minima, and a finite sample size also means that the minimum found can be slightly different."
  },
  {
    "objectID": "posts/em/index.html#the-em-algorithm-1",
    "href": "posts/em/index.html#the-em-algorithm-1",
    "title": "The EM Algorithm",
    "section": "",
    "text": "** Expectation-maximization (EM)** method is an iterative method for maximizing difficult likelihood (or posterior) problems. It was first introduced by Dempster, Laird, and Rubin (1977).\nEM recognizes that if the data were fully observed, then ML/ MAP estimates would be easy to compute. It thus alternates between inferring the missing values given the parameters (E step), and then optimizing the parameters given the “filled in” data (M step). The idea is to find a lower-bound on the log-likelihood \\(\\ell\\) (E-step) and the optimize the lower-bound (M-step).\nWe want to use EM wherever we dont know how to optimize simply the x-data likelihood.\nThe EM algorithm is naturally useful for missing data, truncated data, censored data and grouped data problems. But it is also useful at places where you have clusters like mixture models, and you are working in an unsupervised or semi-supervised mode. Basically, any place you can set up a latent variable model or a data augmentation procedure, EM is useful.\nThere are applications in astronomical image analysis, genetics, engineering, etc\nThe basic idea is:\ncalculate MLE estimates for the incomplete data problem by using the complete-data likelihood. To create complete data, augment the observed data with manufactured data\nSorta like, just assign points to clusters to start with and iterate.\nThen, at each iteration, replace the augmented data by its conditional expectation given current observed data and parameter estimates. (E-step) Maximize the full-data likelihood to do the M-step."
  },
  {
    "objectID": "posts/em/index.html#why-does-em-work",
    "href": "posts/em/index.html#why-does-em-work",
    "title": "The EM Algorithm",
    "section": "",
    "text": "To understand why EM works we will start with our old friend the KL-divergence. All images in this section are stolen from Bishop\nWe cast the problem as a MLE problem, but in general any problem in which there is a \\(\\theta, x, z\\) triad will work.\nWe start with:\n\\[p(x) = \\sum_z p(x, z)\\]\nSpecifically, lets cast these as likelihoods\n\\[p(x \\vert \\theta) = \\sum_z p(x, z \\vert \\theta)\\]\nwhere the \\(x\\) and \\(z\\) range over the multiple points in your data set.\nThen \\[\\ell( x \\vert \\theta) = log\\, p(x \\vert \\theta) = log\\, \\sum_z p(x, z \\vert \\theta)\\]\nAssume \\(z\\) has some normalized distribution:\n\\[z \\sim q(z)\\].\nWe wish to compute conditional expectations of the type:\n\\[E_{p(z \\vert x, \\theta)}[z]\\]\nbut we dont know this “posterior”. Lets however say that we somehow know \\(q\\) and thus can define our loss function as the KL-divergence between \\(q\\) and the posterior \\(p(z \\vert x, \\theta)\\), henceforth abbreviated \\(p\\).\nThen (notice new notation to be consistent with Bishop):\n\\[ \\mathrm{KL}(q \\vert\\vert p) = D_{KL}(q, p) = E_q[log \\frac{q}{p}] = -E_q[log \\frac{p}{q}]\\]\n\\[D_{KL}(q, p) = -E_q[log \\frac{p(x, z \\vert \\theta)}{q\\,p(x \\vert \\theta)}]\\]\nwhere we have used Bayes Theorem.\nNow at first blush this does not seem to have bought us anything as we dont know the full data likelihood and we dont know how to optimize the x-data likelihood. But lets persevere:\n\\[D_{KL}(q, p) = - \\left( E_q[log \\frac{p(x, z \\vert \\theta)}{q}] - E_q[log\\,p(x \\vert \\theta)] \\right)\\]\nThe second term does not depend on \\(q\\), and \\(q\\) is normalized so the expectation just gives 1 and we can rewrite the equation in terns of the x-data log likelihoood thus:\n\\[log\\,p(x \\vert \\theta) = E_q[log \\frac{p(x, z \\vert \\theta)}{q}] + D_{KL}(q, p)\\]\nIf we define the ELBO or Evidence Lower bound as:\n\\[\\mathcal{L}(q, \\theta) = E_q[log \\frac{p(x, z \\vert \\theta)}{q}]\\]\n, then\n\\(log\\,p(x \\vert \\theta)\\) = ELBO + KL-divergence\na situation made clear in the diagram below where the ELBO goues upto the blue line and the divergence from the blue to the red. Be careful with the p’s, the one on the right is for the x-data and the one on the left for the latent-variable posterior.\n\n\n\nDecomposition of the log marginal likelihood into the ELBO and KL divergence between the variational distribution q and the true posterior p. From Bishop.\n\n\nNow recall that the Kullback Liebler divergence is 0 only if the distributions as a function of \\(z\\) are the same at every poiny; it is otherwise ALWAYS greater than 0. This tells us that the quantity \\(\\mathcal{L}(q, \\theta)\\), is ALWAYS smaller than or equal to the log-likelihood of \\(p(x  \\vert  \\theta)\\), as illustrated above. In other words, \\(\\mathcal{L}(q, \\theta)\\) is a lower bound on the log-likelihood. This is why its called the ELBO, with the “evidence” aspect of it coming from Variational calculus (next lecture).\nThe ELBO itself is the expected value of the full-data log-likelihood minus the entropy of \\(q\\):\n\\[\\mathcal{L}(q, \\theta) = E_q[log \\frac{p(x, z \\vert \\theta)}{q}] = E_q[log p(x, z \\vert \\theta)] - E_q[log\\, q]\\]\n\n\nThese observations set up the EM algorithm for us. If we choose, in the E-step, at some (possibly initial) value of the parameters \\(\\theta_{old}\\),\n\\[q(z) = p(z  \\vert  x, \\theta_{old}),\\]\nwe then set the Kullback Liebler divergence to 0, and thus \\(\\mathcal{L}(q, \\theta)\\) to the log-likelihood at \\(\\theta_{old}\\), and maximizing the lower bound.\nUsing this missing data posterior, conditioned on observed data, and \\(\\theta_{old}\\), we compute the expectation of the missing data with respect to the posterior and use it later.\n\n\n\nThe E-step of EM: setting q equal to the posterior makes KL(q||p)=0, raising the ELBO to match the log likelihood. From Bishop.\n\n\n\n\n\nSince after the E-step, the lower bound touches the log-likelihood, any maximization of this ELBO from its current value with respect to \\(\\theta\\) will also “push up” on the likelihood itself. Thus M step guaranteedly modifies the parameters \\(\\theta\\) to increase (or keep same) the likelihood of the observed data.\nThus we hold now the distribution \\(q(z)\\) fixed at the hidden variable posterior calculated at \\(\\theta_{old}\\), and maximize \\(\\mathcal{L}(q, \\theta)\\) with respect to \\(\\theta\\) to obtain new parameter values \\(\\theta_{new}\\). This is a regular maximization.\nThe distribution \\(q\\), calculated as it is at \\(\\theta_{old}\\) will not in general equal the new posterior distribution \\(p(z \\vert x,\\theta_{new})\\), and hence there will be a nonzero KL divergence. Thus the increase in the log-likelihood will be greater than the increase in the lower bound \\(\\mathcal{L}\\), as illustrated below.\nThe M in “M-step” and “EM” stands for “maximization”.\n\n\n\nThe M-step of EM: holding q fixed and maximizing the ELBO with respect to theta increases the log likelihood by at least as much. From Bishop.\n\n\n\n\n\nNote that since \\(\\mathcal{L}\\) is maximized with respect to \\(\\theta\\), one can equivalently maximize the expectation of the full-data log likelihood \\(\\mathrm{E_q[\\ell( x,z  \\vert  \\theta)]}\\) in the M-step since the difference is purely a function of \\(q\\). Furthermore, if the joint distribution \\(p(x, z \\vert  \\theta)\\) is a member of the exponential family, the log-likelihood will have a particularly simple form and will lead to a much simpler maximization than that of the incomple-data log-likelihood \\(p(x \\vert \\theta)\\).\nWe now set \\(\\theta_{old} = \\theta_{new}\\) and repeat the process. This EM algorithm is presented and illustrated below:\n\n\n\nEM convergence: the red curve shows log p(X|theta), the green curve shows the ELBO, and each EM iteration moves from theta_old to theta_new, monotonically increasing the likelihood. From Bishop.\n\n\n\nWe start with the log-likelihood \\(p(x  \\vert  \\theta)\\)(red curve) and the initial guess \\(\\theta_{old}\\) of the parameter values\nUntil convergence (the \\(\\theta\\) values dont change too much):\n\nE-step: Evaluate the hidden variable posterior \\(q(z, \\theta_{old}) = p(z  \\vert  x, \\theta_{old})\\) which gives rise to a lower bound function of \\(\\theta\\): \\(\\mathcal{L}(q(z, \\theta_{old}), \\theta)\\)(blue curve) whose value equals the value of \\(p(x  \\vert  \\theta)\\) at \\(\\theta_{old}\\).\nM-step: maximize the lower bound function with respect to \\(\\theta\\) to get \\(\\theta_{new}\\).\nSet \\(\\theta_{old} = \\theta_{new}\\)\n\n\nOne iteration more is illustrated above, where the subsequent E-step constructs a new lower-bound function that is tangential to the log-likelihood at \\(\\theta_{new}\\), and whose value at \\(\\theta_{new}\\) is higher than the lower bound at \\(\\theta_{old}\\) from the previous step.\nThus\n\\[\\ell(\\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t}) = \\ell(\\theta_t)\\]\nThe first equality follows since \\(\\mathcal{L}\\) is a lower bound on \\(\\ell\\), the second from the M-step’s maximization of \\(\\mathcal{L}\\), and the last from the vanishing of the KL-divergence after the E-step. As a consequence, you must observe monotonic increase of the observed-data log likelihood \\(\\ell\\) across iterations. This is a powerful debugging tool for your code.\nNote that as shown above, since each EM iteration can only improve the likelihood, you are guaranteeing convergence to a local maximum. Because it IS local , you must try some different initial values of \\(\\theta_{old}\\) and take the one that gives you the largest \\(\\ell\\)."
  },
  {
    "objectID": "posts/em/index.html#the-em-algorithm-with-indices-laid-out",
    "href": "posts/em/index.html#the-em-algorithm-with-indices-laid-out",
    "title": "The EM Algorithm",
    "section": "",
    "text": "I often find it confusing as to what the indices are actually doing if I dont write them out explicitly. So lets visit the EM derivation once more, focussing on mixtures, and explicitly writing out indices. The derivation does not need mixtures, but I find it helpful to imagine that we are fitting such a model.\nSuppose we have an estimation problem in which we have data consising of \\(m\\) independent examples \\(\\{x_1,\\ldots,x_m\\}\\) . The goal is to fit the parameters of the model, where the log-likelihood is given by \\[\\begin{eqnarray}\n\\ell(x  \\vert  \\theta)&=& \\log \\prod_{i=1}^{m} p(x_i \\vert  \\theta) =   \\sum_{i=1}^{m} \\log \\,p(x_i \\vert  \\theta)  \\\\\n   &=& \\sum_{i=1}^{m} \\log \\zsumi \\,p(x_i,z \\vert  \\theta)  \\\\\n\\end{eqnarray}\\]\nwhere the \\(z\\) are the latent random variables. If \\(z\\) were observed then the maximum likelihood estimation would be easy.\nIndeed then, let us start with the full data log-likelihood,\n\\[\\ell(x, z  \\vert  \\theta) = \\sum_{i=1}^{m}  \\log \\,p(x_i, z_i  \\vert  \\theta),\\]\nwhich is the log-likelihood we’d calculate if we knew all the \\(z_i\\). But we do not know thse, so lets assume the \\(\\{z_i\\}\\) have some normalized distribution \\(q(z)\\), and calculate the expected value of the full data log likelihood with respect to this distribution:\n\\[\\begin{eqnarray}\n\\mathrm{E_q[\\ell( x,z  \\vert  \\theta)]}  &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,p(x_i, z_i  \\vert  \\theta)\\\\\n    &=& \\sum_i \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i  \\vert  \\theta)}{q_{i}(z_i)} +  \\sum_i \\zsumi q_{i}(z_i) \\log \\,q_{i}(z_i)\n\\end{eqnarray}\\]\nThe second term only involves \\(q\\) and is independent of \\(\\theta\\). Looking only at the first term inside the i-summation:\n\\[\\begin{eqnarray}\n\\mathcal{L}(i, q, \\theta) &=&  \\zsumi q_{i}(z_i) \\log \\,\\frac{p(x_i, z_i  \\vert  \\theta)}{q_{i}(z_i)} \\\\\n&=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i \\vert  x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i  \\vert  \\theta)\\right)\n\\end{eqnarray}\\]\nwe can see that, since \\(\\zsumi q_i(z_i) = 1\\):\n\\[\\begin{eqnarray}\n\\mathcal{L}(i, q, \\theta) &=& \\zsumi  q_i(z_i) \\left( \\log \\frac{p(z_i \\vert  x_i,  \\theta)}{ q_i(z_i)} + \\log p(x_i  \\vert  \\theta)\\right)\\\\\n    &=& -\\mathrm{KL}\\left(q_i  \\vert  \\vert  p_i \\right) + \\log p(x_i  \\vert  \\theta)\\\\\n\\end{eqnarray}\\]\nwhere \\(\\mathrm{KL}\\) is the Kullback-Leibler divergence between \\(q(x)\\) and the hidden variable posterior distribution \\(p(z \\vert x,\\theta)\\) at the poin \\(i\\).\nSince the sum over the data-points of the second term is just the log-likelihood we desire, it can then can be written as:\n\\[\\begin{eqnarray}\n\\ell(x  \\vert  \\theta) &=& \\sum_i \\left(\\mathcal{L}(i, q, \\theta) +\\mathrm{KL}\\left(q_i   \\vert  \\vert  p_i \\right)\\right)\\\\\n&=& \\mathcal{L}(q, \\theta) + \\mathrm{KL}\\left(q  \\vert  \\vert  p \\right)\n\\end{eqnarray}\\]\nwhere we are defining:\n\\[\\mathrm{KL}(q  \\vert  \\vert  p) = \\sum_i \\mathrm{KL}\\left(q_i   \\vert  \\vert  p_i \\right)\\]\nas the sum of the KL-divergence at each data point, and \\(\\mathcal{L}(q, \\theta)\\) as the sum of \\(\\mathcal{L}\\) at each data point."
  },
  {
    "objectID": "posts/em/index.html#the-gaussian-mixture-model-using-em",
    "href": "posts/em/index.html#the-gaussian-mixture-model-using-em",
    "title": "The EM Algorithm",
    "section": "",
    "text": "We dont know how to solve for the MLE of the unsupervised problem. The EM algorithm comes to the rescue. As described above here is the algorithm:\n\nRepeat until convergence\nE-step: For each \\(i,j\\) calculate\n\n\\[ w_{i,j} = q_i(z_i=j)=p(z_i=j \\vert  x_i, \\lambda, \\mu, \\Sigma) \\]\n\nM-step: We need to maximize, with respect to our parameters the\n\n\\[\n\\begin{eqnarray}\n\\mathcal{L} &=& \\sum_i \\sum_{z_i} q_i(z_i) \\log \\frac{p(x_i,z_i  \\vert \\lambda, \\mu, \\Sigma)}{q_i(z_i)} \\nonumber \\\\\n\\mathcal{L} &=& \\sum_i \\sum_{j=i}^{k}  q_i(z_i=j) \\log \\frac{p(x_i \\vert z_i=j , \\mu, \\Sigma) p(z_i=j \\vert \\lambda)}{q_i(z_i=j)} \\\\\n\\mathcal{L} & =&  \\sum_{i=1}^{m} \\sum_{j=i}^{k} w_{i,j}  \\log \\left[   \\frac{ \\frac{1}{ (2\\pi)^{n/2} \\vert \\Sigma_j \\vert ^{1/2}} \\exp \\left(    -\\frac{1}{2}(x_i-\\mu_j)^T \\Sigma_j^{-1} (x_i-\\mu_j) \\right)  \\, \\lambda_j   }{w_{i,j}}\\right]\n\\end{eqnarray}\n\\]\nTaking the derivatives yields the following updating formulas:\n\\[\n\\begin{eqnarray}\n\\lambda_j &=& \\frac{1}{m} \\sum_{i=1}^m w_{i,j} \\nonumber \\\\\n\\mu_j&=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, x_i}{ \\sum_{i=1}^m  w_{i,j}} \\nonumber \\\\\n\\Sigma_j &=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, (x_i-\\mu_j)(x_i-\\mu_j)^T}{ \\sum_{i=1}^m  w_{i,j}}\n\\end{eqnarray}\n\\]\nTo calculate the E-step we basically calculating the posterior of the \\(z\\)’s given the \\(x\\)’s and the current estimate of our parameters. We can use Bayes rule\n\\[ w_{i,j}= p(z_i=j \\vert  x_i, \\lambda, \\mu, \\Sigma) = \\frac{p( x_i \\vert  z_i=j,  \\mu, \\Sigma)\\, p(z_i=j \\vert \\lambda)}{\\sum_{l=1}^k p(x_i  \\vert  z_i=l,  \\mu, \\Sigma) \\, p(z_i=l \\vert \\lambda)} \\]\nWhere \\(p(x_i  \\vert  z_i =j,  \\mu, \\Sigma)\\) is the density of the Gaussian with mean \\(\\mu_j\\) and covariance \\(\\Sigma_j\\) at \\(x_i\\) and \\(p(z_i=j \\vert  \\lambda)\\) is simply \\(\\lambda_j\\). If we to compare these formulas in the M-step with the ones we found in GDA we can see that are very similar except that instead of using \\(\\delta\\) functions we use the \\(w\\)’s. Thus the EM algorithm corresponds here to a weighted maximum likelihood and the weights are interpreted as the ‘probability’ of coming from that Gaussian instead of the deterministic \\(\\delta\\) functions. Thus we have achived a soft clustering (as opposed to k-means in the unsupervised case and classification in the supervised case).\n\n#In 1-D\n# True parameter values\nmu_true = [2, 5]\nsigma_true = [0.6, 0.6]\nlambda_true = .4\nn = 1000\n\n# Simulate from each distribution according to mixing proportion psi\nz = np.random.binomial(1, lambda_true, n)\nx = np.array([np.random.normal(mu_true[i], sigma_true[i]) for i in z])\n\nplt.hist(x, bins=20);\n\n\n\n\n\n\n\n\n\n#from Bios366 lecture notes\nfrom scipy.stats.distributions import norm\n\ndef Estep(x, mu, sigma, lam):\n    a = lam * norm.pdf(x, mu[0], sigma[0])\n    b = (1. - lam) * norm.pdf(x, mu[1], sigma[1])\n    return b / (a + b)\n\n\n\ndef Mstep(x, w):\n    lam = np.mean(1.-w) \n    \n    mu = [np.sum((1-w) * x)/np.sum(1-w), np.sum(w * x)/np.sum(w)]\n    \n    sigma = [np.sqrt(np.sum((1-w) * (x - mu[0])**2)/np.sum(1-w)), \n             np.sqrt(np.sum(w * (x - mu[1])**2)/np.sum(w))]\n    \n    return mu, sigma, lam\n\n\nprint(lambda_true, mu_true, sigma_true)\n# Initialize values\nmu = np.random.normal(4, 10, size=2)\nsigma = np.random.uniform(0, 5, size=2)\nlam = np.random.random()\nprint(\"Initials, mu:\", mu)\nprint(\"Initials, sigma:\", sigma)\nprint(\"Initials, lam:\", lam)\n\n# Stopping criterion\ncrit = 1e-15\n\n# Convergence flag\nconverged = False\n\n# Loop until converged\niterations=1\n\n\nwhile not converged:\n    # E-step\n    if np.isnan(mu[0]) or np.isnan(mu[1]) or np.isnan(sigma[0]) or np.isnan(sigma[1]):\n        print(\"Singularity!\")\n        break\n        \n    w = Estep(x, mu, sigma, lam)\n\n    # M-step\n    mu_new, sigma_new, lam_new = Mstep(x, w)\n    \n    # Check convergence\n    converged = ((np.abs(lam_new - lam) &lt; crit) \n                 & np.all(np.abs((np.array(mu_new) - np.array(mu)) &lt; crit))\n                 & np.all(np.abs((np.array(sigma_new) - np.array(sigma)) &lt; crit)))\n    mu, sigma, lam = mu_new, sigma_new, lam_new\n    iterations +=1           \n\nprint(\"Iterations\", iterations)\nprint('A: N({0:.4f}, {1:.4f})\\nB: N({2:.4f}, {3:.4f})\\nlam: {4:.4f}'.format(\n                        mu_new[0], sigma_new[0], mu_new[1], sigma_new[1], lam_new))\n\n0.4 [2, 5] [0.6, 0.6]\nInitials, mu: [  0.72500527 -20.77035111]\nInitials, sigma: [ 4.59386658  3.6262629 ]\nInitials, lam: 0.6261111131564271\nIterations 95\nA: N(5.0083, 0.6288)\nB: N(2.0261, 0.5936)\nlam: 0.4116"
  },
  {
    "objectID": "posts/em/index.html#why-is-em-important",
    "href": "posts/em/index.html#why-is-em-important",
    "title": "The EM Algorithm",
    "section": "",
    "text": "We have motivated the EM algorithm using mixture models and missing data, but that is not its only place of use.\nSince MLE’s can overfit, we often prefer to use MAP estimation. EM is a perfectly reasonable method for MAP estimation in mixture models; you just need to multiply in the prior.\nBasically the EM algorithm has a similar setup to the data augmentation problem and can be used in any problem which has a similar structure. Suppose for example you have two parameters \\(\\phi\\) and \\(\\gamma\\) in a posterior estimation, with daya \\(y\\). Say that we’d like to estimate the posterior \\(p(\\phi  \\vert  y)\\). It may be relatively hard to estimate this, but suppose we can work with \\(p(\\phi  \\vert  \\gamma, y)\\) and \\(p(\\gamma  \\vert  \\phi, y)\\). Then you can use the structure of the EM algorithm to estimate the marginal posterior of any one parameter. Start with:\n\\[log p(\\phi  \\vert  y) = log p(\\gamma, \\phi  \\vert  y) - log p(\\gamma  \\vert  \\phi, y)\\]\nNotice the similarity of this to the above expressions with \\(\\phi\\) as \\(x\\), \\(y\\) as \\(\\theta\\), and \\(\\gamma\\) as \\(z\\). Thus the same derivations apply toany problem with this structure.\nThis structure can also be used in type-2 likelihood or emprical bayes estimation."
  },
  {
    "objectID": "posts/frequentist.html",
    "href": "posts/frequentist.html",
    "title": "Frequentist Statistics",
    "section": "",
    "text": "\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]"
  },
  {
    "objectID": "posts/frequentist.html#what-is-data",
    "href": "posts/frequentist.html#what-is-data",
    "title": "Frequentist Statistics",
    "section": "What is data?",
    "text": "What is data?\nWhat is data? Frequentist statistics is one answer to this philosophical question. It treats data as a sample from an existing population.\nThis notion is probably clearest to you from elections, where some companies like Zogby or CNN take polls. The sample in these polls maybe a 1000 people, but they “represent” the electoral population at large. We attempt to draw inferences about how the population will vote based on these samples.\nWe model the sample we have. This model typically involves having some kind of distribution, some kind of algorithm, some kind of story that characterizes the data. These descriptions have, usually, some parameters which need estimation.\nFrequentist analysis considers these parameters as fixed and data as varying (stochastic), with our data as one possible sample from the population.\n\nThe Data story\nData analysis involves coming up with a story of how the data came to be. This may be a causal story, or a descriptive one (correlational, associative). The critical point is this:\nThe story must be sufficient to specify an algorithm to simulate new data. This is the model we have been talking about: a formal probability model. And once we have pinned it down from our existing sample, using a method such as Maximum Likelihood Estimation talked about below, we can use it as a generating mechanism.\nConsider, for example, tossing a globe in the air and catching it. When you catch it mark whats under your right index finger: W for water, L for land.\nLets say you toss the globe 10 times and get something like WLWWWLWlWW. We wish to analyze this experiment to figure how much of the earth is covered in water (according to the globe, at any rate!).\nLet us say that our model is:\n\nThe true proportion of water is \\(p\\).\nWe use this as a Bernoulli probability for each globe toss, where \\(p\\) is thus the probability that you get a W. This assumption is one of being Identically Distributed.\nEach globe toss is Independent of the other.\n\nAssumptions 2 and 3 taken together are called IID, or Independent and Identially Distributed Data."
  },
  {
    "objectID": "posts/frequentist.html#a-probabilistic-model",
    "href": "posts/frequentist.html#a-probabilistic-model",
    "title": "Frequentist Statistics",
    "section": "A probabilistic model",
    "text": "A probabilistic model\n(from the data story)\nThe components of the model depend upon what kind of statistical analysis we are doing. For Frequentist analysis, the components are:\n\nThe likelihood, or the plausibility of the data under the model\nand the parameters which go into this plausibility.\n\nFor our example, we begin by enumerating the events. These are W and L. There’s nothing else.\nThen we consider N such tosses and ask the question, how often would we see Ws.\nThis given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k \\mid n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\nBinomial distribution for various values of p\n\n\nNow we use a method to fit our model and find the parameter \\(p\\), or rather, the estimate \\(\\hat{p}\\) that we can obtain from our sample. Once we have that, we can use the Binomial distribution to generate new samples.\nNote that there is a problem with this, in that we dont know the true value \\(p^*\\) of the globe-toss model (speaking in the frequentist paradigm). Thus we are generating new samples from our estimate, rather than our true value."
  },
  {
    "objectID": "posts/frequentist.html#maximum-likelihood-estimation",
    "href": "posts/frequentist.html#maximum-likelihood-estimation",
    "title": "Frequentist Statistics",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nOne of the techniques used to estimate parameters in frequentist statistics, from the data in a given sample, is maximum likelihood estimation. Briefly, the idea behind it is:\nThe likelihood for IID data \\(x_1,...,x_n\\), is the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i | \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nAn image can explain this better. We want to choose the distribution that maximizes the product of the vertical lines. Here the blue does better, but it is not clear if the blue is the best.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nOften it is easier and numerically more stable to maximize the log likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nNotice that the definition here is a bit different from that for the question of the globe tosses above: there the data was \\(k\\), the number of W tosses and not the exact order, and so we formulate the question in that form.\nSo dont follow the formula blindly, but think of (a) what is the data, and (b) what is the data generating mechanism!\n\nMLE for binomial.\nThere:\n\\[P(X = k \\mid n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nSo:\n\\[\\ell = log({n\\choose k}) + k log(p) + (n-k) log(1-p)\\]\nDifferentiating with respect to \\(p\\) and setting to 0 yields:\n\\[\\frac{d\\ell}{dp} = \\frac{k}{p}  - \\frac{n -k}{1-p} = 0\\]\nwhich gives us:\n\\[p_{MLE} = \\frac{k}{n}\\]\nwhich you might have intuitively expected."
  },
  {
    "objectID": "posts/frequentist.html#point-estimates",
    "href": "posts/frequentist.html#point-estimates",
    "title": "Frequentist Statistics",
    "section": "Point estimates",
    "text": "Point estimates\nIn frequentist statistics, the data we have in hand, is viewed as a sample from a population. So if we want to calculate some quantity of the population, like say the mean, we estimate it on the sample.\nThis is because we’ve been given only one sample. Ideally we’d want to see the population, but we have no such luck.\nThe parameter estimate is computed by applying an estimator \\(F\\) to the sample data \\(D\\), so \\(\\est{\\mu} = F(D)\\).\nThe parameter is viewed as fixed and the data as random, which is the exact opposite of the Bayesian approach which you will learn later in this class.\nIf you assume that your model describes the true generating process for the data, then there is some true \\(\\trueval{\\mu}\\) . We dont know this. The best we can do to start with is to estimate the \\(\\est{\\mu}\\) from the data set we have.\n\nFrom single to multiple estimates\nNow, imagine that I let you peek at the entire population in this way: I gave you some M data sets drawn from the population, and you can now find \\(\\mu\\) on each such dataset, of which the one we have here is one. So, we’d have M estimates of the \\(\\mu\\).\nThus if we had many replications of this data set: that is, an ensemble of data sets, for example, we can compute other \\(\\est{\\mu}\\), and begin to construct what is called the sampling distribution of \\(\\mu\\).\nBut we dont."
  },
  {
    "objectID": "posts/frequentist.html#sampling-distribution-of-the-parameter",
    "href": "posts/frequentist.html#sampling-distribution-of-the-parameter",
    "title": "Frequentist Statistics",
    "section": "Sampling Distribution of the parameter",
    "text": "Sampling Distribution of the parameter\nWhat you are doing is sampling M Data Sets \\(D_i\\) from the true population. We will now calculate M \\(\\est{\\mu}_i\\), one for each dataset. As we let \\(M \\rightarrow \\infty\\), the distribution induced on \\(\\est{\\mu}\\) is the sampling distribution of the estimator.\nOur estimation could be of anything, even for example the \\(\\lambda\\) we were tying to find with MLE (F would be the MLE estimation process).\nWe can use the sampling distribution to put confidence intervals on the estimation of the parameters, for example."
  },
  {
    "objectID": "posts/frequentist.html#bootstrap",
    "href": "posts/frequentist.html#bootstrap",
    "title": "Frequentist Statistics",
    "section": "Bootstrap",
    "text": "Bootstrap\nBootstrap tries to approximate our sampling distribution. If we knew the true parameters of the population, we could generate M fake datasets. Then we could compute the parameter (or another estimator) on each one of these, to get a empirical sampling distribution of the parameter or estimator.\n\nParametric Bootstrap\nBut we dont have the true parameter. So we generate these samples, using the parameter we calculated. This is the parametric bootstrap. The process is illustrated in the diagram below, taken from Shalizi:\n\n\n\nThe parametric bootstrap process\n\n\nThere are 3 sources of error with respect to the sampling distribution that come from the bootstrap:\n\nsimulation error: the number of samples M is finite. This can be made arbitrarily small by making M large\nstatistical error: resampling from an estimated parameter is not the “true” data generating process. Often though, the distribution of an estimator from the samples around the truth is more invariant, so subtraction is a good choice in reducing the sampling error\nspecification error: the model isnt quite good.\n\n\n\nNon-parametric bootstrap\nTo address specification error, alternatively, we sample with replacement the X from our original sample D, generating many fake datasets, and then compute the distribution on the parameters as before. This is the non parametric bootstrap. We want to sample with replacement, for if we do so, more typical values will be represented more often in the multiple datasets we create.\nHere we are using the empirical distribution, since it comes without any model preconceptions. This process may be illustrated so:\n\n\n\nThe non-parametric bootstrap process"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html",
    "href": "posts/bayes_withsampling/index.html",
    "title": "Bayesian Statistics",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as  sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn.apionly as sns\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#frequentist-statistics",
    "href": "posts/bayes_withsampling/index.html#frequentist-statistics",
    "title": "Bayesian Statistics",
    "section": "Frequentist Statistics",
    "text": "Frequentist Statistics\nIn frequentist approach, a parameter estimate is computed by using some function on the data \\(D\\). In the world of frequentist there is a true value of the parameter, which is fixed, however the data are random.\nIn other words, assuming that a given distribution is a good description of the model, there is a true parameter \\(\\theta^{\\star}\\) that characterizes the population. Given the data set(sample) we have, we can estimate the parameter \\(\\hat{\\theta}\\). If we could replicate the experiment many times we would have the sampling distribution of \\(\\theta\\) which can be used to estimate the error on the parameter estimation. By sampling \\(M\\) Data sets \\(D_i\\), each of size \\(N\\), from some true model characterized by \\(\\theta^{\\star}\\) we are able to calculate \\(\\hat{\\theta}_i\\), one for each dataset. This is the sampling distribution.\n\nMaximum Likelihood\nA basic approach to parameter estimation is maximum likelihood (MLE). The goal here is to find the parameter estimates that maximize the likelihood.\nThe likelihood gives us a measure of how likely it is to observe values \\(D={d_1,...,d_n}\\) given the parameters \\(\\theta\\).\nAssumming iid, the likelihood is\n\\[L=\\Lik = \\prod_{i=1}^{n} p(d_i \\vert \\theta)\\]\nHow likely are the observations if the model is true?\nThis corresponds to maximizing the likelihood as a function of \\(\\theta\\) for a given set of observations.\n\\[ \\theta_{ML} = \\arg \\! \\max_{\\theta} \\Lik \\]\nNotice that this method wants to account for every point in the “training set”. So it overfits."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#the-bayesian-approach",
    "href": "posts/bayes_withsampling/index.html#the-bayesian-approach",
    "title": "Bayesian Statistics",
    "section": "The Bayesian Approach",
    "text": "The Bayesian Approach\nIn its essence, the Bayesian approach has two parts.\n\ntreat \\(\\theta\\) as a random variable instead, and to fix the data set. So we dont talk anymore about the data set as a sample from a population, but assume that its all we know about the world.\n\n\nAssociate with the parameter \\(\\theta\\) a prior distribution \\(p(\\theta)\\).\n\nThe prior distribution generally represents our belief on the parameter values when we have not observed any data yet. (I use the wiggle word generally as we might estimate this prior itself from data. This is a useful idea, although philosophically-bayesian purists will frown on it)\n\nPosterior Distribution\nIn a Bayesian context, the first goal is to estimate the posterior distribution over parameter values given our data. This is also known as posterior inference. In other words, we would like to know \\(p(\\theta \\vert D)\\) or \\(p(\\theta \\vert y)\\).\n\\[ p(\\theta \\vert y) = \\frac{p(y \\vert \\theta)\\,p(\\theta)}{p(y)} \\]\nwith the evidence \\(p(D)\\) or \\(p(y)\\) being given by the average of the likelihood (on existing data points) over the prior \\(E_{p(\\theta)}[\\cal{L}]\\):\n\\[p(y) = \\int d\\theta p(y \\vert \\theta) p(\\theta).\\]\nThe evidence is basically the normalization constant. But as we have seen, when we sample, we dont usually worry about the normalization…\nYou can remember this as:\n\\[ posterior = \\frac{likelihood \\times prior}{evidence} \\]\nThis diagram from McElreath’s book gives you an idea of what this might look like, and how the prior might affect the posterior in the absence of copius data…\n\n\n\nPrior times likelihood is proportional to posterior: three rows show how different priors (uniform, step, informative) combine with the same likelihood to produce different posteriors. From McElreath, Statistical Rethinking.\n\n\nWhat if \\(\\theta\\) is multidimensional, as it usually is? Then one can calculate the marginal posterior of one of the parameters by integrating over the other one:\n\\[p(\\theta_1 \\vert D) = \\int d\\theta_{-1} p(\\theta \\vert D).\\]\n\n\nPosterior Predictive\nRemember though at the end of the day, we care about how we are going to make predictions on future data, and not the values of the parameters. Thus what we wish to find is the distribution of a future data point \\(y^*\\), also known as the posterior predictive distribution:\n\\[p(y^* \\vert D=\\{y\\}) = \\int d\\theta p(y^* \\vert \\theta)p(\\theta \\vert \\{y\\})\\].\nIf you like, this is the average of the likelihood at a new point(s) \\(E_{p(\\theta \\vert D)}[p(y \\vert \\theta)]\\).\nIf you think about this, for example, from the perspective of a regression problem, this is the distribution for y at a new x (which in many cases is gaussian). This is not different from the frequentist case. But there the different y’s come from the different samples (typically realized in practice as bootstrap samples).\nWhere do priors come from? They are engineering assumptions we put in to help our models learn. Usually they have some regularizing effect. There is a branch of philosophy that takes the attitude that priors can be based on subjective belief. We dont usually do that in the sciences, but as long as you consistently define a probability system, subjective priors are fine to use.\n\n\nMaximum a posteriori\nThe posterior distribution is specified by a simple product of the likelihood (how likely is the data given the model that uses these parameter estimates) and the prior. In Bayesian data analysis, one way to apply a model to data is to find the maximum a posteriori (MAP) parameter values. The goal in this case is to find the parameter that maximize the posterior probability of the parameters given the data. In other words, we find the mode of the posterior distribution. This corresponds to:\n\\[\n\\begin{eqnarray}\n\\theta_{{\\rm MAP}} &=& \\arg \\max_{\\theta} \\, p(\\theta \\vert D)  \\nonumber \\\\\n                               & =& \\arg \\max_{\\theta}  \\frac{\\Lik \\, p(\\theta)}{p(D)}  \\nonumber \\\\\n                               & =& \\arg \\max_{\\theta}  \\, \\Lik \\, p(\\theta) \\nonumber \\\\\n\\end{eqnarray}\n\\]\nThis looks similar to the maximum likelihood estimation procedure. The difference is that the prior we set over the parameters does influence the parameter estimation.\nThe MAP is an example of a pont-estimate. In general point estimates come from decision risks. For example, the mean comes from a squared-errror risk. The MAP comes from 1-0 loss with equal weghts for all errors. We’ll come to this later.\n\n\nThe posterior predictive\nAt the end of the day we want to make predictions, here for the number of coin tosses (or globe throws) that come up heads (or water). This is given us by the postrior predictive, which is the average of the likelihood at the points where the data is wanted with the posterior.\nThe entire process is illustrated in this diagram, where the posterior is multiplied by the likelihood, one at each point \\(y\\) (number of samples), and then integrated over the parameters.\n\n\n\nThe posterior predictive distribution as a mixture: each parameter value implies a sampling distribution, weighted by the posterior probability, producing the marginal prediction. From McElreath, Statistical Rethinking."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#the-normal-model",
    "href": "posts/bayes_withsampling/index.html#the-normal-model",
    "title": "Bayesian Statistics",
    "section": "The Normal Model",
    "text": "The Normal Model\nA random variable \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Thus its density is given by :\n\\[ p(y \\vert \\mu, \\sigma^2) =  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{y-\\mu}{2 \\sigma})^2} \\]\nSuppose our model is \\(\\{y_1, \\ldots, y_n \\vert \\mu, \\sigma^2 \\} \\sim N(\\mu, \\sigma^2)\\) then the likelihood is\n\\[\np(y_1, \\ldots, y_n \\vert \\mu, \\sigma^2) =\n\\prod_{i=1}^{n} p(y_i \\vert \\mu, \\sigma^2)=\\prod_{i=1}^{n}  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{(y_i-\\mu)^2}{2\\sigma^2})} =\n\\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}}   \\exp \\left\\{  - \\frac{1}{2}  \\sum_i \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right\\}\n\\]\nWe can now write the posterior for this model thus:\n\\[ p( \\mu, \\sigma^2 \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{ - \\frac{1}{2\\sigma^2} \\sum (y_i - \\mu)^2 } \\, p(\\mu, \\sigma^2)\\]\nLets see the posterior of \\(\\mu\\) assuming we know \\(\\sigma^2\\).\n\nNormal Model for fixed \\(\\sigma\\)\nNow we wish to condition on a known \\(\\sigma^2\\). The prior probability distribution for it can then be written as:\n\\[p(\\sigma^2) = \\delta(\\sigma^2 -\\sigma_0^2)\\]\n(which does integrate to 1).\nNow, keep in mind that \\(p(\\mu, \\sigma^2) = p(\\mu \\vert \\sigma^2) p(\\sigma^2)\\) and we must carry out the integral over \\(\\sigma^2\\) to get the \\(\\mu\\) prior. Because of the delta distribution means that we can do everything by just substituting \\(\\sigma_0^2\\) in\nThus, we get the posterior:\n\\[ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2 = \\sigma_0^2)  \\propto p(\\mu \\vert \\sigma^2=\\sigma_0^2) \\,e^{ - \\frac{1}{2\\sigma_0^2} \\sum (y_i - \\mu)^2 }\\]\nwhere I have dropped the \\(\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\) factor as there is no stochasticity in it (its fixed)."
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#example-of-the-normal-model-for-fixed-sigma",
    "href": "posts/bayes_withsampling/index.html#example-of-the-normal-model-for-fixed-sigma",
    "title": "Bayesian Statistics",
    "section": "Example of the normal model for fixed \\(\\sigma\\)",
    "text": "Example of the normal model for fixed \\(\\sigma\\)\nWe have data on the wing length in millimeters of a nine members of a particular species of moth. We wish to make inferences from those measurements on the population mean \\(\\mu\\). Other studies show the wing length to be around 19 mm. We also know that the length must be positive. We can choose a prior that is normal and most of the density is above zero (\\(\\mu=19.5,\\tau=10\\)). This is only a marginally informative prior.\nMany bayesians would prefer you choose relatively uninformative priors.\nThe measurements were: 16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8 giving \\(\\bar{y}=18.14\\).\n\nY = [16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8]\n#Data Quantities\nsig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\nmu_data = np.mean(Y)\nn = len(Y)\nprint(\"sigma\", sig, \"mu\", mu_data, \"n\", n)\n\nsigma 1.33092374864 mu 18.1444444444 n 9\n\n\n\n# Prior mean\nmu_prior = 19.5\n# prior std\nstd_prior = 10"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#sampling-by-code",
    "href": "posts/bayes_withsampling/index.html#sampling-by-code",
    "title": "Bayesian Statistics",
    "section": "Sampling by code",
    "text": "Sampling by code\nWe now set up code to do metropolis using logs of distributions:\n\ndef metropolis(logp, qdraw, stepsize, nsamp, xinit):\n    samples=np.empty(nsamp)\n    x_prev = xinit\n    accepted = 0\n    for i in range(nsamp):\n        x_star = qdraw(x_prev, stepsize)\n        logp_star = logp(x_star)\n        logp_prev = logp(x_prev)\n        logpdfratio = logp_star -logp_prev\n        u = np.random.uniform()\n        if np.log(u) &lt;= logpdfratio:\n            samples[i] = x_star\n            x_prev = x_star\n            accepted += 1\n        else:#we always get a sample\n            samples[i]= x_prev\n            \n    return samples, accepted\n\n\ndef prop(x, step):\n    return np.random.normal(x, step)\n\nRemember, that up to normalization, the posterior is the likelihood times the prior. Thus the log of the posterior is the sum of the logs of the likelihood and the prior.\n\nfrom scipy.stats import norm\nlogprior = lambda mu: norm.logpdf(mu, loc=mu_prior, scale=std_prior)\nloglike = lambda mu: np.sum(norm.logpdf(Y, loc=mu, scale=np.std(Y)))\nlogpost = lambda mu: loglike(mu) + logprior(mu)\n\nNow we sample:\n\nx0=np.random.uniform()\nnsamps=40000\nsamps, acc = metropolis(logpost, prop, 1, nsamps, x0)\n\nThe acceptance rate is reasonable. You should shoot for somewhere between 20 and 50%.\n\nacc/nsamps\n\n0.459925\n\n\n\ndef corrplot(trace, maxlags=50):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\n\nWhile thinning is not strictly needed, appropriately thinned, we lose any correlation faster and store less\n\ncorrplot(samps)\n\n\n\n\n\n\n\n\n\ncorrplot(samps[20000::]);\n\n\n\n\n\n\n\n\n\ncorrplot(samps[20000::4]);\n\n\n\n\n\n\n\n\n\nsns.distplot(samps[20000::4], bins=25);\nsns.distplot(samps[20000::], bins=25);\n\n\n\n\n\n\n\n\n\nlike_samples = norm.rvs(loc = mu_data, scale=sig, size=5000)\npost_samples = samps[20000::4]\nprior_samples = norm.rvs(loc = mu_prior, scale=tau, size=5000)"
  },
  {
    "objectID": "posts/bayes_withsampling/index.html#comparing-distributions",
    "href": "posts/bayes_withsampling/index.html#comparing-distributions",
    "title": "Bayesian Statistics",
    "section": "Comparing distributions",
    "text": "Comparing distributions\nWe plot samples from the prior against those from the sampling distribution (likelihood considered as a distribution in \\(\\theta\\) and those from the posterior.\n\nplt.hist(like_samples, bins=25, label=\"likelihood (sampling dist)\", alpha=0.3)\nplt.hist(prior_samples, bins=25, label=\"prior\", alpha=0.1)\nplt.hist(post_samples, bins=25, label=\"posterior\", alpha=0.4)\nplt.legend();\n\n\n\n\n\n\n\n\n\npost_pred_func = lambda post: norm.rvs(loc = post, scale = sig)\npost_pred_samples = post_pred_func(post_samples)\n\nWe then plot the posterior predictive against the sampling distribution. These are pretty close. Notice that both are wider than the distribution of the posterior, as they are distributions of \\(y\\) rather than \\(\\mu\\). It just so happens that these distributions are on the scale so it makes sense to plot them together here and compare them.\n\nplt.hist(like_samples, bins=25, label=\"likelihood (sampling dist)\", alpha=0.5)\nplt.hist(post_pred_samples, bins=25, label=\"posterior predictive\", alpha=0.3)\nplt.hist(post_samples, bins=25, label=\"posterior\", alpha=0.4)\nplt.legend();\n\n\n\n\n\n\n\n\n\nsns.distplot(like_samples);\nsns.distplot(post_pred_samples);"
  },
  {
    "objectID": "posts/expectations/index.html",
    "href": "posts/expectations/index.html",
    "title": "Expectations and the Law of Large Numbers",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/expectations/index.html#expectations",
    "href": "posts/expectations/index.html#expectations",
    "title": "Expectations and the Law of Large Numbers",
    "section": "Expectations",
    "text": "Expectations\n\\[ \\newcommand{\\E}[1]{E[#1]}\\]\nThe expectation value of a quantity with respect to the a density or probability mass function is the weighted sum of the quantity where the weights are probabilties from the distribution. For example, for the discrete random variable \\(X\\):\n\\[E_f[X] = \\sum_x x\\,f(x).\\]\nIn the continuous case the sum is replaced by an integral over the density:\n\\[E_f[X] = \\int x\\,f(x) dx = \\int x dF(x),\\]\nwhere the latter form makes it clear that you are weighing with probabilities from the distribution even in the continuous case.\nThe latter form is often used to establish notation. Thus, the expected value, or mean, or first moment, of X is defined to be \\[\nE_{f}{X} = \\int x dF(x) =\n\\begin{cases}\n\\sum_x x f(x) & \\text{if X is discrete}\\\\\n\\int x f(x) dx & \\text{if X is continuous}\n\\end{cases}\n\\] assuming that the sum (or integral) is well defined. The notation is a unifying notation which nevertheless has a grounding in measure theory; the discrete sum can be said to be an integral with respect to a counting measure.\nA note on notation: we’ll use \\(E_f\\) or sometimes even \\(E_F\\) when we need to make clear what the distribution is. If its clear (or we are being lazy) we might just drop the subscript. Nevertheless, wheneve you see an expectation, YOU MUST ASK, with what density/mass-function or distribution is it with respect to.\n\nThe mean of a distribution\n\\(E_f[X]\\) if often just called the mean of the mass function or density. This definition is analogous to the one for the arithmetic mean of a dataset: the only difference is that we want to give more weight to more probable values.\n\n\nLOTUS: Law of the unconscious statistician\nAlso known as The rule of the lazy statistician.\nTheorem:\nif \\(Y = r(X)\\), \\[\n\\E{Y} = \\int r(x) dF(x)\n\\]\nExample:\nSpecifically, let A be an event and let \\(r(x) = I_A (x)\\) where \\(I_A (x) = 1\\) if \\(x \\in A\\) and \\(I_A (x) = 0\\) if \\(x \\notin A\\). Then: \\[\n\\E{I_A (X)} = \\int I_A (x) dF(x) = \\int_A f_X (x) dx = p(X \\in A)\n\\]\n\n\nVariance of a distribution\nThe variance of a distribution is defined analogous to that of a dataset:\n\\[V_f[X] = E_f[(X-E_f[X])^2]\\].\nFor the Bernoulli distribution \\(f(x)=p=constant\\), and you are summing it over ones as opposed to 0’s, so the mean is just p. The variance is \\((1-p)^2\\times p +(-p)^2\\times (1-p) = p(1-p)(1-p+p) = p(1-p)\\).\nIn general, we can find this mean that by obtaining a large bunch of samples from the distribution and find their arithmetic mean. The justification for this is the Law of large numbers, which we’ll come to soon.\nHowever the intuition is obvious: for a large number of samples, the frequencies will tract probabilities well, so high probability samples with roughly the same value will re-occur, and a simple arithmetic sun will capture the curves of the distribution."
  },
  {
    "objectID": "posts/expectations/index.html#the-law-of-large-numbers",
    "href": "posts/expectations/index.html#the-law-of-large-numbers",
    "title": "Expectations and the Law of Large Numbers",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nImagine a sequence of length n of coin flips. Lets keep increasing the length of the sequence of coin flips n, and compute a running average \\(S_n\\) of the coin-flip random variables, \\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i .\\] We plot this running mean, and notice that it converges to the mean of the distribution from which the random variables are plucked, ie the Bernoulli distribution with p=0.5.\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\nrandom_flips = throw_a_coin(10000)\nrunning_means = np.zeros(10000)\nsequence_lengths = np.arange(1,10001,1)\nfor i in sequence_lengths:\n    running_means[i-1] = np.mean(random_flips[:i])\n\n\nplt.plot(sequence_lengths, running_means);\nplt.xscale('log')\n\n\n\n\n\n\n\n\nThis is an example of a very important theorem in statistics, the law of large numbers, which says this:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) values from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to the mean of \\(X\\) \\(\\mu\\) as \\(n \\to \\infty\\):\n\\[ S_n \\to \\mu \\, as \\, n \\to \\infty. \\]"
  },
  {
    "objectID": "posts/expectations/index.html#frequentist-interpretation-of-probability",
    "href": "posts/expectations/index.html#frequentist-interpretation-of-probability",
    "title": "Expectations and the Law of Large Numbers",
    "section": "Frequentist interpretation of probability",
    "text": "Frequentist interpretation of probability\nThe law of large numbers is what makes the frequentist interpretation of probability possible to use in practise.\nWe saw above from the LOTUS that if we consider any event \\(A\\) from a probability distribution \\(F\\) with random variable X, and consider the indicator function \\(I_A\\) such that:\n\\[\\begin{eqnarray}\nI_A(x) = 1 \\,&& if \\, x \\in A\\\\\nI_A(x) = 0 \\,&&  otherwise\n\\end{eqnarray}\\]\nwe have that:\n\\[E_{F}[I_A (X)] = p(X \\in A)\\]\nOne can think of variable \\(Z=I_A(X)\\) as Bernoulli random variable with parameter and thus p = P(A). The question then arises: how do we estimate this expectation value and thus the probability?\nNow if we take a long sequence from \\(X\\) and thus \\(Z\\), then the frequency of successes (where success means being in A) will converge by the law of large numbers to the true probability p."
  },
  {
    "objectID": "posts/normalmodel/index.html",
    "href": "posts/normalmodel/index.html",
    "title": "The Normal Model",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pylab as plt \nimport seaborn as sn\n\nfrom scipy.stats import norm\nA random variable \\(Y\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Thus its density is given by :\n\\[ p(y \\vert \\mu, \\sigma^2) =  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{y-\\mu}{2 \\sigma})^2} \\]\nSuppose our model is \\(\\{y_1, \\ldots, y_n \\vert \\mu, \\sigma^2 \\} \\sim N(\\mu, \\sigma^2)\\) then the likelihood is\n\\[\np(y_1, \\ldots, y_n \\vert \\mu, \\sigma^2) =\n\\prod_{i=1}^{n} p(y_i \\vert \\mu, \\sigma^2)=\\prod_{i=1}^{n}  \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{-( \\frac{(y_i-\\mu)^2}{2\\sigma^2})} =\n\\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}}   \\exp \\left\\{  - \\frac{1}{2}  \\sum_i \\frac{(y_i-\\mu)^2}{\\sigma^2} \\right\\}\n\\]\nWe can now write the posterior for this model thus:\n\\[ p( \\mu, \\sigma^2 \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\frac{1}{ \\sqrt{ 2 \\pi \\sigma^2}} e^{ - \\frac{1}{2\\sigma^2} \\sum (y_i - \\mu)^2 } \\, p(\\mu, \\sigma^2)\\]\nLets see the posterior of \\(\\mu\\) assuming we know \\(\\sigma^2\\)."
  },
  {
    "objectID": "posts/normalmodel/index.html#normal-model-for-fixed-sigma",
    "href": "posts/normalmodel/index.html#normal-model-for-fixed-sigma",
    "title": "The Normal Model",
    "section": "Normal Model for fixed \\(\\sigma\\)",
    "text": "Normal Model for fixed \\(\\sigma\\)\nNow we wish to condition on a known \\(\\sigma^2\\). The prior probability distribution for it can then be written as:\n\\[p(\\sigma^2) = \\delta(\\sigma^2 -\\sigma_0^2)\\]\n(which does integrate to 1).\nNow, keeping in mind that \\(p(\\mu, \\sigma^2) = p(\\mu \\vert \\sigma^2) p(\\sigma^2)\\) and carrying out the integral over \\(\\sigma^2\\) which because of the delta distribution means that we must just substitute \\(\\sigma_0^2\\) in, we get:\n\\[ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2 = \\sigma_0^2)  \\propto p(\\mu \\vert \\sigma^2=\\sigma_0^2) \\,e^{ - \\frac{1}{2\\sigma_0^2} \\sum (y_i - \\mu)^2 }\\]\nwhere I have dropped the \\(\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}\\) factor as there is no stochasticity in it (its fixed).\nSay we have the prior\n\\[ p(\\mu \\vert \\sigma^2) = \\exp \\left\\{ -\\frac{1}{2 \\tau^2} (\\hat{\\mu}-\\mu)^2 \\right\\} \\]\nthen it can be shown that the posterior is\n\\[  p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2) \\propto \\exp \\left\\{ -\\frac{a}{2} (\\mu-b/a)^2 \\right\\} \\] where \\[ a = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma_0^2} , \\;\\;\\;\\;\\; b = \\frac{\\hat{\\mu}}{\\tau^2} + \\frac{\\sum y_i}{\\sigma_0^2} \\] This is a normal density curve with \\(1/\\sqrt{a}\\) playing the role of the standard deviation and \\(b/a\\) playing the role of the mean. Re-writing this,\n\\[ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{\\mu-b/a}{1/\\sqrt(a)}\\right)^2 \\right\\} \\]\nThe conjugate of the normal is the normal itself.\nDefine $= ^2 / ^2 $ to be the variance of the sample model in units of variance of our prior belief (prior distribution) then the posterior mean is\n\\[\\mu_p = \\frac{b}{a} = \\frac{ \\kappa}{\\kappa + n }  \\hat{\\mu} + \\frac{n}{\\kappa + n} \\bar{y} \\]\nwhich is a weighted average of prior mean and sampling mean. The variance is\n\\[ \\sigma_p^2 = \\frac{1}{1/\\tau^2+n/\\sigma^2} \\] or better\n\\[ \\frac{1}{\\sigma_p^2} = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}. \\]\nYou can see that as \\(n\\) increases, the data dominates the prior and the posterior mean approaches the data mean, with the posterior distribution narrowing…"
  },
  {
    "objectID": "posts/normalmodel/index.html#example-of-the-normal-model-for-fixed-sigma",
    "href": "posts/normalmodel/index.html#example-of-the-normal-model-for-fixed-sigma",
    "title": "The Normal Model",
    "section": "Example of the normal model for fixed \\(\\sigma\\)",
    "text": "Example of the normal model for fixed \\(\\sigma\\)\nWe have data on the wing length in millimeters of a nine members of a particular species of moth. We wish to make inferences from those measurements on the population mean \\(\\mu\\). Other studies show the wing length to be around 19 mm. We also know that the length must be positive. We can choose a prior that is normal and most of the density is above zero (\\(\\mu=19.5,\\tau=10\\)). This is only a marginally informative prior.\nMany bayesians would prefer you choose relatively uninformative (and thus weakly regularizing) priors. This keeps the posterior in-line (it really does help a sampler remain in important regions), but does not add too much information into the problem.\nThe measurements were: 16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8 giving \\(\\bar{y}=18.14\\).\n\nY = [16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8]\n#Data Quantities\nsig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\nmu_data = np.mean(Y)\nn = len(Y)\nprint(\"sigma\", sig, \"mu\", mu_data, \"n\", n)\n\nsigma 1.33092374864 mu 18.1444444444 n 9\n\n\n\n# Prior mean\nmu_prior = 19.5\n# prior std\ntau = 10 \n\n\nkappa = sig**2 / tau**2\nsig_post =np.sqrt(1./( 1./tau**2 + n/sig**2));\n# posterior mean\nmu_post = kappa / (kappa + n) *mu_prior + n/(kappa+n)* mu_data\nprint(\"mu post\", mu_post, \"sig_post\", sig_post)\n\nmu post 18.1471071751 sig_post 0.443205311006\n\n\n\n#samples\nN = 15000\ntheta_prior = np.random.normal(loc=mu_prior, scale=tau, size=N);\ntheta_post = np.random.normal(loc=mu_post, scale=sig_post, size=N);\n\n\nplt.hist(theta_post, bins=30, alpha=0.9, label=\"posterior\");\nplt.hist(theta_prior, bins=30, alpha=0.2, label=\"prior\");\n#plt.xlim([10, 30])\nplt.xlabel(\"wing length (mm)\")\nplt.ylabel(\"Number of samples\")\nplt.legend();\n\n\n\n\n\n\n\n\nIn the case that we dont know \\(\\sigma^2\\) or wont estimate it the way we did above, it turns out that a conjugate prior for the precision (inverse variance) is a gamma distribution. Interested folks can see Murphy’s detailed document here. but you can always just use our MH machinery to draw from any vaguely informative prior for the variance ( a gamma for the precision or even for the variance)."
  },
  {
    "objectID": "posts/divergence/index.html",
    "href": "posts/divergence/index.html",
    "title": "Divergence and Deviance",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/divergence/index.html#the-problem-of-learning",
    "href": "posts/divergence/index.html#the-problem-of-learning",
    "title": "Divergence and Deviance",
    "section": "The problem of learning",
    "text": "The problem of learning\nWe’ve seen cross-validation as a way of minimizing a loss (cost, error, or risk) on the training set, and then obtaining the final model on our validation set, with the possible fitting of a hyperparameter.\nWhat we have done here is, to choose a particular model from a hypothesis set, based on a cost minimization criterion.\nThe basic idea in doing that was to find the out-of-sample, or population loss. Since we showed that we can bound it to within the validation (and test) loss using Hoeffding’s inequality, we can use the latter losses as proxy.\nAnd we have wanted to avoid overfitting, which is, as McElreath calls it, the tendency of the model to get over-excited by the training sample.\nWe also seen regularization in this context. In this case we choose a more complex model than we would have otherwise, but use cross-validation on a changed cost function which then bounds the set of admissible functions from the more complex model.\nWe also saw these issues in the context of supervized learning, where we were trying to solve a classification or regression problem.\nIn the realm of probabilistic models, both supervized learning and unsupervized learning boil down to probability density estimation. For supervized learning we want to find \\(p(y\\vert x)\\) or \\(p(x,y)\\) and in unsupervized learning, we wish to find \\(p(x)\\).\nIn these cases, the problem could be cast in the following form: suppose nature has a true “population” distribution \\(p(x)\\). As usual I am given a sample, and make my effort learning a distribution from this sample, \\(q(x)\\). Our question then is: how good did i do? And what additional uncertainty did I introduce by using \\(q\\) instead of \\(p\\)?"
  },
  {
    "objectID": "posts/divergence/index.html#information-theory-kl-divergence",
    "href": "posts/divergence/index.html#information-theory-kl-divergence",
    "title": "Divergence and Deviance",
    "section": "Information Theory: KL Divergence",
    "text": "Information Theory: KL Divergence\nIn other words, if \\(p\\) is nature’s distribution, we want to know how far we are from “perfect accuracy” by using \\(q\\). In other words we need to develop a distance scale for distances between distributions.\nThis scale is called the Kullback-Leibler (KL) Divergence, introduced in 1951. It is defined thus:\n\\[\\renewcommand{\\kld}{D_{KL}}\\]\n\\[\\kld(p, q) = E_p[log(p) - log(q)] = E_p[log(p/q)] = \\sum_i p_i log(\\frac{p_i}{q_i}) \\,\\,or\\, \\int dP log(\\frac{p}{q})\\]\nThe distance between a distribution and itself is clearly \\(\\kld(p,p) = 0\\).\nWe can use Jensen’s inequality for expectations on a convex function \\(f(x)\\),\n\\[ E[f(X)] \\ge f(E[X]) \\]\nto show that \\(\\kld(p,q) \\ge 0\\) with equality iff (if and only if) \\(q=p\\).\n\\[\\kld(p,q) = E_p[log(p/q)] = E_p[-log(q/p)] \\ge -\\log \\left( E_p[q/p] \\right) = -\\log(\\int dQ) = 0\\]\nwhere we have used the fact that \\(-log(x)\\) is a convex function, and that \\(q(x)\\) normalizes to a distribution. Infact, since \\(-\\log(x)\\) is strictly convex, the equality only happens if \\(q(x) = p(x)\\) for ALL x.\nThus we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x). In frequentist statistics, the KL-divergence is related to the maximum likelihood, in Bayesian statistics the KL divergence can be used as a measure of the information gain in moving from a prior to posterior (with a common goal in Bayesian experimental design to maximise the expected KL divergence between the prior and the posterior). The divergence is also used to understand mutual information in clustering, and in variational bayesian inference.\n\nA simple example\nConsider a Bernoulli distribution with probability parameter \\(p=0.3\\). This is a discrete distribution, defined at 0 and 1. Consider using another Bernoulli with parameter \\(q\\) to approximate it. You can see that the divergence is 0 for \\(q=0.3\\) and always higher for any other \\(q\\).\n\np=0.3\n\ndef kld(p,q):\n    return p*np.log(p/q) + (1-p)*np.log((1-p)/(1-q))\n\nqs=np.linspace(0,1,100)\nplt.plot(qs, [kld(0.3,q) for q in qs]);\n\n\n\n\n\n\n\n\n\n\nRelationship to Entropy\nIf one defines the Cross-Entropy:\n\\[H(p, q) = - E_p[log(q)]\\]\nThen one can write:\n\\[\\kld(p, q) = H(p,q) - H(p) \\]\nSo one can think of the KL-Divergence as the additional entropy introduced by using \\(q\\) instead of \\(p\\).\nNotice that \\(H(p,q)\\) and \\(\\kld(p, q)\\) is not symmetric. This is by design, and indeed is important. The interpretation is that if you use a unusual , low entropy distribution to approximate a usual one, you will be more surprised than if you used a high entropy, many choices one to approximate an unusual one. An example from McElreath provides some intuition: if you went to Mars from Earth you would be less suprised than the other way: Martians have only seen very dry..we’ve seen it all.\nA corollary here is that if we use a high entropy distribution to aproximate the true one, we will incur lesser error."
  },
  {
    "objectID": "posts/divergence/index.html#likelihoods-and-model-comparison",
    "href": "posts/divergence/index.html#likelihoods-and-model-comparison",
    "title": "Divergence and Deviance",
    "section": "Likelihoods and model comparison",
    "text": "Likelihoods and model comparison\nWhen we minimize risk or maximize likelihood, we do it by taking a sum of risks on a point wise basis, or by multiplying likelihood distributions on a point wise basis.\nWe have not really justified that yet, but we do it because its (a) intuitive and (b) we have an intuitive justification at the back of our mind of using the law of large numbers on a sample.\nThat is, we approximate the true population distribution \\(p\\) by a sample-based empirical distribution:\n\\[\\hat{p} = \\frac{1}{N}\\sum_i \\delta (x - x_i),\\]\nwhere we have used the dirac delta function. This is just another way of replacing population integrals by sample sums or averages.\nThe point here is that we dont know \\(p\\), or else why would be doing this in the first place?\n\nMaximum Likelihood justification\n\\[\\kld(p, q) = E_p[log(p/q)] = \\frac{1}{N}\\sum_i (log(p_i) - log(q_i)\\]\nThus minimizing the KL-divergence involves maximizing \\(\\sum_i log(q_i)\\) which is exactly the log likelihood. Hence we can justify the maximum likelihood principle.\n\n\nComparing Models\nBy the same token we can use the KL-Divergences of two different models to do model comparison:\n\\[\\kld(p, q) -\\kld(p, r) = H(p, q) - H(p, r) = E_p[log(r) - log(q)] = E_p[log(\\frac{r}{q})]\\]\nIn the sample approximation we have:\n\\[\\kld(p, q) -\\kld(p, r) = \\frac{1}{N} \\sum_i log(\\frac{r_i}{q_i}) = \\frac{1}{N} log(\\frac{\\prod_i r_i}{\\prod_i q_i}) =  \\frac{1}{N}log(\\frac{\\cal{L}_r}{\\cal{L}_q})\\]\nThis ratio inside the brackets on the right is the likelihood ratio and is used to test goodness of fit. You can read more about it in Wasserman."
  },
  {
    "objectID": "posts/divergence/index.html#from-divergence-to-deviance",
    "href": "posts/divergence/index.html#from-divergence-to-deviance",
    "title": "Divergence and Deviance",
    "section": "From Divergence to Deviance",
    "text": "From Divergence to Deviance\nIf you look at the expression above, you notice that to compare a model with distribution \\(r\\) to one with distribution \\(q\\), you only need the sample averages of the logarithm of \\(r\\) and \\(q\\):\n\\[\\kld(p, q) -\\kld(p, r) = \\langle log(r) \\rangle - \\langle log(q) \\rangle\\]\nwhere the angled brackets mean sample average. If we define the deviance:\n\\[D(q) = -2 \\sum_i log(q_i)\\],\nthen\n\\[\\kld(p, q) -\\kld(p, r) = \\frac{2}{N} (D(q) - D(r))\\]\nso that we can use the deviance’s for model comparison instead. Indeed, this is what we will do, starting in the frequentist realm and moving onto the bayesian realm.\nNotice that deviance is just a negative log likelihood, or risk.\n(Notice that even though we used likelihoods in the last section, I have been vague about the word distribution here. In Bayesian stats we use the posterior averaged likelihood distribution (posterior predictive) instead to do such comparisons.)\n\nBut we are still in-sample\nWe spent a lot of time in machine learning figuring out how to learn out of sample. However, all the machinery developed here has made no mention of it. When we use the empirical distribution and sample quantities here we are working with our training sample.\nClearly we can calculate deviance on the validation and test samples as well to remedy this issue. And the results will be similar to what we found with machine learning, with the training deviance decreasing with complexity and the testing deviance increasing. McElreath has a plot of this for data generated from a gaussian with standard deviation 1 and means:\n\\[\\mu_i = 0.15 x_{1,i} - 0.4 x_{2,i}\\]\nThe deviances in-sample and out-of sample, at 10,000 simulations for each model type, for two sample sizes are shown below.\n\n\n\nIn-sample vs. out-of-sample deviance as model complexity increases, for N=20 and N=100. From McElreath, Statistical Rethinking.\n\n\nNotice:\n\nthe best fit model may not be the original generating model. Remember that the choice of fit depends on the amount of data you have and the less data you have, the less parameters you should use\non average, out of sample deviance must be larger than in-sample deviance, through an individual pair may have that order reversed because of sample peculiarity.\n\nNow when one plots the mean deviances together, we see an interesting phenomenon:\n\n\n\nAIC approximation (dashed) to out-of-sample deviance: the gap between in-sample and out-of-sample deviance grows roughly as twice the number of parameters. From McElreath, Statistical Rethinking.\n\n\nThe test set deviances are \\(2*p\\) above the training set ones, approximately, where \\(p\\) is the number of parameters in the model.\nThis observation leads to an estimate of the out-of-sample deviance by what is called an information criterion, the Akake Information Criterion, or AIC:\n\\[AIC = D_{train} + 2p\\]\nwhich does carry as an assumption the notion that the likelihood is approximately multivariate gaussian, which as we have seen will be true near its peak.\nThis is just a penalized log-likelihood or risk if we choose to identify our distribution with the likelihood, and at higher numbers of parameters, increases the out-of-sample deviance, making them less desirable. In a sense, this penalization is a simple form of regularization on our model.\nWe wont derive the AIC here, but if you are interested, see http://www.stat.cmu.edu/~larry/=stat705/Lecture16.pdf\nWhy would we want to use such information criteria? Cross validation can be expensive, especially with multiple hyper-parameters. We will have more to say about informatiom criterion when we figure how to do model selection in the bayesian context."
  },
  {
    "objectID": "posts/inversetransform/index.html",
    "href": "posts/inversetransform/index.html",
    "title": "The Inverse Transform",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint(\"Setup Finished\")\n\nSetup Finished"
  },
  {
    "objectID": "posts/inversetransform/index.html#the-idea",
    "href": "posts/inversetransform/index.html#the-idea",
    "title": "The Inverse Transform",
    "section": "The idea",
    "text": "The idea\nThe basic idea behind the inverse transform method is to transform uniform samples into samples from a different distribution. That is, by somehow drawing from a uniform distribution, we make it possible to draw from the other distribution in question.\nAt first glance this seems to be a quixotic quest, but the key observation is this: the CDF of a distribution is a function that ranges from 0 to 1. Now assume you use \\[Uniform(0,1)\\] to generate a random number, say 0.63. Now map this number on the range (or y-axis) to a x using the CDF curve to generate a sample. This process is illustrated below:\n\n\n\nThe inverse transform method: uniform samples (left axis of CDF, right panel) are mapped through the inverse CDF to produce samples from the target distribution (left panel).\n\n\nThe right hand side image is the CDF while the left hand side is the pdf we want to sample from.\nNotice that we randomly choose some samples from a uniform on the right hand side image and these correspond to x’s for the samples from the CDF. On the left hand side we can see on the pdf the samples that these correspond to. If you sample from the uniform you will get more samples in the steep part of the cdf as the steep part of the cdf covers a good part of the probability values between 0 and 1. And thus you will get more samples in the higher parts of the pdf than elsewhere.\nClearly, for all this to work, we must be able to invert the cdf function, so that we can invert a uniform sample to get an \\(x\\)."
  },
  {
    "objectID": "posts/inversetransform/index.html#let-us-formalize-this",
    "href": "posts/inversetransform/index.html#let-us-formalize-this",
    "title": "The Inverse Transform",
    "section": "Let us formalize this:",
    "text": "Let us formalize this:\nThis is the process:\n\nget a uniform sample \\(u\\) from \\(Unif(0,1)\\)\nsolve for \\(x\\) yielding a new equation \\(x=F^{-1}(u)\\) where \\(F\\) is the CDF of the distribution we desire.\nrepeat.\n\nWhy does this work?\nFirst note that:\n$F^{-1}(u) = $ smallest x such that \\(F(x) &gt;=u\\)\nWhat distribution does random variable \\(y = F^{-1}(u)\\) follow?\nThe CDF of y is \\(p(y &lt;= x)\\). Since F is monotonic, we can without loss of generality write:\n\\[p(y &lt;= x) = p(F(y) &lt;= F(x)) = p(u &lt;= F(x)) = F(x)\\]\nThus we get the CDF and hence the pdf that we want to sample from!\n\nExample: Draw from the distribution \\(f(x) \\sim \\exp{(-x)}\\)\nFor example, lets assume we would like to generate random numbers that follow the exponential distribution \\(f(x) = \\frac{1}{\\lambda} e^{-x/\\lambda}\\) for \\(x\\ge0\\) and \\(f(x)=0\\) otherwise. Following the recipe from above\n\\[ u = \\int_{0}^{x} \\frac{1}{\\lambda} e^{-x'/\\lambda} dx'  = 1- e^{-x/\\lambda} \\]\nSolving for \\(x\\) \\[ x = - \\lambda \\ln (1-u) \\]\nNow we want the exponential with \\(\\lambda = 1\\). The following code will produce numbers that follow this \\(\\exp{(-x)}\\) distribution. The figure generated by code below shows the resulting histogram of the generated numbers compared to the actual \\(\\exp{(-x)}\\).\n\n# probability distribution we're trying to calculate\np = lambda x: np.exp(-x)\n\n# CDF of p\nCDF = lambda x: 1-np.exp(-x)\n\n# invert the CDF\ninvCDF = lambda r: -np.log(1-r)\n\n# domain limits\nxmin = 0 # the lower limit of our domain\nxmax = 6 # the upper limit of our domain\n\n# range limits\nrmin = CDF(xmin)\nrmax = CDF(xmax)\n\nN = 10000 # the total of samples we wish to generate\n\n# generate uniform samples in our range then invert the CDF\n# to get samples of our target distribution\nR = np.random.uniform(rmin, rmax, N)\nX = invCDF(R)\n\n# get the histogram info\nhinfo = np.histogram(X,100)\n\n# plot the histogram\nplt.hist(X,bins=100, label=u'Samples');\n\n# plot our (normalized) function\nxvals=np.linspace(xmin, xmax, 1000)\nplt.plot(xvals, hinfo[0][0]*p(xvals), 'r', label=u'p(x)')\n\n# turn on the legend\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nBox-Muller algorithm\nIn many cases the integral to calculate the CDF may not be easy to calculate analytically and we need to come with clever algorithms. For example there is no closed form formula for the integral of the normal distribution $ I= _{-}^{x} e{-x’2/2}dx’ $.\nThe Box Muller method is a brilliant trick to overcome this by producing two independent standard normals from two independent uniforms.\nThe idea is this:\nConsider (without loss of generality) the product of two independent normals N(0,1):\n\\[ X \\sim N(0,1), Y \\sim N(0,1) \\implies X,Y \\sim N(0,1)N(0,1)\\]\nThe pdf then is:\n\\[f_{XY}(x,y)  =  \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} \\times \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2} = \\frac{1}{2\\pi} \\times e^{-r^2/2}\\]\nwhere \\(r^2 = x^2 + y^2\\).\nIf you think of this in terms of polar co-ordinates \\(r\\) and \\(\\theta\\), we have\n\\[\\Theta \\sim Unif(0, 2\\pi),  S = R^2 \\sim Exp(1/2)\\]\nFrom the inverse method for the exponential above:\n\\[ s = r^2 = -2 ln(1-u) \\]\nwhere u is a sample from \\(U \\sim Unif(0,1)\\). Now if \\(U \\sim Unif(0,1)\\), then \\(1-U \\sim Unif(0,1)\\).\nThus we can write:\n\\[r = \\sqrt{-2\\,ln(u_1)}, \\theta = 2\\pi\\, u_2\\]\nwhere \\(u_1\\) and \\(u_2\\) are both drawn from a \\(Unif(0,1)\\)s.\nNow we can use:\n\\[x = r\\,cos\\theta, y = r\\,sin\\theta\\]\nto generate samples for the normally distributed random variables \\(x\\) and \\(Y\\).\nWe’ve hand-waved around a bit here in this derivation, so let us ask, what is the pdf in polar co-ordinates? Lets make a few observations.\n\nclearly, no matter what co-ordinates we use \\(\\int dF =1\\). In other words, no matter how we add slivers, the probabilities in these slivers must add to 1.\none can think of cartesian co-ordinate slivers being histogram skyscrapers on a regular grid. In polar co-ordinates the slivers are arranged radially\nWe have in terms of the pdfs:\n\n\\[\\int dx dy f(x,y) = \\int dr d\\theta f2(r, \\theta) = \\int dr d\\theta f2r(r)\\, f2t(\\theta)\\]\nAnd we have seen:\nfpolar() =Unif(0, 2)\nWe might be tempted to think that \\(f2r(r) = e^{-r^2/2}\\). But this is not correct on two counts. First, its not even dimensionally right. Secondly, then you transform the \\(dxdy\\) to polar , you get \\(rdrd\\theta\\).\nWhat this means is that :\n\\[f2r(r) = re^{-r^2/2}\\]\nThis is called the Raleigh distribution.\nAnd now you can see how the transformation \\(s=r^2\\) gives us an exponential in s. And this is why we could take \\(R^2 \\sim Exp(1/2)\\) without much ado..the exponential happily normalizes out to 1 dur to the \\(r\\) multiplying the exponential in the pdf above.\nMore generally, if \\(z=g(x)\\) so that \\(x=g^{-1}(z)\\), let us define the Jacobian \\(J(z)\\) of the transformation \\(x=g^{-1}(z)\\) as the partial derivatives matrix of the transformation.\nThen:\n\\[f_Z(z) = f_X(g^{-1}(z)) \\times det(J(z))\\]\nWe can work this out with \\(z\\) the polar co-ordinates and \\(g^{-1}\\) as \\(x=r\\,cos(\\theta)\\) and \\(y=r\\,sin(\\theta)\\), with \\(g\\) as \\(r=\\sqrt{x^2 + y^2}\\), \\(tan(\\theta) = y/x\\).\n\\[ J =  \\binom{cos(\\theta)\\:sin(\\theta)}{-r sin(\\theta)\\:r cos(\\theta)}\\]\nwhose determinant is \\(r\\), and thus\n\\[f_{R, \\Theta}(r, \\theta) = f_{X,Y}(r cos(\\theta), r sin(\\theta)) \\times r =  \\frac{1}{\\sqrt{2\\pi}} e^{-(r cos(\\theta))^2/2} \\times \\frac{1}{\\sqrt{2\\pi}} e^{-(r sin(\\theta))^2/2} = \\frac{1}{2\\pi} \\times e^{-r^2/2} \\times r\\]."
  },
  {
    "objectID": "posts/tetchygibbs/index.html",
    "href": "posts/tetchygibbs/index.html",
    "title": "A Tetchy Gibbs Sampler",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import beta\nfrom scipy.stats import distributions\nimport matplotlib.pyplot as plt\nimport time\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_context('poster')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nImagine your posterior distribution has the following form:\n\\[ f(x, y \\mid data) =  (1/C)*e^{-\\frac{(x^2*y^2+x^2+y^2-8x-8y)}{2}} \\]\nAs is typical in Bayesian inference, you don’t know what C (the normalizing constant) is, so you can’t sample from this distribution using conventional methods. However, MCMC techniques allow us to sample from probability distributions without knowing this constant, and we will use oGibbs sampling, to do this here.\nGibbs sampling allows you to sample from a probability distribution by iteratively sampling from its conditional distributions. This strategy is very useful in problems where each unknown would have a very simple distribution if we knew all of the other unknowns. In this problem, the posterior distribution \\(f(x, y \\mid data)\\) is over two unknowns, \\(x\\) and \\(y\\). To perform Gibbs sampling, we sample from the distribution of \\(x\\) holding \\(y\\) constant at its current value, then sample from the distribution of \\(y\\) holding \\(x\\) constant at its current value. As it turns out, even though \\(f(x, y \\mid data)\\) is incredibly ugly, the conditional distributions are relatively simple.\n\n\nAfter some simplification (completing the square and throwing all factors that do not involve \\(x\\) into \\(g(y)\\) for the first equation, and vice versa for the second), we find that the conditional distributions have a relatively simple form.\n\\[ p(x \\mid y, data) = g(y) e^{-\\left(x-\\frac{4}{(1+y^2)}\\right)^{2}\\frac{(1+y^2)}{2}} \\]\nand\n\\[ p(y \\mid x, data) = g(x) e^{-\\left(y-\\frac{4}{(1+x^2)}\\right)^{2}\\frac{(1+x^2)}{2}} \\]\nWhat are these distributions? They are in fact normals. Writing this in distributional notation,\n\\[ x \\mid y, data \\sim N\\left(\\frac{4}{1+y^2}, \\sqrt{\\frac{1}{1+y^2}}\\right) \\]\nand similarly\n\\[ y \\mid x, data \\sim N\\left(\\frac{4}{1+x^2}, \\sqrt{\\frac{1}{1+x^2}}\\right) \\].\nWe know how to draw from normal distributions, so if we iterate back and forth, we should be able to sample from \\(f(x, y \\mid data)\\)!\n\nf= lambda x,y: np.exp(-(x*x*y*y+x*x+y*y-8*x-8*y)/2.)\n\n\n\n\nFirst, let’s make a contour plot of the posterior density.\n\nxx=np.linspace(-1,8,100)\nyy=np.linspace(-1,8,100)\nxg,yg = np.meshgrid(xx,yy)\nz=f(xg.ravel(),yg.ravel())\nz2 = z.reshape(xg.shape)\nz2\nplt.contourf(xg,yg,z2)\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s run the sampler, by iteratively drawing from the conditional distribution of \\(x\\) and \\(y\\) given the other.\n\nN = 400000\nxall=np.zeros(N+1)\nyall=np.zeros(N+1)\n#Initialize x and y.\nxall[0]=1.\nyall[0]=6.\nsig = lambda z,i: np.sqrt(1./(1.+z[i]*z[i]))\nmu = lambda z,i: 4./(1.+z[i]*z[i])\n\nfor i in range(1,N,2):\n    sig_x = sig(yall,i-1)\n    mu_x = mu(yall,i-1)\n    xall[i] = np.random.normal(mu_x, sig_x)\n    yall[i] = yall[i-1]\n    \n    sig_y = sig(xall, i)\n    mu_y = mu(xall, i)\n    yall[i+1] = np.random.normal(mu_y, sig_y)\n    xall[i+1] = xall[i]\n    \n\n\nx=xall[N//10::10]\ny=yall[N//10::10]\n\n\n\nTo assess how the sampler is exploring the space, we can plot a traceplot for each dimension. A traceplot plots the value of each sample against the iteration number and gives a sense of how well the sampler is exploring the space.\n\ndef traceplot(z):\n    plt.plot(z, alpha=0.3);\n\n\ntraceplot(x)\n\n\n\n\n\n\n\n\nYou can see from the traceplot the when sampling \\(x\\), the sampler spends long periods of time near zero, and occasionally moves to and hangs out at higher values. These correspond to the two areas of high density in the countour plot.\n\n\n\nWe can also draw a histogram of \\(x\\) to get an estimate of its marginal distribution.\n\nplt.hist(x, bins=50);\n\n\n\n\n\n\n\n\nThis is exactly what we would expect if we projected the distribution in the contour plot down to the \\(x\\) axis.\nWe can do the same plots for \\(y\\).\n\ntraceplot(y)\n\n\n\n\n\n\n\n\n\nplt.hist(y, bins=50);\n\n\n\n\n\n\n\n\n\n\n\nBecause we are in two dimensions, we can also plot the path that the sampler took through the \\(xy\\) plane. Note that the path always takes right angles, because we are alternating between moves that only move in the \\(x\\) direction and only move in the \\(y\\) direction.\n\nplt.contourf(xg,yg,z2, alpha=0.6)\nplt.scatter(xall[::10],yall[::10], alpha=0.01, c='k', s=5)\nplt.plot(xall[:200],yall[:200], c='r', alpha=0.3, lw=1)\n\n\n\n\n\n\n\n\n\n\n\nTo see how effective the samples we have drawn will be at approximating summaries of the posterior distribution (for example the posterior mean), we can look at the autocorrelation of the samples. High autocorrelation would mean that the sample average that we take to approximate the posterior mean would be higher than expected if we had taken independent samples from the posterior distribution.\n\ndef corrplot(trace, maxlags=50):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\ncorrplot(xall[N//10:])\n\n\n\n\n\n\n\n\n\ncorrplot(y)\n\n\n\n\n\n\n\n\n\n\n\nIn both \\(x\\) and \\(y\\), we can see that the autocorrelation is quite high. This is not a big problem though because the sampler is so simple that we can draw millions of samples to make up for the high autocorrelation.\nTo figure out exactly how many samples we would have to draw, we can compute effective sample size, a measure of how many independent samples our our samples are equivalent to. This usese the same quantities that were used to compute the autocorrelation plot above. The following code is taken from https://code.google.com/p/biopy/source/browse/trunk/biopy/bayesianStats.py?r=67. You don’t need to try to understand the function – it’s just here to run it, and this is a rather slow implementation.\n\ndef effectiveSampleSize(data, stepSize = 1) :\n  \"\"\" Effective sample size, as computed by BEAST Tracer.\"\"\"\n  samples = len(data)\n\n  assert len(data) &gt; 1,\"no stats for short sequences\"\n  \n  maxLag = min(samples//3, 1000)\n\n  gammaStat = [0,]*maxLag\n  #varGammaStat = [0,]*maxLag\n\n  varStat = 0.0;\n\n  if type(data) != np.ndarray :\n    data = np.array(data)\n\n  normalizedData = data - data.mean()\n  \n  for lag in range(maxLag) :\n    v1 = normalizedData[:samples-lag]\n    v2 = normalizedData[lag:]\n    v = v1 * v2\n    gammaStat[lag] = sum(v) / len(v)\n    #varGammaStat[lag] = sum(v*v) / len(v)\n    #varGammaStat[lag] -= gammaStat[0] ** 2\n\n    # print lag, gammaStat[lag], varGammaStat[lag]\n    \n    if lag == 0 :\n      varStat = gammaStat[0]\n    elif lag % 2 == 0 :\n      s = gammaStat[lag-1] + gammaStat[lag]\n      if s &gt; 0 :\n         varStat += 2.0*s\n      else :\n        break\n      \n  # standard error of mean\n  # stdErrorOfMean = Math.sqrt(varStat/samples);\n\n  # auto correlation time\n  act = stepSize * varStat / gammaStat[0]\n\n  # effective sample size\n  ess = (stepSize * samples) / act\n\n  return ess\n\nNow we can compute effective sample size for x and y.\n\nesx = effectiveSampleSize(xall)\nesy = effectiveSampleSize(yall)\nprint(\"Effective Size for x: \", esx, \" of \", len(x), \" samples, rate of\", esx/len(x)*100, \"%.\")\nprint(\"Effective Size for y: \", esy, \" of \", len(y), \" samples, rate of\", esy/len(y)*100, \"%.\")\n\nEffective Size for x:  10115.8236073  of  36001  samples, rate of 28.0987295001 %.\nEffective Size for y:  10264.5088434  of  36001  samples, rate of 28.5117325726 %.\n\n\nNote that while the effective size is only just over 25% of the actual sample size, we can draw samples so quickly from the posterior that this is not a major hindrance."
  },
  {
    "objectID": "posts/tetchygibbs/index.html#a-tetchy-posterior",
    "href": "posts/tetchygibbs/index.html#a-tetchy-posterior",
    "title": "A Tetchy Gibbs Sampler",
    "section": "",
    "text": "Imagine your posterior distribution has the following form:\n\\[ f(x, y \\mid data) =  (1/C)*e^{-\\frac{(x^2*y^2+x^2+y^2-8x-8y)}{2}} \\]\nAs is typical in Bayesian inference, you don’t know what C (the normalizing constant) is, so you can’t sample from this distribution using conventional methods. However, MCMC techniques allow us to sample from probability distributions without knowing this constant, and we will use oGibbs sampling, to do this here.\nGibbs sampling allows you to sample from a probability distribution by iteratively sampling from its conditional distributions. This strategy is very useful in problems where each unknown would have a very simple distribution if we knew all of the other unknowns. In this problem, the posterior distribution \\(f(x, y \\mid data)\\) is over two unknowns, \\(x\\) and \\(y\\). To perform Gibbs sampling, we sample from the distribution of \\(x\\) holding \\(y\\) constant at its current value, then sample from the distribution of \\(y\\) holding \\(x\\) constant at its current value. As it turns out, even though \\(f(x, y \\mid data)\\) is incredibly ugly, the conditional distributions are relatively simple.\n\n\nAfter some simplification (completing the square and throwing all factors that do not involve \\(x\\) into \\(g(y)\\) for the first equation, and vice versa for the second), we find that the conditional distributions have a relatively simple form.\n\\[ p(x \\mid y, data) = g(y) e^{-\\left(x-\\frac{4}{(1+y^2)}\\right)^{2}\\frac{(1+y^2)}{2}} \\]\nand\n\\[ p(y \\mid x, data) = g(x) e^{-\\left(y-\\frac{4}{(1+x^2)}\\right)^{2}\\frac{(1+x^2)}{2}} \\]\nWhat are these distributions? They are in fact normals. Writing this in distributional notation,\n\\[ x \\mid y, data \\sim N\\left(\\frac{4}{1+y^2}, \\sqrt{\\frac{1}{1+y^2}}\\right) \\]\nand similarly\n\\[ y \\mid x, data \\sim N\\left(\\frac{4}{1+x^2}, \\sqrt{\\frac{1}{1+x^2}}\\right) \\].\nWe know how to draw from normal distributions, so if we iterate back and forth, we should be able to sample from \\(f(x, y \\mid data)\\)!\n\nf= lambda x,y: np.exp(-(x*x*y*y+x*x+y*y-8*x-8*y)/2.)\n\n\n\n\nFirst, let’s make a contour plot of the posterior density.\n\nxx=np.linspace(-1,8,100)\nyy=np.linspace(-1,8,100)\nxg,yg = np.meshgrid(xx,yy)\nz=f(xg.ravel(),yg.ravel())\nz2 = z.reshape(xg.shape)\nz2\nplt.contourf(xg,yg,z2)"
  },
  {
    "objectID": "posts/tetchygibbs/index.html#gibbs-sampler",
    "href": "posts/tetchygibbs/index.html#gibbs-sampler",
    "title": "A Tetchy Gibbs Sampler",
    "section": "",
    "text": "Now let’s run the sampler, by iteratively drawing from the conditional distribution of \\(x\\) and \\(y\\) given the other.\n\nN = 400000\nxall=np.zeros(N+1)\nyall=np.zeros(N+1)\n#Initialize x and y.\nxall[0]=1.\nyall[0]=6.\nsig = lambda z,i: np.sqrt(1./(1.+z[i]*z[i]))\nmu = lambda z,i: 4./(1.+z[i]*z[i])\n\nfor i in range(1,N,2):\n    sig_x = sig(yall,i-1)\n    mu_x = mu(yall,i-1)\n    xall[i] = np.random.normal(mu_x, sig_x)\n    yall[i] = yall[i-1]\n    \n    sig_y = sig(xall, i)\n    mu_y = mu(xall, i)\n    yall[i+1] = np.random.normal(mu_y, sig_y)\n    xall[i+1] = xall[i]\n    \n\n\nx=xall[N//10::10]\ny=yall[N//10::10]\n\n\n\nTo assess how the sampler is exploring the space, we can plot a traceplot for each dimension. A traceplot plots the value of each sample against the iteration number and gives a sense of how well the sampler is exploring the space.\n\ndef traceplot(z):\n    plt.plot(z, alpha=0.3);\n\n\ntraceplot(x)\n\n\n\n\n\n\n\n\nYou can see from the traceplot the when sampling \\(x\\), the sampler spends long periods of time near zero, and occasionally moves to and hangs out at higher values. These correspond to the two areas of high density in the countour plot.\n\n\n\nWe can also draw a histogram of \\(x\\) to get an estimate of its marginal distribution.\n\nplt.hist(x, bins=50);\n\n\n\n\n\n\n\n\nThis is exactly what we would expect if we projected the distribution in the contour plot down to the \\(x\\) axis.\nWe can do the same plots for \\(y\\).\n\ntraceplot(y)\n\n\n\n\n\n\n\n\n\nplt.hist(y, bins=50);\n\n\n\n\n\n\n\n\n\n\n\nBecause we are in two dimensions, we can also plot the path that the sampler took through the \\(xy\\) plane. Note that the path always takes right angles, because we are alternating between moves that only move in the \\(x\\) direction and only move in the \\(y\\) direction.\n\nplt.contourf(xg,yg,z2, alpha=0.6)\nplt.scatter(xall[::10],yall[::10], alpha=0.01, c='k', s=5)\nplt.plot(xall[:200],yall[:200], c='r', alpha=0.3, lw=1)\n\n\n\n\n\n\n\n\n\n\n\nTo see how effective the samples we have drawn will be at approximating summaries of the posterior distribution (for example the posterior mean), we can look at the autocorrelation of the samples. High autocorrelation would mean that the sample average that we take to approximate the posterior mean would be higher than expected if we had taken independent samples from the posterior distribution.\n\ndef corrplot(trace, maxlags=50):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\ncorrplot(xall[N//10:])\n\n\n\n\n\n\n\n\n\ncorrplot(y)\n\n\n\n\n\n\n\n\n\n\n\nIn both \\(x\\) and \\(y\\), we can see that the autocorrelation is quite high. This is not a big problem though because the sampler is so simple that we can draw millions of samples to make up for the high autocorrelation.\nTo figure out exactly how many samples we would have to draw, we can compute effective sample size, a measure of how many independent samples our our samples are equivalent to. This usese the same quantities that were used to compute the autocorrelation plot above. The following code is taken from https://code.google.com/p/biopy/source/browse/trunk/biopy/bayesianStats.py?r=67. You don’t need to try to understand the function – it’s just here to run it, and this is a rather slow implementation.\n\ndef effectiveSampleSize(data, stepSize = 1) :\n  \"\"\" Effective sample size, as computed by BEAST Tracer.\"\"\"\n  samples = len(data)\n\n  assert len(data) &gt; 1,\"no stats for short sequences\"\n  \n  maxLag = min(samples//3, 1000)\n\n  gammaStat = [0,]*maxLag\n  #varGammaStat = [0,]*maxLag\n\n  varStat = 0.0;\n\n  if type(data) != np.ndarray :\n    data = np.array(data)\n\n  normalizedData = data - data.mean()\n  \n  for lag in range(maxLag) :\n    v1 = normalizedData[:samples-lag]\n    v2 = normalizedData[lag:]\n    v = v1 * v2\n    gammaStat[lag] = sum(v) / len(v)\n    #varGammaStat[lag] = sum(v*v) / len(v)\n    #varGammaStat[lag] -= gammaStat[0] ** 2\n\n    # print lag, gammaStat[lag], varGammaStat[lag]\n    \n    if lag == 0 :\n      varStat = gammaStat[0]\n    elif lag % 2 == 0 :\n      s = gammaStat[lag-1] + gammaStat[lag]\n      if s &gt; 0 :\n         varStat += 2.0*s\n      else :\n        break\n      \n  # standard error of mean\n  # stdErrorOfMean = Math.sqrt(varStat/samples);\n\n  # auto correlation time\n  act = stepSize * varStat / gammaStat[0]\n\n  # effective sample size\n  ess = (stepSize * samples) / act\n\n  return ess\n\nNow we can compute effective sample size for x and y.\n\nesx = effectiveSampleSize(xall)\nesy = effectiveSampleSize(yall)\nprint(\"Effective Size for x: \", esx, \" of \", len(x), \" samples, rate of\", esx/len(x)*100, \"%.\")\nprint(\"Effective Size for y: \", esy, \" of \", len(y), \" samples, rate of\", esy/len(y)*100, \"%.\")\n\nEffective Size for x:  10115.8236073  of  36001  samples, rate of 28.0987295001 %.\nEffective Size for y:  10264.5088434  of  36001  samples, rate of 28.5117325726 %.\n\n\nNote that while the effective size is only just over 25% of the actual sample size, we can draw samples so quickly from the posterior that this is not a major hindrance."
  },
  {
    "objectID": "posts/gradientdescent/index.html",
    "href": "posts/gradientdescent/index.html",
    "title": "Gradient Descent and SGD",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom scipy import stats \n\nfrom sklearn.datasets.samples_generator import make_regression\nA lot of the animations here were adapted from: http://tillbergmann.com/blog/python-gradient-descent.html\nA great discussion (and where momentum image was stolen from) is at http://sebastianruder.com/optimizing-gradient-descent/\nGradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. Gradient descent is a way to minimize an objective function \\(J_{\\theta}\\) parameterized by a model’s parameters \\(\\theta \\in \\mathbb{R}^d\\) by updating the parameters in the opposite direction of the gradient of the objective function \\(\\nabla_J J(\\theta)\\) w.r.t. to the parameters. The learning rate \\(\\eta\\) determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley.\nThere are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update."
  },
  {
    "objectID": "posts/gradientdescent/index.html#example-linear-regression",
    "href": "posts/gradientdescent/index.html#example-linear-regression",
    "title": "Gradient Descent and SGD",
    "section": "Example: Linear regression",
    "text": "Example: Linear regression\nLet’s see briefly how gradient descent can be useful to us in least squares regression. Let’s asssume we have an output variable \\(y\\) which we think depends linearly on the input vector \\(x\\). We approximate \\(y\\) by\n\\[f_\\theta (x) =\\theta^T x\\]\nThe cost function for our linear least squares regression will then be\n\\[J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (f_\\theta (x^{(i)}-y^{(i)})^2\\]\nWe create a regression problem using sklearn’s make_regression function:\n\n#code adapted from http://tillbergmann.com/blog/python-gradient-descent.html\nx, y = make_regression(n_samples = 100, \n                       n_features=1, \n                       n_informative=1, \n                       noise=20,\n                       random_state=2017)\n\n\nx = x.flatten()\n\n\nslope, intercept, _,_,_ = stats.linregress(x,y)\nbest_fit = np.vectorize(lambda x: x * slope + intercept)\n\n\nplt.plot(x,y, 'o', alpha=0.5)\ngrid = np.arange(-3,3,0.1)\nplt.plot(grid,best_fit(grid), '.')"
  },
  {
    "objectID": "posts/gradientdescent/index.html#batch-gradient-descent",
    "href": "posts/gradientdescent/index.html#batch-gradient-descent",
    "title": "Gradient Descent and SGD",
    "section": "Batch gradient descent",
    "text": "Batch gradient descent\nAssume that we have a vector of paramters \\(\\theta\\) and a cost function \\(J(\\theta)\\) which is simply the variable we want to minimize (our objective function). Typically, we will find that the objective function has the form:\n\\[J(\\theta) =\\sum_{i=1}^m J_i(\\theta)\\]\nwhere \\(J_i\\) is associated with the i-th observation in our data set. The batch gradient descent algorithm, starts with some initial feasible \\(\\theta\\) (which we can either fix or assign randomly) and then repeatedly performs the update:\n\\[\\theta := \\theta - \\eta \\nabla_{\\theta} J(\\theta) = \\theta -\\eta \\sum_{i=1}^m \\nabla J_i(\\theta)\\]\nwhere \\(\\eta\\) is a constant controlling step-size and is called the learning rate. Note that in order to make a single update, we need to calculate the gradient using the entire dataset. This can be very inefficient for large datasets.\nIn code, batch gradient descent looks like this:\nfor i in range(n_epochs):\n  params_grad = evaluate_gradient(loss_function, data, params)\n  params = params - learning_rate * params_grad`\nFor a given number of epochs \\(n_{epochs}\\), we first evaluate the gradient vector of the loss function using ALL examples in the data set, and then we update the parameters with a given learning rate. This is where Theano and automatic differentiation come in handy, and you will learn about them in lab.\nBatch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.\nIn the linear example it’s easy to see that our update step then takes the form:\n\\[\\theta_j := \\theta_j + \\alpha \\sum_{i=1}^m (y^{(i)}-f_\\theta (x^{(i)})) x_j^{(i)}\\] for every \\(j\\) (note \\(\\theta_j\\) is simply the j-th component of the \\(\\theta\\) vector).\n\ndef gradient_descent(x, y, theta_init, step=0.001, maxsteps=0, precision=0.001, ):\n    costs = []\n    m = y.size # number of data points\n    theta = theta_init\n    history = [] # to store all thetas\n    preds = []\n    counter = 0\n    oldcost = 0\n    pred = np.dot(x, theta)\n    error = pred - y \n    currentcost = np.sum(error ** 2) / (2 * m)\n    preds.append(pred)\n    costs.append(currentcost)\n    history.append(theta)\n    counter+=1\n    while abs(currentcost - oldcost) &gt; precision:\n        oldcost=currentcost\n        gradient = x.T.dot(error)/m \n        theta = theta - step * gradient  # update\n        history.append(theta)\n        \n        pred = np.dot(x, theta)\n        error = pred - y \n        currentcost = np.sum(error ** 2) / (2 * m)\n        costs.append(currentcost)\n        \n        if counter % 25 == 0: preds.append(pred)\n        counter+=1\n        if maxsteps:\n            if counter == maxsteps:\n                break\n        \n    return history, costs, preds, counter\n\n\nnp.random.rand(2)\n\narray([ 0.75307882,  0.98388838])\n\n\n\nxaug = np.c_[np.ones(x.shape[0]), x]\ntheta_i = [-15, 40] + np.random.rand(2)\nhistory, cost, preds, iters = gradient_descent(xaug, y, theta_i)\ntheta = history[-1]\n\n\nprint(\"Gradient Descent: {:.2f}, {:.2f} {:d}\".format(theta[0], theta[1], iters))\nprint(\"Least Squares: {:.2f}, {:.2f}\".format(intercept, slope))\n\nGradient Descent: -3.93, 81.67 4454\nLeast Squares: -3.71, 82.90\n\n\n\ntheta\n\narray([ -3.92778924,  81.67155225])\n\n\nOne can plot the reduction of cost:\n\nplt.plot(range(len(cost)), cost);\n\n\n\n\n\n\n\n\nThe following animation shows how the regression line forms:\n\nfrom JSAnimation import IPython_display\n\n\ndef init():\n    line.set_data([], [])\n    return line,\n\ndef animate(i):\n    ys = preds[i]\n    line.set_data(xaug[:, 1], ys)\n    return line,\n\n\n\nfig = plt.figure(figsize=(10,6))\nax = plt.axes(xlim=(-3, 2.5), ylim=(-170, 170))\nax.plot(xaug[:,1],y, 'o')\nline, = ax.plot([], [], lw=2)\nplt.plot(xaug[:,1], best_fit(xaug[:,1]), 'k-', color = \"r\")\n\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                        frames=len(preds), interval=100)\nanim.save('images/gdline.mp4')\nanim\n\nIOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n\n\nRemember that the linear regression cost function is convex, and more precisely quadratic. We can see the path that gradient descent takes in arriving at the optimum:\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef error(X, Y, THETA):\n    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n\ndef make_3d_plot(xfinal, yfinal, zfinal, hist, cost, xaug, y):\n    ms = np.linspace(xfinal - 20 , xfinal + 20, 20)\n    bs = np.linspace(yfinal - 40 , yfinal + 40, 40)\n    M, B = np.meshgrid(ms, bs)\n    zs = np.array([error(xaug, y, theta) \n                   for theta in zip(np.ravel(M), np.ravel(B))])\n    Z = zs.reshape(M.shape)\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.1)\n    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=30)\n    ax.set_xlabel('Intercept')\n    ax.set_ylabel('Slope')\n    ax.set_zlabel('Cost')\n    ax.view_init(elev=30., azim=30)\n    ax.plot([xfinal], [yfinal], [zfinal] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7);\n    ax.plot([t[0] for t in hist], [t[1] for t in hist], cost , markerfacecolor='b', markeredgecolor='b', marker='.', markersize=5);\n    ax.plot([t[0] for t in hist], [t[1] for t in hist], 0 , alpha=0.5, markerfacecolor='r', markeredgecolor='r', marker='.', markersize=5)\n    \ndef gd_plot(xaug, y, theta, cost, hist):\n    make_3d_plot(theta[0], theta[1], cost[-1], hist, cost, xaug, y)\n\n\ngd_plot(xaug, y, theta, cost, history)"
  },
  {
    "objectID": "posts/gradientdescent/index.html#stochastic-gradient-descent",
    "href": "posts/gradientdescent/index.html#stochastic-gradient-descent",
    "title": "Gradient Descent and SGD",
    "section": "Stochastic gradient descent",
    "text": "Stochastic gradient descent\nAs noted, the gradient descent algorithm makes intuitive sense as it always proceeds in the direction of steepest descent (the gradient of \\(J\\)) and guarantees that we find a local minimum (global under certain assumptions on \\(J\\)). When we have very large data sets, however, the calculation of \\(\\nabla (J(\\theta))\\) can be costly as we must process every data point before making a single step (hence the name “batch”). An alternative approach, the stochastic gradient descent method, is to update \\(\\theta\\) sequentially with every observation. The updates then take the form:\n\\[\\theta := \\theta - \\alpha \\nabla_{\\theta} J_i(\\theta)\\]\nThis stochastic gradient approach allows us to start making progress on the minimization problem right away. It is computationally cheaper, but it results in a larger variance of the loss function in comparison with batch gradient descent.\nGenerally, the stochastic gradient descent method will get close to the optimal \\(\\theta\\) much faster than the batch method, but will never fully converge to the local (or global) minimum. Thus the stochastic gradient descent method is useful when we want a quick and dirty approximation for the solution to our optimization problem. A full recipe for stochastic gradient descent follows:\n\nInitialize the parameter vector \\(\\theta\\) and set the learning rate \\(\\alpha\\)\nRepeat until an acceptable approximation to the minimum is obtained:\n\nRandomly reshuffle the instances in the training data.\nFor \\(i=1,2,...m\\) do: \\(\\theta := \\theta - \\alpha \\nabla_\\theta J_i(\\theta)\\)\n\n\nThe reshuffling of the data is done to avoid a bias in the optimization algorithm by providing the data examples in a particular order. In code, the algorithm should look something like this:\nfor i in range(nb_epochs):\n  np.random.shuffle(data)\n  for example in data:\n    params_grad = evaluate_gradient(loss_function, example, params)\n    params = params - learning_rate * params_grad\nFor a given epoch, we first reshuffle the data, and then for a single example, we evaluate the gradient of the loss function and then update the params with the chosen learning rate.\nThe update for linear regression is:\n\\[\\theta_j := \\theta_j + \\alpha (y^{(i)}-f_\\theta (x^{(i)})) x_j^{(i)}\\]\n\ndef sgd(x, y, theta_init, step=0.001, maxsteps=0, precision=0.001, ):\n    costs = []\n    currentcosts = []\n    m = y.size # number of data points\n    oldtheta = 0\n    theta = theta_init\n    history = [] # to store all thetas\n    preds = []\n    grads = []\n    xs = []\n    ys = []\n    counter = 0\n    oldcost = 0\n    epoch = 0\n    i = 0 #index\n    xs.append(x[i,:])\n    ys.append([y[i]])\n    pred = np.dot(x[i,:], theta)\n    error = pred - y[i]\n    gradient = x[i,:].T*error\n    grads.append(gradient)\n    currentcost = np.sum(error ** 2) / 2\n    print(\"Init\", gradient, x[i,:],y[i])\n    print (\"Init2\", currentcost, theta)\n    currentcosts.append(currentcost)\n    counter+=1\n    preds.append(pred)\n    costsum = currentcost\n    costs.append(costsum/counter)\n    history.append(theta)\n    print(\"start\",counter, costs, oldcost)\n    while 1:\n        #while abs(costs[counter-1] - oldcost) &gt; precision:\n        #while np.linalg.norm(theta - oldtheta) &gt; precision:\n        gradient = x[i,:].T*error\n        grads.append(gradient)\n        oldtheta = theta\n        theta = theta - step * gradient  # update\n        history.append(theta)\n        i += 1\n        if i == m:#reached one past the end.\n            #break\n            epoch +=1\n            neworder = np.random.permutation(m)\n            x = x[neworder]\n            y = y[neworder]\n            i = 0\n        xs.append(x[i,:])\n        ys.append(y[i])\n        pred = np.dot(x[i,:], theta)\n        error = pred - y[i]\n        currentcost = np.sum(error ** 2) / 2\n        currentcosts.append(currentcost)\n        \n        #print(\"e/cc\",error, currentcost)\n        if counter % 25 == 0: preds.append(pred)\n        counter+=1\n        costsum += currentcost\n        oldcost = costs[counter-2]\n        costs.append(costsum/counter)\n        #print(counter, costs, oldcost)\n        if maxsteps:\n            #print(\"in maxsteps\")\n            if counter == maxsteps:\n                break\n        \n    return history, costs, preds, grads, counter, epoch, xs, ys, currentcosts\n\n\nhistory2, cost2, preds2, grads2, iters2, epoch2, x2, y2, cc2 = sgd(xaug, y, theta_i, maxsteps=5000, step=0.01)\n\nInit [-24.81938748  -0.8005103 ] [ 1.          0.03225343] 11.5348518902\nInit2 308.000997364 [-14.58474841  40.31239274]\nstart 1 [308.00099736389291] 0\n\n\n\nprint(iters2, history2[-1], epoch2, grads2[-1])\n\n5000 [ -3.18011506  82.93875465] 49 [ 10.47922297  -5.63332413]\n\n\n\nplt.plot(range(len(cost2[-10000:])), cost2[-10000:], alpha=0.4);\n\n\n\n\n\n\n\n\n\ngd_plot(xaug, y, theta, cost2, history2)\n\n\n\n\n\n\n\n\n\nplt.plot([t[0] for t in history2], [t[1] for t in history2],'o-', alpha=0.1)\n\n\n\n\n\n\n\n\n\nAnimating SGD\nHere is some code to make an animation of SGD. It shows how the risk surfaces being minimized change, and how the minimum desired is approached.\n\ndef error2(X, Y, THETA):\n    #print(\"XYT\", THETA, np.sum((X.dot(THETA) - Y)**2))\n    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n\n\ndef make_3d_plot2(num, it, xfinal, yfinal, zfinal, hist, cost, xaug, y):\n    ms = np.linspace(xfinal - 20 , xfinal + 20, 20)\n    bs = np.linspace(yfinal - 50 , yfinal + 50, 40)\n    M, B = np.meshgrid(ms, bs)\n    zs = np.array([error2(xaug, y, theta) \n                   for theta in zip(np.ravel(M), np.ravel(B))])\n    Z = zs.reshape(M.shape)\n    fig = plt.figure(figsize=(10, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.1)\n    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=30)\n    ax.set_xlabel('Intercept')\n    ax.set_ylabel('Slope')\n    ax.set_zlabel('Cost')\n    ax.view_init(elev=30., azim=30)\n    #print(\"hist\", xaug, y, hist, cost)\n    ax.plot([xfinal], [yfinal], [zfinal] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7);\n    #ax.plot([t[0] for t in hist], [t[1] for t in hist], cost , markerfacecolor='b', markeredgecolor='b', marker='.', markersize=5);\n    ax.plot([t[0] for t in hist], [t[1] for t in hist], 0 , alpha=0.5, markerfacecolor='r', markeredgecolor='r', marker='.', markersize=5)\n    ax.set_zlim([0, 3000])\n    plt.title(\"Iteration {}\".format(it))\n    plt.savefig(\"images/3danim{0:03d}.png\".format(num))\n    plt.close()\n\n\nprint(\"fthetas\",theta[0], theta[1], \"len\", len(history2))\nST = list(range(0, 750, 10)) + list(range(750, 5000, 250))\nlen(ST)\n\nfthetas -3.92778923799 81.6715522496 len 5000\n\n\n92\n\n\n\nfor i in range(len(ST)):\n    #print(history2[i*ST[i]], cc2[i*ST[i]])\n    make_3d_plot2(i, ST[i], theta[0], theta[1], cost2[-1], [history2[ST[i]]], [cc2[ST[i]]], np.array([x2[ST[i]]]), np.array([y2[ST[i]]]))\n\nUsing Imagemagick we can produce a gif animation: (convert -delay 20 -loop 1 3danim*.png animsgd.gif)\n(I set this animation to repeat just once. (loop 1). Reload this cell to see it again. On the web page right clicking the image might allow for an option to loop again)"
  },
  {
    "objectID": "posts/gradientdescent/index.html#mini-batch-gradient-descent",
    "href": "posts/gradientdescent/index.html#mini-batch-gradient-descent",
    "title": "Gradient Descent and SGD",
    "section": "Mini-batch gradient descent",
    "text": "Mini-batch gradient descent\nWhat if instead of single example from the dataset, we use a batch of data examples witha given size every time we calculate the gradient:\n\\[\\theta = \\theta - \\eta \\nabla_{\\theta} J(\\theta; x^{(i:i+n)}; y^{(i:i+n)})\\]\nThis is what mini-batch gradient descent is about. Using mini-batches has the advantage that the variance in the loss function is reduced, while the computational burden is still reasonable, since we do not use the full dataset. The size of the mini-batches becomes another hyper-parameter of the problem. In standard implementations it ranges from 50 to 256. In code, mini-batch gradient descent looks like this:\nfor i in range(mb_epochs):\n  np.random.shuffle(data)\n  for batch in get_batches(data, batch_size=50):\n    params_grad = evaluate_gradient(loss_function, batch, params)\n    params = params - learning_rate * params_grad\nThe difference with SGD is that for each update we use a batch of 50 examples to estimate the gradient."
  },
  {
    "objectID": "posts/gradientdescent/index.html#variations-on-a-theme",
    "href": "posts/gradientdescent/index.html#variations-on-a-theme",
    "title": "Gradient Descent and SGD",
    "section": "Variations on a theme",
    "text": "Variations on a theme\n\nMomentum\nOften, the cost function has ravines near local optima, ie. areas where the shape of the function is significantly steeper in certain dimensions than in others. This migh result in a slow convergence to the optimum, since standard gradient descent will keep oscillating about these ravines. In the figures below, the left panel shows convergence without momentum, and the right panel shows the effect of adding momentum:\n\n\n\n&lt;img src=“http://sebastianruder.com/content/images/2015/12/without_momentum.gif”, width=300, height=300&gt;\n\n\n&lt;img src=“http://sebastianruder.com/content/images/2015/12/with_momentum.gif”, width=300, height=300&gt;\n\n\n\nOne way to overcome this problem is by using the concept of momentum, which is borrowed from physics. At each iteration, we remember the update \\(v = \\Delta \\theta\\) and use this velocity vector (which as the same dimension as \\(\\theta\\)) in the next update, which is constructed as a combination of the cost gradient and the previous update:\n\\[v_t = \\gamma v_{t-1} +  \\eta \\nabla_{\\theta} J(\\theta)\\] \\[\\theta = \\theta - v_t\\]\nThe effect of this is the following: the momentum terms increases for dimensions whose gradients point in the same direction, and reduces the importance of dimensions whose gradients change direction. This avoids oscillations and improves the chances of rapid convergence. The concept is analog to the a rock rolling down a hill: the gravitational field (cost function) accelerates the particule (weights vector), which accumulates momentum, becomes faster and faster and tends to keep travelling in the same direction. A commonly used value for the momentum parameter is \\(\\gamma = 0.5\\)."
  },
  {
    "objectID": "posts/samplingclt/index.html",
    "href": "posts/samplingclt/index.html",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/samplingclt/index.html#samples-from-a-population-of-coin-flips",
    "href": "posts/samplingclt/index.html#samples-from-a-population-of-coin-flips",
    "title": "Sampling and the Central Limit Theorem",
    "section": "Samples from a population of coin flips",
    "text": "Samples from a population of coin flips\nLets do some more coin flips; this time we’ll do them in many replications. We’ll establish some terminology at first.\nWe will do a large set of replications M, in each of which we will do many coin flips N. We’ll call the result of each coin flip an observation, and a single replication a sample of observations. Thus the number of samples is M, and the sample size is N. These samples have been chosen from a population of size \\(n &gt;&gt; N\\).\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\n\ndef make_throws(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.mean(start, axis=1)\n\nWe show the mean over the observations, or sample mean, for a sample size of 10, with 20 replications. There are thus 20 means.\n\nmake_throws(number_of_samples=20, sample_size=10)\n\narray([ 0.5,  0.8,  0.5,  1. ,  0.7,  0.7,  0.6,  0.6,  0.7,  1. ,  0.7,\n        0.4,  0.4,  0.4,  0.4,  0.7,  0.7,  0.5,  0.5,  0.5])\n\n\nLet us now do 200 replications, each of which has a sample size of 1000 flips, and store the 200 means for each sample size from 1 to 1000 in sample_means.\n\nsample_sizes=np.arange(1,1001,1)\nsample_means = [make_throws(number_of_samples=200, sample_size=i) for i in sample_sizes]\n\nLets formalize what we are up to. Lets call the N random variables in the \\(m^{th}\\) sample \\(x_{m1},x_{m2},...,x_{mN}\\) and lets define the sample mean\n\\[\\bar{x_m}(N) = \\frac{1}{N}\\, \\sum_{i=1}^{N} x_{mi} \\]\nNow imagine the size of the sample becoming large, asymptoting to the size of an infinite or very large population (ie the sample becomes the population). Then you would expect the sample mean to approach the mean of the population distribution. This is just a restatement of the law of large numbers.\nOf course, if you drew many different samples of a size N (which is not infinite), the sample means \\(\\bar{x_1}\\), \\(\\bar{x_2}\\), etc would all be a bit different from each other. But the law of large numbers intuitively indicates that as the sample size gets very large and becomes an infinite population size, these slightly differeing means would all come together and converge to the population (or distribution) mean.\nTo see this lets define, instead, the mean or expectation of the sample means over the set of samples or replications, at a sample size N:\n\\[E_{\\{R\\}}(\\bar{x}) = \\frac{1}{M} \\,\\sum_{m=1}^{M} \\bar{x_m}(N) ,\\] where \\(\\{R\\}\\) is the set of M replications, and calculate and plot this quantity.\n\nmean_of_sample_means = [np.mean(means) for means in sample_means]\n\n\nplt.plot(sample_sizes, mean_of_sample_means);\nplt.ylim([0.480,0.520]);\n\n\n\n\n\n\n\n\nNot surprisingly, the mean of the sample means converges to the distribution mean as the sample size N gets very large."
  },
  {
    "objectID": "posts/samplingclt/index.html#the-notion-of-a-sampling-distribution",
    "href": "posts/samplingclt/index.html#the-notion-of-a-sampling-distribution",
    "title": "Sampling and the Central Limit Theorem",
    "section": "The notion of a Sampling Distribution",
    "text": "The notion of a Sampling Distribution\nIn data science, we are always interested in understanding the world from incomplete data, in other words from a sample or a few samples of a population at large. Our experience with the world tells us that even if we are able to repeat an experiment or process, we will get more or less different answers the next time. If all of the answers were very different each time, we would never be able to make any predictions.\nBut some kind of answers differ only a little, especially as we get to larger sample sizes. So the important question then becomes one of the distribution of these quantities from sample to sample, also known as a sampling distribution.\nSince, in the real world, we see only one sample, this distribution helps us do inference, or figure the uncertainty of the estimates of quantities we are interested in. If we can somehow cook up samples just somewhat different from the one we were given, we can calculate quantities of interest, such as the mean on each one of these samples. By seeing how these means vary from one sample to the other, we can say how typical the mean in the sample we were given is, and whats the uncertainty range of this quantity. This is why the mean of the sample means is an interesting quantity; it characterizes the sampling distribution of the mean, or the distribution of sample means.\nWe can see this mathematically by writing the mean or expectation value of the sample means thus:\n\\[E_{\\{R\\}}(N\\,\\bar{x}) = E_{\\{R\\}}(x_1 + x_2 + ... + x_N) = E_{\\{R\\}}(x_1) + E_{\\{R\\}}(x_2) + ... + E_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the expectations in the right hand side can be replaced by the population mean using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nE_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\mu\\\\\nE_{\\{R\\}}(\\bar{x}) &=& \\mu\n\\end{eqnarray}\\]\nwhich tells us that in the limit of a large number of replications the expectation value of the sampling means converges to the population mean. This limit gives us the true sampling distribution, as opposed to what we might estimate from our finite set of replicates. (Thus there our \\(E_{\\{R\\}}\\) would be replaced by some \\(E_{fs}\\) where by \\(fs\\) we wish to indicate the pmf or density of the sampling distribution).\n\nThe sampling distribution as a function of sample size\nWe can see what the estimated sampling distribution of the mean looks like at different sample sizes.\n\nsample_means_at_size_10=sample_means[9]\nsample_means_at_size_100=sample_means[99]\nsample_means_at_size_1000=sample_means[999]\n\n\nplt.hist(sample_means_at_size_10, bins=np.arange(0,1,0.01), alpha=0.5);\nplt.hist(sample_means_at_size_100, bins=np.arange(0,1,0.01), alpha=0.4);\nplt.hist(sample_means_at_size_1000, bins=np.arange(0,1,0.01), alpha=0.3);\n\n\n\n\n\n\n\n\nThe distribution is much tighter at large sample sizes, and that you can have way low and way large means at small sample sizes. Indeed there are means as small as 0.1 at a sample size of 10, and as small as 0.3 at a sample size of 100.\nLets plot the distribution of the mean as a function of sample size.\n\nfor i in sample_sizes:\n    if i %50 ==0 and i &lt; 1000:\n        plt.scatter([i]*200, sample_means[i], alpha=0.05);\nplt.xlim([0,1000])\nplt.ylim([0.25,0.75]);\n\n\n\n\n\n\n\n\n\n\nThe variation of the sample mean\nLet the underlying distribution from which we have drawn our samples have, additionally to a well defined mean \\(\\mu\\), a well defined variance \\(\\sigma^2\\).\nThen, as before:\n\\[V_{\\{R\\}}(N\\,\\bar{x}) = V_{\\{R\\}}(x_1 + x_2 + ... + x_N) = V_{\\{R\\}}(x_1) + V_{\\{R\\}}(x_2) + ... + V_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the variances in the right hand side can be replaced by the population variance using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nV_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\sigma^2\\\\\nV(\\bar{x}) &=& \\frac{\\sigma^2}{N}\n\\end{eqnarray}\\]\nThis simple formula is called De-Moivre’s formula, and explains the tell-tale triangular plot we saw above, with lots of variation at low sample sizes turning into a tight distribution at large sample size(N).\nThe square root of \\(V\\), or the standard deviation of the sampling distribution of the mean (in other words, the distribution of sample means) is also called the Standard Error.\nWe can obtain the standard deviation of the sampling distribution of the mean at different sample sizes and plot it against the sample size, to confirm the \\(1/\\sqrt(N)\\) behaviour.\n\nstd_of_sample_means = [np.std(means) for means in sample_means]\n\n\nplt.plot(np.log10(sample_sizes), np.log10(std_of_sample_means));\n\n\n\n\n\n\n\n\nLet us plot again the distribution of sample means at a large sample size, \\(N=1000\\). What distribution is this?\n\nplt.hist(sample_means_at_size_1000, bins=np.arange(0.4,0.6,0.002));\n\n\n\n\n\n\n\n\nLets step back and try and think about what this all means. As an example, say I have a weight-watchers’ study of 1000 people, whose average weight is 150 lbs with standard deviation of 30lbs. If I was to randomly choose many samples of 100 people each, the mean weights of those samples would cluster around 150lbs with a standard error of 30/\\(\\sqrt{100}\\) = 3lbs. Now if i gave you a different sample of 100 people with an average weight of 170lbs, this weight would be more than 6 standard errors beyond the population mean, 1 and would thus be very unlikely to be from the weight watchers group.\n1 this example is motivated by the crazy bus example in Charles Whelan’s excellent Naked Statistics Book\n\nThe Gaussian Distribution\nWe saw in the last section that the sampling distribution of the mean itself has a mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\). This distribution is called the Gaussian or Normal Distribution, and is probably the most important distribution in all of statistics.\nThe probability density of the normal distribution is given as:\n\\[ N(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{(x-\\mu)^2}{2s^2} } .\\]\nThe expected value of the Gaussian distribution is \\(E[X]=\\mu\\) and the variance is \\(Var[X]=s^2\\).\n\nnorm =  sp.stats.norm\nx = np.linspace(-5,5, num=200)\n\n\nfig = plt.figure(figsize=(12,6))\nfor mu, sigma, c in zip([0.5]*3, [0.2, 0.5, 0.8], sns.color_palette()[:3]):\n    plt.plot(x, norm.pdf(x, mu, sigma), lw=2, \n             c=c, label = r\"$\\mu = {0:.1f}, s={1:.1f}$\".format(mu, sigma))\n    plt.fill_between(x, norm.pdf(x, mu, sigma), color=c, alpha = .4)\n    \n    \nplt.xlim([-5,5])\nplt.legend(loc=0)\nplt.ylabel(\"PDF at $x$\")\nplt.xlabel(\"$x$\");\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/samplingclt/index.html#the-central-limit-theorem",
    "href": "posts/samplingclt/index.html#the-central-limit-theorem",
    "title": "Sampling and the Central Limit Theorem",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nThe reason for the distribution’s importance is the Central Limit Theorem(CLT). The theorem is stated as thus, very similar to the law of large numbers:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nIn other words:\n\\[s^2 = \\frac{\\sigma^2}{N}.\\]\nThis is true, regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nStrictly speaking, under some conditions called Lyapunov conditions, the variables \\(x_i\\) dont have to be identically distributed, as long as \\(\\mu\\) is the mean of the means and \\(\\sigma^2\\) is the sum of the individual variances. This has major consequences, for the importance of this theorem.\nMany random variables can be thought of as having come from the sum of a large number of small and independent effects. For example human height or weight can be thought of as the sum as a large number of genetic and environmental factors, which add to increase or decrease height or weight respectively. Or think of a measurement of a height. There are lots of ways things could go wrong: frayed tapes, stretched tapes, smudged marks, bad lining up of the eye, etc. These are all independent and have no systematic error in one direction or the other.\nThen the sum of these factors, as long as there are a large number of them, will be distributed as a gaussian.[this has nothing to do with the sampling distribution of the mean but is part of the origin story of the gaussian distribution]\nAs a rule of thumb, the CLT starts holding at \\(N \\sim 30\\).\n\nWhat does this all mean?\nThe sample mean, or mean of the random variables \\(x_{mi}\\) in the sample \\(m\\), has a sampling distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\), as shown before. Now for large sample sizes we can go further and use the CLT theorem to say that this distribution is the normal distribution,\n\\[S_N \\sim N(\\mu, \\frac{\\sigma^2}{N})\\].\nThe preciseness of saying that we have a gaussian is a huge gain in our expository power. For example, for the case of the weight-watchers program above, a separation of 20lbs is more than 3 standard errors away, which corresponds to being way in the tail of a gaussian distribution. Because we can now quantify the area under the curve, we can say that 99.7% of the sample means lie within 9lbs of 150. Thus you can way easily reject the possibility that the new sample is from the weight-watchers program with 99.7% confidence.\nIndeed, the CLT allows us to take the reduction in variance we get from large samples, and make statements in different cases that are quite strong:\n\nif we know a lot about the population, and randomly sampled 100 points from it, the sample mean would be with 99.7% confidence within \\(0.3\\sigma\\) of the population mean. And thus, if \\(\\sigma\\) is small, the sample mean is quite representative of the population mean.\nThe reverse: if we have a well sampled 100 data points, we could make strong statements about the population as a whole. This is indeed how election polling and other sampling works.\nwe can infer, as we just did, if a sample is consistent with a population\nby the same token, you can compare two samples and infer if they are from the same population."
  },
  {
    "objectID": "posts/samplingclt/index.html#the-sampling-distribution-of-the-variance",
    "href": "posts/samplingclt/index.html#the-sampling-distribution-of-the-variance",
    "title": "Sampling and the Central Limit Theorem",
    "section": "The sampling distribution of the Variance",
    "text": "The sampling distribution of the Variance\nAt this point you might be curious about what the sampling distribution of the variance looks like, and what can we surmise from it about the variance of the entire sample. We can do this, just like we did for the means. We’ll stick with a high number of replicates and plot the mean of the sample variances as well as the truish sampling distribution of the variances at a sample size of 100.\n\ndef make_throws_var(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.var(start, axis=1)\nsample_vars_1000_replicates = [make_throws_var(number_of_samples=1000, sample_size=i) for i in sample_sizes]\nmean_of_sample_vars_1000 = [np.mean(vars) for vars in sample_vars_1000_replicates]\nplt.plot(sample_sizes, mean_of_sample_vars_1000);\nplt.xscale(\"log\");\n\n\n\n\n\n\n\n\nThe “mean sample variance” asymptotes to the true variance of 0.25 by a sample size of 100.\nHow well does the sample variance estimate the true variance?\nIf \\(V_m\\) denotes the variance of a sample,\n\\[ N\\,V_m = \\sum_{i=1}^{N} (x_{mi} - \\bar{x_m})^2 = \\sum_{i=1}^{N}(x_{mi} - \\mu)^2 - N\\,(\\bar{x_m} - \\mu)^2. \\]\nThen \\[E_{\\{R\\}}(N\\,V_m) = E_{\\{R\\}}(\\sum_{i=1}^{N}(x_{mi} - \\mu)^2) - E_{\\{R\\}}(N\\,(\\bar{x_m} - \\mu)^2)\\] In the asymptotic limit of a very large number of replicates, we can then write \\[E(N\\,V) = N\\,\\sigma^2 - \\sigma^2, \\] and thus we have \\[E(V) = \\frac{N-1}{N} \\,\\sigma^2\\].\nIn other words, the expected value of the sample variance is LESS than the actual variance. This should not be surprising: consider for example a sample of size 1 from the population. There is zero variance! More generally, whenever you sample a population, you tend to pick the more likely members of the population, and so the variance in the sample is less than the variance in the population.\nAn interesting application of this idea, as Shalizi points out in http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/, is that the loss of variability due to sampling of genes is indeed the origin of genetic drift. More prosaically, the fact that the above graph of expected sample variance against sample size asymptotes to 0.25 is as \\(\\frac{N-1}{N}\\) if very close to 1 at large N.\nOr put another way, you ought to correct your sample variances by a factor of \\(\\frac{n}{n-1}\\) to estimate the population variance, which itself works as the sampling distribution of the sample variance is rather tight, as seen below.\nThat is, defining the sample variance with \\(n-1\\) in the denominator instead of \\(n\\) gives you an unbiased eatimator of the true variance. This is why, for example, Pandas will do this by default for series and dataframes. (numpy wont, so beware!).\n\nplt.hist(sample_vars_1000_replicates[99], bins=np.arange(0.2,0.26,0.001), alpha=0.2, normed=True);"
  },
  {
    "objectID": "posts/seasons/index.html",
    "href": "posts/seasons/index.html",
    "title": "Why Do We Have Seasons?",
    "section": "",
    "text": "Seasons aren’t caused by Earth’s distance from the Sun. The secret is Earth’s 23.5° axial tilt. As Earth orbits, different hemispheres tilt toward the Sun, receiving more direct sunlight — and that’s what makes it warm.\nUse the buttons below to jump between seasons and see how the sun’s rays strike Earth differently. The golden rays show parallel sunlight beams; the markers on Earth’s surface show where light is concentrated (direct, small footprint = hot) versus spread out (angled, large footprint = cool)."
  },
  {
    "objectID": "posts/seasons/index.html#how-it-works",
    "href": "posts/seasons/index.html#how-it-works",
    "title": "Why Do We Have Seasons?",
    "section": "How It Works",
    "text": "How It Works\nDirect sunlight concentrates energy in a small area — that’s summer. Angled sunlight spreads the same energy over a larger area — that’s winter. It’s like a flashlight: shine it straight down and you get a bright spot; tilt it and the light spreads out and dims.\nA mind-blowing fact: Earth is actually 3 million miles closer to the Sun during Northern Hemisphere winter! But 3 million out of 93 million is only ~3%. The angle of sunlight matters far more than distance.\nFor a deeper exploration with orbits, speed controls, and camera modes, see the full Earth & Sun Explorer."
  },
  {
    "objectID": "posts/introgibbs/index.html",
    "href": "posts/introgibbs/index.html",
    "title": "Introduction to Gibbs Sampling",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm, gamma\nfrom scipy.stats import distributions\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nsns.set_style('whitegrid')\nsns.set_context('poster')\n\n\n\nGibbs determined the energy states of gases at equilibrium by cycling through all the particles, drawing from each one of them conditionally given the enerygy levels of the others, taking the time average. He showed that the time average approached the equilibrium distribution. Geman and Geman used this idea to denoise images.\nSuppose you have a density of two variables \\(f_{XY}(x,y)\\). You wish to sample from this density.\nThe definition of the \\(X\\) marginal is\n\\[f_X(x) = \\int f_{XY}(x,y) dy\\]\n(we’ll drop subscripts from this point onwards)\n\\[f(x) = \\int f(x \\vert y) f(y) dy = \\int dy f(x \\vert y) \\int dx' f( y \\vert x') f(x') \\]\nThus:\n\\[f(x) = \\int h(x, x') f(x') dx'\\]\nwhere\n\\[h(x,x') = \\int dy f(x \\vert y) f(y \\vert x').\\]\nNow consider an iterative scheme in which the “transition kernel” \\(h(x, x')\\) is used to create a proposal for metropolis-hastings moves. This looks like:\n\\[f(x_t) = \\int h(x_t, x_{t-1}) f(x_{t-1}) dx_{t-1}\\]\nwhich is the equation of a stationary distribution.\nThe big idea, then, here, as in the case of markov chains, is that the above equation can be thought of as a fixed-point integral equation, and thus we can think of an iterative process which at first does not satisy this condition but then does as time goes on and we reach stationarity.\nBut now look at the equation for the kernel \\(h(x,x')\\). You have seen an equation like this before when we write a posterior predictive by integrating over the parameters \\(\\theta\\). There we draw \\(\\theta\\) from the posterior and then \\(y^*\\) from the likelihood given \\(\\theta\\), thus giving us samples of the joint distribution \\(p(y^*, \\theta)\\).\nSimilarly here, if we draw \\(y\\), from the conditional \\(f(y \\vert x')\\) and then \\(x\\) again from \\(f(x \\vert y)\\) we will be able to get the marginal distribution of \\(x\\). Symmetrically we can get the marginal for \\(y\\).\nNow, if I can draw from the \\(x\\) marginal, and the \\(y \\vert x\\) conditional, i can draw from the \\(x, y\\) joint, and I am done.\n\n\n\nLet us consider a 2-D distribution which we wish to sample from.\n\\[f(x,y) = x^2 {\\rm exp}[-xy^2 - y^2 + 2y - 4x]\\]\nHere’s what the distribution really looks like.\n\n\nfunc = lambda x,y: x**2*np.exp( -x*y**2 - y**2 + 2*y - 4*x )\n\nnumgridpoints=400\nx = np.linspace(0,2,numgridpoints)\ny = np.linspace(-1,2.5,numgridpoints)\nxx,yy = np.meshgrid(x,y)\nzz = np.zeros((numgridpoints,numgridpoints))\nfor i in np.arange(0,numgridpoints):\n    for j in np.arange(0,numgridpoints):\n        zz[i,j]=func(xx[i,j],yy[i,j])\n        \nplt.contourf(xx,yy,zz);\n\n\n\n\n\n\n\n\nOn first glance the functional form of this distribution looks terrible and difficult to deal with.\nHowever, Gibbs Sampling comes to the rescue. The power of the Gibbs sampler is that you can directly sample from the conditionals alternately.\nLet us do a little math to construct the functional form of the conditionals and express them in terms of distributions we know how to sample from.\nWe have:\n\\[f(x,y) = x^2 {\\rm exp}[-xy^2 - y^2 + 2y - 4x]\\]\nNow assume we hold \\(y\\) constant:\n\\[f(x|y) = x^2 {\\rm exp}[-x (y^2 +4)]{\\rm exp}[- y^2 + 2y]\\]\n\\[f(x|y) = x^2 {\\rm exp}[-x (y^2 +4)] g(y)\\]\nwhere \\(g(y) = {\\rm exp}[- y^2 + 2y]\\). Thus:\n\\[f(x|y) = g(y){\\rm Gamma}(3,y^2+4)\\]\n\ndef xcond(y):\n    return gamma.rvs(3, scale=1/(y*y + 4))\n\nthen assume we hold \\(x\\) constant…\n\\[f(y|x) = x^2 {\\rm exp}[-y^2(1+x) + 2y]{\\rm exp}[-4x]\\]\n\\[f(y|x) = x^2 {\\rm exp}[-y^2(1+x) +2y - \\frac{1}{1+x}]\\, {\\rm exp}[-4x]\\,{\\rm exp}[\\frac{1}{1+x}]\\]\n\\[f(y|x) = x^2 {\\rm exp}[-(1+x)(y^2 - 2y\\frac{1}{1+x} + \\frac{1}{(1+x)^2})]{\\rm exp}[-4x]\\]\nWriting \\(h(x) = x^2 {\\rm exp}[-4x]\\) we have\n\\[f(y|x) = N(\\frac{1}{1+x},\\frac{1}{\\sqrt{(2(1+x))}}) h(x)\\]\n\ndef ycond(x):\n    return norm.rvs(1/(1+x), scale=1.0/np.sqrt(2*(x+1)))\n\nSo we can use Gibbs sampler to select directly from the functional forms of the conditionals because we know how to sample from a Normal distribution and we know how to sample from a Gamma distribution.\nThe key thing which makes Gibbs useful is knowing how to directly sample from the conditionals.\n\n\n\n\ndef gibbs(xgiveny_sample, ygivenx_sample, N, start = [0,0]):\n    x=start[0]\n    y=start[1]\n    samples=np.zeros((N+1, 2))\n    samples[0,0]=x\n    samples[0,1]=y\n    \n    for i in range(1,N,2):\n        x=xgiveny_sample(y)\n        samples[i,0]=x\n        samples[i, 1]=y\n        \n        y=ygivenx_sample(x)\n        samples[i+1,0]=x\n        samples[i+1,1]=y\n        \n    return samples\n\n\nout=gibbs(xcond, ycond, 100000)\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nplt.hist2d(out[10000:,0],out[10000:,1], normed=True, bins=100, cmap=cmap)\nplt.contour(xx,yy,zz)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLet’s look at the single steps the Gibbs sampler is taking:\n\nout=gibbs(xcond, ycond, 10000, start = [2,2])\nnr_t = 50\nplt.contour(xx,yy,zz)\nplt.plot(out[:nr_t, 0],out[:nr_t, 1], '.',alpha=0.8)\nplt.plot(out[:nr_t, 0],out[:nr_t, 1], c='r', alpha=0.5, lw=1)\n\n\n\n\n\n\n\n\n\n\n\nLooking at the autocorrelation of our samples we are in pretty good shape and probably don’t have to worry much about thinning here. Note that this is not always the case for Gibbs sampling. The other notebook has an example where we need to have a lot of thinning to account for the autocorrelation of the samples. It all depends on the shape of our target distribution.\n\ndef corrplot(trace, maxlags=50):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\ncorrplot(out[4000:,0], 100)\n\n\n\n\n\n\n\n\n\ncorrplot(out[4000:,1], 100)\n\n\n\n\n\n\n\n\nGenerally, gibbs samplers can show a lot of autocorrelation, needing quite a bit of thinning. This is especially true when the functions you are sampling from have lots of correlation amongst the components. But as in all MCMC algorithms, the number of samples needed depends on what you are doing. Fully characterizing a posterior needs a lot of samples, but you might be able to get away with less in computing expectations…"
  },
  {
    "objectID": "posts/introgibbs/index.html#the-basic-idea",
    "href": "posts/introgibbs/index.html#the-basic-idea",
    "title": "Introduction to Gibbs Sampling",
    "section": "",
    "text": "Gibbs determined the energy states of gases at equilibrium by cycling through all the particles, drawing from each one of them conditionally given the enerygy levels of the others, taking the time average. He showed that the time average approached the equilibrium distribution. Geman and Geman used this idea to denoise images.\nSuppose you have a density of two variables \\(f_{XY}(x,y)\\). You wish to sample from this density.\nThe definition of the \\(X\\) marginal is\n\\[f_X(x) = \\int f_{XY}(x,y) dy\\]\n(we’ll drop subscripts from this point onwards)\n\\[f(x) = \\int f(x \\vert y) f(y) dy = \\int dy f(x \\vert y) \\int dx' f( y \\vert x') f(x') \\]\nThus:\n\\[f(x) = \\int h(x, x') f(x') dx'\\]\nwhere\n\\[h(x,x') = \\int dy f(x \\vert y) f(y \\vert x').\\]\nNow consider an iterative scheme in which the “transition kernel” \\(h(x, x')\\) is used to create a proposal for metropolis-hastings moves. This looks like:\n\\[f(x_t) = \\int h(x_t, x_{t-1}) f(x_{t-1}) dx_{t-1}\\]\nwhich is the equation of a stationary distribution.\nThe big idea, then, here, as in the case of markov chains, is that the above equation can be thought of as a fixed-point integral equation, and thus we can think of an iterative process which at first does not satisy this condition but then does as time goes on and we reach stationarity.\nBut now look at the equation for the kernel \\(h(x,x')\\). You have seen an equation like this before when we write a posterior predictive by integrating over the parameters \\(\\theta\\). There we draw \\(\\theta\\) from the posterior and then \\(y^*\\) from the likelihood given \\(\\theta\\), thus giving us samples of the joint distribution \\(p(y^*, \\theta)\\).\nSimilarly here, if we draw \\(y\\), from the conditional \\(f(y \\vert x')\\) and then \\(x\\) again from \\(f(x \\vert y)\\) we will be able to get the marginal distribution of \\(x\\). Symmetrically we can get the marginal for \\(y\\).\nNow, if I can draw from the \\(x\\) marginal, and the \\(y \\vert x\\) conditional, i can draw from the \\(x, y\\) joint, and I am done."
  },
  {
    "objectID": "posts/introgibbs/index.html#an-example",
    "href": "posts/introgibbs/index.html#an-example",
    "title": "Introduction to Gibbs Sampling",
    "section": "",
    "text": "Let us consider a 2-D distribution which we wish to sample from.\n\\[f(x,y) = x^2 {\\rm exp}[-xy^2 - y^2 + 2y - 4x]\\]\nHere’s what the distribution really looks like.\n\n\nfunc = lambda x,y: x**2*np.exp( -x*y**2 - y**2 + 2*y - 4*x )\n\nnumgridpoints=400\nx = np.linspace(0,2,numgridpoints)\ny = np.linspace(-1,2.5,numgridpoints)\nxx,yy = np.meshgrid(x,y)\nzz = np.zeros((numgridpoints,numgridpoints))\nfor i in np.arange(0,numgridpoints):\n    for j in np.arange(0,numgridpoints):\n        zz[i,j]=func(xx[i,j],yy[i,j])\n        \nplt.contourf(xx,yy,zz);\n\n\n\n\n\n\n\n\nOn first glance the functional form of this distribution looks terrible and difficult to deal with.\nHowever, Gibbs Sampling comes to the rescue. The power of the Gibbs sampler is that you can directly sample from the conditionals alternately.\nLet us do a little math to construct the functional form of the conditionals and express them in terms of distributions we know how to sample from.\nWe have:\n\\[f(x,y) = x^2 {\\rm exp}[-xy^2 - y^2 + 2y - 4x]\\]\nNow assume we hold \\(y\\) constant:\n\\[f(x|y) = x^2 {\\rm exp}[-x (y^2 +4)]{\\rm exp}[- y^2 + 2y]\\]\n\\[f(x|y) = x^2 {\\rm exp}[-x (y^2 +4)] g(y)\\]\nwhere \\(g(y) = {\\rm exp}[- y^2 + 2y]\\). Thus:\n\\[f(x|y) = g(y){\\rm Gamma}(3,y^2+4)\\]\n\ndef xcond(y):\n    return gamma.rvs(3, scale=1/(y*y + 4))\n\nthen assume we hold \\(x\\) constant…\n\\[f(y|x) = x^2 {\\rm exp}[-y^2(1+x) + 2y]{\\rm exp}[-4x]\\]\n\\[f(y|x) = x^2 {\\rm exp}[-y^2(1+x) +2y - \\frac{1}{1+x}]\\, {\\rm exp}[-4x]\\,{\\rm exp}[\\frac{1}{1+x}]\\]\n\\[f(y|x) = x^2 {\\rm exp}[-(1+x)(y^2 - 2y\\frac{1}{1+x} + \\frac{1}{(1+x)^2})]{\\rm exp}[-4x]\\]\nWriting \\(h(x) = x^2 {\\rm exp}[-4x]\\) we have\n\\[f(y|x) = N(\\frac{1}{1+x},\\frac{1}{\\sqrt{(2(1+x))}}) h(x)\\]\n\ndef ycond(x):\n    return norm.rvs(1/(1+x), scale=1.0/np.sqrt(2*(x+1)))\n\nSo we can use Gibbs sampler to select directly from the functional forms of the conditionals because we know how to sample from a Normal distribution and we know how to sample from a Gamma distribution.\nThe key thing which makes Gibbs useful is knowing how to directly sample from the conditionals."
  },
  {
    "objectID": "posts/introgibbs/index.html#the-sampler",
    "href": "posts/introgibbs/index.html#the-sampler",
    "title": "Introduction to Gibbs Sampling",
    "section": "",
    "text": "def gibbs(xgiveny_sample, ygivenx_sample, N, start = [0,0]):\n    x=start[0]\n    y=start[1]\n    samples=np.zeros((N+1, 2))\n    samples[0,0]=x\n    samples[0,1]=y\n    \n    for i in range(1,N,2):\n        x=xgiveny_sample(y)\n        samples[i,0]=x\n        samples[i, 1]=y\n        \n        y=ygivenx_sample(x)\n        samples[i+1,0]=x\n        samples[i+1,1]=y\n        \n    return samples\n\n\nout=gibbs(xcond, ycond, 100000)\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nplt.hist2d(out[10000:,0],out[10000:,1], normed=True, bins=100, cmap=cmap)\nplt.contour(xx,yy,zz)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLet’s look at the single steps the Gibbs sampler is taking:\n\nout=gibbs(xcond, ycond, 10000, start = [2,2])\nnr_t = 50\nplt.contour(xx,yy,zz)\nplt.plot(out[:nr_t, 0],out[:nr_t, 1], '.',alpha=0.8)\nplt.plot(out[:nr_t, 0],out[:nr_t, 1], c='r', alpha=0.5, lw=1)\n\n\n\n\n\n\n\n\n\n\n\nLooking at the autocorrelation of our samples we are in pretty good shape and probably don’t have to worry much about thinning here. Note that this is not always the case for Gibbs sampling. The other notebook has an example where we need to have a lot of thinning to account for the autocorrelation of the samples. It all depends on the shape of our target distribution.\n\ndef corrplot(trace, maxlags=50):\n    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n    plt.xlim([0, maxlags])\ncorrplot(out[4000:,0], 100)\n\n\n\n\n\n\n\n\n\ncorrplot(out[4000:,1], 100)\n\n\n\n\n\n\n\n\nGenerally, gibbs samplers can show a lot of autocorrelation, needing quite a bit of thinning. This is especially true when the functions you are sampling from have lots of correlation amongst the components. But as in all MCMC algorithms, the number of samples needed depends on what you are doing. Fully characterizing a posterior needs a lot of samples, but you might be able to get away with less in computing expectations…"
  },
  {
    "objectID": "posts/vizasstory.html",
    "href": "posts/vizasstory.html",
    "title": "Visualization As Story",
    "section": "",
    "text": "There is this pretty famous book by Steve Krug, called “Dont Make Me Think”. Its a call to respect conventions for web elements, such as shopping carts (a cart should be on the upper right), so that the web experience is obvious to users.\n\n\n\nIn visualization, as in web development, your audience does not want to spend cognitive effort on things you could just show them, by convention, or by explicit writing. So, just point out the key facts and insights.\nFor example, in this great article in the financial times https://www.ft.com/content/0f11b219-0f1b-420e-8188-6651d1e749ff?hcb=1, the main point “Vaccines have made Covid-19 far less lethal” is written up-front.\n\n\n\nThe implications are made clear in the second sentence, comparing vaccinated 80 year-olds to un-vaccinated 50 year-olds. This implication is illustrated in the visualization as well, with a horizontal black line, and a caption.\nInstead of point markers, downwards pointing arrows are used on lines to reinforce the notion of lower risk. Captions and annotations are used to point out key insights. Extraneous frames and tick marks are removed.\nThis is an example of framing. It grabs the audience and leads it through the insights you want to share.\n\n\n\nThere’s been a lot of worry about breakthrough vaccination, especially with the news about the Provincetown cluster. Here is another visualization from the same article, telling us why the large number of breakthrough infections are to be expected.\n\n\n\nIt walks us through the entire calculation visually. And does it in two scenarios: high vaccination rates and low vaccination rates. We can ourselves see the larger hospitalization numbers in the low-vaccination scenario.\nThe visualization and explanation could have been framed in terms of base rates and conditional probabilities, but by illustrating the concepts with an example, they are made accessible to everyone. And the framing drives home the story: go get your shot!\nRead more on how to make good visualizations using R in this book by @khealy . If you are a pythonista, learn how to make good plots in @matplotlib using https://end-to-end-machine-learning.teachable.com/p/navigating-matplotlib-tutorial-how-to/ by @_brohrer_ ."
  },
  {
    "objectID": "posts/hmcidea/index.html",
    "href": "posts/hmcidea/index.html",
    "title": "The Idea of Hamiltonian Monte Carlo",
    "section": "",
    "text": "The neighborhoods around the maxima of probability distributions feature a lot of probability density, but, especially in a large number of dimensions, or in long tailed distributions, they do not feature much volume. In other words, the “sliver” size \\(dP\\) tends to be small there.\nThe typical set refers to the portion of space where most of these slivers live. This is typically an interplay of density and volume, and thus is likely to be found as a more-concentrated space not containing the modes in higher dimensions.\n\n\n\nThe typical set: in high dimensions, the region of highest probability mass lies away from the mode, in a shell where density and volume balance.\n\n\nThus if we design a sampler which efficiently traverses the typical set, we can tolerate poor sampling elsewhere and not incur large penalties in calculating our expectations. So far, we have used MCMC methods which stumble around like a drunk using proposal distributions. These proposals have issues: too large a step size and you propose to go outside the typical set (especially in large dimensions) and are rejected, too small and you spend a large time in a small part of the typical set, as illustrated below.\n\n\n\nStep size issues in random-walk MCMC: too large leads to rejection, too small leads to slow exploration of the typical set.\n\n\nFurthermore, we have problems in pinches and areas of large curvature and separated modes as transitioning is hard and oscillatory behavior is more likely"
  },
  {
    "objectID": "posts/hmcidea/index.html#exploring-the-typical-set",
    "href": "posts/hmcidea/index.html#exploring-the-typical-set",
    "title": "The Idea of Hamiltonian Monte Carlo",
    "section": "",
    "text": "The neighborhoods around the maxima of probability distributions feature a lot of probability density, but, especially in a large number of dimensions, or in long tailed distributions, they do not feature much volume. In other words, the “sliver” size \\(dP\\) tends to be small there.\nThe typical set refers to the portion of space where most of these slivers live. This is typically an interplay of density and volume, and thus is likely to be found as a more-concentrated space not containing the modes in higher dimensions.\n\n\n\nThe typical set: in high dimensions, the region of highest probability mass lies away from the mode, in a shell where density and volume balance.\n\n\nThus if we design a sampler which efficiently traverses the typical set, we can tolerate poor sampling elsewhere and not incur large penalties in calculating our expectations. So far, we have used MCMC methods which stumble around like a drunk using proposal distributions. These proposals have issues: too large a step size and you propose to go outside the typical set (especially in large dimensions) and are rejected, too small and you spend a large time in a small part of the typical set, as illustrated below.\n\n\n\nStep size issues in random-walk MCMC: too large leads to rejection, too small leads to slow exploration of the typical set.\n\n\nFurthermore, we have problems in pinches and areas of large curvature and separated modes as transitioning is hard and oscillatory behavior is more likely"
  },
  {
    "objectID": "posts/hmcidea/index.html#gliding-instead-of-stumbling",
    "href": "posts/hmcidea/index.html#gliding-instead-of-stumbling",
    "title": "The Idea of Hamiltonian Monte Carlo",
    "section": "Gliding instead of stumbling",
    "text": "Gliding instead of stumbling\nGiven infinite resources, we will ultimately travel the whole typical set and more of the density. But we dont have these, especially in larger dimensions, and thus we would do better if we found a way to glide around the typical set, rather than making random walk transitions which would keep us on the typical set at times and off at other times.\n\n\n\nThe need for gliding: instead of random walks that stumble on and off the typical set, we want smooth traversal along it.\n\n\nIn other words, we’d like to explore the typical-set surface smoothly. To do this we must first characterize the surface, something we can do via a gradient. To do this we must identify the equation of the typical set surface so that we can find the gradient which is perpendicular to the surface. And once we do that, we are not done, as the gradient points towards regions of higher density (modes) from the surface of the typical set.\nTo do this imagine a sliver \\(dP = p(q)dq\\) thats in the typical set. If \\(dq\\) (we are using \\(q\\) instead of \\(x\\)) is small enough, then we can consider the typical set as a collection of foliates \\(\\{q_i\\}\\) each of constant probability density \\(p(q)=c_i\\) where \\(c_i\\) is a constant. Thus there are n such foliates, or “orbits”, or level sets. Now we know that the gradient is perpendicular to such level-sets and we can use it to characterize these sets."
  },
  {
    "objectID": "posts/hmcidea/index.html#mechanics-to-the-rescue",
    "href": "posts/hmcidea/index.html#mechanics-to-the-rescue",
    "title": "The Idea of Hamiltonian Monte Carlo",
    "section": "Mechanics to the rescue",
    "text": "Mechanics to the rescue\nWe can make our usual connection to the energy of a physical system by tentatively identifying the energy \\(E = - log(p(q))\\). This is what we did to find distributions in Metropolis and the inverse \\(p(q) = e^{-E(q)/T}\\) the minima of functions in simulated annealing. There we proposed a move using a proposal distribution, creating a random walk.\nWe dont want to do that here, preferring something that will move us smoothly along a level set. We use our newly acquired knowledge of data-augmentation and gibbs-sampling from an augmented distribution instead. The basic idea is to add a momentum variable \\(p\\) for each \\(q\\) in our probability density, creating a joint pdf \\(p(p,q)\\).\nHow would this work? And why momentum? Lets think about a rocket (or a satellite with thrusters it can fire) in orbit around the earth\n\n\n\nA rocket in orbit: with just the right momentum, it counterbalances gravitational potential and moves along a constant-energy level set.\n\n\nIf this rocket had no velocity, it would simply fall down to the earth because it would not be able to counterbalance earth’s gravitational potential (the equivalent of the energy we formulated above and have used in simulated annealing). On the other hand, if it had too much velocity, it would escape earth’s gravity and take off to mars or similar.\nIf we add just the right amount of momentum, it will exactly counterbalance the gravitational force, and the satellite will continue to move in its orbit. And this is an orbit of minimum energy (and therefore constant energy, since a system at minumum energy wont lift from it unless kicked(perhaps stochastically) to do so). Thus the satellite will move exactly along a level-curve of the energy level, in a direction exactly perpendicular to the gradient.\nThus the key to moving a sampler along a probability level curve is to give the sampler momentum and thus kinetic energy via the augmented momemtum variable. In other words, we must carry out an augmentation with an additional momentum which leaves the energy Hamiltonian\n\\[H(p, q) = \\frac{p^2}{2m} +  V(q) = E_i,\\]\nwith \\(E_i\\) constants (constant energies) for each level-set foliate and where the potential energy \\(V(q) = -log(p(q))\\) replaces the energy term we had earlier in simulated annealing. The first term above is called the kinetic energy \\(K(p,q)\\) and has a mass parameter \\(m(q)\\), as you might be familiar with from introductory physics (where \\(m\\) is a constant. One can consider generalizations of the kinetic energy term, but with a quadratic in \\(p\\) and if \\(V(q) = \\frac{1}{2}q^2\\) our distribution is gaussian and the level sets are ellipses of constant energy, as illustrated below.\n\n\n\nHamiltonian level sets: with quadratic kinetic and potential energy, the constant-energy orbits form ellipses in (p,q) phase space.\n\n\nOur distribution over \\(p\\) and \\(q\\) is now:\n\\[p(p,q) = e^{-H(p,q)} = e^{-K(p, q)}e^{-V(q)} = p(p \\vert q)p(q)\\]\nand thus:\n\\[H(p,q) = -log(p(p,q)) = -log p(p \\vert q) - log p(q)\\]\nThe choice of a kinetic energy term then is the choice of a conditional probability distribution over the “augmented” momentum which ensures that\n\\[\\int dp p(p, q) = \\int dp p(p \\vert q) p(q) = p(q) \\int p(p \\vert q) dp = p(q).\\]\nThe game now is to sample from this two-N-dimensional distribution and marginalize over the momenta to get the distribution from the \\(q\\). To carry out this sampling, we’ll use the physics equations of motion in the Hamiltonian Formalism (thus leading to the name Hamiltonian Monte Carlo) to “glide” over a level set.\nBut this then leaves us with the problem of having to go from one level-set to another: after all, we wish to explore the entire typical set. The solution to this is simple..after exploring a given level set for a while, we resample the momentum, and off to another level-set we go, as illustrated below:\n\n\n\nMomentum resampling: after gliding along one energy level set, resample the momentum to jump to a different level set and continue exploring.\n\n\nThis is like a rocket firing its thrusters.\nThere remains the issue of ensuring reversibility, and showing that we have an appropriate MCMC transition. This requires delving into the deeper mechanical and statistical-mechanics of the process, and we shall come to that soon. We also need to understand how much time we should spend exploring a level set vs momentum resampling…these are all practicalities we will tackle soon!"
  },
  {
    "objectID": "posts/importancesampling/index.html",
    "href": "posts/importancesampling/index.html",
    "title": "Importance Sampling",
    "section": "",
    "text": "Importance sampling is directly a method to calculate integrals or expectations, which is one of our main goals at the end of things.\nThe basic idea behind importance sampling is that we want to draw more samples where \\(h(x)\\), a function whose integral or expectation we desire, is large. In the case we are doing an expectation, it would indeed be even better to draw more samples where \\(h(x)f(x)\\) is large, where \\(f(x)\\) is the pdf we are calculating the integral with respect to.\nWhy is this important? Often, in the computation of an expectation or other integral, the integrand has a very small value on a dominant fraction of the whole integration volume. If the points are chosen evenly in the integration volume, the small minority of the points close to the ‘peak’ give the dominant contribution to the integral.\nFor example lets look at the expectation\n\\[ E_f[h] = \\int_V f(x) h(x) dx. \\]\nChoose a distribution \\(g(x)\\), which is close to the function \\(f(x)\\), but which is simple enough so that it is possible to generate random \\(x\\)-values from this distribution. The integral can now be re-written as:\n\\[ E_f[h] = \\int h(x) g(x) \\frac{f(x)}{g(x)} dx \\]\nTherefore if we choose random numbers \\(x_i\\) from distribution \\(g(x)\\), we obtain\n\\[ E_f[h] = \\lim_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{x_{i}\\sim g(.)} h(x_i)\\frac{f(x_i)}{g(x_i)} \\]\nUsually you might have written:\n\\[E_f[h] = \\lim_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{x_{i}\\sim f(.)} h(x_i) \\]\nbut now we have a reweighting with \\(w(x_i) =  \\frac{f(x_i)}{g(x_i)}\\) and the samples are drawn from \\(g(x)\\):\n\\[ E_f[h] = \\lim_{N\\rightarrow \\infty} \\frac{1}{N} \\sum_{x_{i}\\sim g(.)} w(x_i) h(x_i) \\]\nUnlike rejection sampling we have used all samples!\nNow remember that the variance of our montecarlo estimate is given to us by\n\\[\\hat{V} = \\frac{V_f[h(x)]}{N}\\]\nwhere \\(N\\) is the sample size.\nWith importance sampling this formula has now changed to\n\\[\\hat{V} = \\frac{V_g[w(x)h(x)]}{N}\\]\nOur game here now is to try and minimize \\(V_g[w(x)h(x)]\\).\nAs a somewhat absurd notion, this variance would be sent to zero, if:\n\\[w(x)h(x) = C \\implies f(x) h(x) = C g(x),\\]\nwhich leads to (since g(x) is a density we must normalize)\n\\[g(x) = \\frac{f(x)h(x)}{\\int f(x) h(x) dx} = \\frac{f(x)h(x)}{E_f[h(x)]}\\]\nThe expectation was what we were trying to estimate in the first place so our tautological absurdity seems to grow..\nBut, ignoring the denominator, this formula tells us that to achieve low variance, we must have \\(g(x)\\) large where the product \\(f(x)h(x)\\) is large. After all, maximizing the latter in some fashion was our original intuition.\nOr to put it another way, \\(\\frac{g(x)}{f(x)}\\) ought to be large where \\(h(x)\\) is large. This means that, as we said earlier, choose more samples near the peak.\nSo now we have the ingredients of our method. We have a \\(f\\) that we might or might not know. We have a pdf \\(g\\) which we choose to be higher than \\(f\\) at the points where \\(h\\) has peaks. Now what we are left to do is to sample from \\(g\\), and this will give us an oversampling at the place \\(h\\) has peaks, and thus we must correct this there by multiplying by weights \\(w  = \\frac{f}{g} \\lt 1\\) in thse places.\nBe careful to choose \\(g(x)\\) appropriately, it should have thicker tails than f, or the ratio \\(f/g\\) will be too big and count contribute too much in the tails.\nAll of these considerations may be seen in the diagram below:\n\n\n\nImportance sampling: a proposal g(x) concentrates samples where the integrand f(x)·h(x) is large, reweighting by f/g to correct the bias.\n\n\nAnother way of seeing this whole thing is that we will draw the sample from a proposal distribution and re-weight the integral appropriately so that the expectation with respect to the correct distribution is used. And since \\(f/g\\) is flatter than \\(f\\), the variance of \\(h \\times f/g\\) is smaller that the variance of \\(h \\times f\\) and therefore the error will be smaller for all \\(N\\).\n\nExample: Calculate $_{0}^{} (x) , x , dx $\nThe function has a shape that is similar to Gaussian and therefore we choose here a Gaussian as importance sampling distribution.\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nmu = 2;\nsig =.7;\n\nf = lambda x: np.sin(x)*x\ninfun = lambda x: np.sin(x)-x*np.cos(x)\np = lambda x: (1/np.sqrt(2*np.pi*sig**2))*np.exp(-(x-mu)**2/(2.0*sig**2))\nnormfun = lambda x:  norm.cdf(x-mu, scale=sig)\n\n\nplt.figure(figsize=(18,8))  # set the figure size\n\n\n# range of integration\nxmax =np.pi \nxmin =0\n\n# Number of draws \nN =1000\n\n# Just want to plot the function\nx=np.linspace(xmin, xmax, 1000)\nplt.subplot(1,2,1)\nplt.plot(x, f(x), 'b', label=u'Original  $x\\sin(x)$')\nplt.plot( x, p(x), 'r', label=u'Importance Sampling Function: Normal')\nplt.plot(x, np.ones(1000)/np.pi,'k')\nxis = mu + sig*np.random.randn(N,1);\nplt.plot(xis, 1/(np.pi*p(xis)),'.', alpha=0.1)\nplt.xlim([0, np.pi])\nplt.ylim([0,2])\nplt.xlabel('x')\nplt.legend()\n# =============================================\n# EXACT SOLUTION \n# =============================================\nIexact = infun(xmax)-infun(xmin)\nprint(\"Exact solution is: \", Iexact)\n\n# ============================================\n# VANILLA MONTE CARLO \n# ============================================\nIvmc = np.zeros(1000)\nfor k in np.arange(0,1000):\n    x = np.random.uniform(low=xmin, high=xmax, size=N)\n    Ivmc[k] = (xmax-xmin)*np.mean( f(x))\n\nprint(\"Mean basic MC estimate: \", np.mean(Ivmc))\nprint(\"Standard deviation of our estimates: \", np.std(Ivmc))\n\n# ============================================\n# IMPORTANCE SAMPLING \n# ============================================\n# CHOOSE Gaussian so it similar to the original functions\n\nIis = np.zeros(1000)\nfor k in np.arange(0,1000):\n    # DRAW FROM THE GAUSSIAN mean =2 std = sqrt(0.4) \n    xis = mu + sig*np.random.randn(N,1);\n    #hist(x)\n    xis = xis[ (xis&lt;xmax) & (xis&gt;xmin)] ;\n\n    # normalization for gaussian from 0..pi\n    normal = normfun(np.pi)-normfun(0);\n\n\n    Iis[k] =np.mean(f(xis)/p(xis))*normal;\n\nprint(\"Mean importance sampling MC estimate: \", np.mean(Iis))\nprint(\"Standard deviation of our estimates: \", np.std(Iis))\nplt.subplot(1,2,2)\nplt.hist(Iis,30, histtype='step', label=u'Importance Sampling');\nplt.hist(Ivmc, 30, color='r',histtype='step', label=u'Vanilla MC');\n \nplt.legend()\n \n \n \n\nExact solution is:  3.14159265359\nMean basic MC estimate:  3.14068341144\nStandard deviation of our estimates:  0.0617743877206\nMean importance sampling MC estimate:  3.14197268362\nStandard deviation of our estimates:  0.0161935244302"
  },
  {
    "objectID": "posts/utilityorrisk/index.html",
    "href": "posts/utilityorrisk/index.html",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\nimport pymc3 as pm\n\n\n\nThe basic idea behind decision theory is this: predictions (or actions based on predictions) are described by a utility or loss function, whose values can be computed given the observed data.\nIndeed one can consider prediction itself as actions to be undertaken with respect to a particular utility.\nRemember that there are two key distributions arising from the Bayesian scenario: the posterior \\(p(\\theta \\vert D)\\) and the posterior-predictive \\(p(y^* \\vert D) = \\int d\\theta\\, p(y^* \\vert \\theta) p(\\theta \\vert D)\\). Either of these can be used to make decisions: it depends upon what we have information about. If we have information about the true values of params, the posterior might be fine to use. But typically, we dont have this, and are more interested in predictiona from the model, so we formulate the problem in terms of the posterior predictive.\nEither way, the components of the decision problem are\n\n\\(a \\in A\\), available actions for the decision problem\n\\(\\omega \\in \\Omega\\), a state in the set of states of the world. If \\(\\Omega\\) is the set of all future \\(y\\), then \\(\\omega = y^*\\), a future \\(y\\). if \\(\\Omega\\) is the posterior, then \\(\\omega = \\theta\\) is a value of a parameter(s)\n\\(p(\\omega \\vert D)\\) which tells us out current beliefs about the world. This is either the posterior distribution (for \\(\\theta\\)) or the posterior predictive distribution (for \\(y^*\\))\nA utility function \\(u(a, \\omega): A \\times \\Omega \\rightarrow R\\) that awards a score/utility/profit to each action \\(a\\) when the state of the universe is \\(\\omega\\). This can be also formulated as a risk/loss.\n\nThe game then is to maximize the distribution-expected-utility amongst all possible actions a (or minimize the risk).\nIn other words we first define the distribution-averaged utility:\n\\[\\bar{u}(a) = \\int d\\omega \\, u(a, \\omega) \\, p(\\omega \\vert D)\\]\nWe then find the \\(a\\) that maximizes this utility:\n\\[ \\hat{a} = \\arg\\max_a \\bar{u}(a)\\]\nThis action is called the bayes action.\nThe resulting maximized expected utility is given by:\n\\[\\bar{u}(\\hat{a}, p) = \\bar{u}(\\hat{a}) = \\int d\\omega \\, u(\\hat{a}, \\omega) \\, p(\\omega \\vert D)\\]\nThis maximized utility is sometimes referred to as the entropy function, and an associate divergence can be defined:\n\\[ d(a,p) = \\bar{u}(p, p) - \\bar{u}(a, p)\\]\nThen one can think of minimizing \\(d(a,p)\\) with respect to \\(a\\) to get \\(\\hat{a}\\), so that this discrepancy can be thought of as a loss function.\n\n\nTo make this concrete consider the problem in which \\(\\omega\\) is a future observation \\(y^*\\). We will then get a posterior predictive distribution with respect to some model \\(M\\) that we shall put into our conditioned-upon variables as well (the reason to do this is that we’ll consider later, averaging with respect to sufficiently expressive or true distributions, rather than any particular posterior predictive).\nWith this in hand we can write:\n\\[\\bar{u}(a) = \\int dy^* \\, u(a, y^*) \\, p(y^* \\vert D, M)\\]\nand\n\\[\\bar{u}(\\hat{a}, p) = \\bar{u}(\\hat{a}) = \\int dy^* \\, u(\\hat{a}, y^*) \\, p(y^* \\vert D, M)\\]\nEverything we have talked above works for linear regression ansd other supervised learning: we must just carry an additional \\(x\\) along with us in the formulae:\n\\[\\bar{u}(a(x)) = \\int dy^* \\, u(a(x), y^*) \\, p(y^* \\vert x^*, D, M)\\]\n\\[ \\hat{a}(x) = \\arg\\max_a \\bar{u}(a(x))\\]\n\n\n\n\nWe have not specified above what the action \\(a\\) is. This is on purpose. Sometimes we want to make point predictions. In this case \\(a\\) is a single number. Sometimes we want to find a distribution. And sometimes we want to compare multiple models. We’ll see all of these below…\n\n\nThe squared error is an example of a risk defined to make a point estimate. Given a posterior predictive distribution, how do you communicate one number to your boss from it? How do you make a point-prediction. (You can think of it as a quadratic approximation to the general class of convex loss functions).\n\\[l(a, y^*) = (a - y^*)^2\\]\nThe optimal point prediction that minimizes the expected loss (negative expected utility):\n\\[\\bar{l}(a) = \\int dy^* \\, (a - y^*)^2 \\, p(y^* \\vert D, M),\\]\nis the posterior predictive mean:\n\\[\\hat{a} = E_p[y^*].\\]\nThis can be easily seen by differentiating with respect to \\(a\\).\nThe expected loss then becomes:\n\\[\\bar{l}(\\hat{a}) = \\int dy^* \\, (\\hat{a} - y^*)^2 \\, p(y^* \\vert D, M) = \\int dy^* \\, (E_p[y^*] - y^*)^2 \\, p(y^* \\vert D, M) = Var_p[y^*]\\]\nUsing such a loss thus indicates that you only care about the first two moments about the distribution, and that there is no gain to considering things like skewness and kurtosis.\n\n\n\nThis is a regression example.\nWe generate some data to have returns as a function of stock price or trading signals. We generate it as a line with noise, which is perhaps the most unlikely stock market signal…:-)\n\n## Code to create artificial data\nN = 100\nX = 0.025*np.random.randn(N)\nY = 0.5*X + 0.01*np.random.randn(N) \n\nls_coef_ = np.cov(X, Y)[0,1]/np.var(X)\nls_intercept = Y.mean() - ls_coef_*X.mean()\n\nplt.scatter(X, Y, c=\"k\")\nplt.xlabel(\"trading signal\")\nplt.ylabel(\"returns\")\nplt.title(\"Empirical returns vs trading signal\")\nplt.plot(X, ls_coef_*X + ls_intercept, label = \"Least-squares line\")\nplt.xlim(X.min(), X.max())\nplt.ylim(Y.min(), Y.max())\nplt.legend(loc=\"upper left\");\n\n\n\n\n\n\n\n\nA squared-error loss is agnostic to the signage and would penalize a prediction of -0.01 equally as bad a prediction of 0.03:\n\\[(0.01−(−0.01))^2=(0.01−0.03)^2=0.004\\]\nThus we want to define a loss that is sensitive to the difference between the prediction and the posterior-predictive sample.\nNotice that the loss is quadratic when the sign is different, as this will create an even higher penalty. There is still a penalty for guessing wrong with the same sign, since you will overcommit money that could have been used more usefully elsewhere.\n\ndef stock_loss(stock_return, pred, alpha = 100.):\n    if stock_return * pred &lt; 0:\n        #opposite signs, not good\n        return alpha*pred**2 - np.sign(stock_return)*pred \\\n                        + abs(stock_return) \n    else:\n        return abs(stock_return - pred)\n\n\ntrue_value = .05\npred = np.linspace(-.12, .12, 75)\n\nplt.plot(pred, [stock_loss(true_value, _p) for _p in pred], \\\n        label = \"Loss associated with\\n prediction if true value = 0.05\", lw =3) \nplt.vlines(0, 0, .25, linestyles=\"--\")\n\nplt.xlabel(\"prediction\")\nplt.ylabel(\"loss\")\nplt.xlim(-0.12, .12)\nplt.ylim(0, 0.25)\n\ntrue_value = -.02\nplt.plot(pred, [stock_loss(true_value, _p) for _p in pred], alpha = 0.6, \\\n        label = \"Loss associated with\\n prediction if true value = -0.02\", lw =3) \nplt.legend()\nplt.title(\"Stock returns loss if true value = 0.05, -0.02\");\n\n\n\n\n\n\n\n\nNotice how the loss changes after you cross the 0 line.\nLet us fit our returns model\n\nimport pymc3 as pm\n\nwith pm.Model() as model:\n    std = pm.Uniform(\"std\", 0, 100)\n    \n    beta = pm.Normal(\"beta\", mu=0, sd=100)\n    alpha = pm.Normal(\"alpha\", mu=0, sd=100)\n    \n    mean = pm.Deterministic(\"mean\", alpha + beta*X)\n    \n    obs = pm.Normal(\"obs\", mu=mean, sd=std, observed=Y)\n    \n    trace = pm.sample(100000, step=pm.Metropolis())\n    burned_trace = trace[20000:]  \n\n//anaconda/envs/py35/lib/python3.5/site-packages/pymc3/sampling.py:163: UserWarning: Instantiated step methods cannot be automatically initialized. init argument ignored.\n  warnings.warn('Instantiated step methods cannot be automatically initialized. init argument ignored.')\n100%|██████████| 100000/100000 [00:28&lt;00:00, 3488.01it/s]| 3/100000 [00:00&lt;55:42, 29.92it/s]\n\n\n\npm.plots.traceplot(trace=burned_trace, varnames=[\"std\", \"beta\", \"alpha\"]);\n\n\n\n\n\n\n\n\nWe seem to have converged. Now, the game is to find a point estimate y from the predictive samples at each point(possible_outcomes) that minimizes the stock loss, instead of the standard least squares line which is just the mean of the posterior predictive.\n\nfrom scipy.optimize import fmin\n\n\ndef stock_loss(price, pred, coef = 500):\n    \"\"\"vectorized for numpy\"\"\"\n    sol = np.zeros_like(price)\n    ix = price*pred &lt; 0 \n    sol[ix] = coef*pred**2 - np.sign(price[ix])*pred + abs(price[ix])\n    sol[~ix] = abs(price[~ix] - pred)\n    return sol\n\nstd_samples = burned_trace[\"std\"]\nalpha_samples = burned_trace[\"alpha\"]\nbeta_samples = burned_trace[\"beta\"]\n\nN = std_samples.shape[0]\n\nnoise = std_samples*np.random.randn(N) \n\npossible_outcomes = lambda signal: alpha_samples + beta_samples*signal + noise\n\n\nopt_predictions = np.zeros(50)\ntrading_signals =  np.linspace(X.min(), X.max(), 50)\nfor i, _signal in enumerate(trading_signals):\n        _possible_outcomes = possible_outcomes(_signal)\n        tomin = lambda pred: stock_loss(_possible_outcomes, pred).mean()\n        opt_predictions[i] = fmin(tomin, 0, disp = False)\n        \n        \nplt.xlabel(\"trading signal\")\nplt.ylabel(\"prediction\")\nplt.title(\"Least-squares prediction vs. Bayes action prediction\")\nplt.plot(X, ls_coef_*X + ls_intercept, label =\"Least-squares prediction\")\nplt.xlim(X.min(), X.max())\nplt.plot(trading_signals, opt_predictions, label =\"Bayes action prediction\")\nplt.legend(loc=\"upper left\");\n\n\n\n\n\n\n\n\nThe plot above takes the posterior-predictive distribution at each trading signal, applies the risk to it by calculating the integral as a mean over posterior-predictive samples. Then we minimize over the action \\(a\\) which here is a prediction at each trading signal. This is plotted as the green line above. Notice that when the signal is close to 0, our prediction is close to 0, we take no position as its very easy for the sign to be different. Far away from 0, we approach the suared risk more…\n\n\n\n\nThe logaraithmic utility is used for probabilistic prediction when the unknown state is a future observation \\(y^*\\). This is a subtle point, the squared error loss was used for a non-probabilistic point-prediction from the posterior predictive. But here we want to find our future observations themselves (and the entire distribution of them).\nThe utility is defined as:\n\\[u(a, y^*) = log a(y^*),\\]\nThe expected utility then is\n\\[\\bar{u}(a) = \\int dy^* \\, log(a(y^*))\\, p(y^* \\vert D, M).\\]\nThe \\(a\\) that maximizes this utility is the posterior-predictive itself!\n\\[\\hat{a}(y^*) = p(y^* \\vert D, M)\\]\nThe maximized utility then is:\n\\[\\bar{u}(a) = \\int dy^* \\, log(p(y^* \\vert D, M))\\, p(y^* \\vert D, M).\\]\nThis is just the negative entropy of the posterior predictive distribution, and the associated divergence is our old friend the KL-divergence.\nOur entire analysis here seems to be tautological, but is indeed at the base of model comparison. There we started from the KL-divergence to motivate the use of log scores that went into deriving the AIC, DIC, and WAIC. But decision theory generalizes this notion to the making of any point predictions or probabilistic predictions.\n\n\nWe have so far considered the notion above of predicting a single value from a future dataset. If you had such a dataset (like a test dataset) you can think of this as trying to predict the marginal predictive distribution.\nThe theoretical derivation is the same for the joint. One can consider the joint to be derived step by step from updated posterior predictives, as the new data points “come in”.\nIn practice we often use n-marginal distributions for the n future points with respect to the “old” dataset D. Clearly the product of n-marginals is not the joint. Butthese methods are commonly used as:\n\nwe might not have all the new data yet\nthe marginal predictives are easier to calculate\nsome utilities do not make a difference between the two.\n\n\n\n\n\nSo far we have considered the distribution over which we calculate the expectation of the risk to be the posterior predictive distribution of a given model \\(M\\). But if we want to compare models, for example, using the log score as a utility, it does not make sense to do the comparision with respect to one of the distributions being evaluated.\nIn such a case we consider a “true distribution” of the unseen \\(y\\), which we of-course do not know. But in a model comparison scenario, where we are interested in comparing other distributions, we can do so without knowing the true model. This is the essential idea behind taking the difference in the KL-divergences or equivalent divergences which allow us to create a relative scale on which quantities like the DIC and WAIC can be compared.\nThus we can define the generalization utility:\n\\[ \\bar{u}_t(\\hat{a}) = \\int dy^* u(\\hat{a}, y^*) p_t(y^*)\\]\nwhere \\(p_t(y^*)\\) is the true predictive distribution. Notice here that we have used \\(\\hat{a}\\) because we are already considering the action as optimal with respect to a models posterior predictive. In other words, for example, in model comparison, we are considering the actions \\(log(p1)\\) and \\(log(p2)\\) to compare to each other, but with respect to the true predictive distribution in computing the overall expected utility. We have seen this method used, in conjunction with marginal posterior-predictives for single points in the definition of the WAIC.\nSome researchers actually try and approximate the true distribution by a true belief distribution for comparison and other purposes. The idea behind this distribution is that we consider a rich enough model which we believe to capture our phenomenon well after doing posterior predictive checking from it. This might be a non-parametric model like a gaussian process which we shall see soon, or an ensemble model of the type we have seen earlier and which we shall describe in a little more detail below.\nThis is useful for calculating the difference in predictions between the distribution used and such a true belief distribution: this allows us to see how much worse we are doing. We shall not go further down the line on that, but see Vehtari and Ojanen if you are interested.\n\n\nInstead, let us briefly dwell on the idea of Bayesian Model averaging. We have seen this earlier, where in a very ad hoc fashion, we weighted models we were comparing by their WAIC weight, and averaged them together. These averaged models typically gave better predictions with more sensible posterior-predictive envelopes.\nA simple parametric model is often not enough to provide a rich enough model to serve as a true belief model, or to capture all aspects of our data to make good predictions. Thus we indulge in model averaging:\n\\[p_{BMA}(y^* \\vert x^*, D) = \\sum_k p(y^* \\vert x^*, D, M_k) p(M_k \\vert D)\\]\nwhere the averaging is with repect to weights \\(w_k = p(M_k \\vert D)\\), the posterior probabilities of the models \\(M_k\\), which is precisely what the Akaike weights purport to be.\nWe can use the true belief models derived thus at places where we want to use the “true distribution:.\n\n\n\nNote that you might have chosen expressive and best fit models, but if the true generating process is outside the hypothesis set of the models you are using, then you will never capture the true predictive distribution. This is called misfit or bias. Sometimes, your hypothesis set might be too expressive: this is called overfitting and the true generating process is simpler.\nThe former is a problem for finding the true belief distribution, and is especially a problem in mechanisms like cross-validation, which we will talk about soon, where holding out data means that we can only fit a less expressive model. The latter needs amelioration by regularization with stronger priors\n\n\n\n\nThe key idea in model comparison is that we will sort our average utilities in some order. The exact values are not important, and may be computed with respect to some true distribution or true-belief distribution \\(M_{tb}\\). Remember that the utility is computed (and maximized) with respect to some model \\(M_k\\) whereas the average of the utility is computed with respect to either the true, or true belief distribution.\n\\[\\bar{u}(M_k, \\hat{a}_k) = \\int dy^* u(\\hat{a}_k, y^*) p(y^* \\vert D, M_{tb})\\]\nwhere \\(a_k\\) is the optimal prediction under the model \\(M_k\\). Now we compare the actions, that is, we want:\n\\[\\hat{M} = \\arg\\max_k \\bar{u}(M_k, \\hat{a}_k)\\]\nThere is no-calibration of these actions. However, calculating the standard error of the difference can be used to see if the difference is significant, as we did with the WAIC score.\nFor the log score we first get the \\(M_k\\) optimal prediction by\n\\[\\bar{u}(M_k, a_k) = \\int dy^* log a_k(y^*) p(y^* \\vert D, M_{k})\\]\nAs we know, for this, \\(a_k = p((y^* \\vert D, M_{k})\\) which we then plug in to get:\n\\[\\bar{u}(M_k, a_k) = \\int dy^* p(y^* \\vert D, M_{k}) p(y^* \\vert D, M_{tb})\\]\nWe now maximize this over \\(M_k\\). This is equivalent to minimizing the KL-divergence as it is the negative KL divergence upto a \\(M_k\\) independent constant. This is the approach we used to develop model comparison information criteria.\nFor the squared loss the first step gives us \\(\\hat{a}_k = E_{p(y^* \\vert D,M_k)}[y^*]\\). We then plug this in to get the expected utility under the true belief model\n\\[\\bar{l}(\\hat{a_k}) = \\int dy^* \\, (\\hat{a}_k - y^*)^2 \\, p(y^* \\vert D, M_{tb}) = \\int dy^* \\, (E_{p_k}[y^*] - y^*)^2 \\, p(y^* \\vert D, M_{tb}) = Var_{p_{tb}}[y^*] + (E_{p_{tb}}[y^*] - E_{p_{k}}[y^*])^2\\]\nThus if we are model comparing for the squared error, we want the model whose expectation is closest to the true-belief model.\n\n\n\nNow consider the problem in which \\(\\omega\\), the unknown state of the world is some \\(\\theta\\) posterior parameter \\(\\in \\Theta\\). Then our utility function is of the form \\(u(a, \\theta)\\) and our belief about the unknown state of the world is captured by the posterior distribution \\(p(\\theta \\vert D, M)\\).\nThe optimal prediction can be found by calculating the expected utility over the posterior:\n\\[\\bar{u}(a) = \\int d\\theta u(a, \\theta) p(\\theta \\vert D, M)\\]\n\\[\\hat{a} = \\arg\\max_a \\bar{u}(a)\\]\nand then the optimal utility is\n\\[\\bar{u}(\\hat{a}) = \\int d\\theta u(\\hat{a}, \\theta) p(\\theta \\vert D, M)\\]\nIndeed, bayesian decision theory is often formulated with respect to the posterior rather than the posterior predictive, as especially with analytically derivable utilities, it is simple to use sampling to construct these expectations over the posterior.\nIf we identify the \\(\\theta\\) space utility as an average over the sampling distribution:\n\\[u(a, \\theta) = \\int u(a, y^*) p(y^* \\vert \\theta, M) dy^*\\]\nthe two approaches are equivalent and we have merely changed the order of integration.\nThis approach can be used to give us point estimates from the posterior such as means and medians."
  },
  {
    "objectID": "posts/utilityorrisk/index.html#decision-theory",
    "href": "posts/utilityorrisk/index.html#decision-theory",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "The basic idea behind decision theory is this: predictions (or actions based on predictions) are described by a utility or loss function, whose values can be computed given the observed data.\nIndeed one can consider prediction itself as actions to be undertaken with respect to a particular utility.\nRemember that there are two key distributions arising from the Bayesian scenario: the posterior \\(p(\\theta \\vert D)\\) and the posterior-predictive \\(p(y^* \\vert D) = \\int d\\theta\\, p(y^* \\vert \\theta) p(\\theta \\vert D)\\). Either of these can be used to make decisions: it depends upon what we have information about. If we have information about the true values of params, the posterior might be fine to use. But typically, we dont have this, and are more interested in predictiona from the model, so we formulate the problem in terms of the posterior predictive.\nEither way, the components of the decision problem are\n\n\\(a \\in A\\), available actions for the decision problem\n\\(\\omega \\in \\Omega\\), a state in the set of states of the world. If \\(\\Omega\\) is the set of all future \\(y\\), then \\(\\omega = y^*\\), a future \\(y\\). if \\(\\Omega\\) is the posterior, then \\(\\omega = \\theta\\) is a value of a parameter(s)\n\\(p(\\omega \\vert D)\\) which tells us out current beliefs about the world. This is either the posterior distribution (for \\(\\theta\\)) or the posterior predictive distribution (for \\(y^*\\))\nA utility function \\(u(a, \\omega): A \\times \\Omega \\rightarrow R\\) that awards a score/utility/profit to each action \\(a\\) when the state of the universe is \\(\\omega\\). This can be also formulated as a risk/loss.\n\nThe game then is to maximize the distribution-expected-utility amongst all possible actions a (or minimize the risk).\nIn other words we first define the distribution-averaged utility:\n\\[\\bar{u}(a) = \\int d\\omega \\, u(a, \\omega) \\, p(\\omega \\vert D)\\]\nWe then find the \\(a\\) that maximizes this utility:\n\\[ \\hat{a} = \\arg\\max_a \\bar{u}(a)\\]\nThis action is called the bayes action.\nThe resulting maximized expected utility is given by:\n\\[\\bar{u}(\\hat{a}, p) = \\bar{u}(\\hat{a}) = \\int d\\omega \\, u(\\hat{a}, \\omega) \\, p(\\omega \\vert D)\\]\nThis maximized utility is sometimes referred to as the entropy function, and an associate divergence can be defined:\n\\[ d(a,p) = \\bar{u}(p, p) - \\bar{u}(a, p)\\]\nThen one can think of minimizing \\(d(a,p)\\) with respect to \\(a\\) to get \\(\\hat{a}\\), so that this discrepancy can be thought of as a loss function.\n\n\nTo make this concrete consider the problem in which \\(\\omega\\) is a future observation \\(y^*\\). We will then get a posterior predictive distribution with respect to some model \\(M\\) that we shall put into our conditioned-upon variables as well (the reason to do this is that we’ll consider later, averaging with respect to sufficiently expressive or true distributions, rather than any particular posterior predictive).\nWith this in hand we can write:\n\\[\\bar{u}(a) = \\int dy^* \\, u(a, y^*) \\, p(y^* \\vert D, M)\\]\nand\n\\[\\bar{u}(\\hat{a}, p) = \\bar{u}(\\hat{a}) = \\int dy^* \\, u(\\hat{a}, y^*) \\, p(y^* \\vert D, M)\\]\nEverything we have talked above works for linear regression ansd other supervised learning: we must just carry an additional \\(x\\) along with us in the formulae:\n\\[\\bar{u}(a(x)) = \\int dy^* \\, u(a(x), y^*) \\, p(y^* \\vert x^*, D, M)\\]\n\\[ \\hat{a}(x) = \\arg\\max_a \\bar{u}(a(x))\\]"
  },
  {
    "objectID": "posts/utilityorrisk/index.html#point-prediction",
    "href": "posts/utilityorrisk/index.html#point-prediction",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "We have not specified above what the action \\(a\\) is. This is on purpose. Sometimes we want to make point predictions. In this case \\(a\\) is a single number. Sometimes we want to find a distribution. And sometimes we want to compare multiple models. We’ll see all of these below…\n\n\nThe squared error is an example of a risk defined to make a point estimate. Given a posterior predictive distribution, how do you communicate one number to your boss from it? How do you make a point-prediction. (You can think of it as a quadratic approximation to the general class of convex loss functions).\n\\[l(a, y^*) = (a - y^*)^2\\]\nThe optimal point prediction that minimizes the expected loss (negative expected utility):\n\\[\\bar{l}(a) = \\int dy^* \\, (a - y^*)^2 \\, p(y^* \\vert D, M),\\]\nis the posterior predictive mean:\n\\[\\hat{a} = E_p[y^*].\\]\nThis can be easily seen by differentiating with respect to \\(a\\).\nThe expected loss then becomes:\n\\[\\bar{l}(\\hat{a}) = \\int dy^* \\, (\\hat{a} - y^*)^2 \\, p(y^* \\vert D, M) = \\int dy^* \\, (E_p[y^*] - y^*)^2 \\, p(y^* \\vert D, M) = Var_p[y^*]\\]\nUsing such a loss thus indicates that you only care about the first two moments about the distribution, and that there is no gain to considering things like skewness and kurtosis.\n\n\n\nThis is a regression example.\nWe generate some data to have returns as a function of stock price or trading signals. We generate it as a line with noise, which is perhaps the most unlikely stock market signal…:-)\n\n## Code to create artificial data\nN = 100\nX = 0.025*np.random.randn(N)\nY = 0.5*X + 0.01*np.random.randn(N) \n\nls_coef_ = np.cov(X, Y)[0,1]/np.var(X)\nls_intercept = Y.mean() - ls_coef_*X.mean()\n\nplt.scatter(X, Y, c=\"k\")\nplt.xlabel(\"trading signal\")\nplt.ylabel(\"returns\")\nplt.title(\"Empirical returns vs trading signal\")\nplt.plot(X, ls_coef_*X + ls_intercept, label = \"Least-squares line\")\nplt.xlim(X.min(), X.max())\nplt.ylim(Y.min(), Y.max())\nplt.legend(loc=\"upper left\");\n\n\n\n\n\n\n\n\nA squared-error loss is agnostic to the signage and would penalize a prediction of -0.01 equally as bad a prediction of 0.03:\n\\[(0.01−(−0.01))^2=(0.01−0.03)^2=0.004\\]\nThus we want to define a loss that is sensitive to the difference between the prediction and the posterior-predictive sample.\nNotice that the loss is quadratic when the sign is different, as this will create an even higher penalty. There is still a penalty for guessing wrong with the same sign, since you will overcommit money that could have been used more usefully elsewhere.\n\ndef stock_loss(stock_return, pred, alpha = 100.):\n    if stock_return * pred &lt; 0:\n        #opposite signs, not good\n        return alpha*pred**2 - np.sign(stock_return)*pred \\\n                        + abs(stock_return) \n    else:\n        return abs(stock_return - pred)\n\n\ntrue_value = .05\npred = np.linspace(-.12, .12, 75)\n\nplt.plot(pred, [stock_loss(true_value, _p) for _p in pred], \\\n        label = \"Loss associated with\\n prediction if true value = 0.05\", lw =3) \nplt.vlines(0, 0, .25, linestyles=\"--\")\n\nplt.xlabel(\"prediction\")\nplt.ylabel(\"loss\")\nplt.xlim(-0.12, .12)\nplt.ylim(0, 0.25)\n\ntrue_value = -.02\nplt.plot(pred, [stock_loss(true_value, _p) for _p in pred], alpha = 0.6, \\\n        label = \"Loss associated with\\n prediction if true value = -0.02\", lw =3) \nplt.legend()\nplt.title(\"Stock returns loss if true value = 0.05, -0.02\");\n\n\n\n\n\n\n\n\nNotice how the loss changes after you cross the 0 line.\nLet us fit our returns model\n\nimport pymc3 as pm\n\nwith pm.Model() as model:\n    std = pm.Uniform(\"std\", 0, 100)\n    \n    beta = pm.Normal(\"beta\", mu=0, sd=100)\n    alpha = pm.Normal(\"alpha\", mu=0, sd=100)\n    \n    mean = pm.Deterministic(\"mean\", alpha + beta*X)\n    \n    obs = pm.Normal(\"obs\", mu=mean, sd=std, observed=Y)\n    \n    trace = pm.sample(100000, step=pm.Metropolis())\n    burned_trace = trace[20000:]  \n\n//anaconda/envs/py35/lib/python3.5/site-packages/pymc3/sampling.py:163: UserWarning: Instantiated step methods cannot be automatically initialized. init argument ignored.\n  warnings.warn('Instantiated step methods cannot be automatically initialized. init argument ignored.')\n100%|██████████| 100000/100000 [00:28&lt;00:00, 3488.01it/s]| 3/100000 [00:00&lt;55:42, 29.92it/s]\n\n\n\npm.plots.traceplot(trace=burned_trace, varnames=[\"std\", \"beta\", \"alpha\"]);\n\n\n\n\n\n\n\n\nWe seem to have converged. Now, the game is to find a point estimate y from the predictive samples at each point(possible_outcomes) that minimizes the stock loss, instead of the standard least squares line which is just the mean of the posterior predictive.\n\nfrom scipy.optimize import fmin\n\n\ndef stock_loss(price, pred, coef = 500):\n    \"\"\"vectorized for numpy\"\"\"\n    sol = np.zeros_like(price)\n    ix = price*pred &lt; 0 \n    sol[ix] = coef*pred**2 - np.sign(price[ix])*pred + abs(price[ix])\n    sol[~ix] = abs(price[~ix] - pred)\n    return sol\n\nstd_samples = burned_trace[\"std\"]\nalpha_samples = burned_trace[\"alpha\"]\nbeta_samples = burned_trace[\"beta\"]\n\nN = std_samples.shape[0]\n\nnoise = std_samples*np.random.randn(N) \n\npossible_outcomes = lambda signal: alpha_samples + beta_samples*signal + noise\n\n\nopt_predictions = np.zeros(50)\ntrading_signals =  np.linspace(X.min(), X.max(), 50)\nfor i, _signal in enumerate(trading_signals):\n        _possible_outcomes = possible_outcomes(_signal)\n        tomin = lambda pred: stock_loss(_possible_outcomes, pred).mean()\n        opt_predictions[i] = fmin(tomin, 0, disp = False)\n        \n        \nplt.xlabel(\"trading signal\")\nplt.ylabel(\"prediction\")\nplt.title(\"Least-squares prediction vs. Bayes action prediction\")\nplt.plot(X, ls_coef_*X + ls_intercept, label =\"Least-squares prediction\")\nplt.xlim(X.min(), X.max())\nplt.plot(trading_signals, opt_predictions, label =\"Bayes action prediction\")\nplt.legend(loc=\"upper left\");\n\n\n\n\n\n\n\n\nThe plot above takes the posterior-predictive distribution at each trading signal, applies the risk to it by calculating the integral as a mean over posterior-predictive samples. Then we minimize over the action \\(a\\) which here is a prediction at each trading signal. This is plotted as the green line above. Notice that when the signal is close to 0, our prediction is close to 0, we take no position as its very easy for the sign to be different. Far away from 0, we approach the suared risk more…"
  },
  {
    "objectID": "posts/utilityorrisk/index.html#the-logarithmic-utility-function-and-probabilistic-prediction",
    "href": "posts/utilityorrisk/index.html#the-logarithmic-utility-function-and-probabilistic-prediction",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "The logaraithmic utility is used for probabilistic prediction when the unknown state is a future observation \\(y^*\\). This is a subtle point, the squared error loss was used for a non-probabilistic point-prediction from the posterior predictive. But here we want to find our future observations themselves (and the entire distribution of them).\nThe utility is defined as:\n\\[u(a, y^*) = log a(y^*),\\]\nThe expected utility then is\n\\[\\bar{u}(a) = \\int dy^* \\, log(a(y^*))\\, p(y^* \\vert D, M).\\]\nThe \\(a\\) that maximizes this utility is the posterior-predictive itself!\n\\[\\hat{a}(y^*) = p(y^* \\vert D, M)\\]\nThe maximized utility then is:\n\\[\\bar{u}(a) = \\int dy^* \\, log(p(y^* \\vert D, M))\\, p(y^* \\vert D, M).\\]\nThis is just the negative entropy of the posterior predictive distribution, and the associated divergence is our old friend the KL-divergence.\nOur entire analysis here seems to be tautological, but is indeed at the base of model comparison. There we started from the KL-divergence to motivate the use of log scores that went into deriving the AIC, DIC, and WAIC. But decision theory generalizes this notion to the making of any point predictions or probabilistic predictions.\n\n\nWe have so far considered the notion above of predicting a single value from a future dataset. If you had such a dataset (like a test dataset) you can think of this as trying to predict the marginal predictive distribution.\nThe theoretical derivation is the same for the joint. One can consider the joint to be derived step by step from updated posterior predictives, as the new data points “come in”.\nIn practice we often use n-marginal distributions for the n future points with respect to the “old” dataset D. Clearly the product of n-marginals is not the joint. Butthese methods are commonly used as:\n\nwe might not have all the new data yet\nthe marginal predictives are easier to calculate\nsome utilities do not make a difference between the two."
  },
  {
    "objectID": "posts/utilityorrisk/index.html#predictions-with-respect-to-which-modeldistribution",
    "href": "posts/utilityorrisk/index.html#predictions-with-respect-to-which-modeldistribution",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "So far we have considered the distribution over which we calculate the expectation of the risk to be the posterior predictive distribution of a given model \\(M\\). But if we want to compare models, for example, using the log score as a utility, it does not make sense to do the comparision with respect to one of the distributions being evaluated.\nIn such a case we consider a “true distribution” of the unseen \\(y\\), which we of-course do not know. But in a model comparison scenario, where we are interested in comparing other distributions, we can do so without knowing the true model. This is the essential idea behind taking the difference in the KL-divergences or equivalent divergences which allow us to create a relative scale on which quantities like the DIC and WAIC can be compared.\nThus we can define the generalization utility:\n\\[ \\bar{u}_t(\\hat{a}) = \\int dy^* u(\\hat{a}, y^*) p_t(y^*)\\]\nwhere \\(p_t(y^*)\\) is the true predictive distribution. Notice here that we have used \\(\\hat{a}\\) because we are already considering the action as optimal with respect to a models posterior predictive. In other words, for example, in model comparison, we are considering the actions \\(log(p1)\\) and \\(log(p2)\\) to compare to each other, but with respect to the true predictive distribution in computing the overall expected utility. We have seen this method used, in conjunction with marginal posterior-predictives for single points in the definition of the WAIC.\nSome researchers actually try and approximate the true distribution by a true belief distribution for comparison and other purposes. The idea behind this distribution is that we consider a rich enough model which we believe to capture our phenomenon well after doing posterior predictive checking from it. This might be a non-parametric model like a gaussian process which we shall see soon, or an ensemble model of the type we have seen earlier and which we shall describe in a little more detail below.\nThis is useful for calculating the difference in predictions between the distribution used and such a true belief distribution: this allows us to see how much worse we are doing. We shall not go further down the line on that, but see Vehtari and Ojanen if you are interested.\n\n\nInstead, let us briefly dwell on the idea of Bayesian Model averaging. We have seen this earlier, where in a very ad hoc fashion, we weighted models we were comparing by their WAIC weight, and averaged them together. These averaged models typically gave better predictions with more sensible posterior-predictive envelopes.\nA simple parametric model is often not enough to provide a rich enough model to serve as a true belief model, or to capture all aspects of our data to make good predictions. Thus we indulge in model averaging:\n\\[p_{BMA}(y^* \\vert x^*, D) = \\sum_k p(y^* \\vert x^*, D, M_k) p(M_k \\vert D)\\]\nwhere the averaging is with repect to weights \\(w_k = p(M_k \\vert D)\\), the posterior probabilities of the models \\(M_k\\), which is precisely what the Akaike weights purport to be.\nWe can use the true belief models derived thus at places where we want to use the “true distribution:.\n\n\n\nNote that you might have chosen expressive and best fit models, but if the true generating process is outside the hypothesis set of the models you are using, then you will never capture the true predictive distribution. This is called misfit or bias. Sometimes, your hypothesis set might be too expressive: this is called overfitting and the true generating process is simpler.\nThe former is a problem for finding the true belief distribution, and is especially a problem in mechanisms like cross-validation, which we will talk about soon, where holding out data means that we can only fit a less expressive model. The latter needs amelioration by regularization with stronger priors"
  },
  {
    "objectID": "posts/utilityorrisk/index.html#model-comparison",
    "href": "posts/utilityorrisk/index.html#model-comparison",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "The key idea in model comparison is that we will sort our average utilities in some order. The exact values are not important, and may be computed with respect to some true distribution or true-belief distribution \\(M_{tb}\\). Remember that the utility is computed (and maximized) with respect to some model \\(M_k\\) whereas the average of the utility is computed with respect to either the true, or true belief distribution.\n\\[\\bar{u}(M_k, \\hat{a}_k) = \\int dy^* u(\\hat{a}_k, y^*) p(y^* \\vert D, M_{tb})\\]\nwhere \\(a_k\\) is the optimal prediction under the model \\(M_k\\). Now we compare the actions, that is, we want:\n\\[\\hat{M} = \\arg\\max_k \\bar{u}(M_k, \\hat{a}_k)\\]\nThere is no-calibration of these actions. However, calculating the standard error of the difference can be used to see if the difference is significant, as we did with the WAIC score.\nFor the log score we first get the \\(M_k\\) optimal prediction by\n\\[\\bar{u}(M_k, a_k) = \\int dy^* log a_k(y^*) p(y^* \\vert D, M_{k})\\]\nAs we know, for this, \\(a_k = p((y^* \\vert D, M_{k})\\) which we then plug in to get:\n\\[\\bar{u}(M_k, a_k) = \\int dy^* p(y^* \\vert D, M_{k}) p(y^* \\vert D, M_{tb})\\]\nWe now maximize this over \\(M_k\\). This is equivalent to minimizing the KL-divergence as it is the negative KL divergence upto a \\(M_k\\) independent constant. This is the approach we used to develop model comparison information criteria.\nFor the squared loss the first step gives us \\(\\hat{a}_k = E_{p(y^* \\vert D,M_k)}[y^*]\\). We then plug this in to get the expected utility under the true belief model\n\\[\\bar{l}(\\hat{a_k}) = \\int dy^* \\, (\\hat{a}_k - y^*)^2 \\, p(y^* \\vert D, M_{tb}) = \\int dy^* \\, (E_{p_k}[y^*] - y^*)^2 \\, p(y^* \\vert D, M_{tb}) = Var_{p_{tb}}[y^*] + (E_{p_{tb}}[y^*] - E_{p_{k}}[y^*])^2\\]\nThus if we are model comparing for the squared error, we want the model whose expectation is closest to the true-belief model."
  },
  {
    "objectID": "posts/utilityorrisk/index.html#risk-from-the-posterior-posterior-points",
    "href": "posts/utilityorrisk/index.html#risk-from-the-posterior-posterior-points",
    "title": "Utility, Risk, and Decision Theory",
    "section": "",
    "text": "Now consider the problem in which \\(\\omega\\), the unknown state of the world is some \\(\\theta\\) posterior parameter \\(\\in \\Theta\\). Then our utility function is of the form \\(u(a, \\theta)\\) and our belief about the unknown state of the world is captured by the posterior distribution \\(p(\\theta \\vert D, M)\\).\nThe optimal prediction can be found by calculating the expected utility over the posterior:\n\\[\\bar{u}(a) = \\int d\\theta u(a, \\theta) p(\\theta \\vert D, M)\\]\n\\[\\hat{a} = \\arg\\max_a \\bar{u}(a)\\]\nand then the optimal utility is\n\\[\\bar{u}(\\hat{a}) = \\int d\\theta u(\\hat{a}, \\theta) p(\\theta \\vert D, M)\\]\nIndeed, bayesian decision theory is often formulated with respect to the posterior rather than the posterior predictive, as especially with analytically derivable utilities, it is simple to use sampling to construct these expectations over the posterior.\nIf we identify the \\(\\theta\\) space utility as an average over the sampling distribution:\n\\[u(a, \\theta) = \\int u(a, y^*) p(y^* \\vert \\theta, M) dy^*\\]\nthe two approaches are equivalent and we have merely changed the order of integration.\nThis approach can be used to give us point estimates from the posterior such as means and medians."
  },
  {
    "objectID": "posts/globemodellab/index.html",
    "href": "posts/globemodellab/index.html",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")"
  },
  {
    "objectID": "posts/globemodellab/index.html#formulation-of-the-problem",
    "href": "posts/globemodellab/index.html#formulation-of-the-problem",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Formulation of the problem",
    "text": "Formulation of the problem\nThis problem, taken from McElreath’s book, involves a seal (or a well trained human) tossing a globe, catching it on the nose, and noting down if the globe came down on water or land.\nThe seal tells us that the first 9 samples were:\nWLWWWLWLW.\nWe wish to understand the evolution of belief in the fraction of water on earth as the seal tosses the globe.\nSuppose \\(\\theta\\) is the true fraction of water covering the globe. Our data story if that \\(\\theta\\) then is the probability of the nose landing on water, with each throw or toss of the globe being independent.\nNow we build a probabilistic model for the problem, which we shall use to guide a process of Bayesian updating of the model as data comes in.\n\\[\\cal{L} = p(n,k|\\theta) = Binom(n,k, \\theta)=\\frac{n!}{k! (n-k)! } \\, \\theta^k \\, (1-\\theta)^{(n-k)} \\]\nSince our seal hasnt really seen any water or land, (strange, I know), it assigns equal probabilities, ie uniform probability to any value of \\(\\theta\\).\nThis is our prior information\nFor reasons of conjugacy we choose as prior the beta distribution, with \\(Beta(1,1)\\) being the uniform prior."
  },
  {
    "objectID": "posts/globemodellab/index.html#how-to-do-the-bayesian-process",
    "href": "posts/globemodellab/index.html#how-to-do-the-bayesian-process",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "How to do the Bayesian Process",
    "text": "How to do the Bayesian Process\nBayes theorem and the things we will go through\n\nGrid approximation\nQuadratic (Laplace) Approximation\nConjugate Priors\nMCMC (later)\nModel Checking"
  },
  {
    "objectID": "posts/globemodellab/index.html#grid-approximation",
    "href": "posts/globemodellab/index.html#grid-approximation",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Grid Approximation",
    "text": "Grid Approximation\n\nfrom scipy.stats import binom\n\n\nprior_pdf = lambda p: 1\nlike_pdf = lambda p: binom.pmf(k=6, n=9, p=p)\npost_pdf = lambda p: like_pdf(p)*prior_pdf(p)\n\n\np_grid = np.linspace(0., 1., 20)\np_grid\n\narray([ 0.        ,  0.05263158,  0.10526316,  0.15789474,  0.21052632,\n        0.26315789,  0.31578947,  0.36842105,  0.42105263,  0.47368421,\n        0.52631579,  0.57894737,  0.63157895,  0.68421053,  0.73684211,\n        0.78947368,  0.84210526,  0.89473684,  0.94736842,  1.        ])\n\n\n\nplt.plot(p_grid, post_pdf(p_grid),'o-');\n\n\n\n\n\n\n\n\n\np_grid = np.linspace(0., 1., 1000)\npost_vals = post_pdf(p_grid)\npost_vals_normed = post_vals/np.sum(post_vals)\ngrid_post_samples = np.random.choice(p_grid, size=10000, replace=True, p=post_vals_normed)\n\n\nplt.plot(p_grid, post_vals)\n\n\n\n\n\n\n\n\n\nsns.distplot(grid_post_samples)"
  },
  {
    "objectID": "posts/globemodellab/index.html#laplace-approximation",
    "href": "posts/globemodellab/index.html#laplace-approximation",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\n\np_start = 0.5\nfrom scipy.optimize import minimize\npost_pdf_inv = lambda p: -post_pdf(p)\nres = minimize(post_pdf_inv, p_start, method = 'Nelder-Mead', options={'disp': True})\n\nOptimization terminated successfully.\n         Current function value: -0.273129\n         Iterations: 13\n         Function evaluations: 26\n\n\n\nres\n\n final_simplex: (array([[ 0.66669922],\n       [ 0.66660156]]), array([-0.27312909, -0.27312907]))\n           fun: -0.27312909031345828\n       message: 'Optimization terminated successfully.'\n          nfev: 26\n           nit: 13\n        status: 0\n       success: True\n             x: array([ 0.66669922])\n\n\n\npost_MAP = res.x[0]\npost_MAP\n\n0.66669921875000038\n\n\n\ninsertbefore = np.searchsorted(p_grid, post_MAP)\ninsertbefore\n\n667\n\n\n\npostmapval = (post_vals[insertbefore-1] + post_vals[insertbefore])/2.\npostmapval\n\n0.27312632244812729\n\n\n\nplt.plot(p_grid, post_vals);\nplt.plot(p_grid, norm.pdf(p_grid, loc=post_MAP, scale=0.16))\n\n\n\n\n\n\n\n\n\nzq = lambda sigma: sigma*postmapval*np.sqrt(2*np.pi)\ndef fit_loss(sigma):\n    vec = (post_vals/zq(sigma)) - norm.pdf(p_grid, loc=post_MAP, scale=sigma)\n    return np.dot(vec, vec)\n\n\nres2 = minimize(fit_loss, 0.2, method = 'Nelder-Mead', options={'disp': True})\n\nOptimization terminated successfully.\n         Current function value: 23.987144\n         Iterations: 12\n         Function evaluations: 24\n\n\n\nres2\n\n final_simplex: (array([[ 0.14921875],\n       [ 0.14917969]]), array([ 23.9871437 ,  23.98715773]))\n           fun: 23.987143699357638\n       message: 'Optimization terminated successfully.'\n          nfev: 24\n           nit: 12\n        status: 0\n       success: True\n             x: array([ 0.14921875])\n\n\n\npost_SIG = res2.x[0]\npost_SIG\n\n0.14921875000000009\n\n\n\nfrozen_laplace = norm(post_MAP, post_SIG)\n\n\nplt.plot(p_grid, post_pdf(p_grid)/zq(post_SIG), label = \"normalized posterior\");\nplt.plot(p_grid, frozen_laplace.pdf(p_grid), label = \"laplace approx\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nzq(post_SIG)\n\n0.10215906016979832\n\n\nNow we can get samples from here:\n\nsns.distplot(frozen_laplace.rvs(10000))"
  },
  {
    "objectID": "posts/globemodellab/index.html#conjugate-priors",
    "href": "posts/globemodellab/index.html#conjugate-priors",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Conjugate Priors",
    "text": "Conjugate Priors\nThe mean of \\(Beta(\\alpha, \\beta)\\) is \\(\\mu = \\frac{\\alpha}{\\alpha+\\beta}\\) while the variance is\n\\[V=\\mu (1- \\mu)/(\\alpha + \\beta + 1)\\]\n\nfrom scipy.stats import beta\nx=np.linspace(0., 1., 100)\nplt.plot(x, beta.pdf(x, 1, 1));\nplt.plot(x, beta.pdf(x, 1, 9));\nplt.plot(x, beta.pdf(x, 1.2, 9));\nplt.plot(x, beta.pdf(x, 2, 18));\n\n\n\n\n\n\n\n\nWe shall choose \\(\\alpha=1\\) and \\(\\beta=1\\) to be uniform.\n\\[ p(\\theta) = {\\rm Beta}(\\theta,\\alpha, \\beta) = \\frac{\\theta^{\\alpha-1} (1-x)^{\\beta-1} }{B(\\alpha, \\beta)} \\] where \\(B(\\alpha, \\beta)\\) is independent of \\(\\theta\\) and it is the normalization factor.\nFrom Bayes theorem, the posterior for \\(\\theta\\) is\n\\[ p(\\theta|D) \\propto  p(\\theta) \\, p(n,k|\\theta)  =  Binom(n,k, \\theta) \\,  {\\rm Beta}(\\theta,\\alpha, \\beta)  \\]\nwhich can be shown to be\n\\[{\\rm Beta}(\\theta, \\alpha+k, \\beta+n-k)\\]\n\nfrom scipy.stats import beta, binom\n\nplt.figure(figsize=( 15, 18))\n\nprior_params = np.array( [1.,1.] )  # FLAT \n\nx = np.linspace(0.00, 1, 125)\ndatastring = \"WLWWWLWLW\"\ndata=[]\nfor c in datastring:\n    data.append(1*(c=='W'))\ndata=np.array(data)\nprint(data)\nchoices=['Land','Water']\n\n\nfor i,v in enumerate(data):\n    plt.subplot(9,1,i+1)\n    prior_pdf = beta.pdf( x, *prior_params)\n    if v==1:\n        water = [1,0]\n    else:\n        water = [0,1]\n    posterior_params = prior_params + np.array( water )    # posteriors beta parameters\n    posterior_pdf = beta.pdf( x, *posterior_params)  # the posterior \n    prior_params = posterior_params\n    plt.plot( x,prior_pdf, label = r\"prior for this step\", lw =1, color =\"#348ABD\" )\n    plt.plot( x, posterior_pdf, label = \"posterior for this step\", lw= 3, color =\"#A60628\" )\n    plt.fill_between( x, 0, prior_pdf, color =\"#348ABD\", alpha = 0.15) \n    plt.fill_between( x, 0, posterior_pdf, color =\"#A60628\", alpha = 0.15) \n    \n    plt.legend(title = \"N=%d, %s\"%(i, choices[v]));\n    #plt.ylim( 0, 10)#\n\n[1 0 1 1 1 0 1 0 1]"
  },
  {
    "objectID": "posts/globemodellab/index.html#interrogating-the-posterior",
    "href": "posts/globemodellab/index.html#interrogating-the-posterior",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Interrogating the posterior",
    "text": "Interrogating the posterior\nSince we can sample from the posterior now after 9 observations, lets do so!\n\nsamples = beta.rvs(*posterior_params, size=10000)\nplt.hist(samples, bins=50, normed=True);\nsns.kdeplot(samples);\n\n\n\n\n\n\n\n\n\nSampling to summarize\nNow we can calculate all sorts of stuff.\nThe probability that the amount of water is less than 50%\n\nnp.mean(samples &lt; 0.5)\n\n0.17180000000000001\n\n\nThe probability by which we get 80% of the samples.\n\nnp.percentile(samples, 80)\n\n0.75998662608698764\n\n\nYou might try and find a credible interval. This, unlike the wierd definition of confidence intervals, is exactly what you think it is, the amount of probability mass between certain percentages, like the middle 95%\n\nnp.percentile(samples, [2.5, 97.5])\n\narray([ 0.35115415,  0.8774055 ])\n\n\nYou can make various point estimates: mean, median\n\nnp.mean(samples), np.median(samples), np.percentile(samples, 50) #last 2 are same\n\n(0.63736799839639757, 0.64714663472562717, 0.64714663472562717)\n\n\nA particularly important and useful point estimate that we just saw is the MAP, or “maximum a-posteriori” estimate, the value of the parameter at which the pdf (num-samples) reach a maximum. It can be obtained from the samples as well.\n\nsampleshisto = np.histogram(samples, bins=50)\n\n\nmaxcountindex = np.argmax(sampleshisto[0])\nmapvalue = sampleshisto[1][maxcountindex]\nprint(maxcountindex, mapvalue)\n\n33 0.694004782956\n\n\nThe mean of the posterior samples corresponds to minimizing the squared loss.\n\nmse = [np.mean((xi-samples)**2) for xi in x]\nplt.plot(x, mse);\nplt.axvline(np.mean(samples), 0, 1, color=\"r\")\nprint(\"Mean\",np.mean(samples));\n\nMean 0.635370253478"
  },
  {
    "objectID": "posts/globemodellab/index.html#sampling-to-simulate-prediction-the-posterior-predictive",
    "href": "posts/globemodellab/index.html#sampling-to-simulate-prediction-the-posterior-predictive",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Sampling to simulate prediction: the posterior predictive",
    "text": "Sampling to simulate prediction: the posterior predictive\nWhy would you want to simulate prediction?\n\nModel Checking\nSoftware Validation\nResearch Design\nForecasting\n\nIts easy to sample from any one probability to get the sampling distribution at a particular \\(\\theta\\)\n\npoint3samps = np.random.binomial( len(data), 0.3, size=10000);\npoint7samps = np.random.binomial( len(data), 0.7, size=10000);\nplt.hist(point3samps, lw=3, alpha=0.5, histtype=\"stepfilled\", bins=np.arange(11));\nplt.hist(point7samps, lw=3, alpha=0.3,histtype=\"stepfilled\", bins=np.arange(11));\n\n\n\n\n\n\n\n\nThe posterior predictive:\n\\[p(y^{*} \\vert D) = \\int d\\theta p(y^{*} \\vert \\theta) p(\\theta \\vert D)\\]\nseems to be a complex integral. But if you parse it, its not so complex. This diagram from McElreath helps:\n\n\n\nThe posterior predictive distribution as a mixture: each parameter value implies a sampling distribution, weighted by the posterior probability, producing the marginal prediction. From McElreath, Statistical Rethinking.\n\n\n\nPlug-in Approximation\nAlso, often, people will use the plug-in approximation by putting the posterior mean or MAP value\n\\[p(\\theta \\vert D) = \\delta(\\theta - \\theta_{MAP})\\]\nand then simply drawing the posterior predictive from :\n\\[p(y^{*} \\vert D) = p(y^{*} \\vert \\theta_{MAP})\\]\n(the same thing could be done for \\(\\theta_{mean}\\)).\n\npluginpreds = np.random.binomial( len(data), mapvalue, size = len(samples))\n\n\nplt.hist(pluginpreds, bins=np.arange(11));\n\n\n\n\n\n\n\n\nThis approximation is just sampling from the likelihood(sampling distribution), at a posterior-obtained value of \\(\\theta\\). It might be useful if the posterior is an expensive MCMC and the MAP is easier to find by optimization, and can be used in conjunction with quadratic (gaussian) approximations to the posterior, as we will see in variational inference. But for now we have all the samples, and it would be inane not to use them…\n\n\nThe posterior predictive from sampling\nBut really from the perspective of sampling, all we have to do is to first draw the thetas from the posterior, then draw y’s from the likelihood, and histogram the likelihood. This is the same logic as marginal posteriors, with the addition of the fact that we must draw y from the likelihood once we drew \\(\\theta\\). You might think that we have to draw multiple \\(y\\)s at a theta, but this is already taken care of for us because of the nature of sampling. We already have multiple \\(\\theta\\)s in a bin.\n\npostpred = np.random.binomial( len(data), samples);\n\n\npostpred\n\narray([7, 7, 5, ..., 6, 7, 5])\n\n\n\nsamples.shape, postpred.shape\n\n((10000,), (10000,))\n\n\n\nplt.hist(postpred, bins=np.arange(11), alpha=0.5, align=\"left\", label=\"predictive\")\nplt.hist(pluginpreds, bins=np.arange(11), alpha=0.2, align=\"left\", label=\"plug-in (MAP)\")\nplt.title('Posterior predictive')\nplt.xlabel('k')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nReplicative predictives\nThere is a different kind of predictive sampling that us useful (and what you might have thought was predictive sampling). This is replicative sampling. It can be used with both priors and posteriors; the former for model callibration and the latter for model checking. We shall see both of these soon.\nThe idea here is to generate an entire dataset from one of the parameter samples in the posterior. So you are not generating 10000 ys for 10000 thetas, but rather 10000 y’s per theta. (you can play the same game with the prior). This kind of inverts the diagram we saw earlier to produce the posterior predictive.\nOur usual sample vs replication 2D setup can come useful here. Consider generating 1000 y’s per replication for each theta.\n\npostpred.shape\n\n(10000,)\n\n\n\nreppostpred =np.empty((1000, 10000))\nfor i in range(1000):\n    reppostpred[i,:] = np.random.binomial( len(data), samples);\nreppostpred.shape\n\n(1000, 10000)\n\n\n\nper_theta_avgs = np.mean(reppostpred, axis=0)\nper_theta_avgs.shape\n\n(10000,)\n\n\n\nplt.scatter(samples, per_theta_avgs, alpha=0.1);\n\n\n\n\n\n\n\n\nIn particular, you will find that the number of switches is not consistent with what you see in our data. This might lead you to question our model…always a good thing..but note that we have very little data as yet to go on\n\ndata\n\narray([1, 0, 1, 1, 1, 0, 1, 0, 1])\n\n\n\ndata[:-1] != data[1:]\n\narray([ True,  True, False, False,  True,  True,  True,  True], dtype=bool)\n\n\n\nnp.sum(data[:-1] != data[1:])\n\n6"
  },
  {
    "objectID": "posts/globemodellab/index.html#exercise",
    "href": "posts/globemodellab/index.html#exercise",
    "title": "Lab: The Beta-Binomial Globe Model",
    "section": "Exercise",
    "text": "Exercise\nYou can interrogate the posterior-predictive, or simulated samples in other ways, asking about the longest run of water tosses, or the number of times the water/land switched. This is left as an exercise."
  },
  {
    "objectID": "posts/MLE/index.html",
    "href": "posts/MLE/index.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]\n\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]"
  },
  {
    "objectID": "posts/MLE/index.html#choosing-a-parametric-model",
    "href": "posts/MLE/index.html#choosing-a-parametric-model",
    "title": "Maximum Likelihood Estimation",
    "section": "Choosing a parametric model",
    "text": "Choosing a parametric model\nWhen we do data analysis in a parametric way, we start by characterizing our particular sample statistically then, using a probability distribution (or mass function). This distribution has some parameters. Lets refer to these as \\(\\theta\\).\nIf we assume that our data was generated by this distribution, then the notion of the true value of the parameter makes sense. Now, usually in life, there is no way of knowing if this was the true generating process, unless we have some physics or similar ideas behind the process. But lets stick with the myth that we can do this. Then let us call the true value of the parameters as \\(\\theta^*\\).\nTo know this true value, we’d typically need the entire large population, not the sample we have been given as data. So the best we can do us to make a parameter estimate \\(\\hat{\\theta}\\) from the data. In the context of frequentist statistics, the assumption is that the parameters are fixed, and that there is this true value (\\(\\theta^*\\)), and that we can make some estimate of this from our sample (\\(\\hat{\\theta}\\)).\nA distribution is induced on this estimate by considering many samples that could have been drawn from the population…remember that frequentist statistics fixes the parameters but considers data stochastic. This distribution is called the sampling distribution of the parameter \\(\\theta\\). (In general a sampling distribution can be considered for anything computed on the sample, such as a mean or variance or other moment).\nOur question is: how do we estimate \\(\\hat{\\theta}\\). And how do we compute this sampling distribution so that we can get a notion of the uncertainty that estimating from a sample rather than the population leaves us with?\nThe first question is tackled by the Maximum Likelihood estimate, or MLE. The second one is tackled by techniques like the bootstrap.\nLets learn about the MLE in the context of a particular distribution, the exponential.\n\nThe idea behind the MLE\nThe diagram below illustrates the idea behind the MLE.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nConsider two distributions in the same family, one with a parameter, lets call it \\(\\theta\\), of value 1.8 (blue) and another of value 5.8. (green). Let’s say we have 3 data points, at \\(x=1,2,3\\).\nMaximum likelihood starts by asking the question: conditional on the fixed value of \\(\\theta\\), which distribution is the data more likely to have come from?\nIn our case the blue is more likely since the product of the height of the 3 vertical blue bars is higher than that of the 3 green bars.\nIndeed the question that MLE asks is: how can we move and scale the distribution, that is, change \\(\\theta\\), until the product of the 3 bars is maximised!\nThat is, the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i \\mid \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nOften it is easier and numerically more stable to maximise the log likelyhood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nThe exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\nIt takes the form: \\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n0 & x &lt; 0.\n\\end{cases}\n\\]\nIn the case of the exponential distribution we have:\n\\[\n\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n\\]\nMaximizing this:\n\\[\n\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n\\]\nand thus:\n\\[\n\\frac{1}{\\est{\\lambda_{MLE}}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nwhich is the sample mean of our sample. Usually one is not so lucky and one must use numerical optimization techniques.\nA crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large."
  },
  {
    "objectID": "posts/MLE/index.html#inference",
    "href": "posts/MLE/index.html#inference",
    "title": "Maximum Likelihood Estimation",
    "section": "Inference",
    "text": "Inference\nJust having an estimate is no good. We will want to put confidence intervals on the estimation of the parameters. This presents a conundrum: we have access to only one sample, but want to compute a error estimate over multiple samples, using an estimator such as the standard deviation.\nAt this point we are wishing for the Lord to have given us other samples drawn from the population. But alas, no such luck…\nSo how then are we to find the sampling distribution of our parameters?\nIn the last two decades, resampling the ONE dataset we have has become computationally feasible. Resampling involves making new samples from the observations, each of which is analysed in the same way as out original dataset. One way to do this is the Bootstrap."
  },
  {
    "objectID": "posts/MLE/index.html#linear-regression-mle",
    "href": "posts/MLE/index.html#linear-regression-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Linear Regression MLE",
    "text": "Linear Regression MLE\nLinear regression is the workhorse algorithm thats used in many sciences, social and natural. The diagram below illustrates the probabilistic interpretation of linear regression, and the idea behind the MLE for linear regression. We illustrate a point \\((x_i, y_i)\\), and the corresponding prediction for \\(x_i\\) using the line, that is \\(yhat_i\\) or \\(\\hat{y}_i\\).\n\n\n\nProbabilistic interpretation of linear regression\n\n\nThe fundamental assumption for the probabilistic analysis of linear regression is that each \\(y_i\\) is gaussian distributed with mean \\(\\v{w}\\cdot\\v{x_i}\\) (the y predicted by the regression line so to speak) and variance \\(\\sigma^2\\):\n\\[ y_i \\sim N(\\v{w}\\cdot\\v{x_i}, \\sigma^2) .\\]\nWe can then write the likelihood:\n\\[\\cal{L} = p(\\v{y} | \\v{x}, \\v{w}, \\sigma) = \\prod_i p(\\v{y}_i | \\v{x}_i, \\v{w}, \\sigma)\\]\nGiven the canonical form of the gaussian:\n\\[N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},\\]\nwe can show that:\n\\[\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .\\]\nThe log likelihood \\(\\ell\\) then is given by:\n\\[\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .\\]\nIf you differentiate this with respect to \\(\\v{w}\\) and \\(\\sigma\\), you get the MLE values of the parameter estimates:\n\\[\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, \\]\nwhere \\(\\v{X}\\) is the design matrix created by stacking rows \\(\\v{x}_i\\), and\n\\[\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . \\]\nThese are the standard results of linear regression."
  },
  {
    "objectID": "posts/MLE/index.html#logistic-regression-mle",
    "href": "posts/MLE/index.html#logistic-regression-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "Logistic Regression MLE",
    "text": "Logistic Regression MLE\nLogistic regression if one of the well known supervized learning algorithms used for classification.\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the ‘1’ samples from the ‘0’ samples, just like in the diagram above. In other words, we wish to find the “regression” line which divides the samples. Now, a line has the form \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) in 2-dimensions. On one side of this line we have\n\\[w_1 x_1 + w_2 x_2 + w_0 \\ge 0,\\]\nand on the other side we have\n\\[w_1 x_1 + w_2 x_2 + w_0 &lt; 0.\\]\nOur classification rule then becomes:\n\\[\n\\begin{eqnarray}\ny = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &if& \\v{w}\\cdot\\v{x} &lt; 0\n\\end{eqnarray}\n\\]\nwhere \\(\\v{x}\\) is the vector \\(\\{1,x_1, x_2,...,x_n\\}\\) where we have also generalized to more than 2 features.\nWhat hypotheses \\(h\\) can we use to achieve this? One way to do so is to use the sigmoid function:\n\\[h(z) = \\frac{1}{1 + e^{-z}}.\\]\nNotice that at \\(z=0\\) this function has the value 0.5. If \\(z &gt; 0\\), \\(h &gt; 0.5\\) and as \\(z \\to \\infty\\), \\(h \\to 1\\). If \\(z &lt; 0\\), \\(h &lt; 0.5\\) and as \\(z \\to -\\infty\\), \\(h \\to 0\\). As long as we identify any value of \\(y &gt; 0.5\\) as 1, and any \\(y &lt; 0.5\\) as 0, we can achieve what we wished above.\nThis function is plotted below:\n\nh = lambda z: 1./(1+np.exp(-z))\nzs=np.arange(-5,5,0.1)\nplt.plot(zs, h(zs), alpha=0.5);\n\n\n\n\n\n\n\n\nSo we then come up with our rule by identifying:\n\\[z = \\v{w}\\cdot\\v{x}.\\]\nThen \\(h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\ge 0\\) and \\(h(\\v{w}\\cdot\\v{x}) \\lt 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\lt 0\\), and:\n\\[\n\\begin{eqnarray}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray}\n\\]\nWe said above that if \\(h &gt; 0.5\\) we ought to identify the sample with \\(y=1\\)? One way of thinking about this is to identify \\(h(\\v{w}\\cdot\\v{x})\\) with the probability that the sample is a ‘1’ (\\(y=1\\)). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a ‘1’ is \\(\\ge 0.5\\).\nSo suppose we say then that the probability of \\(y=1\\) for a given \\(\\v{x}\\) is given by \\(h(\\v{w}\\cdot\\v{x})\\)?\nThen, the conditional probabilities of \\(y=1\\) or \\(y=0\\) given a particular sample’s features \\(\\v{x}\\) are:\n\\[\\begin{eqnarray}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray}\\]\nThese two can be written together as\n\\[P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} \\]\nThen multiplying over the samples we get the probability of the training \\(y\\) given \\(\\v{w}\\) and the \\(\\v{x}\\):\n\\[P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\]\nWhy use probabilities? Earlier, we talked about how the regression function \\(f(x)\\) never gives us the \\(y\\) exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently.\nWe said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(P(x,y)\\). Thus we need to model \\(y\\) at a given \\(x\\), written as \\(P(y \\mid x)\\), and since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) ,\\]\nand can obtain our joint probability (\\(P(x, y))\\).\nIndeed its important to realize that a particular sample can be thought of as a draw from some “true” probability distribution. If for example the probability of classifying a sample point as a ‘0’ was 0.1, and it turns out that the sample point was actually a ‘0’, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a ‘0’! But, of-course its more unlikely than its likely, and having good probabilities means that we’ll be likely right most of the time, which is what we want to achieve in classification.\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a ‘1’.\nNow if we maximize \\[P(y \\mid \\v{x},\\v{w})\\], we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This maximum likelihood estimation maximises the likelihood of the sample y,\n\\[\\like = P(y \\mid \\v{x},\\v{w}).\\]\nAgain, we can equivalently maximize\n\\[\\loglike = log(P(y \\mid \\v{x},\\v{w}))\\]\nsince the natural logarithm \\(log\\) is a monotonic function. This is known as maximizing the log-likelihood.\n\\[\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).\\]\nThus\n\\[\\begin{eqnarray}\n\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "posts/generativemodels/index.html",
    "href": "posts/generativemodels/index.html",
    "title": "Generative vs Discriminative Models",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n\\[\n\\renewcommand{\\like}{{\\cal L}}\n\\renewcommand{\\loglike}{{\\ell}}\n\\renewcommand{\\err}{{\\cal E}}\n\\renewcommand{\\dat}{{\\cal D}}\n\\renewcommand{\\hyp}{{\\cal H}}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{{\\mathbf x}}\n\\renewcommand{\\v}[1]{{\\mathbf #1}}\n\\]\nFirst some often used routines.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\ndef cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n    gs.fit(Xtrain, ytrain)\n    print(\"BEST PARAMS\", gs.best_params_)\n    best = gs.best_estimator_\n    return best\ndef do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8):\n    subdf=indf[featurenames]\n    if standardize:\n        subdfstd=(subdf - subdf.mean())/subdf.std()\n    else:\n        subdfstd=subdf\n    X=subdfstd.values\n    y=(indf[targetname].values==target1val)*1\n    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n    clf=clf.fit(Xtrain, ytrain)\n    training_accuracy = clf.score(Xtrain, ytrain)\n    test_accuracy = clf.score(Xtest, ytest)\n    print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n    print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))\n    return clf, Xtrain, ytrain, Xtest, ytest\n\n\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\ncm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\ndef points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n    h = .02\n    X=np.concatenate((Xtr, Xte))\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n\n    #plt.figure(figsize=(10,6))\n    if zfunc:\n        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n        Z=zfunc(p0, p1)\n    else:\n        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    ZZ = Z.reshape(xx.shape)\n    if mesh:\n        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n    if predicted:\n        showtr = clf.predict(Xtr)\n        showte = clf.predict(Xte)\n    else:\n        showtr = ytr\n        showte = yte\n    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, s=psize, alpha=alpha,edgecolor=\"k\")\n    # and testing points\n    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, alpha=alpha, marker=\"s\", s=psize+10)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    return ax,xx,yy\n\ndef points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, colorscale=colorscale, cdiscrete=cdiscrete, psize=psize, alpha=alpha, predicted=True) \n    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14, axes=ax)\n    return ax \n\n\n\n\n\n\nTaxonomy of learning approaches: ERM/discriminant methods versus Bayesian discriminative and generative models.\n\n\n\n\nThe Empirical Risk Maximization (ERM) approach corresponds to estimating the true distribution by the empirical distribution. In this case the Risk R is simply the average of the losses at the individual training points:\n\\[ R(g) = \\frac{1}{N} \\sum_i l(g(x_i), y_i) .\\]\n(Diagrams like the one below are chopped out from http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/240415.pdf . Reading Chapter 13 as a survey of machine learning is especially recommended)\n\n\n\nEmpirical risk minimization: the classifier is learned by minimizing penalized empirical risk over the data distribution. From Bishop.\n\n\nThe optimal decision in the training set is obviously the value of \\(y\\) at the training point, but we are left with undefined action \\(g\\) outside the training set. We thus pick some parametric model \\(g(x;\\theta)\\). Now we can minimize the empirical risk with respect to \\(\\theta\\) to get \\(\\theta_{opt}\\) and use this to make predictions/actions using \\(g\\). Because such an approach can lead to overfitting as we have seen before, we typically add a regularization term with co-efficient \\(\\lambda\\) whose value is found by validation.\nNotice that in all of this any talk of density estimation has gone away, and we are just minimizine the averaged loss over the training set plus regularization, the so-called Structural Risk maximization approach of Vapnik, whose motto is (paraphrased): Never solve a more difficult problem (density estimation) while solving a difficut one (learning). The function \\(g\\) is then sometimes called a discriminant function, and we are choosing it based on minimal risk, which is the quantity we are ultimately interested in.\n\nBut there are drawbacks. It seems crazy to assume that the empirical distribution is a good distribution, especially for small data. A more reasonable assumption for the distribution could take into account likely x,y that could arise. If the loss changes, as it might over time, say in a financial application, then we would need to retrain \\(g\\). There is no way to associate a confidence in this framework, as it wont give you probabilities.\n\n\n\n\n\n\nBayesian decision approach: after fitting the model, predictions minimize expected risk under the posterior. From Bishop.\n\n\nThe alternative is to first do density estimation. We estimate \\(p(x,y)\\) (or \\(p(x,c)\\)) from the training data. (Note that this can be thought of as ERM on risk \\(-log(p)\\)). (In the “Learning Models” lab we said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(p(x,y)\\) rather than some well given function\\(y=f(x)\\).)\nThe joint distribution can be constructed in two ways: generative or discriminative. The discriminative approach gives us:\n\\[p(x,c) = p(c|x)p(x)\\]\nwhereas the generative approach gives us\n\\[p(x,c) = p(x|c) p(c)\\]\nand then bayes theorem can be used to obtain p(c|x).\nThe generative approach corresponds to picking one of the classes with probability p(c) and then getting the density of the features for that class. The discriminative approach models the domain boundary instead. While the data may be distributed in a complex way, the boundary may be easier to model. On the other hand prior information for assymetric situations, conditional independence and other such strategies can only be done in generative models.\n\n\n\nGenerative vs discriminative classification: (a) generative models learn class-conditional densities, (b) discriminative models learn the decision boundary directly. From Bishop.\n\n\nIn either case we can get the joint distribution. In the discriminative case that leads us to density estimation for \\(p(x)\\). Often we have no use for it so we wont do it, as in logistic regression. But do remember that if we want our classifier to have good results we should be using it on test sets which reflect the proper sampling \\(p(x)\\). And if we dont characterize it we might be better of using a generative model as it is easier to adjust for class priors.\nThe Bayesian decision approach is a clean one, in which first one models the environment, independent of the subsequent decision process. If \\(p(y,x|\\theta)\\) is the “true” model of the world, this is optimal. But if this environment model is poor, the action \\(g\\) could be higly inaccurate since the environment is divorced from prediction. In practice one often includes regularization terms in the environment model to reduce the complexity of the distribution and bring it more in line with decision based hyperparameters, set by validation on an empirical loss. See the cs109 (2013) Naives bayes homework for a good example.\nBy the way, the ERM method is the only frequentist method which has a well defined risk. The reason for this is that it dosent depend on both a sample-estimate of the true \\(\\theta\\).\n\ndf=pd.read_csv(\"data/01_heights_weights_genders.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nGender\nHeight\nWeight\n\n\n\n\n0\nMale\n73.847017\n241.893563\n\n\n1\nMale\n68.781904\n162.310473\n\n\n2\nMale\n74.110105\n212.740856\n\n\n3\nMale\n71.730978\n220.042470\n\n\n4\nMale\n69.881796\n206.349801\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nclf_l, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(LogisticRegression(), {\"C\": [0.01, 0.1, 1, 10, 100]}, df, ['Weight', 'Height'], 'Gender','Male')\n\nBEST PARAMS {'C': 0.01}\nAccuracy on training data: 0.92\nAccuracy on test data:     0.91\n\n\n\nplt.figure()\nax=plt.gca()\npoints_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);\n\n\n\n\n\n\n\n\nLet us plot the probabilities obtained from predict_proba, overlayed on the samples with their true labels:\n\nplt.figure()\nax=plt.gca()\npoints_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);\n\n\n\n\n\n\n\n\nNotice that lines of equal probability, as might be expected are stright lines. What the classifier does is very intuitive: if the probability is greater than 0.5, it classifies the sample as type ‘1’ (male), otherwise it classifies the sample to be class ‘0’. Thus in the diagram above, where we have plotted predicted values rather than actual labels of samples, there is a clear demarcation at the 0.5 probability line.\nThis notion of trying to obtain the line or boundary of demarcation is what makes the discriminative classifier. The algorithm tries to find a decision boundary that separates the males from the females. To classify a new sample as male or female, it checks on which side of the decision boundary the sample falls, and makes a prediction. In other words we are asking, given \\(\\v{x}\\), what is the probability of a given \\(y\\), or, what is the likelihood \\(P(y|\\v{x},\\v{w})\\)?\n\n\n\nThis involves finding \\(P(\\v{x} | y)\\), the class conditional probability. Consider that heights and weights of males and females might be expected to be distributed using a bell curve. You might have heard of the reasons for this: so many things go into these heights/weights that the net effect is for them to be distributed as a bell curve, according to the central limit theorem. So why not use this additional information and model the heights and weights of males and females separately as 2-dimensional bell curves or Normal Distributions. In other words:\n\\[p(height, weight | male ) = Bell Curve\\]\ncentered at the mean height, weight for males, and a similar equation holds for females. This is exactly what the linear discriminant analysis classifier does. Lets run it, only on tne training set. What we are doing is fitting the male and female sections of the training set separately, getting two 2-D bell curves, and then inverting as above to decide how to classify new samples from the testing set. (We dont cross-validate here as we are currently not fitting any hyperparameters).\n\nfrom sklearn.lda import LDA\nclflda = LDA(solver=\"svd\", store_covariance=True)\nclflda.fit(Xtrain_l, ytrain_l)\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n\n\nLDA(n_components=None, priors=None, shrinkage=None, solver='svd',\n  store_covariance=True, tol=0.0001)\n\n\n\n#from REF\nfrom scipy import linalg\n\ndef plot_ellipse(splot, mean, cov, color):\n    v, w = linalg.eigh(cov)\n    u = w[0] / linalg.norm(w[0])\n    angle = np.arctan(u[1] / u[0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    # filled Gaussian at 2 standard deviation\n    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n                              180 + angle, color=color, lw=3, fill=False)\n    ell.set_clip_box(splot.bbox)\n    ell1 = mpl.patches.Ellipse(mean, 1 * v[0] ** 0.5, 1 * v[1] ** 0.5,\n                              180 + angle, color=color, lw=3, fill=False)\n    ell1.set_clip_box(splot.bbox)\n    ell3 = mpl.patches.Ellipse(mean, 3 * v[0] ** 0.5, 3 * v[1] ** 0.5,\n                              180 + angle, color=color, lw=3, fill=False)\n    ell3.set_clip_box(splot.bbox)\n    #ell.set_alpha(0.2)\n    splot.add_artist(ell)\n    splot.add_artist(ell1)\n    splot.add_artist(ell3)\n\n\n    #splot.set_xticks(())\n    #splot.set_yticks(())\ndef plot_lda_cov(lda, splot):\n    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n#plt.bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0, mux=0.0, muy=0.0, sigmaxy=0.0)¶\n\nWe plot ellipses for equal probability contours of the two individual \\(P(\\v{x}|y)\\), the \\(P(height, weight | male)\\) and the \\(P(height, weight | female)\\). We also plot the discriminant line created by inverting these probabilities using Bayes theorem: we once again classify a sample as male if \\(P(male | height, weight \\gt 0.5\\) (which will ensure that \\(P(female | height, weight) &lt; 0.5\\) since both must add to 1).\n\nplt.figure()\nax=plt.gca()\nspl,_,_=points_plot(ax,Xtrain_l, Xtest_l, ytrain_l, ytest_l, clflda)\nplot_lda_cov(clflda, spl)\n\n\n\n\n\n\n\n\nWhats happenning here? We have estimated \\(P(\\v{x} | y, \\theta_y)\\) where we use \\(\\theta_y\\) to designate the parameters of the fit (the parameters of the female and male bell curves); the subscript indicates we are fitting a separate parameter set for each class.\nWe can then use Bayes theorem to invert to find \\(P(y | \\v{x})\\), the probability we really want thus:\n\\[P(y | \\v{x}) = \\frac{P(\\v{x} | y, \\theta_{y}) P(y)}{P(x)} = \\frac{P(\\v{x} | y) P(y)}{\\sum_{y} P(\\v{x} | y, \\theta_{y})P(y)} .\\]\nNotice from the first term above that the denominator \\(P(\\v{x})\\) does not involve the parameters, it is simply the input distribution of the features. As such, it is not needed in the prediction process.\nFor our special case of two classes:\n\\[P(y = c_1 | \\v{x}) = \\frac{P(\\v{x} | y = c_1, \\theta_1) P(y=c_1)}{P(\\v{x} | y = c_1, \\theta_1) P(y=c_1) + P(\\v{x} | y = c_0, \\theta_0) P(y=c_0)}\\]\nIn other words, we first look at males, and build a model of the features for the males. Then we do a similar thing for females. To classify a new sample, we match it against the model for males, and then the model for females, and user Bayes theorem to see if the new sample more likely looks like the males rather than the females from the training set.\nAnother way to think about this is that you can generate or simulate a new male or female sample from this model \\(P(\\v{x} | y, \\theta_y)\\) of males or females respectively. The idea is that you first toss a (possibly biased) coin which depends on \\(P(y=c_1)\\), the “prior” probability of a sample being male or female. Once that coin has landed male or female (“once the draw has been made”), generate a sample in feature space using \\(P(\\v{x}|what landed)\\). For example, if the coin landed female, now draw a height and weight according to \\[P(height, wieght | female )\\]. Thus there is a story for how to generate samples in such models. If this story is the “correct” or “nearly correct” one, this is likely to be a very good model, and furthermore, we can draw new training sets to check variance and other measures of our classifier’s accuracy.\nYou can see three important aspects of this method from these formulae already. First, you model the classes separately. Secondly, the formula involves \\(P(y)\\), the prior probability of the sample being in the class. This is usually just taken to be the fraction of a particular class in the training sample.\nBeing able to fold this information is key when the classes are very imbalanced in the training data set. For example, consider again the case of trying to predict if a customer will “churn”. Typically churn rates are low, of the order of 2-3%.In this case having the prior probabilities of churning and not-churning, and modelling thse groups separately in a generative classifier is very useful: it might be hard for a discriminative classifier to draw a boundary between many many samples on one side and just a few on the other: one might rightly expect this boundary to be a bit fragile.\nThe third thing is probably obvious and a bit uncomfortable to you: we are being asked to provide much more information: the priors, the individual class models, all of that. It would seem that to get the same kind of classification we are being asked to provide more information.\nThis is true, but it is not a bad thing when the assumptions that go into individual models are well founded, such as in the case of our male and female heights and weights here.\nWe can also plot the output of predict_proba and we see something interesting: we get back exactly the same probability lines that we got using logistic regression. Indeed, LDA is the generative conterpart of Logistic regression. It is possible to prove this but we shall not do so here.\n\nplt.figure()\nax=plt.gca()\npoints_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clflda);\n\n\n\n\n\n\n\n\nThe important point here is that many generative models, including those with Poisson Likelihoods and Naive Bayes Models have Logistic Regression as their discriminative counterpart. This means that on inverting the \\(P(\\v{x}|y)\\) using Bayes theorem, we get back Logistic Regression. Thus Logistic regression if a relatively robust model, insensitive to many modelling assumptions. This is a big reason to use such Discriminative models.\nBut if \\(p(\\v{x}|y)\\) is indeed Normal, as is here for our two classes of male and female, LDA is what is known as asymptotically efficient. This means that for large amounts of training data, it can be proved that no model can be better than LDA for the estimation of the probability we’d like to use to get our classification, \\(p(y|\\v{x})\\). In particular, it can be shown that LDA will outperform Logistic regression, even for small training set sizes.\nFinally note that there is another major advantage of a generative classifier. We can sum up over class conditional probably density weighted by the priors to estimate the input density from the probability marginalization formula.\n\\[P(x) = \\sum_y P(x|y)P(y)\\]\nThis is not surprising. A lot of elbow-grease went into the generative classifier since we had to model so much. It should pay some dividend.\n\n\n\nThis is perhaps the right time then to revisit the generative vs discriminative argument, now that we have some experience under our belt.\n\nIf we only wish to make classification decisions rather than more complex decisions then discriminative might be all we need, and it is cheaper in terms of computing resources. But asymmetry and the ability to generate synthetic data are strengths of the generative approach.\nsometimes generative models like LDA and Naive Bayes are easy to fit. The discriminative model LogisticRegression requires convex optimization via Gradient descent\nwe can add new classes to a generative classifier without retraining for the previous classes so it might be better for online customer selection problems where customer profiles change\ngenerative classifiers can handle missing data easily\ngenerative classifiers are better at handling unlabelled training data (semi-supervized learning)\npreprocessing data is easier with discriminative classifiers\ndiscriminative classifiers give generally better callibrated probabilities"
  },
  {
    "objectID": "posts/generativemodels/index.html#the-different-kinds-of-learning",
    "href": "posts/generativemodels/index.html#the-different-kinds-of-learning",
    "title": "Generative vs Discriminative Models",
    "section": "",
    "text": "Taxonomy of learning approaches: ERM/discriminant methods versus Bayesian discriminative and generative models.\n\n\n\n\nThe Empirical Risk Maximization (ERM) approach corresponds to estimating the true distribution by the empirical distribution. In this case the Risk R is simply the average of the losses at the individual training points:\n\\[ R(g) = \\frac{1}{N} \\sum_i l(g(x_i), y_i) .\\]\n(Diagrams like the one below are chopped out from http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/240415.pdf . Reading Chapter 13 as a survey of machine learning is especially recommended)\n\n\n\nEmpirical risk minimization: the classifier is learned by minimizing penalized empirical risk over the data distribution. From Bishop.\n\n\nThe optimal decision in the training set is obviously the value of \\(y\\) at the training point, but we are left with undefined action \\(g\\) outside the training set. We thus pick some parametric model \\(g(x;\\theta)\\). Now we can minimize the empirical risk with respect to \\(\\theta\\) to get \\(\\theta_{opt}\\) and use this to make predictions/actions using \\(g\\). Because such an approach can lead to overfitting as we have seen before, we typically add a regularization term with co-efficient \\(\\lambda\\) whose value is found by validation.\nNotice that in all of this any talk of density estimation has gone away, and we are just minimizine the averaged loss over the training set plus regularization, the so-called Structural Risk maximization approach of Vapnik, whose motto is (paraphrased): Never solve a more difficult problem (density estimation) while solving a difficut one (learning). The function \\(g\\) is then sometimes called a discriminant function, and we are choosing it based on minimal risk, which is the quantity we are ultimately interested in.\n\nBut there are drawbacks. It seems crazy to assume that the empirical distribution is a good distribution, especially for small data. A more reasonable assumption for the distribution could take into account likely x,y that could arise. If the loss changes, as it might over time, say in a financial application, then we would need to retrain \\(g\\). There is no way to associate a confidence in this framework, as it wont give you probabilities.\n\n\n\n\n\n\nBayesian decision approach: after fitting the model, predictions minimize expected risk under the posterior. From Bishop.\n\n\nThe alternative is to first do density estimation. We estimate \\(p(x,y)\\) (or \\(p(x,c)\\)) from the training data. (Note that this can be thought of as ERM on risk \\(-log(p)\\)). (In the “Learning Models” lab we said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(p(x,y)\\) rather than some well given function\\(y=f(x)\\).)\nThe joint distribution can be constructed in two ways: generative or discriminative. The discriminative approach gives us:\n\\[p(x,c) = p(c|x)p(x)\\]\nwhereas the generative approach gives us\n\\[p(x,c) = p(x|c) p(c)\\]\nand then bayes theorem can be used to obtain p(c|x).\nThe generative approach corresponds to picking one of the classes with probability p(c) and then getting the density of the features for that class. The discriminative approach models the domain boundary instead. While the data may be distributed in a complex way, the boundary may be easier to model. On the other hand prior information for assymetric situations, conditional independence and other such strategies can only be done in generative models.\n\n\n\nGenerative vs discriminative classification: (a) generative models learn class-conditional densities, (b) discriminative models learn the decision boundary directly. From Bishop.\n\n\nIn either case we can get the joint distribution. In the discriminative case that leads us to density estimation for \\(p(x)\\). Often we have no use for it so we wont do it, as in logistic regression. But do remember that if we want our classifier to have good results we should be using it on test sets which reflect the proper sampling \\(p(x)\\). And if we dont characterize it we might be better of using a generative model as it is easier to adjust for class priors.\nThe Bayesian decision approach is a clean one, in which first one models the environment, independent of the subsequent decision process. If \\(p(y,x|\\theta)\\) is the “true” model of the world, this is optimal. But if this environment model is poor, the action \\(g\\) could be higly inaccurate since the environment is divorced from prediction. In practice one often includes regularization terms in the environment model to reduce the complexity of the distribution and bring it more in line with decision based hyperparameters, set by validation on an empirical loss. See the cs109 (2013) Naives bayes homework for a good example.\nBy the way, the ERM method is the only frequentist method which has a well defined risk. The reason for this is that it dosent depend on both a sample-estimate of the true \\(\\theta\\).\n\ndf=pd.read_csv(\"data/01_heights_weights_genders.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nGender\nHeight\nWeight\n\n\n\n\n0\nMale\n73.847017\n241.893563\n\n\n1\nMale\n68.781904\n162.310473\n\n\n2\nMale\n74.110105\n212.740856\n\n\n3\nMale\n71.730978\n220.042470\n\n\n4\nMale\n69.881796\n206.349801\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nclf_l, Xtrain_l, ytrain_l, Xtest_l, ytest_l  = do_classify(LogisticRegression(), {\"C\": [0.01, 0.1, 1, 10, 100]}, df, ['Weight', 'Height'], 'Gender','Male')\n\nBEST PARAMS {'C': 0.01}\nAccuracy on training data: 0.92\nAccuracy on test data:     0.91\n\n\n\nplt.figure()\nax=plt.gca()\npoints_plot(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, alpha=0.2);\n\n\n\n\n\n\n\n\nLet us plot the probabilities obtained from predict_proba, overlayed on the samples with their true labels:\n\nplt.figure()\nax=plt.gca()\npoints_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clf_l, psize=20, alpha=0.1);\n\n\n\n\n\n\n\n\nNotice that lines of equal probability, as might be expected are stright lines. What the classifier does is very intuitive: if the probability is greater than 0.5, it classifies the sample as type ‘1’ (male), otherwise it classifies the sample to be class ‘0’. Thus in the diagram above, where we have plotted predicted values rather than actual labels of samples, there is a clear demarcation at the 0.5 probability line.\nThis notion of trying to obtain the line or boundary of demarcation is what makes the discriminative classifier. The algorithm tries to find a decision boundary that separates the males from the females. To classify a new sample as male or female, it checks on which side of the decision boundary the sample falls, and makes a prediction. In other words we are asking, given \\(\\v{x}\\), what is the probability of a given \\(y\\), or, what is the likelihood \\(P(y|\\v{x},\\v{w})\\)?\n\n\n\nThis involves finding \\(P(\\v{x} | y)\\), the class conditional probability. Consider that heights and weights of males and females might be expected to be distributed using a bell curve. You might have heard of the reasons for this: so many things go into these heights/weights that the net effect is for them to be distributed as a bell curve, according to the central limit theorem. So why not use this additional information and model the heights and weights of males and females separately as 2-dimensional bell curves or Normal Distributions. In other words:\n\\[p(height, weight | male ) = Bell Curve\\]\ncentered at the mean height, weight for males, and a similar equation holds for females. This is exactly what the linear discriminant analysis classifier does. Lets run it, only on tne training set. What we are doing is fitting the male and female sections of the training set separately, getting two 2-D bell curves, and then inverting as above to decide how to classify new samples from the testing set. (We dont cross-validate here as we are currently not fitting any hyperparameters).\n\nfrom sklearn.lda import LDA\nclflda = LDA(solver=\"svd\", store_covariance=True)\nclflda.fit(Xtrain_l, ytrain_l)\n\n//anaconda/envs/py3l/lib/python3.6/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n\n\nLDA(n_components=None, priors=None, shrinkage=None, solver='svd',\n  store_covariance=True, tol=0.0001)\n\n\n\n#from REF\nfrom scipy import linalg\n\ndef plot_ellipse(splot, mean, cov, color):\n    v, w = linalg.eigh(cov)\n    u = w[0] / linalg.norm(w[0])\n    angle = np.arctan(u[1] / u[0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    # filled Gaussian at 2 standard deviation\n    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n                              180 + angle, color=color, lw=3, fill=False)\n    ell.set_clip_box(splot.bbox)\n    ell1 = mpl.patches.Ellipse(mean, 1 * v[0] ** 0.5, 1 * v[1] ** 0.5,\n                              180 + angle, color=color, lw=3, fill=False)\n    ell1.set_clip_box(splot.bbox)\n    ell3 = mpl.patches.Ellipse(mean, 3 * v[0] ** 0.5, 3 * v[1] ** 0.5,\n                              180 + angle, color=color, lw=3, fill=False)\n    ell3.set_clip_box(splot.bbox)\n    #ell.set_alpha(0.2)\n    splot.add_artist(ell)\n    splot.add_artist(ell1)\n    splot.add_artist(ell3)\n\n\n    #splot.set_xticks(())\n    #splot.set_yticks(())\ndef plot_lda_cov(lda, splot):\n    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n#plt.bivariate_normal(X, Y, sigmax=1.0, sigmay=1.0, mux=0.0, muy=0.0, sigmaxy=0.0)¶\n\nWe plot ellipses for equal probability contours of the two individual \\(P(\\v{x}|y)\\), the \\(P(height, weight | male)\\) and the \\(P(height, weight | female)\\). We also plot the discriminant line created by inverting these probabilities using Bayes theorem: we once again classify a sample as male if \\(P(male | height, weight \\gt 0.5\\) (which will ensure that \\(P(female | height, weight) &lt; 0.5\\) since both must add to 1).\n\nplt.figure()\nax=plt.gca()\nspl,_,_=points_plot(ax,Xtrain_l, Xtest_l, ytrain_l, ytest_l, clflda)\nplot_lda_cov(clflda, spl)\n\n\n\n\n\n\n\n\nWhats happenning here? We have estimated \\(P(\\v{x} | y, \\theta_y)\\) where we use \\(\\theta_y\\) to designate the parameters of the fit (the parameters of the female and male bell curves); the subscript indicates we are fitting a separate parameter set for each class.\nWe can then use Bayes theorem to invert to find \\(P(y | \\v{x})\\), the probability we really want thus:\n\\[P(y | \\v{x}) = \\frac{P(\\v{x} | y, \\theta_{y}) P(y)}{P(x)} = \\frac{P(\\v{x} | y) P(y)}{\\sum_{y} P(\\v{x} | y, \\theta_{y})P(y)} .\\]\nNotice from the first term above that the denominator \\(P(\\v{x})\\) does not involve the parameters, it is simply the input distribution of the features. As such, it is not needed in the prediction process.\nFor our special case of two classes:\n\\[P(y = c_1 | \\v{x}) = \\frac{P(\\v{x} | y = c_1, \\theta_1) P(y=c_1)}{P(\\v{x} | y = c_1, \\theta_1) P(y=c_1) + P(\\v{x} | y = c_0, \\theta_0) P(y=c_0)}\\]\nIn other words, we first look at males, and build a model of the features for the males. Then we do a similar thing for females. To classify a new sample, we match it against the model for males, and then the model for females, and user Bayes theorem to see if the new sample more likely looks like the males rather than the females from the training set.\nAnother way to think about this is that you can generate or simulate a new male or female sample from this model \\(P(\\v{x} | y, \\theta_y)\\) of males or females respectively. The idea is that you first toss a (possibly biased) coin which depends on \\(P(y=c_1)\\), the “prior” probability of a sample being male or female. Once that coin has landed male or female (“once the draw has been made”), generate a sample in feature space using \\(P(\\v{x}|what landed)\\). For example, if the coin landed female, now draw a height and weight according to \\[P(height, wieght | female )\\]. Thus there is a story for how to generate samples in such models. If this story is the “correct” or “nearly correct” one, this is likely to be a very good model, and furthermore, we can draw new training sets to check variance and other measures of our classifier’s accuracy.\nYou can see three important aspects of this method from these formulae already. First, you model the classes separately. Secondly, the formula involves \\(P(y)\\), the prior probability of the sample being in the class. This is usually just taken to be the fraction of a particular class in the training sample.\nBeing able to fold this information is key when the classes are very imbalanced in the training data set. For example, consider again the case of trying to predict if a customer will “churn”. Typically churn rates are low, of the order of 2-3%.In this case having the prior probabilities of churning and not-churning, and modelling thse groups separately in a generative classifier is very useful: it might be hard for a discriminative classifier to draw a boundary between many many samples on one side and just a few on the other: one might rightly expect this boundary to be a bit fragile.\nThe third thing is probably obvious and a bit uncomfortable to you: we are being asked to provide much more information: the priors, the individual class models, all of that. It would seem that to get the same kind of classification we are being asked to provide more information.\nThis is true, but it is not a bad thing when the assumptions that go into individual models are well founded, such as in the case of our male and female heights and weights here.\nWe can also plot the output of predict_proba and we see something interesting: we get back exactly the same probability lines that we got using logistic regression. Indeed, LDA is the generative conterpart of Logistic regression. It is possible to prove this but we shall not do so here.\n\nplt.figure()\nax=plt.gca()\npoints_plot_prob(ax, Xtrain_l, Xtest_l, ytrain_l, ytest_l, clflda);\n\n\n\n\n\n\n\n\nThe important point here is that many generative models, including those with Poisson Likelihoods and Naive Bayes Models have Logistic Regression as their discriminative counterpart. This means that on inverting the \\(P(\\v{x}|y)\\) using Bayes theorem, we get back Logistic Regression. Thus Logistic regression if a relatively robust model, insensitive to many modelling assumptions. This is a big reason to use such Discriminative models.\nBut if \\(p(\\v{x}|y)\\) is indeed Normal, as is here for our two classes of male and female, LDA is what is known as asymptotically efficient. This means that for large amounts of training data, it can be proved that no model can be better than LDA for the estimation of the probability we’d like to use to get our classification, \\(p(y|\\v{x})\\). In particular, it can be shown that LDA will outperform Logistic regression, even for small training set sizes.\nFinally note that there is another major advantage of a generative classifier. We can sum up over class conditional probably density weighted by the priors to estimate the input density from the probability marginalization formula.\n\\[P(x) = \\sum_y P(x|y)P(y)\\]\nThis is not surprising. A lot of elbow-grease went into the generative classifier since we had to model so much. It should pay some dividend.\n\n\n\nThis is perhaps the right time then to revisit the generative vs discriminative argument, now that we have some experience under our belt.\n\nIf we only wish to make classification decisions rather than more complex decisions then discriminative might be all we need, and it is cheaper in terms of computing resources. But asymmetry and the ability to generate synthetic data are strengths of the generative approach.\nsometimes generative models like LDA and Naive Bayes are easy to fit. The discriminative model LogisticRegression requires convex optimization via Gradient descent\nwe can add new classes to a generative classifier without retraining for the previous classes so it might be better for online customer selection problems where customer profiles change\ngenerative classifiers can handle missing data easily\ngenerative classifiers are better at handling unlabelled training data (semi-supervized learning)\npreprocessing data is easier with discriminative classifiers\ndiscriminative classifiers give generally better callibrated probabilities"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nopen\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Rahul Dave — physicist, engineer, and practitioner of machine learning, AI, and data science. I also teach.\nI did my Ph.D. in physics at the University of Pennsylvania, where my thesis on quintessence and the cosmic microwave background was among the early works introducing dark energy. After that I spent over a decade at Harvard — as a computational scientist at the Center for Astrophysics, then as a lecturer at SEAS and IACS, where I helped build courses like CS109 (Data Science), AM207 (Stochastic Methods and Bayesian Inference), and CS207 (Systems Development for Computational Science).\nI’m co-founder and Chief Scientist at Univ.AI, where we build AI solutions and teach machine learning. Through Univ.AI, I also consult on ML and data engineering problems — we’ve worked with clients across oil and energy, insurance, finance, and sports.\nI’m a Bayesian at heart. I like interesting problems at the intersection of statistics, machine learning, and computation — and I like explaining them clearly.\n\n\n\nInterests\n\nAI — how it works, why it works, and where it breaks\nBayesian inference and probabilistic modeling\nMachine learning and deep learning\nData visualization and communication\nCosmology and astrophysics\nDevelopment economics\n\n\n\nEducation\n\nPh.D. Physics, University of Pennsylvania, 2002\nB.S. Physics (Honors), St. Xavier’s College, University of Bombay, 1992\n\n\n\nElsewhere\n\nGitHub\nX / Twitter\nBluesky\nGoogle Scholar\nLinkedIn\n\n\n\nGet in Touch\nIf you’d like to work with me — consulting, AI/ML projects, or training — drop me a message below.\nFor questions about blog posts, course material, or anything else, find me on X / Twitter.\n\nName \nEmail \nMessage\n\n\nSend"
  },
  {
    "objectID": "til/open.html",
    "href": "til/open.html",
    "title": "open",
    "section": "",
    "text": "MacOS has a great command open. You can use it to open any file in any folder from the terminal in its default app. For example:\nopen bla.pdf\nwill open a file in Preview.\nSometimes you want another app. Then you can use the -a flag. Like so:\nopen -a /Applications/Typora.app bla.md"
  },
  {
    "objectID": "collections/mysoft/hooksett.html",
    "href": "collections/mysoft/hooksett.html",
    "title": "Hooksett",
    "section": "",
    "text": "Hooksett is a Python library that provides a flexible, extensible hook system for managing parameters, metrics, and artifacts in ML workflows."
  },
  {
    "objectID": "collections/mysoft/hooksett.html#example",
    "href": "collections/mysoft/hooksett.html#example",
    "title": "Hooksett",
    "section": "Example",
    "text": "Example\nAnnotate your ML class with tracked types, wire up a config loader and MLflow output, and every parameter and metric is automatically captured:\nfrom hooksett import tracked, HookManager\nfrom hooksett.hooks import YAMLConfigInput, TypeValidationHook, MLflowOutput\n\ntype Parameter[T] = T\ntype Metric[T] = T\n\n@tracked\nclass Trainer:\n    learning_rate: Parameter[float] = 0.01\n    batch_size: Parameter[int] = 32\n    epochs: Parameter[int] = 100\n    accuracy: Metric[float] = 0.0\n    loss: Metric[float] = 0.0\n\n    def train(self):\n        for epoch in range(self.epochs):\n            # training step ...\n            self.accuracy = evaluate(model)\n            self.loss = compute_loss(model)\n\n# load params from YAML, validate, and log everything to MLflow\nmanager = HookManager()\nmanager.add_input_hook(YAMLConfigInput(\"config.yaml\"))\nmanager.add_input_hook(TypeValidationHook())\nmanager.add_output_hook(MLflowOutput())"
  },
  {
    "objectID": "collections/mysoft/hooksett.html#features",
    "href": "collections/mysoft/hooksett.html#features",
    "title": "Hooksett",
    "section": "Features",
    "text": "Features\n\n@tracked class decorator — monitors attribute changes on class instances via Python descriptors\n@track_function decorator — tracks function parameters and local variables; values are saved to hooks once at function/method exit\nLocal variable tracking — annotate locals with Traced[T] inside methods or functions; only the final value at exit is captured\nAutomatic parameter loading — YAMLConfigInput hook loads configuration from YAML files into tracked attributes\nParameter validation — TypeValidationHook enforces type annotations; RangeValidationHook checks numeric bounds\nCustom type registry — define domain-specific tracked types (Parameter, Metric, Artifact, Prompt, Response, Feature) via register_tracked_type\nPluggable output hooks — TracedOutput for logging, MLflowOutput for experiment tracking, or write your own OutputHook\nSingleton HookManager — register input and output hooks once; all decorated classes and functions use them automatically\nSeparation of config and code — parameters live in YAML, validation in hooks, tracking in type annotations"
  },
  {
    "objectID": "collections/software/hamilton.html",
    "href": "collections/software/hamilton.html",
    "title": "Stitchfix Hamilton",
    "section": "",
    "text": "A scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/hamilton.html#why-choose-this-tool",
    "href": "collections/software/hamilton.html#why-choose-this-tool",
    "title": "Stitchfix Hamilton",
    "section": "",
    "text": "A scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/awk.html",
    "href": "collections/software/awk.html",
    "title": "Awk",
    "section": "",
    "text": "An old goody! For quick command line analysis of data.\n\nThe following examples were taken from the tldr page for awk:\nPrint the fifth column (a.k.a. field) in a space-separated file:\nawk '{print $5}' filename\nPrint the second column of the lines containing “foo” in a space-separated file:\nawk '/foo/ {print $2}' filename\nPrint the last column of each line in a file, using a comma (instead of space) as a field separator:\nawk -F ',' '{print $NF}' filename\nSum the values in the first column of a file and print the total:\nawk '{s+=$1} END {print s}' filename\nPrint every third line starting from the first line:\nawk 'NR%3==1' filename\nPrint different values based on conditions:\nawk '{if ($1 == \"foo\") print \"Exact match foo\"; else if ($1 ~ \"bar\") print \"Partial match bar\"; else print \"Baz\"}' filename\nPrint all lines where the 10th column value equals the specified value:\nawk '($10 == value)'\nPrint all the lines which the 10th column value is between a min and a max:\nawk '($10 &gt;= min_value && $10 &lt;= max_value)'"
  }
]