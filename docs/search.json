[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "Claude Session Context",
    "section": "",
    "text": "This is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io.\n\n\n\n\nAM207 course wiki at ~/Attic/Projects/AM207/2018fall_wiki/wiki/ — lectures, notebooks, markdown notes\nLecture index at ~/Attic/Projects/AM207/2018fall_wiki/lectures/index.md (replace .html with .md for source files)\nEach lecture .md links to wiki notes; check for .ipynb (primary source) before falling back to .md\nUse /import-wiki-notes skill to import notes as blog posts\nHelper scripts in _scripts/:\n\nimport_notebook.py — creates posts/&lt;name&gt;/index.ipynb from wiki .ipynb (adds frontmatter, fixes paths, copies images). Run with --help.\nupdate_captions.py — updates image captions in notebook markdown cells. Used by /caption-images skill.\n\nAfter importing, run /caption-images to add captions with citations to embedded images\n\n\n\n\n\nLecture 1 (Intro & Probability): DONE\n\nboxloop — already existed, skipped\nprobability — notebook, imported to posts/probability/\ndistributions — markdown only, imported to posts/distributions.md\ndistrib-example — notebook, imported to posts/distrib-example/\n\nLecture 2 (Probability, Sampling, Laws, Monte Carlo): DONE\n\ndistributions — already imported (Lecture 1), skipped\nexpectations — notebook, imported to posts/expectations/\nsamplingclt — notebook, imported to posts/samplingclt/\nbasicmontecarlo — notebook, imported to posts/basicmontecarlo/\nmontecarlointegrals — notebook, imported to posts/montecarlointegrals/\n\nLecture 3 (From Monte Carlo to Frequentist Stats): DONE\n\nExpectations — already imported (Lecture 2), skipped\nSamplingCLT — already imported (Lecture 2), skipped\nbasicmontecarlo — already imported (Lecture 2), skipped\nmontecarlointegrals — already imported (Lecture 2), skipped\nfrequentist — markdown only, imported to posts/frequentist.md\nMLE — notebook, imported to posts/MLE/\n\nLectures 4–26: NOT YET IMPORTED\n\n\n\n\n\nCanonical categories are in _categories.txt (root of project), one per line, sorted alphabetically\nCurrent categories: data, elections, integration, interactive, macos, models, montecarlo, orchestration, pipeline, probability, statistics, visualization\nAll categories must be lowercase\nWhen importing, map source keywords to existing categories; propose new ones for user approval\nThe /import-wiki-notes skill enforces this workflow (step 7c)\n\n\n\n\n\nProfile photo: assets/profile.jpg (sourced from GitHub avatar)\nComprehensive bio context: ~/Context/rahul-dave-profile.md\nContact form via Formspree (ID: mykjwlyk) — forwards to rahuldave@univ.ai\nLinks: GitHub, X/Twitter, Bluesky (rahuldave.bsky.social), Google Scholar, LinkedIn\n\n\n\n\n\nincludes/discuss-links.html — “Discuss this post” links (Twitter, Bluesky, LinkedIn) on posts, til, and collections pages only\nUses IIFE (not DOMContentLoaded) because include-after-body runs after DOM is ready\nLinks are constructed at runtime from window.location.href — will use production domain automatically\nOpen Graph and Twitter Card metadata enabled in _quarto.yml\n\n\n\n\n\nAll listing pages sort by date desc: index.qmd, posts.qmd, til.qmd, collections.qmd\n\n\n\n\n\ndesigns/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — fork of depth, modernized with animations/glassmorphism\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nColorBrewer Blues palette (#eff3ff → #08306b)\nDark/light mode support\n\n\n\n--cb-blue-50: #eff3ff   --cb-blue-100: #c6dbef   --cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6  --cb-blue-400: #4292c6   --cb-blue-500: #2171b5\n--cb-blue-600: #08519c  --cb-blue-700: #084594   --cb-blue-800: #08306b\n\n\n\n\n\n\n\n\n_quarto.yml — project config: website type, navbar, SCSS theme (light/dark), TOC, margin references, Open Graph, Twitter Cards\nThemes: styles/modern-light.scss, styles/modern-dark.scss\nIncludes: includes/fonts.html, includes/brand-icon.html, includes/theme-toggle.html, includes/discuss-links.html\n\n\n\n\nNotebooks get their own folder with index.ipynb (or index.qmd for JS demos):\nposts/\n  probability/\n    index.ipynb        # URL becomes /posts/probability/\n    assets/            # images, data files referenced by THIS notebook only\n      venn.png\n      bishop-prob.png\n  votingforcongress/\n    index.ipynb\n    assets/\n      sep7.png\n  earth-demo/\n    index.qmd          # Three.js demo using .qmd format\n    assets/\n      earth-card.png   # Card thumbnail for listing (no content images in post)\nMarkdown posts stay as flat files, images go in shared posts/images/ or posts/data/:\nposts/\n  boxloop.md           # URL becomes /posts/boxloop\n  distributions.md\n  images/              # shared images for flat .md posts\n    2tosscdf.png\n  data/                # shared data for flat .md posts (if needed)\n\n\n\nNotebooks must have a raw cell (cell_type: “raw”) as the first cell with YAML:\n---\ntitle: \"Post Title\"\nsubtitle: \"Catchy one-liner for listing cards.\"\ndescription: \"Two-sentence summary for the post page.\"\ncategories:\n    - probability\n    - statistics\ndate: 2025-01-08\n---\n\nsubtitle appears on listing cards\ndescription is the longer summary\ncategories are used for filtering (must be from _categories.txt)\ndate controls sort order\nimage — optional, for posts without content images (e.g. image: assets/earth-card.png)\n\nFor markdown posts, the same YAML goes in the standard frontmatter block at the top.\n\n\n\nFor posts with no embedded images (e.g. interactive Three.js demos), use the browser agent to screenshot the rendered page, crop to the key visual, save to assets/, and add image: to frontmatter. See skill step 9 for details.\n\n\n\nUse .qmd format (not .ipynb) for JavaScript-heavy interactive content: - Load external JS via format: html: include-in-header: in frontmatter - Use Quarto’s fenced div syntax ::: {.classname} for layout containers - Inline &lt;script&gt; blocks at the end of the .qmd file - Read CSS custom properties (--cb-blue-*, --interactive-*) for theme integration - Example: posts/earth-demo/index.qmd\n\n\n\nmake preview                # Live dev server with hot reload\nmake render                 # Build full site to _site/\nmake build                  # Render + rsync _site/ to docs/ for GitHub Pages\nquarto render posts/probability/index.ipynb  # Render a single post\n\nrender produces _site/; build adds an rsync step to sync changed files into docs/\nCommit and push are intentionally separate from build\n\n\n\n\n\nNotebook posts: assets/filename.png (relative to the notebook’s folder)\nMarkdown posts: images/filename.png (relative to posts/)\nSite-wide assets already in /assets/ (e.g. lawoflargenumbers images) use /assets/... absolute paths\nProfile photo: assets/profile.jpg (JPEG, not PNG — watch for GitHub avatar format mismatch)\n\n\n\n\n\nAll categories must be lowercase — no Statistics, use statistics; no MonteCarlo, use montecarlo\nCanonical list in _categories.txt at project root\nThis prevents duplicate tags in Quarto listing filters\n\n\n\n\n\ntitle — post title\nsubtitle — catchy one-liner shown on listing cards\ndescription — 2-sentence content summary\ncategories — lowercase tags for filtering (from _categories.txt)\ndate — controls sort order (YYYY-MM-DD)\nimage — card thumbnail path (optional, for posts without content images)\n\n\n\n\n\nposts/ — main blog posts (shown on index page)\ntil/ — Today I Learned (shown only on TIL page, NOT on index)\ncollections/software/ — software tools (shown only on Collections page, NOT on index)\nindex.qmd listing contents should only include posts/ and posts/**/\n\n\n\n\n\nAfter adding/moving/renaming files, restart quarto preview — the live server caches resource IDs and will show “Bad resource ID” for changed files\nListing .qmd files must not have a stray --- after the YAML block (causes parse errors)\nListing contents paths should NOT have a leading / (use til/*.qmd not /til/*.qmd)\ninclude-after-body scripts run after DOMContentLoaded — use IIFE, not event listeners\nGitHub avatar downloads are JPEG even with .png URL — always check with file command and use correct extension"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "Claude Session Context",
    "section": "",
    "text": "This is a Quarto-based personal website for data science/ML educational content at rahuldave.github.io."
  },
  {
    "objectID": "CLAUDE.html#content-source",
    "href": "CLAUDE.html#content-source",
    "title": "Claude Session Context",
    "section": "",
    "text": "AM207 course wiki at ~/Attic/Projects/AM207/2018fall_wiki/wiki/ — lectures, notebooks, markdown notes\nLecture index at ~/Attic/Projects/AM207/2018fall_wiki/lectures/index.md (replace .html with .md for source files)\nEach lecture .md links to wiki notes; check for .ipynb (primary source) before falling back to .md\nUse /import-wiki-notes skill to import notes as blog posts\nHelper scripts in _scripts/:\n\nimport_notebook.py — creates posts/&lt;name&gt;/index.ipynb from wiki .ipynb (adds frontmatter, fixes paths, copies images). Run with --help.\nupdate_captions.py — updates image captions in notebook markdown cells. Used by /caption-images skill.\n\nAfter importing, run /caption-images to add captions with citations to embedded images"
  },
  {
    "objectID": "CLAUDE.html#content-import-status-from-am207-wiki",
    "href": "CLAUDE.html#content-import-status-from-am207-wiki",
    "title": "Claude Session Context",
    "section": "",
    "text": "Lecture 1 (Intro & Probability): DONE\n\nboxloop — already existed, skipped\nprobability — notebook, imported to posts/probability/\ndistributions — markdown only, imported to posts/distributions.md\ndistrib-example — notebook, imported to posts/distrib-example/\n\nLecture 2 (Probability, Sampling, Laws, Monte Carlo): DONE\n\ndistributions — already imported (Lecture 1), skipped\nexpectations — notebook, imported to posts/expectations/\nsamplingclt — notebook, imported to posts/samplingclt/\nbasicmontecarlo — notebook, imported to posts/basicmontecarlo/\nmontecarlointegrals — notebook, imported to posts/montecarlointegrals/\n\nLecture 3 (From Monte Carlo to Frequentist Stats): DONE\n\nExpectations — already imported (Lecture 2), skipped\nSamplingCLT — already imported (Lecture 2), skipped\nbasicmontecarlo — already imported (Lecture 2), skipped\nmontecarlointegrals — already imported (Lecture 2), skipped\nfrequentist — markdown only, imported to posts/frequentist.md\nMLE — notebook, imported to posts/MLE/\n\nLectures 4–26: NOT YET IMPORTED"
  },
  {
    "objectID": "CLAUDE.html#category-system",
    "href": "CLAUDE.html#category-system",
    "title": "Claude Session Context",
    "section": "",
    "text": "Canonical categories are in _categories.txt (root of project), one per line, sorted alphabetically\nCurrent categories: data, elections, integration, interactive, macos, models, montecarlo, orchestration, pipeline, probability, statistics, visualization\nAll categories must be lowercase\nWhen importing, map source keywords to existing categories; propose new ones for user approval\nThe /import-wiki-notes skill enforces this workflow (step 7c)"
  },
  {
    "objectID": "CLAUDE.html#about-page",
    "href": "CLAUDE.html#about-page",
    "title": "Claude Session Context",
    "section": "",
    "text": "Profile photo: assets/profile.jpg (sourced from GitHub avatar)\nComprehensive bio context: ~/Context/rahul-dave-profile.md\nContact form via Formspree (ID: mykjwlyk) — forwards to rahuldave@univ.ai\nLinks: GitHub, X/Twitter, Bluesky (rahuldave.bsky.social), Google Scholar, LinkedIn"
  },
  {
    "objectID": "CLAUDE.html#social-discussion-links",
    "href": "CLAUDE.html#social-discussion-links",
    "title": "Claude Session Context",
    "section": "",
    "text": "includes/discuss-links.html — “Discuss this post” links (Twitter, Bluesky, LinkedIn) on posts, til, and collections pages only\nUses IIFE (not DOMContentLoaded) because include-after-body runs after DOM is ready\nLinks are constructed at runtime from window.location.href — will use production domain automatically\nOpen Graph and Twitter Card metadata enabled in _quarto.yml"
  },
  {
    "objectID": "CLAUDE.html#listing-sort-order",
    "href": "CLAUDE.html#listing-sort-order",
    "title": "Claude Session Context",
    "section": "",
    "text": "All listing pages sort by date desc: index.qmd, posts.qmd, til.qmd, collections.qmd"
  },
  {
    "objectID": "CLAUDE.html#design",
    "href": "CLAUDE.html#design",
    "title": "Claude Session Context",
    "section": "",
    "text": "designs/design1-depth/ — CHOSEN BASE DESIGN (Blues sequential palette, clean, scholarly)\ndesigns/design1-modern/ — fork of depth, modernized with animations/glassmorphism\nSerif typography: Bitter (headings) + Source Serif 4 (body) + IBM Plex Mono (code)\nColorBrewer Blues palette (#eff3ff → #08306b)\nDark/light mode support\n\n\n\n--cb-blue-50: #eff3ff   --cb-blue-100: #c6dbef   --cb-blue-200: #9ecae1\n--cb-blue-300: #6baed6  --cb-blue-400: #4292c6   --cb-blue-500: #2171b5\n--cb-blue-600: #08519c  --cb-blue-700: #084594   --cb-blue-800: #08306b"
  },
  {
    "objectID": "CLAUDE.html#quarto-site-structure",
    "href": "CLAUDE.html#quarto-site-structure",
    "title": "Claude Session Context",
    "section": "",
    "text": "_quarto.yml — project config: website type, navbar, SCSS theme (light/dark), TOC, margin references, Open Graph, Twitter Cards\nThemes: styles/modern-light.scss, styles/modern-dark.scss\nIncludes: includes/fonts.html, includes/brand-icon.html, includes/theme-toggle.html, includes/discuss-links.html\n\n\n\n\nNotebooks get their own folder with index.ipynb (or index.qmd for JS demos):\nposts/\n  probability/\n    index.ipynb        # URL becomes /posts/probability/\n    assets/            # images, data files referenced by THIS notebook only\n      venn.png\n      bishop-prob.png\n  votingforcongress/\n    index.ipynb\n    assets/\n      sep7.png\n  earth-demo/\n    index.qmd          # Three.js demo using .qmd format\n    assets/\n      earth-card.png   # Card thumbnail for listing (no content images in post)\nMarkdown posts stay as flat files, images go in shared posts/images/ or posts/data/:\nposts/\n  boxloop.md           # URL becomes /posts/boxloop\n  distributions.md\n  images/              # shared images for flat .md posts\n    2tosscdf.png\n  data/                # shared data for flat .md posts (if needed)\n\n\n\nNotebooks must have a raw cell (cell_type: “raw”) as the first cell with YAML:\n---\ntitle: \"Post Title\"\nsubtitle: \"Catchy one-liner for listing cards.\"\ndescription: \"Two-sentence summary for the post page.\"\ncategories:\n    - probability\n    - statistics\ndate: 2025-01-08\n---\n\nsubtitle appears on listing cards\ndescription is the longer summary\ncategories are used for filtering (must be from _categories.txt)\ndate controls sort order\nimage — optional, for posts without content images (e.g. image: assets/earth-card.png)\n\nFor markdown posts, the same YAML goes in the standard frontmatter block at the top.\n\n\n\nFor posts with no embedded images (e.g. interactive Three.js demos), use the browser agent to screenshot the rendered page, crop to the key visual, save to assets/, and add image: to frontmatter. See skill step 9 for details.\n\n\n\nUse .qmd format (not .ipynb) for JavaScript-heavy interactive content: - Load external JS via format: html: include-in-header: in frontmatter - Use Quarto’s fenced div syntax ::: {.classname} for layout containers - Inline &lt;script&gt; blocks at the end of the .qmd file - Read CSS custom properties (--cb-blue-*, --interactive-*) for theme integration - Example: posts/earth-demo/index.qmd\n\n\n\nmake preview                # Live dev server with hot reload\nmake render                 # Build full site to _site/\nmake build                  # Render + rsync _site/ to docs/ for GitHub Pages\nquarto render posts/probability/index.ipynb  # Render a single post\n\nrender produces _site/; build adds an rsync step to sync changed files into docs/\nCommit and push are intentionally separate from build\n\n\n\n\n\nNotebook posts: assets/filename.png (relative to the notebook’s folder)\nMarkdown posts: images/filename.png (relative to posts/)\nSite-wide assets already in /assets/ (e.g. lawoflargenumbers images) use /assets/... absolute paths\nProfile photo: assets/profile.jpg (JPEG, not PNG — watch for GitHub avatar format mismatch)\n\n\n\n\n\nAll categories must be lowercase — no Statistics, use statistics; no MonteCarlo, use montecarlo\nCanonical list in _categories.txt at project root\nThis prevents duplicate tags in Quarto listing filters\n\n\n\n\n\ntitle — post title\nsubtitle — catchy one-liner shown on listing cards\ndescription — 2-sentence content summary\ncategories — lowercase tags for filtering (from _categories.txt)\ndate — controls sort order (YYYY-MM-DD)\nimage — card thumbnail path (optional, for posts without content images)\n\n\n\n\n\nposts/ — main blog posts (shown on index page)\ntil/ — Today I Learned (shown only on TIL page, NOT on index)\ncollections/software/ — software tools (shown only on Collections page, NOT on index)\nindex.qmd listing contents should only include posts/ and posts/**/\n\n\n\n\n\nAfter adding/moving/renaming files, restart quarto preview — the live server caches resource IDs and will show “Bad resource ID” for changed files\nListing .qmd files must not have a stray --- after the YAML block (causes parse errors)\nListing contents paths should NOT have a leading / (use til/*.qmd not /til/*.qmd)\ninclude-after-body scripts run after DOMContentLoaded — use IIFE, not event listeners\nGitHub avatar downloads are JPEG even with .png URL — always check with file command and use correct extension"
  },
  {
    "objectID": "collections/software/prefect.html",
    "href": "collections/software/prefect.html",
    "title": "Prefect-2.0",
    "section": "",
    "text": "Prefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#why-choose-this-tool",
    "href": "collections/software/prefect.html#why-choose-this-tool",
    "title": "Prefect-2.0",
    "section": "",
    "text": "Prefect is largely regarded as the successor to Airflow. Its API is simpler, and conceptually its easy to understand. It is an open-source piece of software supported by a long running and well funded startup. This abates risk from the company shutting down.\n\nOrchestration is important to run DAG like flows when input sources have changed. Its even more important to run orchestration at regular intervals to support active learning, or retraining of models.\nThis diagram (from https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) provides an idea of how prefect might be used to orchestrate a pipeline:\n\n\n\n\n\n\nFigure 1: Recommendation systems Flow"
  },
  {
    "objectID": "collections/software/prefect.html#more-about-the-tool",
    "href": "collections/software/prefect.html#more-about-the-tool",
    "title": "Prefect-2.0",
    "section": "More about the tool",
    "text": "More about the tool\nPrefect is organized around the notion of fllows. Flows can have subflows, and both of these can have tasks, but tasks cannot have sub-tasks. Flows have implementation as processes or as docker containers.\n\nflows can be run adhoc\nflows can be scheduled\nother DAG based software such as DVC pipelines, hamilton, and dbt can be run as prefect processes\nprefect does not seem to support event based activation of pipelines, although the ability to create deployments in python can enable us to create some such flow\nprefect is well integrated with dask, which we can then use for hyper-parameter optimizations on our cluster or other such distributed computations"
  },
  {
    "objectID": "collections/software/prefect.html#how-to-install",
    "href": "collections/software/prefect.html#how-to-install",
    "title": "Prefect-2.0",
    "section": "How to install",
    "text": "How to install\npip install -U prefect\nThe prefect orion UI will need proxying out of a cluster."
  },
  {
    "objectID": "collections/software/prefect.html#alternatives",
    "href": "collections/software/prefect.html#alternatives",
    "title": "Prefect-2.0",
    "section": "Alternatives",
    "text": "Alternatives\nSeveral alternatives exist. The old airflow and luigi are still around."
  },
  {
    "objectID": "collections/mysoft/deebase.html",
    "href": "collections/mysoft/deebase.html",
    "title": "DeeBase",
    "section": "",
    "text": "DeeBase is an async SQLAlchemy-based database library with a fastlite-inspired API that makes async database operations simple across multiple backends."
  },
  {
    "objectID": "collections/mysoft/deebase.html#example",
    "href": "collections/mysoft/deebase.html#example",
    "title": "DeeBase",
    "section": "Example",
    "text": "Example\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom deebase import Database, ForeignKey, Text\n\n@dataclass\nclass User:\n    id: int\n    name: str\n    email: str\n    status: str = \"active\"\n\n@dataclass\nclass Post:\n    id: int\n    author_id: ForeignKey[int, \"user\"]\n    title: str\n    content: Text\n    created_at: datetime\n\ndb = Database(\"sqlite+aiosqlite:///blog.db\")\nusers = await db.create(User, pk='id')\nposts = await db.create(Post, pk='id')\n\nalice = await users.insert(User(id=None, name=\"Alice\", email=\"alice@example.com\"))\npost = await posts.insert(Post(\n    id=None, author_id=alice.id, title=\"Hello\",\n    content=\"First post!\", created_at=datetime.now()\n))\n\n# FK navigation\nauthor = await posts.fk.author_id(post)\nprint(author.name)  # \"Alice\"\n\n# Query\nall_posts = await posts()\npost = await posts[1]"
  },
  {
    "objectID": "collections/mysoft/deebase.html#features",
    "href": "collections/mysoft/deebase.html#features",
    "title": "DeeBase",
    "section": "Features",
    "text": "Features\n\nAsync/await first — built on SQLAlchemy 2.0+ with aiosqlite and asyncpg drivers\nErgonomic API — await users[1], await users(), await users.lookup(email=\"...\")\nType safety — optional @dataclass support with IDE autocomplete; or start with plain classes and call .dataclass() later\nRich type system — str, Text, int, float, bool, bytes, dict (JSON), datetime, date, time, Optional[T]\nForeign keys and defaults — ForeignKey[int, \"user\"] type annotation; SQL defaults from class field defaults\nFK navigation — await posts.fk.author_id(post) to fetch parent; await users.get_children(user, \"post\", \"author_id\") for children\nIndexes — simple, composite, unique, and named indexes via Index or create_index()\nFull-text search — BM25-ranked search via SQLite FTS5 and PostgreSQL pg_textsearch with automatic index sync\nViews — db.create_view() for JOINs and CTEs with full query API\nxtra() filtering — create scoped table views that auto-apply filters on all operations including inserts\nTransactions — atomic multi-operation commits with rollback\nDatabase reflection — await db.reflect() to work with existing databases; access tables via db.t.tablename\nCode generation — create_mod_from_tables() exports schemas as Python dataclasses\n6 exception types — NotFoundError, IntegrityError, ValidationError, SchemaError, ConnectionError, InvalidOperationError\nCLI — deebase init, deebase table create, deebase migrate, deebase sql, deebase data, and more\nMigrations — schema migrations with up/down support and version tracking\nFastAPI integration — create_crud_router() auto-generates CRUD endpoints with Swagger docs and FK validation\nAdmin interface — Django-like admin UI at /admin/ via deebase api serve --admin\nValidation layer — shared validation for CLI, admin, and API"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Why Do We Have Seasons?\n\n\nEarth’s 23.5° tilt is the secret — not distance from the Sun.\n\n\n\nvisualization\n\ninteractive\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nFind the parameters that make your data most probable.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Statistics\n\n\nFixed parameters, random data — the frequentist creed.\n\n\n\nstatistics\n\nprobability\n\n\n\n\nFeb 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Integration\n\n\nWhen calculus is hard, sample instead.\n\n\n\nmontecarlo\n\nintegration\n\nprobability\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBasic Monte Carlo\n\n\nLet randomness do the heavy lifting.\n\n\n\nmontecarlo\n\nprobability\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nSampling and the Central Limit Theorem\n\n\nWhy everything looks normal in the limit.\n\n\n\nprobability\n\nstatistics\n\nmontecarlo\n\n\n\n\nFeb 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nExpectations and the Law of Large Numbers\n\n\nWhat you expect is what you get — eventually.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions Example: Elections\n\n\nSimulating a presidential election with coin flips.\n\n\n\nprobability\n\nstatistics\n\nelections\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\nThe shapes that randomness takes.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nFrom coin flips to Bayes’ theorem.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\nBuild, compute, critique, repeat.\n\n\n\nmodels\n\nprobability\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\nDoes the president’s party always lose seats in congress?\n\n\n\nelections\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\nFlip enough coins and the truth emerges.\n\n\n\nstatistics\n\nmontecarlo\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\nDon’t make your audience think.\n\n\n\nvisualization\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "collections.html",
    "href": "collections.html",
    "title": "Collections",
    "section": "",
    "text": "Software I’ve Written\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nJan 20, 2025\n\n\nDeeBase\n\n\n \n\n\n\n\n\n\nJan 15, 2025\n\n\nHooksett\n\n\n \n\n\n\n\n\n\nNo matching items\n\n\n\nSoftware I Like\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\nStitchfix Hamilton\n\n\n \n\n\n\n\n\n\nDec 3, 2023\n\n\nAwk\n\n\n \n\n\n\n\n\n\nDec 3, 2022\n\n\nPrefect-2.0\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nRahul’s Lair\n",
    "section": "",
    "text": "Data, Stats, ML and AI\n\n\n\nA collection of notebooks, tutorials, and deep dives into statistical inference, machine learning, AI, and the art of communicating with data."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "\nRahul’s Lair\n",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nWhy Do We Have Seasons?\n\n\nEarth’s 23.5° tilt is the secret — not distance from the Sun.\n\n\n\nvisualization\n\ninteractive\n\n\n\n\nJan 29, 2026\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation\n\n\nFind the parameters that make your data most probable.\n\n\n\nstatistics\n\nmodels\n\n\n\n\nFeb 26, 2025\n\n\n\n\n\n\n\n\n\n\n\nFrequentist Statistics\n\n\nFixed parameters, random data — the frequentist creed.\n\n\n\nstatistics\n\nprobability\n\n\n\n\nFeb 21, 2025\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Integration\n\n\nWhen calculus is hard, sample instead.\n\n\n\nmontecarlo\n\nintegration\n\nprobability\n\n\n\n\nFeb 19, 2025\n\n\n\n\n\n\n\n\n\n\n\nBasic Monte Carlo\n\n\nLet randomness do the heavy lifting.\n\n\n\nmontecarlo\n\nprobability\n\n\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\nSampling and the Central Limit Theorem\n\n\nWhy everything looks normal in the limit.\n\n\n\nprobability\n\nstatistics\n\nmontecarlo\n\n\n\n\nFeb 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nExpectations and the Law of Large Numbers\n\n\nWhat you expect is what you get — eventually.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions Example: Elections\n\n\nSimulating a presidential election with coin flips.\n\n\n\nprobability\n\nstatistics\n\nelections\n\n\n\n\nJan 15, 2025\n\n\n\n\n\n\n\n\n\n\n\nDistributions\n\n\nThe shapes that randomness takes.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\nFrom coin flips to Bayes’ theorem.\n\n\n\nprobability\n\nstatistics\n\n\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\nBox’s Loop\n\n\nBuild, compute, critique, repeat.\n\n\n\nmodels\n\nprobability\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\nSome Data Analysis about Congress\n\n\nDoes the president’s party always lose seats in congress?\n\n\n\nelections\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\nThe LLN\n\n\nFlip enough coins and the truth emerges.\n\n\n\nstatistics\n\nmontecarlo\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\n\n\n\nVisualization As Story\n\n\nDon’t make your audience think.\n\n\n\nvisualization\n\n\n\n\nDec 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/MLE/index.html",
    "href": "posts/MLE/index.html",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]\n\\[\n\\renewcommand{\\like}{\\cal L}\n\\renewcommand{\\loglike}{\\ell}\n\\renewcommand{\\err}{\\cal E}\n\\renewcommand{\\dat}{\\cal D}\n\\renewcommand{\\hyp}{\\cal H}\n\\renewcommand{\\Ex}[2]{E_{#1}[#2]}\n\\renewcommand{\\x}{\\mathbf x}\n\\renewcommand{\\v}[1]{\\mathbf #1}\n\\]\n\n\n\nWhen we do data analysis in a parametric way, we start by characterizing our particular sample statistically then, using a probability distribution (or mass function). This distribution has some parameters. Lets refer to these as \\(\\theta\\).\nIf we assume that our data was generated by this distribution, then the notion of the true value of the parameter makes sense. Now, usually in life, there is no way of knowing if this was the true generating process, unless we have some physics or similar ideas behind the process. But lets stick with the myth that we can do this. Then let us call the true value of the parameters as \\(\\theta^*\\).\nTo know this true value, we’d typically need the entire large population, not the sample we have been given as data. So the best we can do us to make a parameter estimate \\(\\hat{\\theta}\\) from the data. In the context of frequentist statistics, the assumption is that the parameters are fixed, and that there is this true value (\\(\\theta^*\\)), and that we can make some estimate of this from our sample (\\(\\hat{\\theta}\\)).\nA distribution is induced on this estimate by considering many samples that could have been drawn from the population…remember that frequentist statistics fixes the parameters but considers data stochastic. This distribution is called the sampling distribution of the parameter \\(\\theta\\). (In general a sampling distribution can be considered for anything computed on the sample, such as a mean or variance or other moment).\nOur question is: how do we estimate \\(\\hat{\\theta}\\). And how do we compute this sampling distribution so that we can get a notion of the uncertainty that estimating from a sample rather than the population leaves us with?\nThe first question is tackled by the Maximum Likelihood estimate, or MLE. The second one is tackled by techniques like the bootstrap.\nLets learn about the MLE in the context of a particular distribution, the exponential.\n\n\nThe diagram below illustrates the idea behind the MLE.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nConsider two distributions in the same family, one with a parameter, lets call it \\(\\theta\\), of value 1.8 (blue) and another of value 5.8. (green). Let’s say we have 3 data points, at \\(x=1,2,3\\).\nMaximum likelihood starts by asking the question: conditional on the fixed value of \\(\\theta\\), which distribution is the data more likely to have come from?\nIn our case the blue is more likely since the product of the height of the 3 vertical blue bars is higher than that of the 3 green bars.\nIndeed the question that MLE asks is: how can we move and scale the distribution, that is, change \\(\\theta\\), until the product of the 3 bars is maximised!\nThat is, the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i \\mid \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nOften it is easier and numerically more stable to maximise the log likelyhood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nThe exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\nIt takes the form: \\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n0 & x &lt; 0.\n\\end{cases}\n\\]\nIn the case of the exponential distribution we have:\n\\[\n\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n\\]\nMaximizing this:\n\\[\n\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n\\]\nand thus:\n\\[\n\\frac{1}{\\est{\\lambda_{MLE}}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nwhich is the sample mean of our sample. Usually one is not so lucky and one must use numerical optimization techniques.\nA crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large.\n\n\n\n\nJust having an estimate is no good. We will want to put confidence intervals on the estimation of the parameters. This presents a conundrum: we have access to only one sample, but want to compute a error estimate over multiple samples, using an estimator such as the standard deviation.\nAt this point we are wishing for the Lord to have given us other samples drawn from the population. But alas, no such luck…\nSo how then are we to find the sampling distribution of our parameters?\nIn the last two decades, resampling the ONE dataset we have has become computationally feasible. Resampling involves making new samples from the observations, each of which is analysed in the same way as out original dataset. One way to do this is the Bootstrap.\n\n\n\nLinear regression is the workhorse algorithm thats used in many sciences, social and natural. The diagram below illustrates the probabilistic interpretation of linear regression, and the idea behind the MLE for linear regression. We illustrate a point \\((x_i, y_i)\\), and the corresponding prediction for \\(x_i\\) using the line, that is \\(yhat_i\\) or \\(\\hat{y}_i\\).\n\n\n\nProbabilistic interpretation of linear regression\n\n\nThe fundamental assumption for the probabilistic analysis of linear regression is that each \\(y_i\\) is gaussian distributed with mean \\(\\v{w}\\cdot\\v{x_i}\\) (the y predicted by the regression line so to speak) and variance \\(\\sigma^2\\):\n\\[ y_i \\sim N(\\v{w}\\cdot\\v{x_i}, \\sigma^2) .\\]\nWe can then write the likelihood:\n\\[\\cal{L} = p(\\v{y} | \\v{x}, \\v{w}, \\sigma) = \\prod_i p(\\v{y}_i | \\v{x}_i, \\v{w}, \\sigma)\\]\nGiven the canonical form of the gaussian:\n\\[N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},\\]\nwe can show that:\n\\[\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .\\]\nThe log likelihood \\(\\ell\\) then is given by:\n\\[\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .\\]\nIf you differentiate this with respect to \\(\\v{w}\\) and \\(\\sigma\\), you get the MLE values of the parameter estimates:\n\\[\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, \\]\nwhere \\(\\v{X}\\) is the design matrix created by stacking rows \\(\\v{x}_i\\), and\n\\[\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . \\]\nThese are the standard results of linear regression.\n\n\n\nLogistic regression if one of the well known supervized learning algorithms used for classification.\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the ‘1’ samples from the ‘0’ samples, just like in the diagram above. In other words, we wish to find the “regression” line which divides the samples. Now, a line has the form \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) in 2-dimensions. On one side of this line we have\n\\[w_1 x_1 + w_2 x_2 + w_0 \\ge 0,\\]\nand on the other side we have\n\\[w_1 x_1 + w_2 x_2 + w_0 &lt; 0.\\]\nOur classification rule then becomes:\n\\[\n\\begin{eqnarray}\ny = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &if& \\v{w}\\cdot\\v{x} &lt; 0\n\\end{eqnarray}\n\\]\nwhere \\(\\v{x}\\) is the vector \\(\\{1,x_1, x_2,...,x_n\\}\\) where we have also generalized to more than 2 features.\nWhat hypotheses \\(h\\) can we use to achieve this? One way to do so is to use the sigmoid function:\n\\[h(z) = \\frac{1}{1 + e^{-z}}.\\]\nNotice that at \\(z=0\\) this function has the value 0.5. If \\(z &gt; 0\\), \\(h &gt; 0.5\\) and as \\(z \\to \\infty\\), \\(h \\to 1\\). If \\(z &lt; 0\\), \\(h &lt; 0.5\\) and as \\(z \\to -\\infty\\), \\(h \\to 0\\). As long as we identify any value of \\(y &gt; 0.5\\) as 1, and any \\(y &lt; 0.5\\) as 0, we can achieve what we wished above.\nThis function is plotted below:\n\nh = lambda z: 1./(1+np.exp(-z))\nzs=np.arange(-5,5,0.1)\nplt.plot(zs, h(zs), alpha=0.5);\n\n\n\n\n\n\n\n\nSo we then come up with our rule by identifying:\n\\[z = \\v{w}\\cdot\\v{x}.\\]\nThen \\(h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\ge 0\\) and \\(h(\\v{w}\\cdot\\v{x}) \\lt 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\lt 0\\), and:\n\\[\n\\begin{eqnarray}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray}\n\\]\nWe said above that if \\(h &gt; 0.5\\) we ought to identify the sample with \\(y=1\\)? One way of thinking about this is to identify \\(h(\\v{w}\\cdot\\v{x})\\) with the probability that the sample is a ‘1’ (\\(y=1\\)). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a ‘1’ is \\(\\ge 0.5\\).\nSo suppose we say then that the probability of \\(y=1\\) for a given \\(\\v{x}\\) is given by \\(h(\\v{w}\\cdot\\v{x})\\)?\nThen, the conditional probabilities of \\(y=1\\) or \\(y=0\\) given a particular sample’s features \\(\\v{x}\\) are:\n\\[\\begin{eqnarray}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray}\\]\nThese two can be written together as\n\\[P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} \\]\nThen multiplying over the samples we get the probability of the training \\(y\\) given \\(\\v{w}\\) and the \\(\\v{x}\\):\n\\[P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\]\nWhy use probabilities? Earlier, we talked about how the regression function \\(f(x)\\) never gives us the \\(y\\) exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently.\nWe said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(P(x,y)\\). Thus we need to model \\(y\\) at a given \\(x\\), written as \\(P(y \\mid x)\\), and since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) ,\\]\nand can obtain our joint probability (\\(P(x, y))\\).\nIndeed its important to realize that a particular sample can be thought of as a draw from some “true” probability distribution. If for example the probability of classifying a sample point as a ‘0’ was 0.1, and it turns out that the sample point was actually a ‘0’, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a ‘0’! But, of-course its more unlikely than its likely, and having good probabilities means that we’ll be likely right most of the time, which is what we want to achieve in classification.\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a ‘1’.\nNow if we maximize \\[P(y \\mid \\v{x},\\v{w})\\], we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This maximum likelihood estimation maximises the likelihood of the sample y,\n\\[\\like = P(y \\mid \\v{x},\\v{w}).\\]\nAgain, we can equivalently maximize\n\\[\\loglike = log(P(y \\mid \\v{x},\\v{w}))\\]\nsince the natural logarithm \\(log\\) is a monotonic function. This is known as maximizing the log-likelihood.\n\\[\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).\\]\nThus\n\\[\\begin{eqnarray}\n\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "posts/MLE/index.html#choosing-a-parametric-model",
    "href": "posts/MLE/index.html#choosing-a-parametric-model",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "When we do data analysis in a parametric way, we start by characterizing our particular sample statistically then, using a probability distribution (or mass function). This distribution has some parameters. Lets refer to these as \\(\\theta\\).\nIf we assume that our data was generated by this distribution, then the notion of the true value of the parameter makes sense. Now, usually in life, there is no way of knowing if this was the true generating process, unless we have some physics or similar ideas behind the process. But lets stick with the myth that we can do this. Then let us call the true value of the parameters as \\(\\theta^*\\).\nTo know this true value, we’d typically need the entire large population, not the sample we have been given as data. So the best we can do us to make a parameter estimate \\(\\hat{\\theta}\\) from the data. In the context of frequentist statistics, the assumption is that the parameters are fixed, and that there is this true value (\\(\\theta^*\\)), and that we can make some estimate of this from our sample (\\(\\hat{\\theta}\\)).\nA distribution is induced on this estimate by considering many samples that could have been drawn from the population…remember that frequentist statistics fixes the parameters but considers data stochastic. This distribution is called the sampling distribution of the parameter \\(\\theta\\). (In general a sampling distribution can be considered for anything computed on the sample, such as a mean or variance or other moment).\nOur question is: how do we estimate \\(\\hat{\\theta}\\). And how do we compute this sampling distribution so that we can get a notion of the uncertainty that estimating from a sample rather than the population leaves us with?\nThe first question is tackled by the Maximum Likelihood estimate, or MLE. The second one is tackled by techniques like the bootstrap.\nLets learn about the MLE in the context of a particular distribution, the exponential.\n\n\nThe diagram below illustrates the idea behind the MLE.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nConsider two distributions in the same family, one with a parameter, lets call it \\(\\theta\\), of value 1.8 (blue) and another of value 5.8. (green). Let’s say we have 3 data points, at \\(x=1,2,3\\).\nMaximum likelihood starts by asking the question: conditional on the fixed value of \\(\\theta\\), which distribution is the data more likely to have come from?\nIn our case the blue is more likely since the product of the height of the 3 vertical blue bars is higher than that of the 3 green bars.\nIndeed the question that MLE asks is: how can we move and scale the distribution, that is, change \\(\\theta\\), until the product of the 3 bars is maximised!\nThat is, the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i \\mid \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nOften it is easier and numerically more stable to maximise the log likelyhood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nThe exponential distribution occurs naturally when describing the lengths of the inter-arrival times in a homogeneous Poisson process.\nIt takes the form: \\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x} & x \\ge 0, \\\\\n0 & x &lt; 0.\n\\end{cases}\n\\]\nIn the case of the exponential distribution we have:\n\\[\n\\ell(lambda) = \\sum_{i=1}^n ln(\\lambda e^{-\\lambda x_i}) = \\sum_{i=1}^n \\left( ln(\\lambda) - \\lambda x_i \\right).\n\\]\nMaximizing this:\n\\[\n\\frac{d \\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^n x_i = 0\n\\]\nand thus:\n\\[\n\\frac{1}{\\est{\\lambda_{MLE}}} = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\]\nwhich is the sample mean of our sample. Usually one is not so lucky and one must use numerical optimization techniques.\nA crucial property is that, for many commonly occurring situations, maximum likelihood parameter estimators have an approximate normal distribution when n is large."
  },
  {
    "objectID": "posts/MLE/index.html#inference",
    "href": "posts/MLE/index.html#inference",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Just having an estimate is no good. We will want to put confidence intervals on the estimation of the parameters. This presents a conundrum: we have access to only one sample, but want to compute a error estimate over multiple samples, using an estimator such as the standard deviation.\nAt this point we are wishing for the Lord to have given us other samples drawn from the population. But alas, no such luck…\nSo how then are we to find the sampling distribution of our parameters?\nIn the last two decades, resampling the ONE dataset we have has become computationally feasible. Resampling involves making new samples from the observations, each of which is analysed in the same way as out original dataset. One way to do this is the Bootstrap."
  },
  {
    "objectID": "posts/MLE/index.html#linear-regression-mle",
    "href": "posts/MLE/index.html#linear-regression-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Linear regression is the workhorse algorithm thats used in many sciences, social and natural. The diagram below illustrates the probabilistic interpretation of linear regression, and the idea behind the MLE for linear regression. We illustrate a point \\((x_i, y_i)\\), and the corresponding prediction for \\(x_i\\) using the line, that is \\(yhat_i\\) or \\(\\hat{y}_i\\).\n\n\n\nProbabilistic interpretation of linear regression\n\n\nThe fundamental assumption for the probabilistic analysis of linear regression is that each \\(y_i\\) is gaussian distributed with mean \\(\\v{w}\\cdot\\v{x_i}\\) (the y predicted by the regression line so to speak) and variance \\(\\sigma^2\\):\n\\[ y_i \\sim N(\\v{w}\\cdot\\v{x_i}, \\sigma^2) .\\]\nWe can then write the likelihood:\n\\[\\cal{L} = p(\\v{y} | \\v{x}, \\v{w}, \\sigma) = \\prod_i p(\\v{y}_i | \\v{x}_i, \\v{w}, \\sigma)\\]\nGiven the canonical form of the gaussian:\n\\[N(\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-(y - \\mu)^2 / 2\\sigma^2},\\]\nwe can show that:\n\\[\\cal{L} =  (2\\pi\\sigma^2)^{(-n/2)} e^{\\frac{-1}{2\\sigma^2} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2} .\\]\nThe log likelihood \\(\\ell\\) then is given by:\n\\[\\ell = \\frac{-n}{2} log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}  \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2 .\\]\nIf you differentiate this with respect to \\(\\v{w}\\) and \\(\\sigma\\), you get the MLE values of the parameter estimates:\n\\[\\v{w}_{MLE} = (\\v{X}^T\\v{X})^{-1} \\v{X}^T\\v{y}, \\]\nwhere \\(\\v{X}\\) is the design matrix created by stacking rows \\(\\v{x}_i\\), and\n\\[\\sigma^2_{MLE} =  \\frac{1}{n} \\sum_i (y_i -  \\v{w}\\cdot\\v{x}_i)^2  . \\]\nThese are the standard results of linear regression."
  },
  {
    "objectID": "posts/MLE/index.html#logistic-regression-mle",
    "href": "posts/MLE/index.html#logistic-regression-mle",
    "title": "Maximum Likelihood Estimation",
    "section": "",
    "text": "Logistic regression if one of the well known supervized learning algorithms used for classification.\nThe idea behind logistic regression is very simple. We want to draw a line in feature space that divides the ‘1’ samples from the ‘0’ samples, just like in the diagram above. In other words, we wish to find the “regression” line which divides the samples. Now, a line has the form \\(w_1 x_1 + w_2 x_2 + w_0 = 0\\) in 2-dimensions. On one side of this line we have\n\\[w_1 x_1 + w_2 x_2 + w_0 \\ge 0,\\]\nand on the other side we have\n\\[w_1 x_1 + w_2 x_2 + w_0 &lt; 0.\\]\nOur classification rule then becomes:\n\\[\n\\begin{eqnarray}\ny = 1 &if& \\v{w}\\cdot\\v{x} \\ge 0\\\\\ny = 0 &if& \\v{w}\\cdot\\v{x} &lt; 0\n\\end{eqnarray}\n\\]\nwhere \\(\\v{x}\\) is the vector \\(\\{1,x_1, x_2,...,x_n\\}\\) where we have also generalized to more than 2 features.\nWhat hypotheses \\(h\\) can we use to achieve this? One way to do so is to use the sigmoid function:\n\\[h(z) = \\frac{1}{1 + e^{-z}}.\\]\nNotice that at \\(z=0\\) this function has the value 0.5. If \\(z &gt; 0\\), \\(h &gt; 0.5\\) and as \\(z \\to \\infty\\), \\(h \\to 1\\). If \\(z &lt; 0\\), \\(h &lt; 0.5\\) and as \\(z \\to -\\infty\\), \\(h \\to 0\\). As long as we identify any value of \\(y &gt; 0.5\\) as 1, and any \\(y &lt; 0.5\\) as 0, we can achieve what we wished above.\nThis function is plotted below:\n\nh = lambda z: 1./(1+np.exp(-z))\nzs=np.arange(-5,5,0.1)\nplt.plot(zs, h(zs), alpha=0.5);\n\n\n\n\n\n\n\n\nSo we then come up with our rule by identifying:\n\\[z = \\v{w}\\cdot\\v{x}.\\]\nThen \\(h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\ge 0\\) and \\(h(\\v{w}\\cdot\\v{x}) \\lt 0.5\\) if \\(\\v{w}\\cdot\\v{x} \\lt 0\\), and:\n\\[\n\\begin{eqnarray}\ny = 1 &if& h(\\v{w}\\cdot\\v{x}) \\ge 0.5\\\\\ny = 0 &if& h(\\v{w}\\cdot\\v{x}) \\lt 0.5.\n\\end{eqnarray}\n\\]\nWe said above that if \\(h &gt; 0.5\\) we ought to identify the sample with \\(y=1\\)? One way of thinking about this is to identify \\(h(\\v{w}\\cdot\\v{x})\\) with the probability that the sample is a ‘1’ (\\(y=1\\)). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a ‘1’ is \\(\\ge 0.5\\).\nSo suppose we say then that the probability of \\(y=1\\) for a given \\(\\v{x}\\) is given by \\(h(\\v{w}\\cdot\\v{x})\\)?\nThen, the conditional probabilities of \\(y=1\\) or \\(y=0\\) given a particular sample’s features \\(\\v{x}\\) are:\n\\[\\begin{eqnarray}\nP(y=1 | \\v{x}) &=& h(\\v{w}\\cdot\\v{x}) \\\\\nP(y=0 | \\v{x}) &=& 1 - h(\\v{w}\\cdot\\v{x}).\n\\end{eqnarray}\\]\nThese two can be written together as\n\\[P(y|\\v{x}, \\v{w}) = h(\\v{w}\\cdot\\v{x})^y \\left(1 - h(\\v{w}\\cdot\\v{x}) \\right)^{(1-y)} \\]\nThen multiplying over the samples we get the probability of the training \\(y\\) given \\(\\v{w}\\) and the \\(\\v{x}\\):\n\\[P(y|\\v{x},\\v{w}) = P(\\{y_i\\} | \\{\\v{x}_i\\}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} P(y_i|\\v{x_i}, \\v{w}) = \\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\]\nWhy use probabilities? Earlier, we talked about how the regression function \\(f(x)\\) never gives us the \\(y\\) exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently.\nWe said that another way to think about a noisy \\(y\\) is to imagine that our data \\(\\dat\\) was generated from a joint probability distribution \\(P(x,y)\\). Thus we need to model \\(y\\) at a given \\(x\\), written as \\(P(y \\mid x)\\), and since \\(P(x)\\) is also a probability distribution, we have:\n\\[P(x,y) = P(y \\mid x) P(x) ,\\]\nand can obtain our joint probability (\\(P(x, y))\\).\nIndeed its important to realize that a particular sample can be thought of as a draw from some “true” probability distribution. If for example the probability of classifying a sample point as a ‘0’ was 0.1, and it turns out that the sample point was actually a ‘0’, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a ‘0’! But, of-course its more unlikely than its likely, and having good probabilities means that we’ll be likely right most of the time, which is what we want to achieve in classification.\nThus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a ‘1’.\nNow if we maximize \\[P(y \\mid \\v{x},\\v{w})\\], we will maximize the chance that each point is classified correctly, which is what we want to do. This is a principled way of obtaining the highest probability classification. This maximum likelihood estimation maximises the likelihood of the sample y,\n\\[\\like = P(y \\mid \\v{x},\\v{w}).\\]\nAgain, we can equivalently maximize\n\\[\\loglike = log(P(y \\mid \\v{x},\\v{w}))\\]\nsince the natural logarithm \\(log\\) is a monotonic function. This is known as maximizing the log-likelihood.\n\\[\\loglike = log \\like = log(P(y \\mid \\v{x},\\v{w})).\\]\nThus\n\\[\\begin{eqnarray}\n\\loglike &=& log\\left(\\prod_{y_i \\in \\cal{D}} h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} log\\left(h(\\v{w}\\cdot\\v{x_i})^{y_i} \\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\right)\\\\                  \n                  &=& \\sum_{y_i \\in \\cal{D}} log\\,h(\\v{w}\\cdot\\v{x_i})^{y_i} + log\\,\\left(1 - h(\\v{w}\\cdot\\v{x_i}) \\right)^{(1-y_i)}\\\\\n                  &=& \\sum_{y_i \\in \\cal{D}} \\left ( y_i log(h(\\v{w}\\cdot\\v{x})) + ( 1 - y_i) log(1 - h(\\v{w}\\cdot\\v{x})) \\right )\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "posts/seasons/index.html",
    "href": "posts/seasons/index.html",
    "title": "Why Do We Have Seasons?",
    "section": "",
    "text": "Seasons aren’t caused by Earth’s distance from the Sun. The secret is Earth’s 23.5° axial tilt. As Earth orbits, different hemispheres tilt toward the Sun, receiving more direct sunlight — and that’s what makes it warm.\nUse the buttons below to jump between seasons and see how the sun’s rays strike Earth differently. The golden rays show parallel sunlight beams; the markers on Earth’s surface show where light is concentrated (direct, small footprint = hot) versus spread out (angled, large footprint = cool)."
  },
  {
    "objectID": "posts/seasons/index.html#how-it-works",
    "href": "posts/seasons/index.html#how-it-works",
    "title": "Why Do We Have Seasons?",
    "section": "How It Works",
    "text": "How It Works\nDirect sunlight concentrates energy in a small area — that’s summer. Angled sunlight spreads the same energy over a larger area — that’s winter. It’s like a flashlight: shine it straight down and you get a bright spot; tilt it and the light spreads out and dims.\nA mind-blowing fact: Earth is actually 3 million miles closer to the Sun during Northern Hemisphere winter! But 3 million out of 93 million is only ~3%. The angle of sunlight matters far more than distance.\nFor a deeper exploration with orbits, speed controls, and camera modes, see the full Earth & Sun Explorer."
  },
  {
    "objectID": "posts/expectations/index.html",
    "href": "posts/expectations/index.html",
    "title": "Expectations and the Law of Large Numbers",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\\[ \\newcommand{\\E}[1]{E[#1]}\\]\nThe expectation value of a quantity with respect to the a density or probability mass function is the weighted sum of the quantity where the weights are probabilties from the distribution. For example, for the discrete random variable \\(X\\):\n\\[E_f[X] = \\sum_x x\\,f(x).\\]\nIn the continuous case the sum is replaced by an integral over the density:\n\\[E_f[X] = \\int x\\,f(x) dx = \\int x dF(x),\\]\nwhere the latter form makes it clear that you are weighing with probabilities from the distribution even in the continuous case.\nThe latter form is often used to establish notation. Thus, the expected value, or mean, or first moment, of X is defined to be \\[\nE_{f}{X} = \\int x dF(x) =\n\\begin{cases}\n\\sum_x x f(x) & \\text{if X is discrete}\\\\\n\\int x f(x) dx & \\text{if X is continuous}\n\\end{cases}\n\\] assuming that the sum (or integral) is well defined. The notation is a unifying notation which nevertheless has a grounding in measure theory; the discrete sum can be said to be an integral with respect to a counting measure.\nA note on notation: we’ll use \\(E_f\\) or sometimes even \\(E_F\\) when we need to make clear what the distribution is. If its clear (or we are being lazy) we might just drop the subscript. Nevertheless, wheneve you see an expectation, YOU MUST ASK, with what density/mass-function or distribution is it with respect to.\n\n\n\\(E_f[X]\\) if often just called the mean of the mass function or density. This definition is analogous to the one for the arithmetic mean of a dataset: the only difference is that we want to give more weight to more probable values.\n\n\n\nAlso known as The rule of the lazy statistician.\nTheorem:\nif \\(Y = r(X)\\), \\[\n\\E{Y} = \\int r(x) dF(x)\n\\]\nExample:\nSpecifically, let A be an event and let \\(r(x) = I_A (x)\\) where \\(I_A (x) = 1\\) if \\(x \\in A\\) and \\(I_A (x) = 0\\) if \\(x \\notin A\\). Then: \\[\n\\E{I_A (X)} = \\int I_A (x) dF(x) = \\int_A f_X (x) dx = p(X \\in A)\n\\]\n\n\n\nThe variance of a distribution is defined analogous to that of a dataset:\n\\[V_f[X] = E_f[(X-E_f[X])^2]\\].\nFor the Bernoulli distribution \\(f(x)=p=constant\\), and you are summing it over ones as opposed to 0’s, so the mean is just p. The variance is \\((1-p)^2\\times p +(-p)^2\\times (1-p) = p(1-p)(1-p+p) = p(1-p)\\).\nIn general, we can find this mean that by obtaining a large bunch of samples from the distribution and find their arithmetic mean. The justification for this is the Law of large numbers, which we’ll come to soon.\nHowever the intuition is obvious: for a large number of samples, the frequencies will tract probabilities well, so high probability samples with roughly the same value will re-occur, and a simple arithmetic sun will capture the curves of the distribution.\n\n\n\n\nImagine a sequence of length n of coin flips. Lets keep increasing the length of the sequence of coin flips n, and compute a running average \\(S_n\\) of the coin-flip random variables, \\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i .\\] We plot this running mean, and notice that it converges to the mean of the distribution from which the random variables are plucked, ie the Bernoulli distribution with p=0.5.\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\nrandom_flips = throw_a_coin(10000)\nrunning_means = np.zeros(10000)\nsequence_lengths = np.arange(1,10001,1)\nfor i in sequence_lengths:\n    running_means[i-1] = np.mean(random_flips[:i])\n\n\nplt.plot(sequence_lengths, running_means);\nplt.xscale('log')\n\n\n\n\n\n\n\n\nThis is an example of a very important theorem in statistics, the law of large numbers, which says this:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) values from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to the mean of \\(X\\) \\(\\mu\\) as \\(n \\to \\infty\\):\n\\[ S_n \\to \\mu \\, as \\, n \\to \\infty. \\]\n\n\n\nThe law of large numbers is what makes the frequentist interpretation of probability possible to use in practise.\nWe saw above from the LOTUS that if we consider any event \\(A\\) from a probability distribution \\(F\\) with random variable X, and consider the indicator function \\(I_A\\) such that:\n\\[\\begin{eqnarray}\nI_A(x) = 1 \\,&& if \\, x \\in A\\\\\nI_A(x) = 0 \\,&&  otherwise\n\\end{eqnarray}\\]\nwe have that:\n\\[E_{F}[I_A (X)] = p(X \\in A)\\]\nOne can think of variable \\(Z=I_A(X)\\) as Bernoulli random variable with parameter and thus p = P(A). The question then arises: how do we estimate this expectation value and thus the probability?\nNow if we take a long sequence from \\(X\\) and thus \\(Z\\), then the frequency of successes (where success means being in A) will converge by the law of large numbers to the true probability p."
  },
  {
    "objectID": "posts/expectations/index.html#expectations",
    "href": "posts/expectations/index.html#expectations",
    "title": "Expectations and the Law of Large Numbers",
    "section": "",
    "text": "\\[ \\newcommand{\\E}[1]{E[#1]}\\]\nThe expectation value of a quantity with respect to the a density or probability mass function is the weighted sum of the quantity where the weights are probabilties from the distribution. For example, for the discrete random variable \\(X\\):\n\\[E_f[X] = \\sum_x x\\,f(x).\\]\nIn the continuous case the sum is replaced by an integral over the density:\n\\[E_f[X] = \\int x\\,f(x) dx = \\int x dF(x),\\]\nwhere the latter form makes it clear that you are weighing with probabilities from the distribution even in the continuous case.\nThe latter form is often used to establish notation. Thus, the expected value, or mean, or first moment, of X is defined to be \\[\nE_{f}{X} = \\int x dF(x) =\n\\begin{cases}\n\\sum_x x f(x) & \\text{if X is discrete}\\\\\n\\int x f(x) dx & \\text{if X is continuous}\n\\end{cases}\n\\] assuming that the sum (or integral) is well defined. The notation is a unifying notation which nevertheless has a grounding in measure theory; the discrete sum can be said to be an integral with respect to a counting measure.\nA note on notation: we’ll use \\(E_f\\) or sometimes even \\(E_F\\) when we need to make clear what the distribution is. If its clear (or we are being lazy) we might just drop the subscript. Nevertheless, wheneve you see an expectation, YOU MUST ASK, with what density/mass-function or distribution is it with respect to.\n\n\n\\(E_f[X]\\) if often just called the mean of the mass function or density. This definition is analogous to the one for the arithmetic mean of a dataset: the only difference is that we want to give more weight to more probable values.\n\n\n\nAlso known as The rule of the lazy statistician.\nTheorem:\nif \\(Y = r(X)\\), \\[\n\\E{Y} = \\int r(x) dF(x)\n\\]\nExample:\nSpecifically, let A be an event and let \\(r(x) = I_A (x)\\) where \\(I_A (x) = 1\\) if \\(x \\in A\\) and \\(I_A (x) = 0\\) if \\(x \\notin A\\). Then: \\[\n\\E{I_A (X)} = \\int I_A (x) dF(x) = \\int_A f_X (x) dx = p(X \\in A)\n\\]\n\n\n\nThe variance of a distribution is defined analogous to that of a dataset:\n\\[V_f[X] = E_f[(X-E_f[X])^2]\\].\nFor the Bernoulli distribution \\(f(x)=p=constant\\), and you are summing it over ones as opposed to 0’s, so the mean is just p. The variance is \\((1-p)^2\\times p +(-p)^2\\times (1-p) = p(1-p)(1-p+p) = p(1-p)\\).\nIn general, we can find this mean that by obtaining a large bunch of samples from the distribution and find their arithmetic mean. The justification for this is the Law of large numbers, which we’ll come to soon.\nHowever the intuition is obvious: for a large number of samples, the frequencies will tract probabilities well, so high probability samples with roughly the same value will re-occur, and a simple arithmetic sun will capture the curves of the distribution."
  },
  {
    "objectID": "posts/expectations/index.html#the-law-of-large-numbers",
    "href": "posts/expectations/index.html#the-law-of-large-numbers",
    "title": "Expectations and the Law of Large Numbers",
    "section": "",
    "text": "Imagine a sequence of length n of coin flips. Lets keep increasing the length of the sequence of coin flips n, and compute a running average \\(S_n\\) of the coin-flip random variables, \\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i .\\] We plot this running mean, and notice that it converges to the mean of the distribution from which the random variables are plucked, ie the Bernoulli distribution with p=0.5.\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\nrandom_flips = throw_a_coin(10000)\nrunning_means = np.zeros(10000)\nsequence_lengths = np.arange(1,10001,1)\nfor i in sequence_lengths:\n    running_means[i-1] = np.mean(random_flips[:i])\n\n\nplt.plot(sequence_lengths, running_means);\nplt.xscale('log')\n\n\n\n\n\n\n\n\nThis is an example of a very important theorem in statistics, the law of large numbers, which says this:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) values from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to the mean of \\(X\\) \\(\\mu\\) as \\(n \\to \\infty\\):\n\\[ S_n \\to \\mu \\, as \\, n \\to \\infty. \\]"
  },
  {
    "objectID": "posts/expectations/index.html#frequentist-interpretation-of-probability",
    "href": "posts/expectations/index.html#frequentist-interpretation-of-probability",
    "title": "Expectations and the Law of Large Numbers",
    "section": "",
    "text": "The law of large numbers is what makes the frequentist interpretation of probability possible to use in practise.\nWe saw above from the LOTUS that if we consider any event \\(A\\) from a probability distribution \\(F\\) with random variable X, and consider the indicator function \\(I_A\\) such that:\n\\[\\begin{eqnarray}\nI_A(x) = 1 \\,&& if \\, x \\in A\\\\\nI_A(x) = 0 \\,&&  otherwise\n\\end{eqnarray}\\]\nwe have that:\n\\[E_{F}[I_A (X)] = p(X \\in A)\\]\nOne can think of variable \\(Z=I_A(X)\\) as Bernoulli random variable with parameter and thus p = P(A). The question then arises: how do we estimate this expectation value and thus the probability?\nNow if we take a long sequence from \\(X\\) and thus \\(Z\\), then the frequency of successes (where success means being in A) will converge by the law of large numbers to the true probability p."
  },
  {
    "objectID": "posts/votingforcongress/index.html",
    "href": "posts/votingforcongress/index.html",
    "title": "Some Data Analysis about Congress",
    "section": "",
    "text": "PredictWise congressional voting visualization\n\n\n\nimport pandas as pd\n\n\ntbl = pd.read_html(\"https://www.presidency.ucsb.edu/statistics/data/seats-congress-gainedlost-the-presidents-party-mid-term-elections\")\n\n\ndf = tbl[0]\ndf.columns = df.columns.to_flat_index()\ndf\n\n\n\n\n\n\n\n\n(Unnamed: 0_level_0, Year)\n(Unnamed: 1_level_0, Lame Duck?)\n(Unnamed: 2_level_0, President)\n(Unnamed: 3_level_0, President'sParty)\n(President's Job Approval Percentage (Gallup) As of:, Early Aug)\n(President's Job Approval Percentage (Gallup) As of:, Late Aug)\n(President's Job Approval Percentage (Gallup) As of:, Early Sep)\n(President's Job Approval Percentage (Gallup) As of:, Late Sep)\n(President's Job Approval Percentage (Gallup) As of:, Early Oct)\n(President's Job Approval Percentage (Gallup) As of:, Late Oct)\n(President's Party, House Seatsto Defend)\n(President's Party, Senate Seatsto Defend)\n(Seat Change, President's Party, House Seats)\n(Seat Change, President's Party, Senate Seats)\n\n\n\n\n0\n1934\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n--\n313\n14\n+9\n+9\n\n\n1\n1938\nNaN\nFranklin D. Roosevelt\nD\n--\n--\n--\n--\n--\n60\n334\n27\n-81\n-7\n\n\n2\n1942\nNaN\nFranklin D. Roosevelt\nD\n74\n--\n74\n--\n--\n--\n267\n25\n-46\n-9\n\n\n3\n1946\nNaN\nHarry S. Truman\nD\n--\n--\n33\n--\n--\n27\n244\n21\n-45\n-12\n\n\n4\n1950\nLD*\nHarry S. Truman\nD\nnd\n43\n35\n35\n43\n41\n263\n21\n-29\n-6\n\n\n5\n1954\nNaN\nDwight D. Eisenhower\nR\n67\n62\n--\n66\n62\n--\n221\n11\n-18\n-1\n\n\n6\n1958\nLD\nDwight D. Eisenhower\nR\n58\n56\n56\n54\n57\n--\n203\n20\n-48\n-13\n\n\n7\n1962\nNaN\nJohn F. Kennedy\nD\n--\n67\n--\n63\n--\n61\n264\n18\n-4\n+3\n\n\n8\n1966\n†\nLyndon B. Johnson\nD\n51\n47\n--\n--\n44\n44\n295\n21\n-47\n-4\n\n\n9\n1970\nNaN\nRichard Nixon\nR\n55\n55\n57\n51\n58\n--\n192\n7\n-12\n+2\n\n\n10\n1974\n±\nGerald R. Ford (Nixon)\nR\n71\n--\n66\n50\n53\n--\n192\n15\n-48\n-5\n\n\n11\n1978\nNaN\nJimmy Carter\nD\n43\n43\n48\n--\n49\n45\n292\n14\n-15\n-3\n\n\n12\n1982\nNaN\nRonald Reagan\nR\n41\n42\n--\n42\n--\n42\n192\n12\n-26\n+1\n\n\n13\n1986\nLD\nRonald Reagan\nR\n--\n64\n--\n63\n64\n--\n181\n22\n-5\n-8\n\n\n14\n1990\nNaN\nGeorge Bush\nR\n75\n73\n54\n--\n--\n57\n175\n17\n-8\n-1\n\n\n15\n1994\nNaN\nWilliam J. Clinton\nD\n43\n40\n40\n44\n43\n48\n258\n17\n-52\n-8\n\n\n16\n1998\nLD\nWilliam J. Clinton\nD\n65\n62\n63\n66\n65\n65\n207\n18\n+5\n0\n\n\n17\n2002\nNaN\nGeorge W. Bush\nR\n--\n66\n66\n66\n68\n67\n220\n20\n+8\n+2\n\n\n18\n2006\nLD\nGeorge W. Bush\nR\n37\n42\n39\n44\n37\n37\n233\n15\n-30\n-6\n\n\n19\n2010\nNaN\nBarack Obama\nD\n44\n44\n45\n45\n45\n45\n257\n15\n-63\n-6\n\n\n20\n2014\nLD\nBarack Obama\nD\n42\n42\n41\n43\n42\n41\n201\n20\n-13\n-9\n\n\n21\n2018\nNaN\nDonald J. Trump\nR\n41\n41\n39\n41\n44\n44\n241\n9\n-40\n+2\n\n\n22\n2022\nNaN\nJoseph R. Biden\nD\n38\n44\n44\n42\n42\nNaN\n222\n14\nTBD\nTBD"
  },
  {
    "objectID": "posts/lawoflargenumbers.html",
    "href": "posts/lawoflargenumbers.html",
    "title": "The LLN",
    "section": "",
    "text": "Suppose that you toss a fair coin and catch it to see if you got heads or tails. Then you have this intuition that while you might get a streak of several heads in a row, in the long run the heads and tails are balanced.\nThis is actually an example of a famous law: the Law of Large numbers (LLN), which states that if you have a random variable X with a mean, the average value of X over a sample of size N converges i.e. gets close and closer to this mean as N becomes larger and larger.\n\n\n\nThe LLN was first proved by Jakob Bernoulli in Ars Conjectandi, published posthumously by his nephew Niklaus Bernoulli, who appropriated entire passages of it for his treatise on law. It is the basis of much of modern statistics, including the Monte-Carlo method.\nLets parse the law. A random variable is one that can take multiple values, each with some probability. So if X represents the flip of a coin, it will take values Heads and Tails with some probability. We’ll assign Heads the value 1 and Tails the value 0.\nThe probabilities attatched to the values a random variable takes is called a distribution, or probability mass function (pmf). For a fair coin, the “Bernoulli” Distribution attaches the probabilities 0.5 to value 1 and 0.5 to value 0. These probabilities must add to 1.\n\n\n\nAn unfair coin thats more likely to land on heads might have a distribution where 0 has attached probability 0.4 and 1 has attached probability 0.6. In this case the mean µ of the distribution is 0.4 x 0 + 0.6 x 1 = 0.6.\n\n\n\nThis mean does not need to be one of the allowed values of the distribution (here 0 and 1). The mean here simply indicates whats more likely: 0.6 means that heads is more likely than tails. What is the mean in the case of the fair coin?\nNow let us simulate the case of the fair coin. We’ll toss a sample of N coins, or 1 coin N times, using the magic of numpy. We’ll find the average of these N tosses. This is the fraction of heads! We’ll plot this sample average against the sample size N.\n\n\n\nWe find that these sample averages are quite close to 0.5. And, as we increase the sample size N, these sample averages become super close to 0.5. Indeed, as N becomes infinite, the sample averages approach the mean µ=0.5. This is the Law of Large Numbers.\n\n\n\nThe LLN can be tautologically used to define the probability of a fair coin showing heads as the asymptotic (infinite N) sampling average. This is the frequentist definition of “sampling probability”, the population frequency µ.\nBut we might also treat the mean µ as an intrinsic fraction of heads, a “parameter” of the Bernoulli distribution. Where does it come from in the first place? The value µ can be thought of as an “inferential probability” derived from symmetry and lack of knowledge.\n\n\n\nIf you have a coin (2 sides, 2 possibilities), and no additional information about the coin and toss physics (thus fair), you would guess fraction µ=0.5 for heads. The LLN then says that sampling probabilities converge to this “inferential probability”.\n\nwh"
  },
  {
    "objectID": "posts/distributions.html",
    "href": "posts/distributions.html",
    "title": "Distributions",
    "section": "",
    "text": "Remember that a Random Variable is a mapping $ X: $ that assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\) in a sample space \\(\\Omega\\). The definitions below are taken from Larry Wasserman’s All of Statistics."
  },
  {
    "objectID": "posts/distributions.html#cumulative-distribution-function",
    "href": "posts/distributions.html#cumulative-distribution-function",
    "title": "Distributions",
    "section": "Cumulative distribution Function",
    "text": "Cumulative distribution Function\nThe cumulative distribution function, or the CDF, is a function\n\\[F_X : \\mathbb{R} → [0, 1] \\],\ndefined by\n\\[F_X (x) = p(X \\le x).\\]\nA note on notation: \\(X\\) is a random variable while \\(x\\) is a particular value of the random variable.\nLet \\(X\\) be the random variable representing the number of heads in two coin tosses. Then \\(x\\) can take on values 0, 1 and 2. The CDF for this random variable can be drawn thus (taken from All of Stats):\n\n\n\nCDF of the 2-coin-toss distribution\n\n\nNotice that this function is right-continuous and defined for all \\(x\\), even if $x $does not take real values in-between the integers."
  },
  {
    "objectID": "posts/distributions.html#probability-mass-and-distribution-function",
    "href": "posts/distributions.html#probability-mass-and-distribution-function",
    "title": "Distributions",
    "section": "Probability Mass and Distribution Function",
    "text": "Probability Mass and Distribution Function\n\\(X\\) is called a discrete random variable if it takes countably many values \\(\\{x_1, x_2,…\\}\\). We define the probability function or the probability mass function (pmf) for X by:\n\\[f_X(x) = p(X=x)\\]\n\\(f_X\\) is a probability.\nThe pmf for the number of heads in two coin tosses (taken from All of Stats) looks like this:\n\n\n\nPMF of the 2-coin-toss distribution\n\n\nOn the other hand, a random variable is called a continuous random variable if there exists a function \\(f_X\\) such that \\(f_X (x) \\ge 0\\) for all x, \\(\\int_{-\\infty}^{\\infty} f_X (x) dx = 1\\) and for every a ≤ b,\n\\[p(a &lt; X &lt; b) = \\int_{a}^{b} f_X (x) dx\\]\nThe function \\(f_X\\) is called the probability density function (pdf). We have the CDF:\n\\[F_X (x) = \\int_{-\\infty}^{x}f_X (t) dt \\]\nand \\(f_X (x) = \\frac{d F_X (x)}{dx}\\) at all points x at which \\(F_X\\) is differentiable.\nContinuous variables are confusing. Note:\n\n\\(p(X=x) = 0\\) for every \\(x\\). You cant think of \\(f_X(x)\\) as \\(p(X=x)\\). This holds only for discretes. You can only get probabilities from a pdf by integrating, if only over a very small paty of the space.\nA pdf can be bigger than 1 unlike a probability mass function, since probability masses represent actual probabilities.\n\n\nA continuous example: the Uniform Distribution\nSuppose that X has pdf \\[\nf_X (x) =\n\\begin{cases}\n1 & \\text{for } 0 \\leq x\\leq 1\\\\\n    0             & \\text{otherwise.}\n\\end{cases}\n\\] A random variable with this density is said to have a Uniform (0,1) distribution. This is meant to capture the idea of choosing a point at random between 0 and 1. The cdf is given by: \\[\nF_X (x) =\n\\begin{cases}\n0 & x \\le 0\\\\\nx & 0 \\leq x \\leq 1\\\\\n1 & x &gt; 1.\n\\end{cases}\n\\] and can be visualized as so (again from All of Stats):\n\n\n\nCDF of the uniform distribution\n\n\n\n\nA discrete example: the Bernoulli Distribution\nThe Bernoulli Distribution represents the distribution a coin flip. Let the random variable \\(X\\) represent such a coin flip, where \\(X=1\\) is heads, and \\(X=0\\) is tails. Let us further say that the probability of heads is \\(p\\) (\\(p=0.5\\) is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p)\\]\nwhich is to be read as \\(X\\) has distribution \\(Bernoulli(p)\\). The pmf or probability function associated with the Bernoulli distribution is \\[\nf(x) =\n\\begin{cases}\n1 - p & x = 0\\\\\np & x = 1.\n\\end{cases}\n\\]\nfor p in the range 0 to 1. This pmf may be written as\n\\[f(x) = p^x (1-p)^{1-x}\\]\nfor x in the set {0,1}.\n\\(p\\) is called a parameter of the Bernoulli distribution."
  },
  {
    "objectID": "posts/distributions.html#conditional-and-marginal-distributions",
    "href": "posts/distributions.html#conditional-and-marginal-distributions",
    "title": "Distributions",
    "section": "Conditional and Marginal Distributions",
    "text": "Conditional and Marginal Distributions\nMarginal mass functions are defined in analog to probabilities. Thus:\n\\[f_X(x) = p(X=x) =  \\sum_y f(x, y);\\,\\, f_Y(y) = p(Y=y) = \\sum_x f(x,y).\\]\nSimilarly, marginal densities are defined using integrals:\n\\[f_X(x) = \\int dy f(x,y);\\,\\, f_Y(y) = \\int dx f(x,y).\\]\nNotice there is no interpretation of the marginal densities in the continuous case as probabilities. An example here if \\(f(x,y) = e^{-(x+y)}\\) defined on the positive quadrant. The marginal is an exponential defined on the positive part of the line.\nConditional mass function is similarly, just a conditional probability. So:\n\\[f_{X \\mid Y}(x \\mid y) = p(X=x \\mid Y=y) = \\frac{p(X=x, Y=y)}{p(Y=y)} = \\frac{f_{XY}(x,y)}{f_Y(y)}\\]\nThe similar formula for continuous densities might be suspected to a bit more complex, because we are conditioning on the event \\(Y=y\\) which strictly speaking has 0 probability. But it can be proved that the same formula holds for densities with some additional requirements and interpretation:\n\\[f_{X \\mid Y}(x \\mid y)  = \\frac{f_{XY}(x,y)}{f_Y(y)},\\]\nwhere we must assume that \\(f_Y(y) &gt; 0\\). Then we have the interpretation that for some event A:\n\\[p(X \\in A \\mid Y=y) = \\int_{x \\in A} f_{X \\mid Y}(x,y) dx.\\]\nAn example of this is the uniform distribution on the unit square. Suppose then that \\(y=0.3\\). Then the conditional density is a uniform density on the line between 0 and 1 at \\(y=0.3\\)."
  },
  {
    "objectID": "posts/montecarlointegrals/index.html",
    "href": "posts/montecarlointegrals/index.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nLet us formalize the basic idea behind Monte Carlo Integration in 1-D.\nConsider the definite integral:\n\\[ I = \\int_{a}^{b} f(x) \\, dx \\]\nConsider:\n\\[ J = \\int_{a}^{b} f(x) U_{ab}(x) \\, dx \\]\nIf \\(V\\) is the support of the uniform distribution on a to b then the pdf \\[ U_{ab}(x) = \\frac{1}{V} = \\frac{1}{b-a}\\]\nThen from LOTUS and the law of large numbers:\n\\[J = \\frac{1}{V}  \\int_{a}^{b} f(x) \\, dx  =  \\frac{I}{V} = E_{U}[f] = \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nor\n\\[I  = V  \\times \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nPractically speaking, our estimate will only be as exact as the number of samples we draw, but more on this soon..\n\n\n**Calculate the integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx. $**\nWe know from calculus that the anti-derivative is \\[ x^3/3 + 4\\sin(x) -4x\\cos(x). \\]\nTo solve this using MC, we simply draw \\(N\\) random numbers from 2 to 3 and then take the average of all the values \\(f(x)=x^2 + 4 \\, x \\,\\sin(x)\\) and normalized over the volume; this case the volume is 1 (3-2=1).\n\ndef f(x):\n    return x**2 + 4*x*np.sin(x) \n\ndef intf(x): \n    return x**3/3.0+4.0*np.sin(x) - 4.0*x*np.cos(x) \n\n\na = 2;    \nb = 3; \n\n# use N draws \nN= 10000\n\nX = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \nY =f(X)   # CALCULATE THE f(x) \nV = b-a\nImc= V * np.sum(Y)/ N;\n\nexactval=intf(b)-intf(a)\n\nprint(\"Monte Carlo estimation=\",Imc, \"Exact number=\", intf(b)-intf(a))\n\nMonte Carlo estimation= 11.8120823531 Exact number= 11.8113589251\n\n\n\n\n\nThat is nice but how about a multidimensional case?\nLet us calculate the two dimensional integral \\(I=\\int \\int f(x, y) dx dy\\) where \\(f(x,y) = x^2 +y^2\\) over the region deﬁned by the condition \\(x^2 +y^2 ≤ 1\\)\nIn other words we are talking about a uniform distribution on the unit circle\n\nfmd = lambda x,y: x*x + y*y\n\n\n# use N draws \nN= 8000\nX= np.random.uniform(low=-1, high=1, size=N) \nY= np.random.uniform(low=-1, high=1, size=N) \nZ=fmd(X, Y)   # CALCULATE THE f(x) \n\nR = X**2 + Y**2\nV = np.pi*1.0*1.0\nN = np.sum(R&lt;1)\nsumsamples = np.sum(Z[R&lt;1])\n\nprint(\"I=\",V*sumsamples/N, \"actual\", np.pi/2.0) #actual value (change to polar to calculate)\n\nI= 1.56308724855 actual 1.5707963267948966\n\n\n\n\n\n\nHow does the accuracy depends on the number of points(samples)? Lets try the same 1-D integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx $ as a function of the number of points.\n\nImc=np.zeros(1000)\nNa = np.linspace(0,1000,1000)\n\nexactval= intf(b)-intf(a)\n\nfor N in np.arange(0,1000):\n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[N]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.plot(Na[10:],np.sqrt((Imc[10:]-exactval)**2), alpha=0.7)\nplt.plot(Na[10:], 1/np.sqrt(Na[10:]), 'r')\nplt.xlabel(\"N\")\nplt.ylabel(\"sqrt((Imc-ExactValue)$^2$)\")\n\n# \n\n\n\n\n\n\n\n\nObviously this depends on the number of \\(N\\) as \\(1/\\sqrt{N}\\).\n\n\n\nMonte Carlo methods yield approximate answers whose accuracy depends on the number of draws. So far, we have used our knowledge of the exact value to determine that the error in the Monte Carlo method approaches zero as approximately \\(1/\\sqrt{N}\\) for large \\(N\\), where \\(N\\) is the number of trials.\nBut in the usual case, the exact answer is unknown. Why do this otherwise?\nSo, lets repeat the same evaluation \\(m\\) times and check the variance of the estimate.\n\n# multiple MC estimations\nm=1000\nN=10000\nImc=np.zeros(m)\n\n\nfor i in np.arange(m):\n    \n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[i]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.hist(Imc, bins=30)\nplt.xlabel(\"Imc\")\nprint(np.mean(Imc), np.std(Imc))\n\n11.8114651823 0.00398497853806\n\n\n\n\n\n\n\n\n\nThis looks like our telltale Normal distribution.\nThis is not surprising\n\n\nWe know from the CLT that if \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\), and that if \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\).\nThen,\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nThis is true regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nThe sums\n\\[S_n(f) = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\]\nare exactly what we want to calculate for Monte-Carlo Integration(due to the LOTUS) and correspond to the random variable f(X) where X is uniformly distributed on the support.\nWhatever the original variance of f(X) might be, we can see that the variance of the sampling distribution of the mean goes down as \\(1/n\\) and thus the standard error goes down as \\(1/\\sqrt{n}\\) as we discovered when we compared it to the exact value as well.\nWhy is this important?\n\n\n\nWhat if we changed the dimensionality of the integral? The formula for \\(S_n\\) does not change, we just replace \\(g(x_i)\\) by \\(g(x_i, y_i, z_i...)\\). Thus, the CLT still holds and the error still scales as \\(\\frac{1}{\\sqrt{n}}\\).\nOn the other hand, if we divide the \\(a, b\\)-interval into \\(N\\) steps and use some regular integration routine, what is the error? Consider the midpoint rule as illustrated in this diagram from Wikipedia:\n\n\n\nRectangle rule for numerical integration\n\n\nThe basic idea is that the function value at the midpoint of the interval is used as the height of the approximating rectangle. In general, the differing methods consist of choosing different \\(x_i\\) below..with left being at the left end, right being at the right end. \\[I(est) = \\sum_i f(x_i)\\Delta x_i = \\frac{b-a}{n} \\sum_i f(x_i)\\]\nThe error on the estimation of the integral can be shown to decrease as \\(\\frac{1}{n^2}\\). The basic reason for this can be understood on a taylor series expansion of the function to second order. When you integrate on the sub-interval, the linear term vanishes while the quadratic term becomes cubic in \\(\\Delta x\\). So the local error goes as \\(\\frac{1}{n^3}\\) and thus the global as \\(\\frac{1}{n^2}\\).\nMonte-Carlo if clearly not competitive with the midpoint method in 1-D. Its actually not even competitive with left or right rectangle methods.\nThe trapeziod rule uses a line between the sub-interval points while the Simpsons rule uses a quadratic.\nThese integrations can be generalized to multiple dimensions, and the rule for these\n\nleft or right rule: \\(\\propto 1/n\\)\nMidpoint rule: \\(\\propto 1/n^2\\)\nTrapezoid: \\(\\propto 1/n^2\\)\nSimpson: \\(\\propto 1/n^4\\)\n\nwhere \\(n=N^{1/d}\\). MC becomes better than the Simpson method only in 8 dimensions.."
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#the-basic-idea",
    "href": "posts/montecarlointegrals/index.html#the-basic-idea",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "Let us formalize the basic idea behind Monte Carlo Integration in 1-D.\nConsider the definite integral:\n\\[ I = \\int_{a}^{b} f(x) \\, dx \\]\nConsider:\n\\[ J = \\int_{a}^{b} f(x) U_{ab}(x) \\, dx \\]\nIf \\(V\\) is the support of the uniform distribution on a to b then the pdf \\[ U_{ab}(x) = \\frac{1}{V} = \\frac{1}{b-a}\\]\nThen from LOTUS and the law of large numbers:\n\\[J = \\frac{1}{V}  \\int_{a}^{b} f(x) \\, dx  =  \\frac{I}{V} = E_{U}[f] = \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nor\n\\[I  = V  \\times \\lim_{n \\to \\infty} \\frac{1}{N}\\sum_{x_i \\sim U} f(x_i) \\]\nPractically speaking, our estimate will only be as exact as the number of samples we draw, but more on this soon..\n\n\n**Calculate the integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx. $**\nWe know from calculus that the anti-derivative is \\[ x^3/3 + 4\\sin(x) -4x\\cos(x). \\]\nTo solve this using MC, we simply draw \\(N\\) random numbers from 2 to 3 and then take the average of all the values \\(f(x)=x^2 + 4 \\, x \\,\\sin(x)\\) and normalized over the volume; this case the volume is 1 (3-2=1).\n\ndef f(x):\n    return x**2 + 4*x*np.sin(x) \n\ndef intf(x): \n    return x**3/3.0+4.0*np.sin(x) - 4.0*x*np.cos(x) \n\n\na = 2;    \nb = 3; \n\n# use N draws \nN= 10000\n\nX = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \nY =f(X)   # CALCULATE THE f(x) \nV = b-a\nImc= V * np.sum(Y)/ N;\n\nexactval=intf(b)-intf(a)\n\nprint(\"Monte Carlo estimation=\",Imc, \"Exact number=\", intf(b)-intf(a))\n\nMonte Carlo estimation= 11.8120823531 Exact number= 11.8113589251\n\n\n\n\n\nThat is nice but how about a multidimensional case?\nLet us calculate the two dimensional integral \\(I=\\int \\int f(x, y) dx dy\\) where \\(f(x,y) = x^2 +y^2\\) over the region deﬁned by the condition \\(x^2 +y^2 ≤ 1\\)\nIn other words we are talking about a uniform distribution on the unit circle\n\nfmd = lambda x,y: x*x + y*y\n\n\n# use N draws \nN= 8000\nX= np.random.uniform(low=-1, high=1, size=N) \nY= np.random.uniform(low=-1, high=1, size=N) \nZ=fmd(X, Y)   # CALCULATE THE f(x) \n\nR = X**2 + Y**2\nV = np.pi*1.0*1.0\nN = np.sum(R&lt;1)\nsumsamples = np.sum(Z[R&lt;1])\n\nprint(\"I=\",V*sumsamples/N, \"actual\", np.pi/2.0) #actual value (change to polar to calculate)\n\nI= 1.56308724855 actual 1.5707963267948966"
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#monte-carlo-as-a-function-of-number-of-samples",
    "href": "posts/montecarlointegrals/index.html#monte-carlo-as-a-function-of-number-of-samples",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "How does the accuracy depends on the number of points(samples)? Lets try the same 1-D integral $ I= _{2}^{3} [x^2 + 4 , x ,(x)] , dx $ as a function of the number of points.\n\nImc=np.zeros(1000)\nNa = np.linspace(0,1000,1000)\n\nexactval= intf(b)-intf(a)\n\nfor N in np.arange(0,1000):\n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[N]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.plot(Na[10:],np.sqrt((Imc[10:]-exactval)**2), alpha=0.7)\nplt.plot(Na[10:], 1/np.sqrt(Na[10:]), 'r')\nplt.xlabel(\"N\")\nplt.ylabel(\"sqrt((Imc-ExactValue)$^2$)\")\n\n# \n\n\n\n\n\n\n\n\nObviously this depends on the number of \\(N\\) as \\(1/\\sqrt{N}\\)."
  },
  {
    "objectID": "posts/montecarlointegrals/index.html#errors-in-mc",
    "href": "posts/montecarlointegrals/index.html#errors-in-mc",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "Monte Carlo methods yield approximate answers whose accuracy depends on the number of draws. So far, we have used our knowledge of the exact value to determine that the error in the Monte Carlo method approaches zero as approximately \\(1/\\sqrt{N}\\) for large \\(N\\), where \\(N\\) is the number of trials.\nBut in the usual case, the exact answer is unknown. Why do this otherwise?\nSo, lets repeat the same evaluation \\(m\\) times and check the variance of the estimate.\n\n# multiple MC estimations\nm=1000\nN=10000\nImc=np.zeros(m)\n\n\nfor i in np.arange(m):\n    \n    X = np.random.uniform(low=a, high=b, size=N) # N values uniformly drawn from a to b \n    Y =f(X)   # CALCULATE THE f(x) \n\n    Imc[i]= (b-a) * np.sum(Y)/ N;\n    \n    \nplt.hist(Imc, bins=30)\nplt.xlabel(\"Imc\")\nprint(np.mean(Imc), np.std(Imc))\n\n11.8114651823 0.00398497853806\n\n\n\n\n\n\n\n\n\nThis looks like our telltale Normal distribution.\nThis is not surprising\n\n\nWe know from the CLT that if \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\), and that if \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\).\nThen,\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nThis is true regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nThe sums\n\\[S_n(f) = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\]\nare exactly what we want to calculate for Monte-Carlo Integration(due to the LOTUS) and correspond to the random variable f(X) where X is uniformly distributed on the support.\nWhatever the original variance of f(X) might be, we can see that the variance of the sampling distribution of the mean goes down as \\(1/n\\) and thus the standard error goes down as \\(1/\\sqrt{n}\\) as we discovered when we compared it to the exact value as well.\nWhy is this important?\n\n\n\nWhat if we changed the dimensionality of the integral? The formula for \\(S_n\\) does not change, we just replace \\(g(x_i)\\) by \\(g(x_i, y_i, z_i...)\\). Thus, the CLT still holds and the error still scales as \\(\\frac{1}{\\sqrt{n}}\\).\nOn the other hand, if we divide the \\(a, b\\)-interval into \\(N\\) steps and use some regular integration routine, what is the error? Consider the midpoint rule as illustrated in this diagram from Wikipedia:\n\n\n\nRectangle rule for numerical integration\n\n\nThe basic idea is that the function value at the midpoint of the interval is used as the height of the approximating rectangle. In general, the differing methods consist of choosing different \\(x_i\\) below..with left being at the left end, right being at the right end. \\[I(est) = \\sum_i f(x_i)\\Delta x_i = \\frac{b-a}{n} \\sum_i f(x_i)\\]\nThe error on the estimation of the integral can be shown to decrease as \\(\\frac{1}{n^2}\\). The basic reason for this can be understood on a taylor series expansion of the function to second order. When you integrate on the sub-interval, the linear term vanishes while the quadratic term becomes cubic in \\(\\Delta x\\). So the local error goes as \\(\\frac{1}{n^3}\\) and thus the global as \\(\\frac{1}{n^2}\\).\nMonte-Carlo if clearly not competitive with the midpoint method in 1-D. Its actually not even competitive with left or right rectangle methods.\nThe trapeziod rule uses a line between the sub-interval points while the Simpsons rule uses a quadratic.\nThese integrations can be generalized to multiple dimensions, and the rule for these\n\nleft or right rule: \\(\\propto 1/n\\)\nMidpoint rule: \\(\\propto 1/n^2\\)\nTrapezoid: \\(\\propto 1/n^2\\)\nSimpson: \\(\\propto 1/n^4\\)\n\nwhere \\(n=N^{1/d}\\). MC becomes better than the Simpson method only in 8 dimensions.."
  },
  {
    "objectID": "posts/boxloop.html",
    "href": "posts/boxloop.html",
    "title": "Box’s Loop",
    "section": "",
    "text": "In the 1960’s, the great statistician Box, along with his collaborators, formulated the notion of a loop to understand the nature of the scientific method. This loop is called Box’s loop by Blei et. al., 1, and illustrated in the diagram (taken from the above linked paper) below:\n1 Blei, David M. “Build, compute, critique, repeat: Data analysis with latent variable models.” Annual Review of Statistics and Its Application 1 (2014): 203-232.\n\n\nBox’s loop: Build, Infer, Criticize, Apply\n\n\nBox himself focussed on the scientific method, but the loop is applicable at large to other examples of probabilistic modelling, such as the building of an information retrieval or recommendation system, exploratory data analysis, etc, etc\nWe:\n\nfirst build a model. This is as much as an art as a science if we are of the philosophical bent that we desire explainability. We bring in domain experts.\nWe compute a model using the observed data.\nWe then critique our model, studying how they succeed or fail and how they predict future data or on held out sets.\nIf we are satisfied with the performance of our model we apply it in the context of a predictive or explanatory system. If we are not, we go back to 1.\n\nIf we are Bayesians, we compute the posterior distribution (the distribution of the parameters conditioned on the data) of the (hidden) parameters of the model. Here we assume that the data is fixed and our stochasticity is in the parameters.\nIf we are Frequentists, we assume our data is a sample from a population and compute the parameters of our models abd confidence intervals for those parameters. Here we assume that the data is stochastic as in we could get multiple different samplkes, but that the parameter is fixed and given.\nWe could have mis-specified our model. It might be too simple or too complex. If so we go back to (1) and try again with another model specification."
  },
  {
    "objectID": "posts/distrib-example/index.html",
    "href": "posts/distrib-example/index.html",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\nIn the last section, we made a simple simulation of a coin-toss on the computer from a fair-coin model which associated equal probability with heads and tails. Let us consider another model here, a table of probabilities that PredictWise made on October 2, 2012 for the US presidential elections. PredictWise aggregated polling data and, for each state, estimated the probability that the Obama or Romney would win. Here are those estimated probabilities:\n\npredictwise = pd.read_csv('assets/predictwise.csv').set_index('States')\npredictwise.head()\n\n\n\n\n\n\n\nObama\nRomney\nVotes\n\n\nStates\n\n\n\n\n\n\n\nAlabama\n0.000\n1.000\n9\n\n\nAlaska\n0.000\n1.000\n3\n\n\nArizona\n0.062\n0.938\n11\n\n\nArkansas\n0.000\n1.000\n6\n\n\nCalifornia\n1.000\n0.000\n55\n\n\n\n\n\n\n\nSay you toss a coin and have a model which says that the probability of heads is 0.5 (you have figured this out from symmetry, or physics, or something). Still, there will be sequences of flips in which more or less than half the flips are heads. These fluctuations induce a distribution on the number of heads (say k) in N coin tosses (this is a binomial distribution).\nSimilarly, here, if the probability of Romney winning in Arizona is 0.938, it means that if somehow, there were 10000 replications (as if we were running the election in 10000 parallel universes) with an election each, Romney would win in 9380 of those Arizonas on the average across the replications. And there would be some replications with Romney winning more, and some with less. We can run these simulated universes or replications on a computer though not in real life.\n\n\nTo do this, we will assume that the outcome in each state is the result of an independent coin flip whose probability of coming up Obama is given by the Predictwise state-wise win probabilities. Lets write a function simulate_election that uses this predictive model to simulate the outcome of the election given a table of probabilities.\n\n\nThe Bernoulli Distribution represents the distribution for coin flips. Let the random variable X represent such a coin flip, where X=1 is heads, and X=0 is tails. Let us further say that the probability of heads is p (p=0.5 is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p),\\]\nwhich is to be read as X has distribution Bernoulli(p). The probability distribution function (pdf) or probability mass function associated with the Bernoulli distribution is\n\\[\\begin{eqnarray}\nP(X = 1) &=& p \\\\\nP(X = 0) &=& 1 - p\n\\end{eqnarray}\\]\nfor p in the range 0 to 1. The pdf, or the probability that random variable \\(X=x\\) may thus be written as\n\\[P(X=x) = p^x(1-p)^{1-x}\\]\nfor x in the set {0,1}.\nThe Predictwise probability of Obama winning in each state is a Bernoulli Parameter. You can think of it as a different loaded coin being tossed in each state, and thus there is a bernoulli distribution for each state\nNote: some of the code, and ALL of the visual style for the distribution plots below was shamelessly stolen from https://gist.github.com/mattions/6113437/ .\n\nfrom scipy.stats import bernoulli\n#bernoulli random variable\nbrv=bernoulli(p=0.3)\nprint(brv.rvs(size=20))\nevent_space=[0,1]\nplt.figure(figsize=(12,8))\ncolors=sns.color_palette()\nfor i, p in enumerate([0.1, 0.2, 0.5, 0.7]):\n    ax = plt.subplot(1, 4, i+1)\n    plt.bar(event_space, bernoulli.pmf(event_space, p), label=p, color=colors[i], alpha=0.5)\n    plt.plot(event_space, bernoulli.cdf(event_space, p), color=colors[i], alpha=0.5)\n\n    ax.xaxis.set_ticks(event_space)\n   \n    plt.ylim((0,1))\n    plt.legend(loc=0)\n    if i == 0:\n        plt.ylabel(\"PDF at $k$\")\nplt.tight_layout()\n\n[1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0]\n\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code below, each column simulates a single outcome from the 50 states + DC by choosing a random number between 0 and 1. Obama wins that simulation if the random number is \\(&lt;\\) the win probability. If he wins that simulation, we add in the electoral votes for that state, otherwise we dont. We do this n_sim times and return a list of total Obama electoral votes in each simulation.\n\ndef simulate_election(model, n_sim):\n    simulations = np.random.uniform(size=(51, n_sim))\n    obama_votes = (simulations &lt; model.Obama.values.reshape(-1, 1)) * model.Votes.values.reshape(-1, 1)\n    #summing over rows gives the total electoral votes for each simulation\n    return obama_votes.sum(axis=0)\n\nThe first thing to pick up on here is that np.random.uniform gives you a random number between 0 and 1, uniformly. In other words, the number is equally likely to be between 0 and 0.1, 0.1 and 0.2, and so on. This is a very intuitive idea, but it is formalized by the notion of the Uniform Distribution.\nWe then say:\n\\[X \\sim Uniform([0,1),\\]\nwhich is to be read as X has distribution Uniform([0,1]). The probability distribution function (pdf) associated with the Uniform distribution is\n\\[\\begin{eqnarray}\nP(X = x) &=& 1 \\, for \\, x \\in [0,1] \\\\\nP(X = x) &=& 0 \\, for \\, x \\notin [0,1]\n\\end{eqnarray}\\]\nWhat assigning the vote to Obama when the random variable drawn from the Uniform distribution is less than the Predictwise probability of Obama winning (which is a Bernoulli Parameter) does for us is this: if we have a large number of simulations and \\(p_{Obama}=0.7\\) , then 70% of the time, the random numbes drawn will be below 0.7. And then, assigning those as Obama wins will hew to the frequentist notion of probability of the Obama win. But remember, of course, that in 30% of the simulations, Obama wont win, and this will induce fluctuations and a distribution on the total number of electoral college votes that Obama gets. And this is what we see in the histogram below.\nThe following code takes the necessary probabilities for the Predictwise data, and runs 10000 simulations. If you think of this in terms of our coins, think of it as having 51 biased coins, one for each state, and tossing them 10,000 times each.\nWe use the results to compute the number of simulations, according to this predictive model, that Obama wins the election (i.e., the probability that he receives 269 or more electoral college votes)\n\nresult = simulate_election(predictwise, 10000)\nprint((result &gt;= 269).sum())\n\n9955\n\n\n\nresult\n\narray([303, 326, 329, ..., 332, 281, 324])\n\n\nThere are roughly only 50 simulations in which Romney wins the election!\n\n\n\n\nNow, lets visualize the simulation. We will build a histogram from the result of simulate_election. We will normalize the histogram by dividing the frequency of a vote tally by the number of simulations. We’ll overplot the “victory threshold” of 269 votes as a vertical black line and the result (Obama winning 332 votes) as a vertical red line.\nWe also compute the number of votes at the 5th and 95th quantiles, which we call the spread, and display it (this is an estimate of the outcome’s uncertainty). By 5th quantile we mean that if we ordered the number of votes Obama gets in each simulation in increasing order, the 5th quantile is the number below which 5% of the simulations lie.\nWe also display the probability of an Obama victory\n\ndef plot_simulation(simulation):    \n    plt.hist(simulation, bins=np.arange(200, 538, 1), \n             label='simulations', align='left', normed=True)\n    plt.axvline(332, 0, .5, color='r', label='Actual Outcome')\n    plt.axvline(269, 0, .5, color='k', label='Victory Threshold')\n    p05 = np.percentile(simulation, 5.)\n    p95 = np.percentile(simulation, 95.)\n    iq = int(p95 - p05)\n    pwin = ((simulation &gt;= 269).mean() * 100)\n    plt.title(\"Chance of Obama Victory: %0.2f%%, Spread: %d votes\" % (pwin, iq))\n    plt.legend(frameon=False, loc='upper left')\n    plt.xlabel(\"Obama Electoral College Votes\")\n    plt.ylabel(\"Probability\")\n    sns.despine()\n\n\nplot_simulation(result)\n\n\n\n\n\n\n\n\nThe model created by combining the probabilities we obtained from Predictwise with the simulation of a biased coin flip corresponding to the win probability in each states leads us to obtain a histogram of election outcomes. We are plotting the probabilities of a prediction, so we call this distribution over outcomes the predictive distribution. Simulating from our model and plotting a histogram allows us to visualize this predictive distribution. In general, such a set of probabilities is called a probability mass function.\n\n\n\nThis is an empirical Probability Mass Function.\nLets summarize: the way the mass function arose here that we did ran 10,000 tosses (for each state), and depending on the value, assigned the state to Obama or Romney, and then summed up the electoral votes over the states.\nThere is a second, very useful question, we can ask of any such probability mass or probability density: what is the probability that a random variable is less than some value. In other words: \\(P(X &lt; x)\\). This is also a probability distribution and is called the Cumulative Distribution Function, or CDF (sometimes just called the distribution, as opposed to the density, or mass function). Its obtained by “summing” the probability density function for all \\(X\\) less than \\(x\\).\n\nCDF = lambda x: np.float(np.sum(result &lt; x))/result.shape[0]\nfor votes in [200, 300, 320, 340, 360, 400, 500]:\n    print(\"Obama Win CDF at votes=\", votes, \" is \", CDF(votes))\n\nObama Win CDF at votes= 200  is  0.0\nObama Win CDF at votes= 300  is  0.1447\nObama Win CDF at votes= 320  is  0.4439\nObama Win CDF at votes= 340  is  0.839\nObama Win CDF at votes= 360  is  0.9979\nObama Win CDF at votes= 400  is  1.0\nObama Win CDF at votes= 500  is  1.0\n\n\n\nvotelist=np.arange(0, 540, 5)\nplt.plot(votelist, [CDF(v) for v in votelist], '.-');\nplt.xlim([200,400])\nplt.ylim([-0.1,1.1])\nplt.xlabel(\"votes for Obama\")\nplt.ylabel(\"probability of Obama win\");\n\n\n\n\n\n\n\n\n\n\n\nLet us consider a population of coinflips, n of them to be precise, \\(x_1,x_2,...,x_n\\). The distribution of coin flips is the binomial distribution. By this we mean that each coin flip represents a bernoulli random variable (or comes from a bernoulli distribution) with \\(p=0.5\\).\nAt this point, you might want to ask the question, what is the probability of obtaining \\(k\\) heads in \\(n\\) flips of the coin. We have seen this before, when we flipped 2 coins. What happens when when we flip 3?\n(This diagram is taken from the Feynman Lectures on Physics, volume 1. The chapter on probability is http://www.feynmanlectures.caltech.edu/I_06.html) \nWe draw a possibilities diagram like we did with the 2 coin flips, and see that there are different probabilities associated with the events of 0, 1,2, and 3 heads with 1 and 2 heads being the most likely. The probability of each of these events is given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k; n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\n\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\n\n\n\n\n\n\n\nConsider the binomial distribution Binomial(n,k, p) in the limit of large n. The number of successes k in n trials can be regarded as the sum of n IID Bernoulli variables with values 1 or 0. Call these \\(x_i\\).\nThen:\n\\[S_n = \\frac{1}{n} \\sum_i x_i .\\]\nThe CLT tells us then that for large n, we have:\n\\[S_n \\sim N(p, \\frac{p(1-p)}{n}),\\]\nsince the mean of a Bernoulli is \\(p\\), and its variance \\(p*(1-p)\\).\nThis means that we can replace the binomial distribution at large n by a gaussian where k is now a continuous variable, and whose mean is the mean of the binomial \\(np\\) (\\(nS_n\\), since the binomial distribution is on the sum, not on the average) and whose variance is \\(np(1-p)\\).\nThe accuracy of this approximation depends on the variance. A large variance makes for a broad distribution spanning many discrete k, thus justifying the transition from a discrete to a continuous distribution.\nThis approximation is used a lot in studying elections. For example, suppose I told you that I’d polled 1000 people in Ohio and found that 600 would vote Democratic, and 400 republican. Imagine that this 1000 is a “sample” drawn from the voting “population” of Ohio. Assume then that these are 1000 independent bernoulli trials with p=600/1000 = 0.6. Then we can say that, from the CLT, the mean of the sampling distribution of the mean of the bernoulli or is 0.6 (equivalently the binomial’s mean is 600), with a variance of \\(0.6*0.4/1000 = 0.00024\\) (equivalently the binomials variance is 240). Thus the standard deviation is 0.015 for a mean of 0.6, or 1.5% on a mean of 60% voting Democratic. This 1.5% if part of what pollsters quote as the margin of error of a candidates winning; they often include other factors such as errors in polling methodology.\n\n\n\nEarlier we had used the Predictwise probabilities from Octover 12th to create a predictive model for the elections. This time we will try to estimate our own win probabilities to plug into our predictive model.\nWe will start with a simple forecast model. We will try to predict the outcome of the election based the estimated proportion of people in each state who identify with one one political party or the other.\nGallup measures the political leaning of each state, based on asking random people which party they identify or affiliate with. Here’s the data they collected from January-June of 2012:\n\ngallup_2012=pd.read_csv(\"assets/g12.csv\").set_index('State')\ngallup_2012[\"Unknown\"] = 100 - gallup_2012.Democrat - gallup_2012.Republican\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\n\n\nState\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n\n\n\n\n\n\n\nEach row lists a state, the percent of surveyed individuals who identify as Democrat/Republican, the percent whose identification is unknown or who haven’t made an affiliation yet, the margin between Democrats and Republicans (Dem_Adv: the percentage identifying as Democrats minus the percentage identifying as Republicans), and the number N of people surveyed.\nThe most obvious source of error in the Gallup data is the finite sample size – Gallup did not poll everybody in America, and thus the party affilitions are subject to sampling errors. How much uncertainty does this introduce? Lets estimate the sampling error using the definition of the standard error (we use N-1 rather than N; see the sample error section in the page on the CLT).\n\ngallup_2012[\"SE_percentage\"]=100.0*np.sqrt((gallup_2012.Democrat/100.)*((100. - gallup_2012.Democrat)/100.)/(gallup_2012.N -1))\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\nSE_percentage\n\n\nState\n\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n0.849059\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n2.395543\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n0.744384\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n1.082971\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n0.392658\n\n\n\n\n\n\n\nOn their webpage discussing these data, Gallup notes that the sampling error for the states is between 3 and 6%, with it being 3% for most states. This is more than what we find, so lets go with what Gallup says.\nWe now use Gallup’s estimate of 3% to build a Gallup model with some uncertainty. We will, using the CLT, assume that the sampling distribution of the Obama win percentage is a gaussian with mean the democrat percentage and standard error the sampling error of 3%.\nWe’ll build the model in the function uncertain_gallup_model, and return a forecast where the probability of an Obama victory is given by the probability that a sample from the Dem_Adv Gaussian is positive.\nTo do this we simply need to find the area under the curve of a Gaussian that is on the positive side of the x-axis. The probability that a sample from a Gaussian with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) exceeds a threhold \\(z\\) can be found using the the Cumulative Distribution Function of a Gaussian:\n\\[\nCDF(z) = \\frac1{2}\\left(1 + {\\mathrm erf}\\left(\\frac{z - \\mu}{\\sqrt{2 \\sigma^2}}\\right)\\right)\n\\]\n\nfrom scipy.special import erf\ndef uncertain_gallup_model(gallup):\n    sigma = 3\n    prob =  .5 * (1 + erf(gallup.Dem_Adv / np.sqrt(2 * sigma**2)))\n    return pd.DataFrame(dict(Obama=prob), index=gallup.index)\n\n\nmodel = uncertain_gallup_model(gallup_2012)\nmodel = model.join(predictwise.Votes)\n\n\nprediction = simulate_election(model, 10000)\nplot_simulation(prediction)\n\n\n\n\n\n\n\n\n\n\n\nIf one has results from multiple pollsters, one can now treat them as independent samples from the voting population. Now we use the CLT again. Then the average from these samples will approach the average in the population, with the sample means distributed normally around it. So we can average the averages of the samples to get the population mean, and estimate the variance around this population mean as well."
  },
  {
    "objectID": "posts/distrib-example/index.html#simulating-a-simple-election-model",
    "href": "posts/distrib-example/index.html#simulating-a-simple-election-model",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "To do this, we will assume that the outcome in each state is the result of an independent coin flip whose probability of coming up Obama is given by the Predictwise state-wise win probabilities. Lets write a function simulate_election that uses this predictive model to simulate the outcome of the election given a table of probabilities.\n\n\nThe Bernoulli Distribution represents the distribution for coin flips. Let the random variable X represent such a coin flip, where X=1 is heads, and X=0 is tails. Let us further say that the probability of heads is p (p=0.5 is a fair coin).\nWe then say:\n\\[X \\sim Bernoulli(p),\\]\nwhich is to be read as X has distribution Bernoulli(p). The probability distribution function (pdf) or probability mass function associated with the Bernoulli distribution is\n\\[\\begin{eqnarray}\nP(X = 1) &=& p \\\\\nP(X = 0) &=& 1 - p\n\\end{eqnarray}\\]\nfor p in the range 0 to 1. The pdf, or the probability that random variable \\(X=x\\) may thus be written as\n\\[P(X=x) = p^x(1-p)^{1-x}\\]\nfor x in the set {0,1}.\nThe Predictwise probability of Obama winning in each state is a Bernoulli Parameter. You can think of it as a different loaded coin being tossed in each state, and thus there is a bernoulli distribution for each state\nNote: some of the code, and ALL of the visual style for the distribution plots below was shamelessly stolen from https://gist.github.com/mattions/6113437/ .\n\nfrom scipy.stats import bernoulli\n#bernoulli random variable\nbrv=bernoulli(p=0.3)\nprint(brv.rvs(size=20))\nevent_space=[0,1]\nplt.figure(figsize=(12,8))\ncolors=sns.color_palette()\nfor i, p in enumerate([0.1, 0.2, 0.5, 0.7]):\n    ax = plt.subplot(1, 4, i+1)\n    plt.bar(event_space, bernoulli.pmf(event_space, p), label=p, color=colors[i], alpha=0.5)\n    plt.plot(event_space, bernoulli.cdf(event_space, p), color=colors[i], alpha=0.5)\n\n    ax.xaxis.set_ticks(event_space)\n   \n    plt.ylim((0,1))\n    plt.legend(loc=0)\n    if i == 0:\n        plt.ylabel(\"PDF at $k$\")\nplt.tight_layout()\n\n[1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 1 0]\n\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\n\n\n\nIn the code below, each column simulates a single outcome from the 50 states + DC by choosing a random number between 0 and 1. Obama wins that simulation if the random number is \\(&lt;\\) the win probability. If he wins that simulation, we add in the electoral votes for that state, otherwise we dont. We do this n_sim times and return a list of total Obama electoral votes in each simulation.\n\ndef simulate_election(model, n_sim):\n    simulations = np.random.uniform(size=(51, n_sim))\n    obama_votes = (simulations &lt; model.Obama.values.reshape(-1, 1)) * model.Votes.values.reshape(-1, 1)\n    #summing over rows gives the total electoral votes for each simulation\n    return obama_votes.sum(axis=0)\n\nThe first thing to pick up on here is that np.random.uniform gives you a random number between 0 and 1, uniformly. In other words, the number is equally likely to be between 0 and 0.1, 0.1 and 0.2, and so on. This is a very intuitive idea, but it is formalized by the notion of the Uniform Distribution.\nWe then say:\n\\[X \\sim Uniform([0,1),\\]\nwhich is to be read as X has distribution Uniform([0,1]). The probability distribution function (pdf) associated with the Uniform distribution is\n\\[\\begin{eqnarray}\nP(X = x) &=& 1 \\, for \\, x \\in [0,1] \\\\\nP(X = x) &=& 0 \\, for \\, x \\notin [0,1]\n\\end{eqnarray}\\]\nWhat assigning the vote to Obama when the random variable drawn from the Uniform distribution is less than the Predictwise probability of Obama winning (which is a Bernoulli Parameter) does for us is this: if we have a large number of simulations and \\(p_{Obama}=0.7\\) , then 70% of the time, the random numbes drawn will be below 0.7. And then, assigning those as Obama wins will hew to the frequentist notion of probability of the Obama win. But remember, of course, that in 30% of the simulations, Obama wont win, and this will induce fluctuations and a distribution on the total number of electoral college votes that Obama gets. And this is what we see in the histogram below.\nThe following code takes the necessary probabilities for the Predictwise data, and runs 10000 simulations. If you think of this in terms of our coins, think of it as having 51 biased coins, one for each state, and tossing them 10,000 times each.\nWe use the results to compute the number of simulations, according to this predictive model, that Obama wins the election (i.e., the probability that he receives 269 or more electoral college votes)\n\nresult = simulate_election(predictwise, 10000)\nprint((result &gt;= 269).sum())\n\n9955\n\n\n\nresult\n\narray([303, 326, 329, ..., 332, 281, 324])\n\n\nThere are roughly only 50 simulations in which Romney wins the election!"
  },
  {
    "objectID": "posts/distrib-example/index.html#displaying-the-prediction",
    "href": "posts/distrib-example/index.html#displaying-the-prediction",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "Now, lets visualize the simulation. We will build a histogram from the result of simulate_election. We will normalize the histogram by dividing the frequency of a vote tally by the number of simulations. We’ll overplot the “victory threshold” of 269 votes as a vertical black line and the result (Obama winning 332 votes) as a vertical red line.\nWe also compute the number of votes at the 5th and 95th quantiles, which we call the spread, and display it (this is an estimate of the outcome’s uncertainty). By 5th quantile we mean that if we ordered the number of votes Obama gets in each simulation in increasing order, the 5th quantile is the number below which 5% of the simulations lie.\nWe also display the probability of an Obama victory\n\ndef plot_simulation(simulation):    \n    plt.hist(simulation, bins=np.arange(200, 538, 1), \n             label='simulations', align='left', normed=True)\n    plt.axvline(332, 0, .5, color='r', label='Actual Outcome')\n    plt.axvline(269, 0, .5, color='k', label='Victory Threshold')\n    p05 = np.percentile(simulation, 5.)\n    p95 = np.percentile(simulation, 95.)\n    iq = int(p95 - p05)\n    pwin = ((simulation &gt;= 269).mean() * 100)\n    plt.title(\"Chance of Obama Victory: %0.2f%%, Spread: %d votes\" % (pwin, iq))\n    plt.legend(frameon=False, loc='upper left')\n    plt.xlabel(\"Obama Electoral College Votes\")\n    plt.ylabel(\"Probability\")\n    sns.despine()\n\n\nplot_simulation(result)\n\n\n\n\n\n\n\n\nThe model created by combining the probabilities we obtained from Predictwise with the simulation of a biased coin flip corresponding to the win probability in each states leads us to obtain a histogram of election outcomes. We are plotting the probabilities of a prediction, so we call this distribution over outcomes the predictive distribution. Simulating from our model and plotting a histogram allows us to visualize this predictive distribution. In general, such a set of probabilities is called a probability mass function."
  },
  {
    "objectID": "posts/distrib-example/index.html#empirical-distribution",
    "href": "posts/distrib-example/index.html#empirical-distribution",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "This is an empirical Probability Mass Function.\nLets summarize: the way the mass function arose here that we did ran 10,000 tosses (for each state), and depending on the value, assigned the state to Obama or Romney, and then summed up the electoral votes over the states.\nThere is a second, very useful question, we can ask of any such probability mass or probability density: what is the probability that a random variable is less than some value. In other words: \\(P(X &lt; x)\\). This is also a probability distribution and is called the Cumulative Distribution Function, or CDF (sometimes just called the distribution, as opposed to the density, or mass function). Its obtained by “summing” the probability density function for all \\(X\\) less than \\(x\\).\n\nCDF = lambda x: np.float(np.sum(result &lt; x))/result.shape[0]\nfor votes in [200, 300, 320, 340, 360, 400, 500]:\n    print(\"Obama Win CDF at votes=\", votes, \" is \", CDF(votes))\n\nObama Win CDF at votes= 200  is  0.0\nObama Win CDF at votes= 300  is  0.1447\nObama Win CDF at votes= 320  is  0.4439\nObama Win CDF at votes= 340  is  0.839\nObama Win CDF at votes= 360  is  0.9979\nObama Win CDF at votes= 400  is  1.0\nObama Win CDF at votes= 500  is  1.0\n\n\n\nvotelist=np.arange(0, 540, 5)\nplt.plot(votelist, [CDF(v) for v in votelist], '.-');\nplt.xlim([200,400])\nplt.ylim([-0.1,1.1])\nplt.xlabel(\"votes for Obama\")\nplt.ylabel(\"probability of Obama win\");"
  },
  {
    "objectID": "posts/distrib-example/index.html#binomial-distribution",
    "href": "posts/distrib-example/index.html#binomial-distribution",
    "title": "Distributions Example: Elections",
    "section": "",
    "text": "Let us consider a population of coinflips, n of them to be precise, \\(x_1,x_2,...,x_n\\). The distribution of coin flips is the binomial distribution. By this we mean that each coin flip represents a bernoulli random variable (or comes from a bernoulli distribution) with \\(p=0.5\\).\nAt this point, you might want to ask the question, what is the probability of obtaining \\(k\\) heads in \\(n\\) flips of the coin. We have seen this before, when we flipped 2 coins. What happens when when we flip 3?\n(This diagram is taken from the Feynman Lectures on Physics, volume 1. The chapter on probability is http://www.feynmanlectures.caltech.edu/I_06.html) \nWe draw a possibilities diagram like we did with the 2 coin flips, and see that there are different probabilities associated with the events of 0, 1,2, and 3 heads with 1 and 2 heads being the most likely. The probability of each of these events is given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k; n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\n\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\n\n\n\n\n\n\n\nConsider the binomial distribution Binomial(n,k, p) in the limit of large n. The number of successes k in n trials can be regarded as the sum of n IID Bernoulli variables with values 1 or 0. Call these \\(x_i\\).\nThen:\n\\[S_n = \\frac{1}{n} \\sum_i x_i .\\]\nThe CLT tells us then that for large n, we have:\n\\[S_n \\sim N(p, \\frac{p(1-p)}{n}),\\]\nsince the mean of a Bernoulli is \\(p\\), and its variance \\(p*(1-p)\\).\nThis means that we can replace the binomial distribution at large n by a gaussian where k is now a continuous variable, and whose mean is the mean of the binomial \\(np\\) (\\(nS_n\\), since the binomial distribution is on the sum, not on the average) and whose variance is \\(np(1-p)\\).\nThe accuracy of this approximation depends on the variance. A large variance makes for a broad distribution spanning many discrete k, thus justifying the transition from a discrete to a continuous distribution.\nThis approximation is used a lot in studying elections. For example, suppose I told you that I’d polled 1000 people in Ohio and found that 600 would vote Democratic, and 400 republican. Imagine that this 1000 is a “sample” drawn from the voting “population” of Ohio. Assume then that these are 1000 independent bernoulli trials with p=600/1000 = 0.6. Then we can say that, from the CLT, the mean of the sampling distribution of the mean of the bernoulli or is 0.6 (equivalently the binomial’s mean is 600), with a variance of \\(0.6*0.4/1000 = 0.00024\\) (equivalently the binomials variance is 240). Thus the standard deviation is 0.015 for a mean of 0.6, or 1.5% on a mean of 60% voting Democratic. This 1.5% if part of what pollsters quote as the margin of error of a candidates winning; they often include other factors such as errors in polling methodology.\n\n\n\nEarlier we had used the Predictwise probabilities from Octover 12th to create a predictive model for the elections. This time we will try to estimate our own win probabilities to plug into our predictive model.\nWe will start with a simple forecast model. We will try to predict the outcome of the election based the estimated proportion of people in each state who identify with one one political party or the other.\nGallup measures the political leaning of each state, based on asking random people which party they identify or affiliate with. Here’s the data they collected from January-June of 2012:\n\ngallup_2012=pd.read_csv(\"assets/g12.csv\").set_index('State')\ngallup_2012[\"Unknown\"] = 100 - gallup_2012.Democrat - gallup_2012.Republican\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\n\n\nState\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n\n\n\n\n\n\n\nEach row lists a state, the percent of surveyed individuals who identify as Democrat/Republican, the percent whose identification is unknown or who haven’t made an affiliation yet, the margin between Democrats and Republicans (Dem_Adv: the percentage identifying as Democrats minus the percentage identifying as Republicans), and the number N of people surveyed.\nThe most obvious source of error in the Gallup data is the finite sample size – Gallup did not poll everybody in America, and thus the party affilitions are subject to sampling errors. How much uncertainty does this introduce? Lets estimate the sampling error using the definition of the standard error (we use N-1 rather than N; see the sample error section in the page on the CLT).\n\ngallup_2012[\"SE_percentage\"]=100.0*np.sqrt((gallup_2012.Democrat/100.)*((100. - gallup_2012.Democrat)/100.)/(gallup_2012.N -1))\ngallup_2012.head()\n\n\n\n\n\n\n\nDemocrat\nRepublican\nDem_Adv\nN\nUnknown\nSE_percentage\n\n\nState\n\n\n\n\n\n\n\n\n\n\nAlabama\n36.0\n49.6\n-13.6\n3197\n14.4\n0.849059\n\n\nAlaska\n35.9\n44.3\n-8.4\n402\n19.8\n2.395543\n\n\nArizona\n39.8\n47.3\n-7.5\n4325\n12.9\n0.744384\n\n\nArkansas\n41.5\n40.8\n0.7\n2071\n17.7\n1.082971\n\n\nCalifornia\n48.3\n34.6\n13.7\n16197\n17.1\n0.392658\n\n\n\n\n\n\n\nOn their webpage discussing these data, Gallup notes that the sampling error for the states is between 3 and 6%, with it being 3% for most states. This is more than what we find, so lets go with what Gallup says.\nWe now use Gallup’s estimate of 3% to build a Gallup model with some uncertainty. We will, using the CLT, assume that the sampling distribution of the Obama win percentage is a gaussian with mean the democrat percentage and standard error the sampling error of 3%.\nWe’ll build the model in the function uncertain_gallup_model, and return a forecast where the probability of an Obama victory is given by the probability that a sample from the Dem_Adv Gaussian is positive.\nTo do this we simply need to find the area under the curve of a Gaussian that is on the positive side of the x-axis. The probability that a sample from a Gaussian with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) exceeds a threhold \\(z\\) can be found using the the Cumulative Distribution Function of a Gaussian:\n\\[\nCDF(z) = \\frac1{2}\\left(1 + {\\mathrm erf}\\left(\\frac{z - \\mu}{\\sqrt{2 \\sigma^2}}\\right)\\right)\n\\]\n\nfrom scipy.special import erf\ndef uncertain_gallup_model(gallup):\n    sigma = 3\n    prob =  .5 * (1 + erf(gallup.Dem_Adv / np.sqrt(2 * sigma**2)))\n    return pd.DataFrame(dict(Obama=prob), index=gallup.index)\n\n\nmodel = uncertain_gallup_model(gallup_2012)\nmodel = model.join(predictwise.Votes)\n\n\nprediction = simulate_election(model, 10000)\nplot_simulation(prediction)\n\n\n\n\n\n\n\n\n\n\n\nIf one has results from multiple pollsters, one can now treat them as independent samples from the voting population. Now we use the CLT again. Then the average from these samples will approach the average in the population, with the sample means distributed normally around it. So we can average the averages of the samples to get the population mean, and estimate the variance around this population mean as well."
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', False)\nimport seaborn as sns\nsns.set_style(\"white\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nSuppose you were to flip a coin. Then you expect not to be able to say whether the next toss would yield a heads or a tails. You might tell a friend that the odds of getting a heads is equal to to the odds of getting a tails, and that both are \\(1/2\\).\nThis intuitive notion of odds is a probability. It comes about because of our physical model of the world: say that because of our faith in the U.S. Mint, we might be willing to, without having seen any tosses, say that the coin is fair. In other words, there are two choices, both of which are equally likely.\n\n\nConsider another example. If we were tossing a ‘fair’ six-sided dice, we may thus equivalently say that the odds of the dice falling on any one of its sides is \\(1/6\\). Indeed if there are \\(C\\) different equally likely possibilities, we’d expect that the probability of any one particular outcome would be \\(1/C\\).\nThe examples of the coin as well as the dice illustrate the notion of probability springing from symmetry. Here we think of probability of of the number 4 on the dice as the ratio:\n\\[\\frac{Number\\: of\\: cases\\: for\\: number\\: 4}{number\\: of\\: possibilities} = \\frac{1}{6},\\] assuming equally likely possibilities.\nIn other words, the symmetry refers to the notion that when there are multiple ways for an event to happen, and that then we have an intuitive model of fairness between these ways that tells us that none of these are any more likely than the other.\n\n\n\nThus one might think of symmetry as providing a model. There are also other kinds of models.\nThink of an event like an election, say a presidential election. You cant exactly run multiple trials of the election: its a one-off event. But you still want to talk about the likelyhood of a candidate winning. However people do make models of elections, based on inputs such as race, age, income, sampling polls, etc. They assign likeyhoods of candidates winning and run large numbers of simulations of the election, making predictions based on that. Forecasters like Nate Silver, Sam Wang, And Drew Linzer, made incredibly successfull predictions of the 2012 elections.\nOr consider what a weather forecaster means when he or she says there is a 90% chance of rain today. Presumably, this conclusion has been made from many computer simulations which take in the weather conditions known in the past, and propagated using physics to the current day. The simulations give different results based on the uncertainty in the measurement of past weather, and the inability of the physics to capture the phenomenon exactly (all physics is some approximation to the natural world). But 90% of these simulations show rain.\nIn all of these cases, there is either a model (a fair coin, an election forecasting model, a weather differential equation), or an experiment ( a large number of coin tosses) that is used to estimate a probability, or the odds, of an event \\(E\\) occuring.\n\n\nIn all of these cases, probability is something we speak of, for observations we are to make in the future. And it is something we assign, based on the model or belief of the world we have, or on the basis of past observations that we have made, or that we might even imagine that we would make.\nConsider some additional examples. You might ask the probability of the Yankees winning the next baseball game against the Red Sox. Or you might ask for the probability of a launch failure for the next missile protecting Tel-Aviv. These are not academic questions: lots of betting money and lives depend upon them respectively. In both cases there is some past data, and some other inputs such as say, weather conditions, which might be used to construct a model, which is then used to predict the fate of the next game or launch.\nThey key takeaway is this: for some reasons, and possibly using some data, we have constructed a model of the universe. In other words, we have combined prior beliefs and past frequencies respectively. This notion of such combination is yet another notion of probability, called the Bayesian notion of probability. And we can now use this model to make predictions, such us the future odds of a particular event happening.\n\n\n\n\nConsider doing a large number of coin flips. You would do, or imagine doing, a large number of flips or trials \\(N\\), and finding the number of times you got heads \\(N_H\\). Then the probability of getting heads would be \\[\\frac{N_H}{N}.\\]\nThis is the notion of probability as a relative frequency: if there are multiple ways an event like the tossing of a coin can happen, lets look at multiple trials of the event and see the fraction of times one or other of these ways happened.\nThis jibes with our general notion of probability from symmetry: indeed you can think of it as an experimental verificaltion of a symmetry based model.\n\n\nWe dont have a coin right now. So let us simulate this process on a computer. To do this we will use a form of the random number generator built into numpy. In particular, we will use the function np.random.choice, which will with equal probability for all items pick an item from a list (thus if the list is of size 6, it will pick one of the six list items each time, with a probability 1/6).\n\ndef throw_a_coin(N):\n    return np.random.choice(['H','T'], size=N)\nthrows=throw_a_coin(40)\nprint(\"Throws:\",\" \".join(throws))\nprint(\"Number of Heads:\", np.sum(throws=='H'))\nprint(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/40.)\n\nThrows: T H T H T H H H H H T H H H T H T H T H T H H H T H H H H T T T T H T T H T T T\nNumber of Heads: 22\np1 = Number of Heads/Total Throws: 0.55\n\n\nNotice that you do not necessarily get 20 heads.\nNow say that we run the entire process again, a second replication to obtain a second sample. Then we ask the same question: what is the fraction of heads we get this time? Lets call the odds of heads in sample 2, then, \\(p_2\\):\n\ndef make_throws(N):\n    throws=throw_a_coin(N)\n    if N &lt;= 100:\n        print(\"Throws:\",\" \".join(throws))\n    else:\n        print(\"First 100 Throws:\",\" \".join(throws[:100]))\n    print(\"Number of Heads:\", np.sum(throws=='H'))\n    print(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/N)\nmake_throws(40)\n\nThrows: H T H T H H H H T H H H T T T H T H H H H T H H T T H H T H T H T H H H T T H H\nNumber of Heads: 25\np1 = Number of Heads/Total Throws: 0.625\n\n\nLet’s do many more trials\n\nmake_throws(1000)\n\nFirst 100 Throws: H H H T H H T T T H T H H H H T T T H H H H H H T H T T T H T H T T T H H T H T H H H H T H T H T T H H T T T T T T H T H H T H T H T H T H T T H T H H H T H T T T H H T H T H T H T H H H H T T T T H\nNumber of Heads: 521\np1 = Number of Heads/Total Throws: 0.521\n\n\nAnd even more:\n\nmake_throws(10000)\n\nFirst 100 Throws: H T T T H T T T H T T T T H T T H T H H H H T H T H T T H T H T H H H H T T H H H H T H H H H T H T T T H T H H T T T H T H T H T T T H T T T T H T H H H H H T T H H H T T H H H H H H T H T H T T T H\nNumber of Heads: 5047\np1 = Number of Heads/Total Throws: 0.5047\n\n\nAs you can see, the larger number of trials we do, the closer we seem to get to half the tosses showing up heads. Lets see this more systematically:\n\ntrials=np.arange(0, 40000, 1000)\nplt.plot(trials, [np.sum(throw_a_coin(j)=='H')/np.float(j) for j in trials], 'o-', alpha=0.2);\nplt.axhline(0.5, 0, 1, color='r');\nplt.xlabel('number of trials');\nplt.ylabel('probability of heads from simulation');\nplt.title('frequentist probability of heads');\n\n\n\n\n\n\n\n\nThus, the true odds fluctuate about their long-run value of 0.5, in accordance with the model of a fair coin (which we encoded in our simulation by having np.random.choice choose between two possibilities with equal probability), with the fluctuations becoming much smaller (we shall talk a lot more about this later in the book). These fluctations are what give rise to probability distributions.\nEach finite length run is called a sample, which has been obtained from the generative model of our fair coin. Its called generative as we can use the model to generate, using simulation, a set of samples we can play with to understand a model. Such simulation from a model is a key technique which we will come back to again and again in learning from data.\n\n\n\n\n\nWe have seen multiple notions of probability so far. One might assign probabilities based on symmetry, for eg, 2 sides of a fair coin, or six sides of a fair dice. One might assign probabilities based on doing an experiment. such as the long run number of heads in many coin flips. One might assign probabilities based on beliefs; and one might even assign probabilities to events that have no chance of repeating, such as the 2012 presidential election, or the probability of rain between 2pm and 6pm today.\nThus, the very definition of probability seems to be wishy-washy and subjective. Thus you might wonder how you might work with such probabilities. For this, we turn to the rules of probability.\nThe rules dont care where our probabilities come from, as to how we estimated them, as long as they behave in intuitively sensible ways.\nConsider an example:\nE is the event of getting a heads in a first coin toss, and F is the same for a second coin toss. Here \\(\\Omega\\), the set of all possibilities that can happen when you toss two coins is \\(\\{HH, HT, TH, TT\\}\\). Since E only specifies that the first toss is heads, \\(E=\\{HT, HH\\}\\). Similarly \\(F= {HH, TH}\\) The set of all events that are not E then is \\(\\tilde{E} = {TH, TT}\\).\nThese sets, along with some others are captured in the venn diagram below:\n\n\n\nVenn diagram for the 2-coin-toss sample space\n\n\n\n\nIf E and F are independent events, the probability of both events happening together \\(P(EF)\\) or \\(P(E \\cap F)\\) (read as E and F or E intersection F, respectively) is the multiplication of the individual probabilities.\n\\[ P(EF) = P(E) P(F) .\\]\nIf you made the two independent coin tosses in our example, and you had a fair coin, the probability of both coming up heads is \\((1/2)*(1/2) = 1/4\\). This makes intuitive sense: half the time the first coin comes up heads, and then 1/2 the time the second coin comes up heads, so its 1/4 of the times that both come up heads.\n\n\n\nWe can now ask the question, what is \\(P(E+F)\\), the odds of E alone, F alone, or both together. Translated into English, we are asking, whats the probability that only the first toss was heads, or only the second toss was heads, or that both came up heads? Or in other words, what are the odds of at least one heads? The answer to this question is given by the rule:\n\\[P(E+F) = P(E) + P(F) - P(EF),\\]\nthe “plus” formula, where E+F, read as E or F (also \\(E \\cup F\\), reads as E union F) means “E alone, F alone, or both together”. This rule is a hard one to understand and has a lot of notation, so lets examine it in some detail.\nThere are four ways that these two tosses can arrange themselves, as illustrated by this diagram, adapted from the probability chapter in Feynman’s lectures on Physics..you should read it!.\n\n\n\nTree diagram for 2 coin flips\n\n\nWe can have a HH, HT, TH, or TT. In three out of 4 of these cases, either the first toss was heads, or the second was heads. Thus \\(P(E+F)=3/4\\).\nThe formula says, add the odds that “the first toss was a heads, without worrying about the second one (1/2), to the probability that the second toss was a heads, without worrying about the first one” (1/2). Since this double counts the situation where both are heads; subtract that (1/4):\n\\[\\begin{eqnarray}\nP(E+F) \\, & = &\\, P(E) + P(F) - P(EF)\\\\\n\\frac{3}{4} \\, & = &\\, \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4}\n\\end{eqnarray}\\]\nArmed with these two formulas, we can tackle the world of conditional and marginal probabilities, and Bayes theorem!\n\n\n\nIf \\(X\\) and \\(Y\\) are two events and \\(p(X)\\) is the probability of the event \\(X\\) to happen. $X^- $ is the complement of \\(X\\), the event which is all the occurrences which are not in \\(X\\). \\(X+Y\\) is the union of \\(X\\) and \\(Y\\); \\(X,Y\\) is the intersection of \\(X\\) and \\(Y\\). (Both \\(X+Y\\) and \\(X,Y\\) are also events.)\n\n\n\n\n\\(p(X) &gt;=0\\); probability must be non-negative\n\\(0 ≤ p(X) ≤ 1 \\;\\) \\(X\\) has probability range from 0 to 1.\n\\(p(X)+p(X^-)=1 \\;\\) \\(X\\) must either happen or not happen. These last two aximoms can be thought of as saying that the probabilities if all events put tohether must sum to 1.\n\\(p(X+Y)=p(X)+p(Y)−p(X,Y) \\;\\) \\(X\\) can happen and \\(Y\\) can happen but we must subtract the cases that are happening together so we do not over-count.\n\n\n\n\n\nTo link the notion of events such as \\(E\\) and collections of events, or probability spaces \\(\\Omega\\) to data, we must introduce the concept of random variables. The following definition is taken from Larry Wasserman’s All of Stats.\nDefinition. A random variable is a mapping\n\\[ X: \\Omega \\rightarrow \\mathbb{R}\\]\nthat assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\). \\(\\Omega\\) is the sample space. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. Subsets of \\(\\Omega\\) are called Events. Say \\(\\omega = HHTTTTHTT\\) then \\(X(\\omega) = 3\\) if defined as number of heads in the sequence \\(\\omega\\).\nWe will assign a real number P(A) to every event A, called the probability of A. We also call P a probability distribution or a probability measure. To qualify as a probability, P must satisfy the three axioms (non-negative, \\(P(\\Omega)=1\\), disjoint probs add).\n\n\n\nThe diagram below taken from Bishop may be used to illustrate the concepts of conditionals and marginals. Consider two random variables, \\(X\\), which takes the values \\({x_i}\\) where \\(i = 1,...,M\\), and \\(Y\\), which takes the values \\({y_j}\\) where \\(j = 1,...,L\\). The number of instances for which \\(X = x_i\\) and \\(Y = y_j\\) is \\(n_{ij}\\). The number of points in column i where \\(X=x_i\\) is \\(c_i\\), and for the row where \\(Y = y_j\\) is \\(r_j\\).\n\n\n\nJoint probability table notation\n\n\nThen the joint probability of having \\(p(X = x_i, Y= y_j)\\) is in the asymptotic limit of large numbers in the frequency sense of probability \\(n_{ij}/N\\) where is the total number of instances. The \\(X\\) marginal, \\(p(X=x_i)\\) can be obtained by summing instances in all the cells in the i’th column:\n\\[p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)\\]\nLets consider next only those instances for which \\(X=x_i\\). This means that we are limiting our analysis to the ith row. Then, we write the conditional probability of \\(Y = y_j\\) given \\(X = x_i\\) as \\(p(Y = y_j \\mid X = x_i)\\). This is the asymptotic fraction of these instances where \\(Y = y_j\\) and is obtained by dividing the instances in the cell by those in the comumn as\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i}.\\]\nA little algebraic rearrangement gives:\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i} = \\frac{n_{ij}}{N} / \\frac{c_i}{N},\\]\nor:\n\\[p(Y = y_j \\mid X = x_i) \\times p(X=x_i) =  p(X=x_i, Y=y_j).\\]\nThis is the product rule of probability with conditionals involved.\nLet us simplify the notation by dropping the \\(X=\\) and \\(Y=\\).\nThen we can write the marginal probability of x as a sum over the joint distribution of x and y where we sum over all possibilities of y,\n\\[p(x) = \\sum_y p(x,y) \\].\nWe can rewrite a joint distribution as a product of a conditional and marginal probability,\n\\[ p(x,y) = p(y\\mid x) p(x) \\]\nThe product rule is applied repeatedly to give expressions for the joint probability involving more than two variables. For example, the joint distribution over three variables can be factorized into a product of conditional probabilities:\n\\[ p(x,y,z) = p(x|y,z) \\, p(y,z) = p(x |y,z) \\, p(y|z) p(z) \\]\n\n\nObserve that\n\\[ p(x,y) = p(y\\mid x) p(x) = P(x\\mid y)p(y).\\]\nGiven the product rule one can derive the Bayes rule, which plays a central role in a lot of the things we will be talking:\n\\[ p(y\\mid x) = \\frac{p(x\\mid y) \\, p(y) }{p(x)} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x,y')} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x\\mid y')p(y')}\\]\n\n\n\nTwo variables are said to be independent if their joint distribution factorizes into a product of two marginal probabilities:\n\\[ p(x,y) = p(x) \\, p(y) \\]\nAnother consequence of independence is that if \\(x\\) and \\(y\\) are independent, the conditional probability of \\(x\\) given \\(y\\) is just the probability of \\(x\\):\n\\[ p(x|y) = p(x) \\]\nIn other words, by conditioning on a particular \\(y\\), we have learned nothing about \\(x\\) because of independence. Two variables \\(x\\) and \\(y\\) and said to be conditionally independent of \\(z\\) if the following holds:\n\\[ p(x,y|z) = p(x|z) p(y|z) \\]\nTherefore, if we learn about z, x and y become independent. Another way to write that \\(x\\) and \\(y\\) are conditionally independent of \\(z\\) is\n\\[ p(x| z, y) = p(x|z) \\]\nIn other words, if we condition on \\(z\\), and now also learn about \\(y\\), this is not going to change the probability of \\(x\\). It is important to realize that conditional independence between \\(x\\) and \\(y\\) does not imply independence between \\(x\\) and \\(y\\).\n\n\n\n\nSally Clark, a lawyer who lost her first son at 11 weeks and her second at 8 weeks, was convicted in 1999. A prominent pediatrician, Sir Roy Meadow, had testified for the prosecution about Sudden Infant Death Syndrome, known as SIDS in the U.S. and cot death in Britain. Citing a government study, Meadow said the incidence of one SIDS death was one in 8,500 in a family like Clark’s–stable, affluent, nonsmoking, with a mother more than 26 years old.\n\n\nThen, despite the fact that some families are predisposed to SIDS, Meadow assumed erroneously that each sibling’s death occurred independently of the other. Multiplying 8,500 by 8,500, he calculated that the chance of two children dying in a family like Sally Clark’s was so rare–one in 73 million–that they must have been murdered.\n\n(from http://www.mcgrayne.com/disc.htm)\np(child 1 dying of sids) = 1/8500\n\nP(child 2 dying of sids) = 1/100\n\nFirst, we look at natural causes of sudden infant death. The chance of one random infant dying from SIDS was about 1 in 1,300 during this period in Britain. Meadow’s argument was flawed and produced a much slimmer chance of natural death. The estimated odds of a second SIDS death in the same family was much larger, perhaps one in 100, because family members can share a common environmental or genetic propensity for SIDS.\n\n\nSecond, we turn to the hypothesis that the babies were murdered. Only about 30 children out of 650,000 annual births in England, Scotland, and Wales were known to have been murdered by their mothers. The number of double murders must be much lower, estimated as 10 times less likely.\n\np(S2 = both children dying of sids) =  0.000007\np(notS2 = not both dying of sids) =  0.999993\n\nData: both children died unexpectedly\nSo now ask, whats:\np(data | S2) = 1\np(data | notS2) = ? both died but not SIDS. Murder? =  30/650000    × 1/10 = 0.000005\nWe want to calculate the “posterior probability”:\np(S2 | data) = P(data | S2) P(S2) /(P(data | S2) P(S2) + P(data|notS2)P(notS2))\n= 1*0.000007/(1*0.000007 + 0.000005*0.999993)\n=0.58\n58% chance of having died from SIDS!\nSally Clark spent 3 years in jail."
  },
  {
    "objectID": "posts/probability/index.html#what-is-probability",
    "href": "posts/probability/index.html#what-is-probability",
    "title": "Probability",
    "section": "",
    "text": "Suppose you were to flip a coin. Then you expect not to be able to say whether the next toss would yield a heads or a tails. You might tell a friend that the odds of getting a heads is equal to to the odds of getting a tails, and that both are \\(1/2\\).\nThis intuitive notion of odds is a probability. It comes about because of our physical model of the world: say that because of our faith in the U.S. Mint, we might be willing to, without having seen any tosses, say that the coin is fair. In other words, there are two choices, both of which are equally likely.\n\n\nConsider another example. If we were tossing a ‘fair’ six-sided dice, we may thus equivalently say that the odds of the dice falling on any one of its sides is \\(1/6\\). Indeed if there are \\(C\\) different equally likely possibilities, we’d expect that the probability of any one particular outcome would be \\(1/C\\).\nThe examples of the coin as well as the dice illustrate the notion of probability springing from symmetry. Here we think of probability of of the number 4 on the dice as the ratio:\n\\[\\frac{Number\\: of\\: cases\\: for\\: number\\: 4}{number\\: of\\: possibilities} = \\frac{1}{6},\\] assuming equally likely possibilities.\nIn other words, the symmetry refers to the notion that when there are multiple ways for an event to happen, and that then we have an intuitive model of fairness between these ways that tells us that none of these are any more likely than the other.\n\n\n\nThus one might think of symmetry as providing a model. There are also other kinds of models.\nThink of an event like an election, say a presidential election. You cant exactly run multiple trials of the election: its a one-off event. But you still want to talk about the likelyhood of a candidate winning. However people do make models of elections, based on inputs such as race, age, income, sampling polls, etc. They assign likeyhoods of candidates winning and run large numbers of simulations of the election, making predictions based on that. Forecasters like Nate Silver, Sam Wang, And Drew Linzer, made incredibly successfull predictions of the 2012 elections.\nOr consider what a weather forecaster means when he or she says there is a 90% chance of rain today. Presumably, this conclusion has been made from many computer simulations which take in the weather conditions known in the past, and propagated using physics to the current day. The simulations give different results based on the uncertainty in the measurement of past weather, and the inability of the physics to capture the phenomenon exactly (all physics is some approximation to the natural world). But 90% of these simulations show rain.\nIn all of these cases, there is either a model (a fair coin, an election forecasting model, a weather differential equation), or an experiment ( a large number of coin tosses) that is used to estimate a probability, or the odds, of an event \\(E\\) occuring.\n\n\nIn all of these cases, probability is something we speak of, for observations we are to make in the future. And it is something we assign, based on the model or belief of the world we have, or on the basis of past observations that we have made, or that we might even imagine that we would make.\nConsider some additional examples. You might ask the probability of the Yankees winning the next baseball game against the Red Sox. Or you might ask for the probability of a launch failure for the next missile protecting Tel-Aviv. These are not academic questions: lots of betting money and lives depend upon them respectively. In both cases there is some past data, and some other inputs such as say, weather conditions, which might be used to construct a model, which is then used to predict the fate of the next game or launch.\nThey key takeaway is this: for some reasons, and possibly using some data, we have constructed a model of the universe. In other words, we have combined prior beliefs and past frequencies respectively. This notion of such combination is yet another notion of probability, called the Bayesian notion of probability. And we can now use this model to make predictions, such us the future odds of a particular event happening.\n\n\n\n\nConsider doing a large number of coin flips. You would do, or imagine doing, a large number of flips or trials \\(N\\), and finding the number of times you got heads \\(N_H\\). Then the probability of getting heads would be \\[\\frac{N_H}{N}.\\]\nThis is the notion of probability as a relative frequency: if there are multiple ways an event like the tossing of a coin can happen, lets look at multiple trials of the event and see the fraction of times one or other of these ways happened.\nThis jibes with our general notion of probability from symmetry: indeed you can think of it as an experimental verificaltion of a symmetry based model.\n\n\nWe dont have a coin right now. So let us simulate this process on a computer. To do this we will use a form of the random number generator built into numpy. In particular, we will use the function np.random.choice, which will with equal probability for all items pick an item from a list (thus if the list is of size 6, it will pick one of the six list items each time, with a probability 1/6).\n\ndef throw_a_coin(N):\n    return np.random.choice(['H','T'], size=N)\nthrows=throw_a_coin(40)\nprint(\"Throws:\",\" \".join(throws))\nprint(\"Number of Heads:\", np.sum(throws=='H'))\nprint(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/40.)\n\nThrows: T H T H T H H H H H T H H H T H T H T H T H H H T H H H H T T T T H T T H T T T\nNumber of Heads: 22\np1 = Number of Heads/Total Throws: 0.55\n\n\nNotice that you do not necessarily get 20 heads.\nNow say that we run the entire process again, a second replication to obtain a second sample. Then we ask the same question: what is the fraction of heads we get this time? Lets call the odds of heads in sample 2, then, \\(p_2\\):\n\ndef make_throws(N):\n    throws=throw_a_coin(N)\n    if N &lt;= 100:\n        print(\"Throws:\",\" \".join(throws))\n    else:\n        print(\"First 100 Throws:\",\" \".join(throws[:100]))\n    print(\"Number of Heads:\", np.sum(throws=='H'))\n    print(\"p1 = Number of Heads/Total Throws:\", np.sum(throws=='H')/N)\nmake_throws(40)\n\nThrows: H T H T H H H H T H H H T T T H T H H H H T H H T T H H T H T H T H H H T T H H\nNumber of Heads: 25\np1 = Number of Heads/Total Throws: 0.625\n\n\nLet’s do many more trials\n\nmake_throws(1000)\n\nFirst 100 Throws: H H H T H H T T T H T H H H H T T T H H H H H H T H T T T H T H T T T H H T H T H H H H T H T H T T H H T T T T T T H T H H T H T H T H T H T T H T H H H T H T T T H H T H T H T H T H H H H T T T T H\nNumber of Heads: 521\np1 = Number of Heads/Total Throws: 0.521\n\n\nAnd even more:\n\nmake_throws(10000)\n\nFirst 100 Throws: H T T T H T T T H T T T T H T T H T H H H H T H T H T T H T H T H H H H T T H H H H T H H H H T H T T T H T H H T T T H T H T H T T T H T T T T H T H H H H H T T H H H T T H H H H H H T H T H T T T H\nNumber of Heads: 5047\np1 = Number of Heads/Total Throws: 0.5047\n\n\nAs you can see, the larger number of trials we do, the closer we seem to get to half the tosses showing up heads. Lets see this more systematically:\n\ntrials=np.arange(0, 40000, 1000)\nplt.plot(trials, [np.sum(throw_a_coin(j)=='H')/np.float(j) for j in trials], 'o-', alpha=0.2);\nplt.axhline(0.5, 0, 1, color='r');\nplt.xlabel('number of trials');\nplt.ylabel('probability of heads from simulation');\nplt.title('frequentist probability of heads');\n\n\n\n\n\n\n\n\nThus, the true odds fluctuate about their long-run value of 0.5, in accordance with the model of a fair coin (which we encoded in our simulation by having np.random.choice choose between two possibilities with equal probability), with the fluctuations becoming much smaller (we shall talk a lot more about this later in the book). These fluctations are what give rise to probability distributions.\nEach finite length run is called a sample, which has been obtained from the generative model of our fair coin. Its called generative as we can use the model to generate, using simulation, a set of samples we can play with to understand a model. Such simulation from a model is a key technique which we will come back to again and again in learning from data."
  },
  {
    "objectID": "posts/probability/index.html#the-rules-of-probability",
    "href": "posts/probability/index.html#the-rules-of-probability",
    "title": "Probability",
    "section": "",
    "text": "We have seen multiple notions of probability so far. One might assign probabilities based on symmetry, for eg, 2 sides of a fair coin, or six sides of a fair dice. One might assign probabilities based on doing an experiment. such as the long run number of heads in many coin flips. One might assign probabilities based on beliefs; and one might even assign probabilities to events that have no chance of repeating, such as the 2012 presidential election, or the probability of rain between 2pm and 6pm today.\nThus, the very definition of probability seems to be wishy-washy and subjective. Thus you might wonder how you might work with such probabilities. For this, we turn to the rules of probability.\nThe rules dont care where our probabilities come from, as to how we estimated them, as long as they behave in intuitively sensible ways.\nConsider an example:\nE is the event of getting a heads in a first coin toss, and F is the same for a second coin toss. Here \\(\\Omega\\), the set of all possibilities that can happen when you toss two coins is \\(\\{HH, HT, TH, TT\\}\\). Since E only specifies that the first toss is heads, \\(E=\\{HT, HH\\}\\). Similarly \\(F= {HH, TH}\\) The set of all events that are not E then is \\(\\tilde{E} = {TH, TT}\\).\nThese sets, along with some others are captured in the venn diagram below:\n\n\n\nVenn diagram for the 2-coin-toss sample space\n\n\n\n\nIf E and F are independent events, the probability of both events happening together \\(P(EF)\\) or \\(P(E \\cap F)\\) (read as E and F or E intersection F, respectively) is the multiplication of the individual probabilities.\n\\[ P(EF) = P(E) P(F) .\\]\nIf you made the two independent coin tosses in our example, and you had a fair coin, the probability of both coming up heads is \\((1/2)*(1/2) = 1/4\\). This makes intuitive sense: half the time the first coin comes up heads, and then 1/2 the time the second coin comes up heads, so its 1/4 of the times that both come up heads.\n\n\n\nWe can now ask the question, what is \\(P(E+F)\\), the odds of E alone, F alone, or both together. Translated into English, we are asking, whats the probability that only the first toss was heads, or only the second toss was heads, or that both came up heads? Or in other words, what are the odds of at least one heads? The answer to this question is given by the rule:\n\\[P(E+F) = P(E) + P(F) - P(EF),\\]\nthe “plus” formula, where E+F, read as E or F (also \\(E \\cup F\\), reads as E union F) means “E alone, F alone, or both together”. This rule is a hard one to understand and has a lot of notation, so lets examine it in some detail.\nThere are four ways that these two tosses can arrange themselves, as illustrated by this diagram, adapted from the probability chapter in Feynman’s lectures on Physics..you should read it!.\n\n\n\nTree diagram for 2 coin flips\n\n\nWe can have a HH, HT, TH, or TT. In three out of 4 of these cases, either the first toss was heads, or the second was heads. Thus \\(P(E+F)=3/4\\).\nThe formula says, add the odds that “the first toss was a heads, without worrying about the second one (1/2), to the probability that the second toss was a heads, without worrying about the first one” (1/2). Since this double counts the situation where both are heads; subtract that (1/4):\n\\[\\begin{eqnarray}\nP(E+F) \\, & = &\\, P(E) + P(F) - P(EF)\\\\\n\\frac{3}{4} \\, & = &\\, \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{4}\n\\end{eqnarray}\\]\nArmed with these two formulas, we can tackle the world of conditional and marginal probabilities, and Bayes theorem!\n\n\n\nIf \\(X\\) and \\(Y\\) are two events and \\(p(X)\\) is the probability of the event \\(X\\) to happen. $X^- $ is the complement of \\(X\\), the event which is all the occurrences which are not in \\(X\\). \\(X+Y\\) is the union of \\(X\\) and \\(Y\\); \\(X,Y\\) is the intersection of \\(X\\) and \\(Y\\). (Both \\(X+Y\\) and \\(X,Y\\) are also events.)\n\n\n\n\n\\(p(X) &gt;=0\\); probability must be non-negative\n\\(0 ≤ p(X) ≤ 1 \\;\\) \\(X\\) has probability range from 0 to 1.\n\\(p(X)+p(X^-)=1 \\;\\) \\(X\\) must either happen or not happen. These last two aximoms can be thought of as saying that the probabilities if all events put tohether must sum to 1.\n\\(p(X+Y)=p(X)+p(Y)−p(X,Y) \\;\\) \\(X\\) can happen and \\(Y\\) can happen but we must subtract the cases that are happening together so we do not over-count."
  },
  {
    "objectID": "posts/probability/index.html#random-variables",
    "href": "posts/probability/index.html#random-variables",
    "title": "Probability",
    "section": "",
    "text": "To link the notion of events such as \\(E\\) and collections of events, or probability spaces \\(\\Omega\\) to data, we must introduce the concept of random variables. The following definition is taken from Larry Wasserman’s All of Stats.\nDefinition. A random variable is a mapping\n\\[ X: \\Omega \\rightarrow \\mathbb{R}\\]\nthat assigns a real number \\(X(\\omega)\\) to each outcome \\(\\omega\\). \\(\\Omega\\) is the sample space. Points \\(\\omega\\) in \\(\\Omega\\) are called sample outcomes, realizations, or elements. Subsets of \\(\\Omega\\) are called Events. Say \\(\\omega = HHTTTTHTT\\) then \\(X(\\omega) = 3\\) if defined as number of heads in the sequence \\(\\omega\\).\nWe will assign a real number P(A) to every event A, called the probability of A. We also call P a probability distribution or a probability measure. To qualify as a probability, P must satisfy the three axioms (non-negative, \\(P(\\Omega)=1\\), disjoint probs add)."
  },
  {
    "objectID": "posts/probability/index.html#marginals-and-conditionals-and-bayes-theorem",
    "href": "posts/probability/index.html#marginals-and-conditionals-and-bayes-theorem",
    "title": "Probability",
    "section": "",
    "text": "The diagram below taken from Bishop may be used to illustrate the concepts of conditionals and marginals. Consider two random variables, \\(X\\), which takes the values \\({x_i}\\) where \\(i = 1,...,M\\), and \\(Y\\), which takes the values \\({y_j}\\) where \\(j = 1,...,L\\). The number of instances for which \\(X = x_i\\) and \\(Y = y_j\\) is \\(n_{ij}\\). The number of points in column i where \\(X=x_i\\) is \\(c_i\\), and for the row where \\(Y = y_j\\) is \\(r_j\\).\n\n\n\nJoint probability table notation\n\n\nThen the joint probability of having \\(p(X = x_i, Y= y_j)\\) is in the asymptotic limit of large numbers in the frequency sense of probability \\(n_{ij}/N\\) where is the total number of instances. The \\(X\\) marginal, \\(p(X=x_i)\\) can be obtained by summing instances in all the cells in the i’th column:\n\\[p(X=x_i) = \\sum_j p(X=x_i, Y=y_j)\\]\nLets consider next only those instances for which \\(X=x_i\\). This means that we are limiting our analysis to the ith row. Then, we write the conditional probability of \\(Y = y_j\\) given \\(X = x_i\\) as \\(p(Y = y_j \\mid X = x_i)\\). This is the asymptotic fraction of these instances where \\(Y = y_j\\) and is obtained by dividing the instances in the cell by those in the comumn as\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i}.\\]\nA little algebraic rearrangement gives:\n\\[p(Y = y_j \\mid X = x_i) = \\frac{n_{ij}}{c_i} = \\frac{n_{ij}}{N} / \\frac{c_i}{N},\\]\nor:\n\\[p(Y = y_j \\mid X = x_i) \\times p(X=x_i) =  p(X=x_i, Y=y_j).\\]\nThis is the product rule of probability with conditionals involved.\nLet us simplify the notation by dropping the \\(X=\\) and \\(Y=\\).\nThen we can write the marginal probability of x as a sum over the joint distribution of x and y where we sum over all possibilities of y,\n\\[p(x) = \\sum_y p(x,y) \\].\nWe can rewrite a joint distribution as a product of a conditional and marginal probability,\n\\[ p(x,y) = p(y\\mid x) p(x) \\]\nThe product rule is applied repeatedly to give expressions for the joint probability involving more than two variables. For example, the joint distribution over three variables can be factorized into a product of conditional probabilities:\n\\[ p(x,y,z) = p(x|y,z) \\, p(y,z) = p(x |y,z) \\, p(y|z) p(z) \\]\n\n\nObserve that\n\\[ p(x,y) = p(y\\mid x) p(x) = P(x\\mid y)p(y).\\]\nGiven the product rule one can derive the Bayes rule, which plays a central role in a lot of the things we will be talking:\n\\[ p(y\\mid x) = \\frac{p(x\\mid y) \\, p(y) }{p(x)} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x,y')} = \\frac{p(x\\mid y) \\, p(y) }{\\sum_{y'} p(x\\mid y')p(y')}\\]\n\n\n\nTwo variables are said to be independent if their joint distribution factorizes into a product of two marginal probabilities:\n\\[ p(x,y) = p(x) \\, p(y) \\]\nAnother consequence of independence is that if \\(x\\) and \\(y\\) are independent, the conditional probability of \\(x\\) given \\(y\\) is just the probability of \\(x\\):\n\\[ p(x|y) = p(x) \\]\nIn other words, by conditioning on a particular \\(y\\), we have learned nothing about \\(x\\) because of independence. Two variables \\(x\\) and \\(y\\) and said to be conditionally independent of \\(z\\) if the following holds:\n\\[ p(x,y|z) = p(x|z) p(y|z) \\]\nTherefore, if we learn about z, x and y become independent. Another way to write that \\(x\\) and \\(y\\) are conditionally independent of \\(z\\) is\n\\[ p(x| z, y) = p(x|z) \\]\nIn other words, if we condition on \\(z\\), and now also learn about \\(y\\), this is not going to change the probability of \\(x\\). It is important to realize that conditional independence between \\(x\\) and \\(y\\) does not imply independence between \\(x\\) and \\(y\\).\n\n\n\n\nSally Clark, a lawyer who lost her first son at 11 weeks and her second at 8 weeks, was convicted in 1999. A prominent pediatrician, Sir Roy Meadow, had testified for the prosecution about Sudden Infant Death Syndrome, known as SIDS in the U.S. and cot death in Britain. Citing a government study, Meadow said the incidence of one SIDS death was one in 8,500 in a family like Clark’s–stable, affluent, nonsmoking, with a mother more than 26 years old.\n\n\nThen, despite the fact that some families are predisposed to SIDS, Meadow assumed erroneously that each sibling’s death occurred independently of the other. Multiplying 8,500 by 8,500, he calculated that the chance of two children dying in a family like Sally Clark’s was so rare–one in 73 million–that they must have been murdered.\n\n(from http://www.mcgrayne.com/disc.htm)\np(child 1 dying of sids) = 1/8500\n\nP(child 2 dying of sids) = 1/100\n\nFirst, we look at natural causes of sudden infant death. The chance of one random infant dying from SIDS was about 1 in 1,300 during this period in Britain. Meadow’s argument was flawed and produced a much slimmer chance of natural death. The estimated odds of a second SIDS death in the same family was much larger, perhaps one in 100, because family members can share a common environmental or genetic propensity for SIDS.\n\n\nSecond, we turn to the hypothesis that the babies were murdered. Only about 30 children out of 650,000 annual births in England, Scotland, and Wales were known to have been murdered by their mothers. The number of double murders must be much lower, estimated as 10 times less likely.\n\np(S2 = both children dying of sids) =  0.000007\np(notS2 = not both dying of sids) =  0.999993\n\nData: both children died unexpectedly\nSo now ask, whats:\np(data | S2) = 1\np(data | notS2) = ? both died but not SIDS. Murder? =  30/650000    × 1/10 = 0.000005\nWe want to calculate the “posterior probability”:\np(S2 | data) = P(data | S2) P(S2) /(P(data | S2) P(S2) + P(data|notS2)P(notS2))\n= 1*0.000007/(1*0.000007 + 0.000005*0.999993)\n=0.58\n58% chance of having died from SIDS!\nSally Clark spent 3 years in jail."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html",
    "href": "posts/basicmontecarlo/index.html",
    "title": "Basic Monte Carlo",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('talk')\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nThe basic idea of a Monte Carlo Algorithm is to use randomness to solve what is often a deterministic problem. In this course, we’ll study their application in 3 different places: optimization, integration, and obtaining draws from a probability distribution. These uses are often intertwined: optimization is needed to find modes of distributions and integration to find expectations.\nWikipedia has a facinating bit of history on the subject, from which I quote:\n\nThe first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a Canfield solitaire laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than “abstract thinking” might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to John von Neumann, and we began to plan actual calculations. –Stanislaw Ulam\n\n\nBeing secret, the work of von Neumann and Ulam required a code name.[citation needed] A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name Monte Carlo, which refers to the Monte Carlo Casino in Monaco where Ulam’s uncle would borrow money from relatives to gamble.\n\n\n\n\nTo understand how randomness can be brought to bear upon solving deterministic problems, consider a very simple example: the value of \\(\\pi\\). If you could uniformly generate random numbers on a square, you could ask, how many of these numbers would fall inside a unit circle embedded in and touching the midpoints of the sides of the square. This ratio would be\n\\[\\frac{\\pi \\times 1^2}{2^2} = \\frac{\\pi}{4}.\\]\n\n#area of the bounding box\nbox_area = 4.0    \n\n#number of samples\nN_total = 10000.0 \n\n#drawing random points uniform between -1 and 1\nX = np.random.uniform(low=-1, high=1, size=N_total)  \nY = np.random.uniform(low=-1, high=1, size=N_total)   \n\n# calculate the distance of the points from the center \ndistance = np.sqrt(X**2+Y**2);  \n \n# check if point is inside the circle    \nis_point_inside = distance&lt;1.0\n\n# sum up the hits inside the circle\nN_inside=np.sum(is_point_inside)\n\n# estimate the circle area\ncircle_area = box_area * N_inside/N_total\n\n# some nice visualization\nplt.scatter(X,Y, c=is_point_inside, s=5.0, edgecolors='none', cmap=plt.cm.Paired)  \nplt.axis('equal')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# text output\nprint(\"Area of the circle = \", circle_area)\nprint(\"pi = \", np.pi)\n\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n\n\nArea of the circle =  3.1344\npi =  3.141592653589793\n\n\n\n\n\n\n\n\n\nIntuitively, one might expect our estimate of \\(\\pi\\) to get better as we draw more and more samples: we are covering the areas with samples much better when we do that.\nLets try to think about the mathematics in the intuition which tells us that we can calculate \\(\\pi\\) in this way.\nThe area of the circle C can be obtained by computing a double integral like so:\n\\[A = \\int_x \\int_y I_{\\in C}(x, y) dx dy = \\int \\int_{\\in C} dx dy \\]\nwhere \\(I_{\\in C} (x, y) = 1\\) if \\(x,y \\in C\\) and \\(I_{\\in C}(x) = 0\\) if \\(x,y \\notin C\\).\nThis is basically adding up all the small area elements inside the circle.\nRemember from the LOTUS:\n\\[E_f[I_{\\in C} (X,Y)] = \\int I_{\\in C} (X,Y) dF(X,Y) = \\int_{\\in C} dF(X,Y) = \\int \\int_{\\in C} f_{X,Y} (x,y) dx dy = p(X,Y \\in C)\\]\nand then we can use the law of large numbers to calculate this expectation and thus this probability.\nThe relationship of the expression all the way on the left to that all the way on the right is simply the law of large numbers we saw before. This is a distribution independent statement.\nBut the critical thing to notice is that:\n\\[\\int \\int_{\\in C} f_{X,Y} (x,y) dx dy  =  \\frac{1}{V} \\int \\int_{\\in C}  dx dy = E_f[I_{\\in C} (X,Y)]\\]\nonce we choose a uniform distribution. Here \\(V\\) is the support, the normalizing factor..here 4. The expectation from the law of large numbers comes from a sequence of identically distributed bernoullis (independent of \\(f\\) which here is uniform). All we have to do, is just like before in the law, count the frequency of samples inside.\n\n\n\nThis simple scenario of inside-or-outside can be used as a general (but poor, as missing increases exponentially with dimension) way to use the generation of samples to carry out integration\n\n\n\nBounding box for hit-or-miss Monte Carlo integration\n\n\nYou basically generate samples from a uniform distribution with support on the rectangle and see how many fall below \\(y(x)\\) at a specific x.\nThis is the basic idea behind rejection sampling"
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#monte-carlo",
    "href": "posts/basicmontecarlo/index.html#monte-carlo",
    "title": "Basic Monte Carlo",
    "section": "",
    "text": "The basic idea of a Monte Carlo Algorithm is to use randomness to solve what is often a deterministic problem. In this course, we’ll study their application in 3 different places: optimization, integration, and obtaining draws from a probability distribution. These uses are often intertwined: optimization is needed to find modes of distributions and integration to find expectations.\nWikipedia has a facinating bit of history on the subject, from which I quote:\n\nThe first thoughts and attempts I made to practice [the Monte Carlo Method] were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires. The question was what are the chances that a Canfield solitaire laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations, I wondered whether a more practical method than “abstract thinking” might not be to lay it out say one hundred times and simply observe and count the number of successful plays. This was already possible to envisage with the beginning of the new era of fast computers, and I immediately thought of problems of neutron diffusion and other questions of mathematical physics, and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations. Later [in 1946], I described the idea to John von Neumann, and we began to plan actual calculations. –Stanislaw Ulam\n\n\nBeing secret, the work of von Neumann and Ulam required a code name.[citation needed] A colleague of von Neumann and Ulam, Nicholas Metropolis, suggested using the name Monte Carlo, which refers to the Monte Carlo Casino in Monaco where Ulam’s uncle would borrow money from relatives to gamble."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#estimate-the-area-of-a-unit-circle",
    "href": "posts/basicmontecarlo/index.html#estimate-the-area-of-a-unit-circle",
    "title": "Basic Monte Carlo",
    "section": "",
    "text": "To understand how randomness can be brought to bear upon solving deterministic problems, consider a very simple example: the value of \\(\\pi\\). If you could uniformly generate random numbers on a square, you could ask, how many of these numbers would fall inside a unit circle embedded in and touching the midpoints of the sides of the square. This ratio would be\n\\[\\frac{\\pi \\times 1^2}{2^2} = \\frac{\\pi}{4}.\\]\n\n#area of the bounding box\nbox_area = 4.0    \n\n#number of samples\nN_total = 10000.0 \n\n#drawing random points uniform between -1 and 1\nX = np.random.uniform(low=-1, high=1, size=N_total)  \nY = np.random.uniform(low=-1, high=1, size=N_total)   \n\n# calculate the distance of the points from the center \ndistance = np.sqrt(X**2+Y**2);  \n \n# check if point is inside the circle    \nis_point_inside = distance&lt;1.0\n\n# sum up the hits inside the circle\nN_inside=np.sum(is_point_inside)\n\n# estimate the circle area\ncircle_area = box_area * N_inside/N_total\n\n# some nice visualization\nplt.scatter(X,Y, c=is_point_inside, s=5.0, edgecolors='none', cmap=plt.cm.Paired)  \nplt.axis('equal')\nplt.xlabel('x')\nplt.ylabel('y')\n\n# text output\nprint(\"Area of the circle = \", circle_area)\nprint(\"pi = \", np.pi)\n\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n//anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n\n\nArea of the circle =  3.1344\npi =  3.141592653589793\n\n\n\n\n\n\n\n\n\nIntuitively, one might expect our estimate of \\(\\pi\\) to get better as we draw more and more samples: we are covering the areas with samples much better when we do that.\nLets try to think about the mathematics in the intuition which tells us that we can calculate \\(\\pi\\) in this way.\nThe area of the circle C can be obtained by computing a double integral like so:\n\\[A = \\int_x \\int_y I_{\\in C}(x, y) dx dy = \\int \\int_{\\in C} dx dy \\]\nwhere \\(I_{\\in C} (x, y) = 1\\) if \\(x,y \\in C\\) and \\(I_{\\in C}(x) = 0\\) if \\(x,y \\notin C\\).\nThis is basically adding up all the small area elements inside the circle.\nRemember from the LOTUS:\n\\[E_f[I_{\\in C} (X,Y)] = \\int I_{\\in C} (X,Y) dF(X,Y) = \\int_{\\in C} dF(X,Y) = \\int \\int_{\\in C} f_{X,Y} (x,y) dx dy = p(X,Y \\in C)\\]\nand then we can use the law of large numbers to calculate this expectation and thus this probability.\nThe relationship of the expression all the way on the left to that all the way on the right is simply the law of large numbers we saw before. This is a distribution independent statement.\nBut the critical thing to notice is that:\n\\[\\int \\int_{\\in C} f_{X,Y} (x,y) dx dy  =  \\frac{1}{V} \\int \\int_{\\in C}  dx dy = E_f[I_{\\in C} (X,Y)]\\]\nonce we choose a uniform distribution. Here \\(V\\) is the support, the normalizing factor..here 4. The expectation from the law of large numbers comes from a sequence of identically distributed bernoullis (independent of \\(f\\) which here is uniform). All we have to do, is just like before in the law, count the frequency of samples inside."
  },
  {
    "objectID": "posts/basicmontecarlo/index.html#hit-or-miss-method",
    "href": "posts/basicmontecarlo/index.html#hit-or-miss-method",
    "title": "Basic Monte Carlo",
    "section": "",
    "text": "This simple scenario of inside-or-outside can be used as a general (but poor, as missing increases exponentially with dimension) way to use the generation of samples to carry out integration\n\n\n\nBounding box for hit-or-miss Monte Carlo integration\n\n\nYou basically generate samples from a uniform distribution with support on the rectangle and see how many fall below \\(y(x)\\) at a specific x.\nThis is the basic idea behind rejection sampling"
  },
  {
    "objectID": "posts/frequentist.html",
    "href": "posts/frequentist.html",
    "title": "Frequentist Statistics",
    "section": "",
    "text": "\\[\n\\newcommand{\\Ex}{\\mathbb{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\SampleAvg}{\\frac{1}{N({S})} \\sum_{s \\in {S}}}\n\\newcommand{\\indic}{\\mathbb{1}}\n\\newcommand{\\avg}{\\overline}\n\\newcommand{\\est}{\\hat}\n\\newcommand{\\trueval}[1]{#1^{*}}\n\\newcommand{\\Gam}[1]{\\mathrm{Gamma}#1}\n\\]"
  },
  {
    "objectID": "posts/frequentist.html#what-is-data",
    "href": "posts/frequentist.html#what-is-data",
    "title": "Frequentist Statistics",
    "section": "What is data?",
    "text": "What is data?\nWhat is data? Frequentist statistics is one answer to this philosophical question. It treats data as a sample from an existing population.\nThis notion is probably clearest to you from elections, where some companies like Zogby or CNN take polls. The sample in these polls maybe a 1000 people, but they “represent” the electoral population at large. We attempt to draw inferences about how the population will vote based on these samples.\nWe model the sample we have. This model typically involves having some kind of distribution, some kind of algorithm, some kind of story that characterizes the data. These descriptions have, usually, some parameters which need estimation.\nFrequentist analysis considers these parameters as fixed and data as varying (stochastic), with our data as one possible sample from the population.\n\nThe Data story\nData analysis involves coming up with a story of how the data came to be. This may be a causal story, or a descriptive one (correlational, associative). The critical point is this:\nThe story must be sufficient to specify an algorithm to simulate new data. This is the model we have been talking about: a formal probability model. And once we have pinned it down from our existing sample, using a method such as Maximum Likelihood Estimation talked about below, we can use it as a generating mechanism.\nConsider, for example, tossing a globe in the air and catching it. When you catch it mark whats under your right index finger: W for water, L for land.\nLets say you toss the globe 10 times and get something like WLWWWLWlWW. We wish to analyze this experiment to figure how much of the earth is covered in water (according to the globe, at any rate!).\nLet us say that our model is:\n\nThe true proportion of water is \\(p\\).\nWe use this as a Bernoulli probability for each globe toss, where \\(p\\) is thus the probability that you get a W. This assumption is one of being Identically Distributed.\nEach globe toss is Independent of the other.\n\nAssumptions 2 and 3 taken together are called IID, or Independent and Identially Distributed Data."
  },
  {
    "objectID": "posts/frequentist.html#a-probabilistic-model",
    "href": "posts/frequentist.html#a-probabilistic-model",
    "title": "Frequentist Statistics",
    "section": "A probabilistic model",
    "text": "A probabilistic model\n(from the data story)\nThe components of the model depend upon what kind of statistical analysis we are doing. For Frequentist analysis, the components are:\n\nThe likelihood, or the plausibility of the data under the model\nand the parameters which go into this plausibility.\n\nFor our example, we begin by enumerating the events. These are W and L. There’s nothing else.\nThen we consider N such tosses and ask the question, how often would we see Ws.\nThis given by the Binomial Distribution, the distribution of the number of successes in a sequence of \\(n\\) independent yes/no experiments, or Bernoulli trials, each of which yields success with probability \\(p\\). The Binomial distribution is an extension of the Bernoulli when \\(n&gt;1\\) or the Bernoulli is the a special case of the Binomial when \\(n=1\\).\n\\[P(X = k \\mid n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nwhere\n\\[{n\\choose k}=\\frac{n!}{k!(n-k)!}\\]\nHow did we obtain this? The \\(p^k(1-p)^{n-k}\\) comes simply from multiplying the probabilities for each bernoulli trial; there are \\(k\\) 1’s or yes’s, and \\(n-k\\) 0’s or no’s. The \\({n\\choose k}\\) comes from counting the number of ways in which each event happens: this corresponds to counting all the paths that give the same number of heads in the diagram above.\nWe show the distribution below for 200 trials.\nfrom scipy.stats import binom\nplt.figure(figsize=(12,6))\nk = np.arange(0, 200)\nfor p, color in zip([0.1, 0.3, 0.7, 0.7, 0.9], colors):\n    rv = binom(200, p)\n    plt.plot(k, rv.pmf(k), '.', lw=2, color=color, label=p)\n    plt.fill_between(k, rv.pmf(k), color=color, alpha=0.5)\nq=plt.legend()\nplt.title(\"Binomial distribution\")\nplt.tight_layout()\nq=plt.ylabel(\"PDF at $k$\")\nq=plt.xlabel(\"$k$\")\n\n\n\nBinomial distribution for various values of p\n\n\nNow we use a method to fit our model and find the parameter \\(p\\), or rather, the estimate \\(\\hat{p}\\) that we can obtain from our sample. Once we have that, we can use the Binomial distribution to generate new samples.\nNote that there is a problem with this, in that we dont know the true value \\(p^*\\) of the globe-toss model (speaking in the frequentist paradigm). Thus we are generating new samples from our estimate, rather than our true value."
  },
  {
    "objectID": "posts/frequentist.html#maximum-likelihood-estimation",
    "href": "posts/frequentist.html#maximum-likelihood-estimation",
    "title": "Frequentist Statistics",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nOne of the techniques used to estimate parameters in frequentist statistics, from the data in a given sample, is maximum likelihood estimation. Briefly, the idea behind it is:\nThe likelihood for IID data \\(x_1,...,x_n\\), is the product\n\\[\nL(\\lambda) = \\prod_{i=1}^n P(x_i | \\lambda)\n\\]\ngives us a measure of how likely it is to observe values \\(x_1,...,x_n\\) given the parameters \\(\\lambda\\). Maximum likelihood fitting consists of choosing the appropriate “likelihood” function \\(L=P(X \\mid \\lambda)\\) to maximize for a given set of observations. How likely are the observations if the model is true?\nAn image can explain this better. We want to choose the distribution that maximizes the product of the vertical lines. Here the blue does better, but it is not clear if the blue is the best.\n\n\n\nTwo Gaussians illustrating maximum likelihood estimation\n\n\nOften it is easier and numerically more stable to maximize the log likelihood:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n ln(P(x_i \\mid \\lambda))\n\\]\nNotice that the definition here is a bit different from that for the question of the globe tosses above: there the data was \\(k\\), the number of W tosses and not the exact order, and so we formulate the question in that form.\nSo dont follow the formula blindly, but think of (a) what is the data, and (b) what is the data generating mechanism!\n\nMLE for binomial.\nThere:\n\\[P(X = k \\mid n, p) = {n\\choose k}p^k(1-p)^{n-k} \\]\nSo:\n\\[\\ell = log({n\\choose k}) + k log(p) + (n-k) log(1-p)\\]\nDifferentiating with respect to \\(p\\) and setting to 0 yields:\n\\[\\frac{d\\ell}{dp} = \\frac{k}{p}  - \\frac{n -k}{1-p} = 0\\]\nwhich gives us:\n\\[p_{MLE} = \\frac{k}{n}\\]\nwhich you might have intuitively expected."
  },
  {
    "objectID": "posts/frequentist.html#point-estimates",
    "href": "posts/frequentist.html#point-estimates",
    "title": "Frequentist Statistics",
    "section": "Point estimates",
    "text": "Point estimates\nIn frequentist statistics, the data we have in hand, is viewed as a sample from a population. So if we want to calculate some quantity of the population, like say the mean, we estimate it on the sample.\nThis is because we’ve been given only one sample. Ideally we’d want to see the population, but we have no such luck.\nThe parameter estimate is computed by applying an estimator \\(F\\) to the sample data \\(D\\), so \\(\\est{\\mu} = F(D)\\).\nThe parameter is viewed as fixed and the data as random, which is the exact opposite of the Bayesian approach which you will learn later in this class.\nIf you assume that your model describes the true generating process for the data, then there is some true \\(\\trueval{\\mu}\\) . We dont know this. The best we can do to start with is to estimate the \\(\\est{\\mu}\\) from the data set we have.\n\nFrom single to multiple estimates\nNow, imagine that I let you peek at the entire population in this way: I gave you some M data sets drawn from the population, and you can now find \\(\\mu\\) on each such dataset, of which the one we have here is one. So, we’d have M estimates of the \\(\\mu\\).\nThus if we had many replications of this data set: that is, an ensemble of data sets, for example, we can compute other \\(\\est{\\mu}\\), and begin to construct what is called the sampling distribution of \\(\\mu\\).\nBut we dont."
  },
  {
    "objectID": "posts/frequentist.html#sampling-distribution-of-the-parameter",
    "href": "posts/frequentist.html#sampling-distribution-of-the-parameter",
    "title": "Frequentist Statistics",
    "section": "Sampling Distribution of the parameter",
    "text": "Sampling Distribution of the parameter\nWhat you are doing is sampling M Data Sets \\(D_i\\) from the true population. We will now calculate M \\(\\est{\\mu}_i\\), one for each dataset. As we let \\(M \\rightarrow \\infty\\), the distribution induced on \\(\\est{\\mu}\\) is the sampling distribution of the estimator.\nOur estimation could be of anything, even for example the \\(\\lambda\\) we were tying to find with MLE (F would be the MLE estimation process).\nWe can use the sampling distribution to put confidence intervals on the estimation of the parameters, for example."
  },
  {
    "objectID": "posts/frequentist.html#bootstrap",
    "href": "posts/frequentist.html#bootstrap",
    "title": "Frequentist Statistics",
    "section": "Bootstrap",
    "text": "Bootstrap\nBootstrap tries to approximate our sampling distribution. If we knew the true parameters of the population, we could generate M fake datasets. Then we could compute the parameter (or another estimator) on each one of these, to get a empirical sampling distribution of the parameter or estimator.\n\nParametric Bootstrap\nBut we dont have the true parameter. So we generate these samples, using the parameter we calculated. This is the parametric bootstrap. The process is illustrated in the diagram below, taken from Shalizi:\n\n\n\nThe parametric bootstrap process\n\n\nThere are 3 sources of error with respect to the sampling distribution that come from the bootstrap:\n\nsimulation error: the number of samples M is finite. This can be made arbitrarily small by making M large\nstatistical error: resampling from an estimated parameter is not the “true” data generating process. Often though, the distribution of an estimator from the samples around the truth is more invariant, so subtraction is a good choice in reducing the sampling error\nspecification error: the model isnt quite good.\n\n\n\nNon-parametric bootstrap\nTo address specification error, alternatively, we sample with replacement the X from our original sample D, generating many fake datasets, and then compute the distribution on the parameters as before. This is the non parametric bootstrap. We want to sample with replacement, for if we do so, more typical values will be represented more often in the multiple datasets we create.\nHere we are using the empirical distribution, since it comes without any model preconceptions. This process may be illustrated so:\n\n\n\nThe non-parametric bootstrap process"
  },
  {
    "objectID": "posts/samplingclt/index.html",
    "href": "posts/samplingclt/index.html",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "# The %... is an iPython thing, and is not part of the Python language.\n# In this case we're just telling the plotting library to draw things on\n# the notebook, instead of on a separate window.\n%matplotlib inline\n# See all the \"as ...\" contructs? They're just aliasing the package names.\n# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\nimport numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport time\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nsns.set_context(\"poster\")\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\nLets do some more coin flips; this time we’ll do them in many replications. We’ll establish some terminology at first.\nWe will do a large set of replications M, in each of which we will do many coin flips N. We’ll call the result of each coin flip an observation, and a single replication a sample of observations. Thus the number of samples is M, and the sample size is N. These samples have been chosen from a population of size \\(n &gt;&gt; N\\).\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\n\ndef make_throws(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.mean(start, axis=1)\n\nWe show the mean over the observations, or sample mean, for a sample size of 10, with 20 replications. There are thus 20 means.\n\nmake_throws(number_of_samples=20, sample_size=10)\n\narray([ 0.5,  0.8,  0.5,  1. ,  0.7,  0.7,  0.6,  0.6,  0.7,  1. ,  0.7,\n        0.4,  0.4,  0.4,  0.4,  0.7,  0.7,  0.5,  0.5,  0.5])\n\n\nLet us now do 200 replications, each of which has a sample size of 1000 flips, and store the 200 means for each sample size from 1 to 1000 in sample_means.\n\nsample_sizes=np.arange(1,1001,1)\nsample_means = [make_throws(number_of_samples=200, sample_size=i) for i in sample_sizes]\n\nLets formalize what we are up to. Lets call the N random variables in the \\(m^{th}\\) sample \\(x_{m1},x_{m2},...,x_{mN}\\) and lets define the sample mean\n\\[\\bar{x_m}(N) = \\frac{1}{N}\\, \\sum_{i=1}^{N} x_{mi} \\]\nNow imagine the size of the sample becoming large, asymptoting to the size of an infinite or very large population (ie the sample becomes the population). Then you would expect the sample mean to approach the mean of the population distribution. This is just a restatement of the law of large numbers.\nOf course, if you drew many different samples of a size N (which is not infinite), the sample means \\(\\bar{x_1}\\), \\(\\bar{x_2}\\), etc would all be a bit different from each other. But the law of large numbers intuitively indicates that as the sample size gets very large and becomes an infinite population size, these slightly differeing means would all come together and converge to the population (or distribution) mean.\nTo see this lets define, instead, the mean or expectation of the sample means over the set of samples or replications, at a sample size N:\n\\[E_{\\{R\\}}(\\bar{x}) = \\frac{1}{M} \\,\\sum_{m=1}^{M} \\bar{x_m}(N) ,\\] where \\(\\{R\\}\\) is the set of M replications, and calculate and plot this quantity.\n\nmean_of_sample_means = [np.mean(means) for means in sample_means]\n\n\nplt.plot(sample_sizes, mean_of_sample_means);\nplt.ylim([0.480,0.520]);\n\n\n\n\n\n\n\n\nNot surprisingly, the mean of the sample means converges to the distribution mean as the sample size N gets very large.\n\n\n\nIn data science, we are always interested in understanding the world from incomplete data, in other words from a sample or a few samples of a population at large. Our experience with the world tells us that even if we are able to repeat an experiment or process, we will get more or less different answers the next time. If all of the answers were very different each time, we would never be able to make any predictions.\nBut some kind of answers differ only a little, especially as we get to larger sample sizes. So the important question then becomes one of the distribution of these quantities from sample to sample, also known as a sampling distribution.\nSince, in the real world, we see only one sample, this distribution helps us do inference, or figure the uncertainty of the estimates of quantities we are interested in. If we can somehow cook up samples just somewhat different from the one we were given, we can calculate quantities of interest, such as the mean on each one of these samples. By seeing how these means vary from one sample to the other, we can say how typical the mean in the sample we were given is, and whats the uncertainty range of this quantity. This is why the mean of the sample means is an interesting quantity; it characterizes the sampling distribution of the mean, or the distribution of sample means.\nWe can see this mathematically by writing the mean or expectation value of the sample means thus:\n\\[E_{\\{R\\}}(N\\,\\bar{x}) = E_{\\{R\\}}(x_1 + x_2 + ... + x_N) = E_{\\{R\\}}(x_1) + E_{\\{R\\}}(x_2) + ... + E_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the expectations in the right hand side can be replaced by the population mean using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nE_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\mu\\\\\nE_{\\{R\\}}(\\bar{x}) &=& \\mu\n\\end{eqnarray}\\]\nwhich tells us that in the limit of a large number of replications the expectation value of the sampling means converges to the population mean. This limit gives us the true sampling distribution, as opposed to what we might estimate from our finite set of replicates. (Thus there our \\(E_{\\{R\\}}\\) would be replaced by some \\(E_{fs}\\) where by \\(fs\\) we wish to indicate the pmf or density of the sampling distribution).\n\n\nWe can see what the estimated sampling distribution of the mean looks like at different sample sizes.\n\nsample_means_at_size_10=sample_means[9]\nsample_means_at_size_100=sample_means[99]\nsample_means_at_size_1000=sample_means[999]\n\n\nplt.hist(sample_means_at_size_10, bins=np.arange(0,1,0.01), alpha=0.5);\nplt.hist(sample_means_at_size_100, bins=np.arange(0,1,0.01), alpha=0.4);\nplt.hist(sample_means_at_size_1000, bins=np.arange(0,1,0.01), alpha=0.3);\n\n\n\n\n\n\n\n\nThe distribution is much tighter at large sample sizes, and that you can have way low and way large means at small sample sizes. Indeed there are means as small as 0.1 at a sample size of 10, and as small as 0.3 at a sample size of 100.\nLets plot the distribution of the mean as a function of sample size.\n\nfor i in sample_sizes:\n    if i %50 ==0 and i &lt; 1000:\n        plt.scatter([i]*200, sample_means[i], alpha=0.05);\nplt.xlim([0,1000])\nplt.ylim([0.25,0.75]);\n\n\n\n\n\n\n\n\n\n\n\nLet the underlying distribution from which we have drawn our samples have, additionally to a well defined mean \\(\\mu\\), a well defined variance \\(\\sigma^2\\).\nThen, as before:\n\\[V_{\\{R\\}}(N\\,\\bar{x}) = V_{\\{R\\}}(x_1 + x_2 + ... + x_N) = V_{\\{R\\}}(x_1) + V_{\\{R\\}}(x_2) + ... + V_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the variances in the right hand side can be replaced by the population variance using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nV_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\sigma^2\\\\\nV(\\bar{x}) &=& \\frac{\\sigma^2}{N}\n\\end{eqnarray}\\]\nThis simple formula is called De-Moivre’s formula, and explains the tell-tale triangular plot we saw above, with lots of variation at low sample sizes turning into a tight distribution at large sample size(N).\nThe square root of \\(V\\), or the standard deviation of the sampling distribution of the mean (in other words, the distribution of sample means) is also called the Standard Error.\nWe can obtain the standard deviation of the sampling distribution of the mean at different sample sizes and plot it against the sample size, to confirm the \\(1/\\sqrt(N)\\) behaviour.\n\nstd_of_sample_means = [np.std(means) for means in sample_means]\n\n\nplt.plot(np.log10(sample_sizes), np.log10(std_of_sample_means));\n\n\n\n\n\n\n\n\nLet us plot again the distribution of sample means at a large sample size, \\(N=1000\\). What distribution is this?\n\nplt.hist(sample_means_at_size_1000, bins=np.arange(0.4,0.6,0.002));\n\n\n\n\n\n\n\n\nLets step back and try and think about what this all means. As an example, say I have a weight-watchers’ study of 1000 people, whose average weight is 150 lbs with standard deviation of 30lbs. If I was to randomly choose many samples of 100 people each, the mean weights of those samples would cluster around 150lbs with a standard error of 30/\\(\\sqrt{100}\\) = 3lbs. Now if i gave you a different sample of 100 people with an average weight of 170lbs, this weight would be more than 6 standard errors beyond the population mean, 1 and would thus be very unlikely to be from the weight watchers group.\n1 this example is motivated by the crazy bus example in Charles Whelan’s excellent Naked Statistics Book\n\n\nWe saw in the last section that the sampling distribution of the mean itself has a mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\). This distribution is called the Gaussian or Normal Distribution, and is probably the most important distribution in all of statistics.\nThe probability density of the normal distribution is given as:\n\\[ N(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{(x-\\mu)^2}{2s^2} } .\\]\nThe expected value of the Gaussian distribution is \\(E[X]=\\mu\\) and the variance is \\(Var[X]=s^2\\).\n\nnorm =  sp.stats.norm\nx = np.linspace(-5,5, num=200)\n\n\nfig = plt.figure(figsize=(12,6))\nfor mu, sigma, c in zip([0.5]*3, [0.2, 0.5, 0.8], sns.color_palette()[:3]):\n    plt.plot(x, norm.pdf(x, mu, sigma), lw=2, \n             c=c, label = r\"$\\mu = {0:.1f}, s={1:.1f}$\".format(mu, sigma))\n    plt.fill_between(x, norm.pdf(x, mu, sigma), color=c, alpha = .4)\n    \n    \nplt.xlim([-5,5])\nplt.legend(loc=0)\nplt.ylabel(\"PDF at $x$\")\nplt.xlabel(\"$x$\");\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe reason for the distribution’s importance is the Central Limit Theorem(CLT). The theorem is stated as thus, very similar to the law of large numbers:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nIn other words:\n\\[s^2 = \\frac{\\sigma^2}{N}.\\]\nThis is true, regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nStrictly speaking, under some conditions called Lyapunov conditions, the variables \\(x_i\\) dont have to be identically distributed, as long as \\(\\mu\\) is the mean of the means and \\(\\sigma^2\\) is the sum of the individual variances. This has major consequences, for the importance of this theorem.\nMany random variables can be thought of as having come from the sum of a large number of small and independent effects. For example human height or weight can be thought of as the sum as a large number of genetic and environmental factors, which add to increase or decrease height or weight respectively. Or think of a measurement of a height. There are lots of ways things could go wrong: frayed tapes, stretched tapes, smudged marks, bad lining up of the eye, etc. These are all independent and have no systematic error in one direction or the other.\nThen the sum of these factors, as long as there are a large number of them, will be distributed as a gaussian.[this has nothing to do with the sampling distribution of the mean but is part of the origin story of the gaussian distribution]\nAs a rule of thumb, the CLT starts holding at \\(N \\sim 30\\).\n\n\nThe sample mean, or mean of the random variables \\(x_{mi}\\) in the sample \\(m\\), has a sampling distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\), as shown before. Now for large sample sizes we can go further and use the CLT theorem to say that this distribution is the normal distribution,\n\\[S_N \\sim N(\\mu, \\frac{\\sigma^2}{N})\\].\nThe preciseness of saying that we have a gaussian is a huge gain in our expository power. For example, for the case of the weight-watchers program above, a separation of 20lbs is more than 3 standard errors away, which corresponds to being way in the tail of a gaussian distribution. Because we can now quantify the area under the curve, we can say that 99.7% of the sample means lie within 9lbs of 150. Thus you can way easily reject the possibility that the new sample is from the weight-watchers program with 99.7% confidence.\nIndeed, the CLT allows us to take the reduction in variance we get from large samples, and make statements in different cases that are quite strong:\n\nif we know a lot about the population, and randomly sampled 100 points from it, the sample mean would be with 99.7% confidence within \\(0.3\\sigma\\) of the population mean. And thus, if \\(\\sigma\\) is small, the sample mean is quite representative of the population mean.\nThe reverse: if we have a well sampled 100 data points, we could make strong statements about the population as a whole. This is indeed how election polling and other sampling works.\nwe can infer, as we just did, if a sample is consistent with a population\nby the same token, you can compare two samples and infer if they are from the same population.\n\n\n\n\n\nAt this point you might be curious about what the sampling distribution of the variance looks like, and what can we surmise from it about the variance of the entire sample. We can do this, just like we did for the means. We’ll stick with a high number of replicates and plot the mean of the sample variances as well as the truish sampling distribution of the variances at a sample size of 100.\n\ndef make_throws_var(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.var(start, axis=1)\nsample_vars_1000_replicates = [make_throws_var(number_of_samples=1000, sample_size=i) for i in sample_sizes]\nmean_of_sample_vars_1000 = [np.mean(vars) for vars in sample_vars_1000_replicates]\nplt.plot(sample_sizes, mean_of_sample_vars_1000);\nplt.xscale(\"log\");\n\n\n\n\n\n\n\n\nThe “mean sample variance” asymptotes to the true variance of 0.25 by a sample size of 100.\nHow well does the sample variance estimate the true variance?\nIf \\(V_m\\) denotes the variance of a sample,\n\\[ N\\,V_m = \\sum_{i=1}^{N} (x_{mi} - \\bar{x_m})^2 = \\sum_{i=1}^{N}(x_{mi} - \\mu)^2 - N\\,(\\bar{x_m} - \\mu)^2. \\]\nThen \\[E_{\\{R\\}}(N\\,V_m) = E_{\\{R\\}}(\\sum_{i=1}^{N}(x_{mi} - \\mu)^2) - E_{\\{R\\}}(N\\,(\\bar{x_m} - \\mu)^2)\\] In the asymptotic limit of a very large number of replicates, we can then write \\[E(N\\,V) = N\\,\\sigma^2 - \\sigma^2, \\] and thus we have \\[E(V) = \\frac{N-1}{N} \\,\\sigma^2\\].\nIn other words, the expected value of the sample variance is LESS than the actual variance. This should not be surprising: consider for example a sample of size 1 from the population. There is zero variance! More generally, whenever you sample a population, you tend to pick the more likely members of the population, and so the variance in the sample is less than the variance in the population.\nAn interesting application of this idea, as Shalizi points out in http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/, is that the loss of variability due to sampling of genes is indeed the origin of genetic drift. More prosaically, the fact that the above graph of expected sample variance against sample size asymptotes to 0.25 is as \\(\\frac{N-1}{N}\\) if very close to 1 at large N.\nOr put another way, you ought to correct your sample variances by a factor of \\(\\frac{n}{n-1}\\) to estimate the population variance, which itself works as the sampling distribution of the sample variance is rather tight, as seen below.\nThat is, defining the sample variance with \\(n-1\\) in the denominator instead of \\(n\\) gives you an unbiased eatimator of the true variance. This is why, for example, Pandas will do this by default for series and dataframes. (numpy wont, so beware!).\n\nplt.hist(sample_vars_1000_replicates[99], bins=np.arange(0.2,0.26,0.001), alpha=0.2, normed=True);"
  },
  {
    "objectID": "posts/samplingclt/index.html#samples-from-a-population-of-coin-flips",
    "href": "posts/samplingclt/index.html#samples-from-a-population-of-coin-flips",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "Lets do some more coin flips; this time we’ll do them in many replications. We’ll establish some terminology at first.\nWe will do a large set of replications M, in each of which we will do many coin flips N. We’ll call the result of each coin flip an observation, and a single replication a sample of observations. Thus the number of samples is M, and the sample size is N. These samples have been chosen from a population of size \\(n &gt;&gt; N\\).\n\nfrom scipy.stats.distributions import bernoulli\ndef throw_a_coin(n):\n    brv = bernoulli(0.5)\n    return brv.rvs(size=n)\n\n\n\ndef make_throws(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.mean(start, axis=1)\n\nWe show the mean over the observations, or sample mean, for a sample size of 10, with 20 replications. There are thus 20 means.\n\nmake_throws(number_of_samples=20, sample_size=10)\n\narray([ 0.5,  0.8,  0.5,  1. ,  0.7,  0.7,  0.6,  0.6,  0.7,  1. ,  0.7,\n        0.4,  0.4,  0.4,  0.4,  0.7,  0.7,  0.5,  0.5,  0.5])\n\n\nLet us now do 200 replications, each of which has a sample size of 1000 flips, and store the 200 means for each sample size from 1 to 1000 in sample_means.\n\nsample_sizes=np.arange(1,1001,1)\nsample_means = [make_throws(number_of_samples=200, sample_size=i) for i in sample_sizes]\n\nLets formalize what we are up to. Lets call the N random variables in the \\(m^{th}\\) sample \\(x_{m1},x_{m2},...,x_{mN}\\) and lets define the sample mean\n\\[\\bar{x_m}(N) = \\frac{1}{N}\\, \\sum_{i=1}^{N} x_{mi} \\]\nNow imagine the size of the sample becoming large, asymptoting to the size of an infinite or very large population (ie the sample becomes the population). Then you would expect the sample mean to approach the mean of the population distribution. This is just a restatement of the law of large numbers.\nOf course, if you drew many different samples of a size N (which is not infinite), the sample means \\(\\bar{x_1}\\), \\(\\bar{x_2}\\), etc would all be a bit different from each other. But the law of large numbers intuitively indicates that as the sample size gets very large and becomes an infinite population size, these slightly differeing means would all come together and converge to the population (or distribution) mean.\nTo see this lets define, instead, the mean or expectation of the sample means over the set of samples or replications, at a sample size N:\n\\[E_{\\{R\\}}(\\bar{x}) = \\frac{1}{M} \\,\\sum_{m=1}^{M} \\bar{x_m}(N) ,\\] where \\(\\{R\\}\\) is the set of M replications, and calculate and plot this quantity.\n\nmean_of_sample_means = [np.mean(means) for means in sample_means]\n\n\nplt.plot(sample_sizes, mean_of_sample_means);\nplt.ylim([0.480,0.520]);\n\n\n\n\n\n\n\n\nNot surprisingly, the mean of the sample means converges to the distribution mean as the sample size N gets very large."
  },
  {
    "objectID": "posts/samplingclt/index.html#the-notion-of-a-sampling-distribution",
    "href": "posts/samplingclt/index.html#the-notion-of-a-sampling-distribution",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "In data science, we are always interested in understanding the world from incomplete data, in other words from a sample or a few samples of a population at large. Our experience with the world tells us that even if we are able to repeat an experiment or process, we will get more or less different answers the next time. If all of the answers were very different each time, we would never be able to make any predictions.\nBut some kind of answers differ only a little, especially as we get to larger sample sizes. So the important question then becomes one of the distribution of these quantities from sample to sample, also known as a sampling distribution.\nSince, in the real world, we see only one sample, this distribution helps us do inference, or figure the uncertainty of the estimates of quantities we are interested in. If we can somehow cook up samples just somewhat different from the one we were given, we can calculate quantities of interest, such as the mean on each one of these samples. By seeing how these means vary from one sample to the other, we can say how typical the mean in the sample we were given is, and whats the uncertainty range of this quantity. This is why the mean of the sample means is an interesting quantity; it characterizes the sampling distribution of the mean, or the distribution of sample means.\nWe can see this mathematically by writing the mean or expectation value of the sample means thus:\n\\[E_{\\{R\\}}(N\\,\\bar{x}) = E_{\\{R\\}}(x_1 + x_2 + ... + x_N) = E_{\\{R\\}}(x_1) + E_{\\{R\\}}(x_2) + ... + E_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the expectations in the right hand side can be replaced by the population mean using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nE_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\mu\\\\\nE_{\\{R\\}}(\\bar{x}) &=& \\mu\n\\end{eqnarray}\\]\nwhich tells us that in the limit of a large number of replications the expectation value of the sampling means converges to the population mean. This limit gives us the true sampling distribution, as opposed to what we might estimate from our finite set of replicates. (Thus there our \\(E_{\\{R\\}}\\) would be replaced by some \\(E_{fs}\\) where by \\(fs\\) we wish to indicate the pmf or density of the sampling distribution).\n\n\nWe can see what the estimated sampling distribution of the mean looks like at different sample sizes.\n\nsample_means_at_size_10=sample_means[9]\nsample_means_at_size_100=sample_means[99]\nsample_means_at_size_1000=sample_means[999]\n\n\nplt.hist(sample_means_at_size_10, bins=np.arange(0,1,0.01), alpha=0.5);\nplt.hist(sample_means_at_size_100, bins=np.arange(0,1,0.01), alpha=0.4);\nplt.hist(sample_means_at_size_1000, bins=np.arange(0,1,0.01), alpha=0.3);\n\n\n\n\n\n\n\n\nThe distribution is much tighter at large sample sizes, and that you can have way low and way large means at small sample sizes. Indeed there are means as small as 0.1 at a sample size of 10, and as small as 0.3 at a sample size of 100.\nLets plot the distribution of the mean as a function of sample size.\n\nfor i in sample_sizes:\n    if i %50 ==0 and i &lt; 1000:\n        plt.scatter([i]*200, sample_means[i], alpha=0.05);\nplt.xlim([0,1000])\nplt.ylim([0.25,0.75]);\n\n\n\n\n\n\n\n\n\n\n\nLet the underlying distribution from which we have drawn our samples have, additionally to a well defined mean \\(\\mu\\), a well defined variance \\(\\sigma^2\\).\nThen, as before:\n\\[V_{\\{R\\}}(N\\,\\bar{x}) = V_{\\{R\\}}(x_1 + x_2 + ... + x_N) = V_{\\{R\\}}(x_1) + V_{\\{R\\}}(x_2) + ... + V_{\\{R\\}}(x_N)\\]\nNow in the limit of a very large number of replications, each of the variances in the right hand side can be replaced by the population variance using the law of large numbers! Thus:\n\\[\\begin{eqnarray}\nV_{\\{R\\}}(N\\,\\bar{x}) &=& N\\, \\sigma^2\\\\\nV(\\bar{x}) &=& \\frac{\\sigma^2}{N}\n\\end{eqnarray}\\]\nThis simple formula is called De-Moivre’s formula, and explains the tell-tale triangular plot we saw above, with lots of variation at low sample sizes turning into a tight distribution at large sample size(N).\nThe square root of \\(V\\), or the standard deviation of the sampling distribution of the mean (in other words, the distribution of sample means) is also called the Standard Error.\nWe can obtain the standard deviation of the sampling distribution of the mean at different sample sizes and plot it against the sample size, to confirm the \\(1/\\sqrt(N)\\) behaviour.\n\nstd_of_sample_means = [np.std(means) for means in sample_means]\n\n\nplt.plot(np.log10(sample_sizes), np.log10(std_of_sample_means));\n\n\n\n\n\n\n\n\nLet us plot again the distribution of sample means at a large sample size, \\(N=1000\\). What distribution is this?\n\nplt.hist(sample_means_at_size_1000, bins=np.arange(0.4,0.6,0.002));\n\n\n\n\n\n\n\n\nLets step back and try and think about what this all means. As an example, say I have a weight-watchers’ study of 1000 people, whose average weight is 150 lbs with standard deviation of 30lbs. If I was to randomly choose many samples of 100 people each, the mean weights of those samples would cluster around 150lbs with a standard error of 30/\\(\\sqrt{100}\\) = 3lbs. Now if i gave you a different sample of 100 people with an average weight of 170lbs, this weight would be more than 6 standard errors beyond the population mean, 1 and would thus be very unlikely to be from the weight watchers group.\n1 this example is motivated by the crazy bus example in Charles Whelan’s excellent Naked Statistics Book\n\n\nWe saw in the last section that the sampling distribution of the mean itself has a mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\). This distribution is called the Gaussian or Normal Distribution, and is probably the most important distribution in all of statistics.\nThe probability density of the normal distribution is given as:\n\\[ N(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{(x-\\mu)^2}{2s^2} } .\\]\nThe expected value of the Gaussian distribution is \\(E[X]=\\mu\\) and the variance is \\(Var[X]=s^2\\).\n\nnorm =  sp.stats.norm\nx = np.linspace(-5,5, num=200)\n\n\nfig = plt.figure(figsize=(12,6))\nfor mu, sigma, c in zip([0.5]*3, [0.2, 0.5, 0.8], sns.color_palette()[:3]):\n    plt.plot(x, norm.pdf(x, mu, sigma), lw=2, \n             c=c, label = r\"$\\mu = {0:.1f}, s={1:.1f}$\".format(mu, sigma))\n    plt.fill_between(x, norm.pdf(x, mu, sigma), color=c, alpha = .4)\n    \n    \nplt.xlim([-5,5])\nplt.legend(loc=0)\nplt.ylabel(\"PDF at $x$\")\nplt.xlabel(\"$x$\");\n\n//anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:892: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n  warnings.warn(self.msg_depr % (key, alt_key))"
  },
  {
    "objectID": "posts/samplingclt/index.html#the-central-limit-theorem",
    "href": "posts/samplingclt/index.html#the-central-limit-theorem",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "The reason for the distribution’s importance is the Central Limit Theorem(CLT). The theorem is stated as thus, very similar to the law of large numbers:\nLet \\(x_1,x_2,...,x_n\\) be a sequence of independent, identically-distributed (IID) random variables from a random variable \\(X\\). Suppose that \\(X\\) has the finite mean \\(\\mu\\) AND finite variance \\(\\sigma^2\\). Then the average of the first n of them:\n\\[S_n = \\frac{1}{n} \\sum_{i=1}^{n} x_i ,\\]\nconverges to a Gaussian Random Variable with mean \\(\\mu\\) and variance \\(\\sigma^2/n\\) as \\(n \\to \\infty\\):\n\\[ S_n \\sim N(\\mu,\\frac{\\sigma^2}{n}) \\, as \\, n \\to \\infty. \\]\nIn other words:\n\\[s^2 = \\frac{\\sigma^2}{N}.\\]\nThis is true, regardless of the shape of \\(X\\), which could be binomial, poisson, or any other distribution.\nStrictly speaking, under some conditions called Lyapunov conditions, the variables \\(x_i\\) dont have to be identically distributed, as long as \\(\\mu\\) is the mean of the means and \\(\\sigma^2\\) is the sum of the individual variances. This has major consequences, for the importance of this theorem.\nMany random variables can be thought of as having come from the sum of a large number of small and independent effects. For example human height or weight can be thought of as the sum as a large number of genetic and environmental factors, which add to increase or decrease height or weight respectively. Or think of a measurement of a height. There are lots of ways things could go wrong: frayed tapes, stretched tapes, smudged marks, bad lining up of the eye, etc. These are all independent and have no systematic error in one direction or the other.\nThen the sum of these factors, as long as there are a large number of them, will be distributed as a gaussian.[this has nothing to do with the sampling distribution of the mean but is part of the origin story of the gaussian distribution]\nAs a rule of thumb, the CLT starts holding at \\(N \\sim 30\\).\n\n\nThe sample mean, or mean of the random variables \\(x_{mi}\\) in the sample \\(m\\), has a sampling distribution with mean \\(\\mu\\) and variance \\(\\frac{\\sigma^2}{N}\\), as shown before. Now for large sample sizes we can go further and use the CLT theorem to say that this distribution is the normal distribution,\n\\[S_N \\sim N(\\mu, \\frac{\\sigma^2}{N})\\].\nThe preciseness of saying that we have a gaussian is a huge gain in our expository power. For example, for the case of the weight-watchers program above, a separation of 20lbs is more than 3 standard errors away, which corresponds to being way in the tail of a gaussian distribution. Because we can now quantify the area under the curve, we can say that 99.7% of the sample means lie within 9lbs of 150. Thus you can way easily reject the possibility that the new sample is from the weight-watchers program with 99.7% confidence.\nIndeed, the CLT allows us to take the reduction in variance we get from large samples, and make statements in different cases that are quite strong:\n\nif we know a lot about the population, and randomly sampled 100 points from it, the sample mean would be with 99.7% confidence within \\(0.3\\sigma\\) of the population mean. And thus, if \\(\\sigma\\) is small, the sample mean is quite representative of the population mean.\nThe reverse: if we have a well sampled 100 data points, we could make strong statements about the population as a whole. This is indeed how election polling and other sampling works.\nwe can infer, as we just did, if a sample is consistent with a population\nby the same token, you can compare two samples and infer if they are from the same population."
  },
  {
    "objectID": "posts/samplingclt/index.html#the-sampling-distribution-of-the-variance",
    "href": "posts/samplingclt/index.html#the-sampling-distribution-of-the-variance",
    "title": "Sampling and the Central Limit Theorem",
    "section": "",
    "text": "At this point you might be curious about what the sampling distribution of the variance looks like, and what can we surmise from it about the variance of the entire sample. We can do this, just like we did for the means. We’ll stick with a high number of replicates and plot the mean of the sample variances as well as the truish sampling distribution of the variances at a sample size of 100.\n\ndef make_throws_var(number_of_samples, sample_size):\n    start=np.zeros((number_of_samples, sample_size), dtype=int)\n    for i in range(number_of_samples):\n        start[i,:]=throw_a_coin(sample_size)\n    return np.var(start, axis=1)\nsample_vars_1000_replicates = [make_throws_var(number_of_samples=1000, sample_size=i) for i in sample_sizes]\nmean_of_sample_vars_1000 = [np.mean(vars) for vars in sample_vars_1000_replicates]\nplt.plot(sample_sizes, mean_of_sample_vars_1000);\nplt.xscale(\"log\");\n\n\n\n\n\n\n\n\nThe “mean sample variance” asymptotes to the true variance of 0.25 by a sample size of 100.\nHow well does the sample variance estimate the true variance?\nIf \\(V_m\\) denotes the variance of a sample,\n\\[ N\\,V_m = \\sum_{i=1}^{N} (x_{mi} - \\bar{x_m})^2 = \\sum_{i=1}^{N}(x_{mi} - \\mu)^2 - N\\,(\\bar{x_m} - \\mu)^2. \\]\nThen \\[E_{\\{R\\}}(N\\,V_m) = E_{\\{R\\}}(\\sum_{i=1}^{N}(x_{mi} - \\mu)^2) - E_{\\{R\\}}(N\\,(\\bar{x_m} - \\mu)^2)\\] In the asymptotic limit of a very large number of replicates, we can then write \\[E(N\\,V) = N\\,\\sigma^2 - \\sigma^2, \\] and thus we have \\[E(V) = \\frac{N-1}{N} \\,\\sigma^2\\].\nIn other words, the expected value of the sample variance is LESS than the actual variance. This should not be surprising: consider for example a sample of size 1 from the population. There is zero variance! More generally, whenever you sample a population, you tend to pick the more likely members of the population, and so the variance in the sample is less than the variance in the population.\nAn interesting application of this idea, as Shalizi points out in http://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/, is that the loss of variability due to sampling of genes is indeed the origin of genetic drift. More prosaically, the fact that the above graph of expected sample variance against sample size asymptotes to 0.25 is as \\(\\frac{N-1}{N}\\) if very close to 1 at large N.\nOr put another way, you ought to correct your sample variances by a factor of \\(\\frac{n}{n-1}\\) to estimate the population variance, which itself works as the sampling distribution of the sample variance is rather tight, as seen below.\nThat is, defining the sample variance with \\(n-1\\) in the denominator instead of \\(n\\) gives you an unbiased eatimator of the true variance. This is why, for example, Pandas will do this by default for series and dataframes. (numpy wont, so beware!).\n\nplt.hist(sample_vars_1000_replicates[99], bins=np.arange(0.2,0.26,0.001), alpha=0.2, normed=True);"
  },
  {
    "objectID": "posts/vizasstory.html",
    "href": "posts/vizasstory.html",
    "title": "Visualization As Story",
    "section": "",
    "text": "There is this pretty famous book by Steve Krug, called “Dont Make Me Think”. Its a call to respect conventions for web elements, such as shopping carts (a cart should be on the upper right), so that the web experience is obvious to users.\n\n\n\nIn visualization, as in web development, your audience does not want to spend cognitive effort on things you could just show them, by convention, or by explicit writing. So, just point out the key facts and insights.\nFor example, in this great article in the financial times https://www.ft.com/content/0f11b219-0f1b-420e-8188-6651d1e749ff?hcb=1, the main point “Vaccines have made Covid-19 far less lethal” is written up-front.\n\n\n\nThe implications are made clear in the second sentence, comparing vaccinated 80 year-olds to un-vaccinated 50 year-olds. This implication is illustrated in the visualization as well, with a horizontal black line, and a caption.\nInstead of point markers, downwards pointing arrows are used on lines to reinforce the notion of lower risk. Captions and annotations are used to point out key insights. Extraneous frames and tick marks are removed.\nThis is an example of framing. It grabs the audience and leads it through the insights you want to share.\n\n\n\nThere’s been a lot of worry about breakthrough vaccination, especially with the news about the Provincetown cluster. Here is another visualization from the same article, telling us why the large number of breakthrough infections are to be expected.\n\n\n\nIt walks us through the entire calculation visually. And does it in two scenarios: high vaccination rates and low vaccination rates. We can ourselves see the larger hospitalization numbers in the low-vaccination scenario.\nThe visualization and explanation could have been framed in terms of base rates and conditional probabilities, but by illustrating the concepts with an example, they are made accessible to everyone. And the framing drives home the story: go get your shot!\nRead more on how to make good visualizations using R in this book by @khealy . If you are a pythonista, learn how to make good plots in @matplotlib using https://end-to-end-machine-learning.teachable.com/p/navigating-matplotlib-tutorial-how-to/ by @_brohrer_ ."
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I Learned",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nAuthor\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nopen\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Rahul Dave — physicist, engineer, and practitioner of machine learning, AI, and data science. I also teach.\nI did my Ph.D. in physics at the University of Pennsylvania, where my thesis on quintessence and the cosmic microwave background was among the early works introducing dark energy. After that I spent over a decade at Harvard — as a computational scientist at the Center for Astrophysics, then as a lecturer at SEAS and IACS, where I helped build courses like CS109 (Data Science), AM207 (Stochastic Methods and Bayesian Inference), and CS207 (Systems Development for Computational Science).\nI’m co-founder and Chief Scientist at Univ.AI, where we build AI solutions and teach machine learning. Through Univ.AI, I also consult on ML and data engineering problems — we’ve worked with clients across oil and energy, insurance, finance, and sports.\nI’m a Bayesian at heart. I like interesting problems at the intersection of statistics, machine learning, and computation — and I like explaining them clearly.\n\n\n\nInterests\n\nAI — how it works, why it works, and where it breaks\nBayesian inference and probabilistic modeling\nMachine learning and deep learning\nData visualization and communication\nCosmology and astrophysics\nDevelopment economics\n\n\n\nEducation\n\nPh.D. Physics, University of Pennsylvania, 2002\nB.S. Physics (Honors), St. Xavier’s College, University of Bombay, 1992\n\n\n\nElsewhere\n\nGitHub\nX / Twitter\nBluesky\nGoogle Scholar\nLinkedIn\n\n\n\nGet in Touch\nIf you’d like to work with me — consulting, AI/ML projects, or training — drop me a message below.\nFor questions about blog posts, course material, or anything else, find me on X / Twitter.\n\nName \nEmail \nMessage\n\n\nSend"
  },
  {
    "objectID": "til/open.html",
    "href": "til/open.html",
    "title": "open",
    "section": "",
    "text": "MacOS has a great command open. You can use it to open any file in any folder from the terminal in its default app. For example:\nopen bla.pdf\nwill open a file in Preview.\nSometimes you want another app. Then you can use the -a flag. Like so:\nopen -a /Applications/Typora.app bla.md"
  },
  {
    "objectID": "collections/mysoft/hooksett.html",
    "href": "collections/mysoft/hooksett.html",
    "title": "Hooksett",
    "section": "",
    "text": "Hooksett is a Python library that provides a flexible, extensible hook system for managing parameters, metrics, and artifacts in ML workflows."
  },
  {
    "objectID": "collections/mysoft/hooksett.html#example",
    "href": "collections/mysoft/hooksett.html#example",
    "title": "Hooksett",
    "section": "Example",
    "text": "Example\nAnnotate your ML class with tracked types, wire up a config loader and MLflow output, and every parameter and metric is automatically captured:\nfrom hooksett import tracked, HookManager\nfrom hooksett.hooks import YAMLConfigInput, TypeValidationHook, MLflowOutput\n\ntype Parameter[T] = T\ntype Metric[T] = T\n\n@tracked\nclass Trainer:\n    learning_rate: Parameter[float] = 0.01\n    batch_size: Parameter[int] = 32\n    epochs: Parameter[int] = 100\n    accuracy: Metric[float] = 0.0\n    loss: Metric[float] = 0.0\n\n    def train(self):\n        for epoch in range(self.epochs):\n            # training step ...\n            self.accuracy = evaluate(model)\n            self.loss = compute_loss(model)\n\n# load params from YAML, validate, and log everything to MLflow\nmanager = HookManager()\nmanager.add_input_hook(YAMLConfigInput(\"config.yaml\"))\nmanager.add_input_hook(TypeValidationHook())\nmanager.add_output_hook(MLflowOutput())"
  },
  {
    "objectID": "collections/mysoft/hooksett.html#features",
    "href": "collections/mysoft/hooksett.html#features",
    "title": "Hooksett",
    "section": "Features",
    "text": "Features\n\n@tracked class decorator — monitors attribute changes on class instances via Python descriptors\n@track_function decorator — tracks function parameters and local variables; values are saved to hooks once at function/method exit\nLocal variable tracking — annotate locals with Traced[T] inside methods or functions; only the final value at exit is captured\nAutomatic parameter loading — YAMLConfigInput hook loads configuration from YAML files into tracked attributes\nParameter validation — TypeValidationHook enforces type annotations; RangeValidationHook checks numeric bounds\nCustom type registry — define domain-specific tracked types (Parameter, Metric, Artifact, Prompt, Response, Feature) via register_tracked_type\nPluggable output hooks — TracedOutput for logging, MLflowOutput for experiment tracking, or write your own OutputHook\nSingleton HookManager — register input and output hooks once; all decorated classes and functions use them automatically\nSeparation of config and code — parameters live in YAML, validation in hooks, tracking in type annotations"
  },
  {
    "objectID": "collections/software/hamilton.html",
    "href": "collections/software/hamilton.html",
    "title": "Stitchfix Hamilton",
    "section": "",
    "text": "A scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/hamilton.html#why-choose-this-tool",
    "href": "collections/software/hamilton.html#why-choose-this-tool",
    "title": "Stitchfix Hamilton",
    "section": "",
    "text": "A scalable general purpose micro-framework for defining dataflows, Allows you to specify a flow of (delayed) execution, that forms a Directed Acyclic Graph (DAG).\n\nHamilton prescribes a way of writing feature transformations as linked sets of functions to form a DAG. These transformations can be connected to drivers which can be pandas dataframes or SQL in a database, or whatever. This provides testable data transformations."
  },
  {
    "objectID": "collections/software/awk.html",
    "href": "collections/software/awk.html",
    "title": "Awk",
    "section": "",
    "text": "An old goody! For quick command line analysis of data.\n\nThe following examples were taken from the tldr page for awk:\nPrint the fifth column (a.k.a. field) in a space-separated file:\nawk '{print $5}' filename\nPrint the second column of the lines containing “foo” in a space-separated file:\nawk '/foo/ {print $2}' filename\nPrint the last column of each line in a file, using a comma (instead of space) as a field separator:\nawk -F ',' '{print $NF}' filename\nSum the values in the first column of a file and print the total:\nawk '{s+=$1} END {print s}' filename\nPrint every third line starting from the first line:\nawk 'NR%3==1' filename\nPrint different values based on conditions:\nawk '{if ($1 == \"foo\") print \"Exact match foo\"; else if ($1 ~ \"bar\") print \"Partial match bar\"; else print \"Baz\"}' filename\nPrint all lines where the 10th column value equals the specified value:\nawk '($10 == value)'\nPrint all the lines which the 10th column value is between a min and a max:\nawk '($10 &gt;= min_value && $10 &lt;= max_value)'"
  }
]